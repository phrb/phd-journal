# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro H R Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Setup
** Julia
#+NAME: install_julia_deps
#+HEADER: :results output :session *julia*
#+BEGIN_SRC julia
Pkg.add("Plots")
Pkg.add("Lint")
Pkg.add("Gadfly")
Pkg.add("ProfileView")
Pkg.add("CSV")
Pkg.add("StatsBase")
Pkg.add("StatsModels")
Pkg.add("GLM")
Pkg.add("RDatasets")
Pkg.add("IterTools")
Pkg.add("Missings")
Pkg.add("RCall")
Pkg.add("DataFrames")
#+END_SRC

#+RESULTS: install_julia_deps
#+begin_example
INFO: Package Plots is already installed
INFO: Package Lint is already installed
INFO: Package Gadfly is already installed
INFO: Cloning cache of Gtk from https://github.com/JuliaGraphics/Gtk.jl.git
INFO: Cloning cache of GtkReactive from https://github.com/JuliaGizmos/GtkReactive.jl.git
INFO: Cloning cache of IntervalSets from https://github.com/JuliaMath/IntervalSets.jl.git
INFO: Cloning cache of ProfileView from https://github.com/timholy/ProfileView.jl.git
INFO: Cloning cache of Reactive from https://github.com/JuliaGizmos/Reactive.jl.git
INFO: Cloning cache of RoundingIntegers from https://github.com/JuliaMath/RoundingIntegers.jl.git
INFO: Installing Gtk v0.13.1
INFO: Installing GtkReactive v0.4.0
INFO: Installing IntervalSets v0.1.1
INFO: Installing ProfileView v0.3.0
INFO: Installing Reactive v0.6.0
INFO: Installing RoundingIntegers v0.0.3
INFO: Building Cairo
INFO: Building Gtk
INFO: Package database updated
INFO: Package CSV is already installed
INFO: Package StatsBase is already installed
INFO: Package StatsModels is already installed
INFO: Package GLM is already installed
INFO: Package RDatasets is already installed
#+end_example

#+NAME: update_julia_pkg
#+HEADER:  :results output :session *julia*
#+BEGIN_SRC julia
Pkg.update()
#+END_SRC

#+RESULTS: update_julia_pkg
: INFO: Updating METADATA...
: WARNING: Package ASTInterpreter: skipping update (dirty)...
: INFO: Updating Gallium master...
: INFO: Computing changes...
: INFO: No packages to install, update or remove

*** NODAL Development                                          :Code:NODAL:
**** Installing NODAL in Julia Nightly
[[https://github.com/phrb/NODAL.jl][NODAL]] is the autotuning library I am developing in the [[https://julialang.org][Julia]]
language. The idea is to provide tools for the implementation of
parallel and distributed autotuners for various problem domains.
***** Download Julia Nightly
****** [[https://julialang.org/downloads][Download Generic Binary]]
****** Downloading from the CLI
You can run the following to install the latest *Julia* version:
#+BEGIN_SRC bash
cd ~ && mkdir .bin && cd .bin
wget https://julialangnightlies-s3.julialang.org/bin/linux/x64/julia-latest-linux64.tar.gz
tar xvf julia-latest-linux64.tar.gz
mv julia-* julia
rm julia-latest-linux64.tar.gz
#+END_SRC
This will put the *Julia* binary at =~/.bin/julia/bin/julia=.
You can use it like that or add an =alias= to your shell.
***** Installing the unregistered version
This will not be needed after registering NODAL to METADATA.
****** [[https://docs.julialang.org/en/latest/manual/packages/#Installing-Unregistered-Packages-1][Documentation]]
****** Julia Commands
#+BEGIN_SRC julia
Pkg.clone("https://github.com/phrb/NODAL.jl")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
***** Installing from the Julia package manager
****** Julia commands
#+BEGIN_SRC julia
Pkg.add("NODAL")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
**** Setting up a new Release
***** Using Attobot
[[https://github.com/attobot][Attobot]] integrates with *GitHub* to automatically register a new package
or a package version to *Julia*'s =METADATA= package repository.  Attobot
only needs a new *GitHub* release to work.
***** Using *Julia*'s =PkgDev=
Check the [[https://docs.julialang.org/en/latest/manual/packages/#Tagging-and-Publishing-Your-Package-1][documentation]] to learn how to register and publish user
packages to =METADATA=.
**** Development Workflow
The process of fixing an [[https://github.com/phrb/NODAL.jl/issues][issue]] or submitting a new
feature is:
0. Fork [[https://github.com/phrb/NODAL.jl][NODAL on GitHub]]

   You will need a GitHub account for this.

1. Make sure you have the latest version
   #+BEGIN_SRC bash
git checkout master
git fetch
   #+END_SRC

   New branches must be made from the =dev= branch:
   #+BEGIN_SRC bash
git checkout dev
   #+END_SRC
2. Checkout a new branch
   #+BEGIN_SRC bash
git checkout -b fix-or-feature
   #+END_SRC
3. Write code and commit to your new branch

   Make sure you write short and descriptive commit
   messages. Something similar to [[https://udacity.github.io/git-styleguide/][Udacity's guidelines]] is preferred
   but not strictly necessary.

4. Open a [[https://github.com/phrb/NODAL.jl/pulls][pull request]] to the =dev= bran

** R
Installing *R* dependencies:
#+NAME: install_r_deps
#+HEADER: :results output :exports both :session *R*
#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",
                 "plotly", "rPref", "pracma", "FrF2", "AlgDesign",
                 "quantreg"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

#+RESULTS: install_r_deps
#+begin_example
Installing packages into â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™
(as â€˜libâ€™ is unspecified)
trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/ggplot2_2.2.1.tar.gz'
Content type 'application/x-gzip' length 2213308 bytes (2.1 MB)
==================================================
downloaded 2.1 MB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/dplyr_0.7.4.tar.gz'
Content type 'application/x-gzip' length 808054 bytes (789 KB)
==================================================
downloaded 789 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/tidyr_0.8.0.tar.gz'
Content type 'application/x-gzip' length 377417 bytes (368 KB)
==================================================
downloaded 368 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rjson_0.2.18.tar.gz'
Content type 'application/x-gzip' length 99478 bytes (97 KB)
==================================================
downloaded 97 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/GGally_1.3.2.tar.gz'
Content type 'application/x-gzip' length 1031885 bytes (1007 KB)
==================================================
downloaded 1007 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/plotly_4.7.1.tar.gz'
Content type 'application/x-gzip' length 1034951 bytes (1010 KB)
==================================================
downloaded 1010 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rPref_1.2.tar.gz'
Content type 'application/x-gzip' length 99297 bytes (96 KB)
==================================================
downloaded 96 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/pracma_2.1.4.tar.gz'
Content type 'application/x-gzip' length 382113 bytes (373 KB)
==================================================
downloaded 373 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/FrF2_1.7-2.tar.gz'
Content type 'application/x-gzip' length 282582 bytes (275 KB)
==================================================
downloaded 275 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/AlgDesign_1.1-7.3.tar.gz'
Content type 'application/x-gzip' length 514391 bytes (502 KB)
==================================================
downloaded 502 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/quantreg_5.35.tar.gz'
Content type 'application/x-gzip' length 1640297 bytes (1.6 MB)
==================================================
downloaded 1.6 MB

,* installing *source* package â€˜ggplot2â€™ ...
,** package â€˜ggplot2â€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (ggplot2)
ERROR: failed to lock directory â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™ for modifying
Try removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/00LOCK-dplyrâ€™
,* installing *source* package â€˜rjsonâ€™ ...
,** package â€˜rjsonâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c dump.cpp -o dump.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c parser.c -o parser.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c register.c -o register.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rjson.so dump.o parser.o register.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rjson/libs
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (rjson)
,* installing *source* package â€˜pracmaâ€™ ...
,** package â€˜pracmaâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** byte-compile and prepare package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (pracma)
,* installing *source* package â€˜FrF2â€™ ...
,** package â€˜FrF2â€™ successfully unpacked and MD5 sums checked
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (FrF2)
,* installing *source* package â€˜AlgDesignâ€™ ...
,** package â€˜AlgDesignâ€™ successfully unpacked and MD5 sums checked
,** libs
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c FederovOpt.c -o FederovOpt.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c OptBlock.c -o OptBlock.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c Utility.c -o Utility.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o AlgDesign.so FederovOpt.o OptBlock.o Utility.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/AlgDesign/libs
,** R
,** data
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (AlgDesign)
,* installing *source* package â€˜quantregâ€™ ...
,** package â€˜quantregâ€™ successfully unpacked and MD5 sums checked
,** libs
gfortran   -fpic  -g -O2  -c akj.f -o akj.o
gfortran   -fpic  -g -O2  -c boot.f -o boot.o
gfortran   -fpic  -g -O2  -c bound.f -o bound.o
gfortran   -fpic  -g -O2  -c boundc.f -o boundc.o
gfortran   -fpic  -g -O2  -c brute.f -o brute.o
gfortran   -fpic  -g -O2  -c chlfct.f -o chlfct.o
gfortran   -fpic  -g -O2  -c cholesky.f -o cholesky.o
gfortran   -fpic  -g -O2  -c combos.f -o combos.o
gfortran   -fpic  -g -O2  -c crq.f -o crq.o
gfortran   -fpic  -g -O2  -c crqfnb.f -o crqfnb.o
gfortran   -fpic  -g -O2  -c dsel05.f -o dsel05.o
gfortran   -fpic  -g -O2  -c etime.f -o etime.o
gfortran   -fpic  -g -O2  -c extract.f -o extract.o
gfortran   -fpic  -g -O2  -c idmin.f -o idmin.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c init.c -o init.o
gfortran   -fpic  -g -O2  -c iswap.f -o iswap.o
gfortran   -fpic  -g -O2  -c kuantile.f -o kuantile.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c mcmb.c -o mcmb.o
gfortran   -fpic  -g -O2  -c penalty.f -o penalty.o
gfortran   -fpic  -g -O2  -c powell.f -o powell.o
gfortran   -fpic  -g -O2  -c rls.f -o rls.o
gfortran   -fpic  -g -O2  -c rq0.f -o rq0.o
gfortran   -fpic  -g -O2  -c rq1.f -o rq1.o
gfortran   -fpic  -g -O2  -c rqbr.f -o rqbr.o
gfortran   -fpic  -g -O2  -c rqfn.f -o rqfn.o
gfortran   -fpic  -g -O2  -c rqfnb.f -o rqfnb.o
gfortran   -fpic  -g -O2  -c rqfnc.f -o rqfnc.o
gfortran   -fpic  -g -O2  -c rqs.f -o rqs.o
gfortran   -fpic  -g -O2  -c sparskit2.f -o sparskit2.o
gfortran   -fpic  -g -O2  -c srqfn.f -o srqfn.o
gfortran   -fpic  -g -O2  -c srqfnc.f -o srqfnc.o
gfortran   -fpic  -g -O2  -c srtpai.f -o srtpai.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o quantreg.so akj.o boot.o bound.o boundc.o brute.o chlfct.o cholesky.o combos.o crq.o crqfnb.o dsel05.o etime.o extract.o idmin.o init.o iswap.o kuantile.o mcmb.o penalty.o powell.o rls.o rq0.o rq1.o rqbr.o rqfn.o rqfnb.o rqfnc.o rqs.o sparskit2.o srqfn.o srqfnc.o srtpai.o -llapack -lblas -lgfortran -lm -lquadmath -lgfortran -lm -lquadmath -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/quantreg/libs
,** R
,** data
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (quantreg)
,* installing *source* package â€˜tidyrâ€™ ...
,** package â€˜tidyrâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c RcppExports.cpp -o RcppExports.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c fill.cpp -o fill.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c melt.cpp -o melt.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c simplifyPieces.cpp -o simplifyPieces.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o tidyr.so RcppExports.o fill.o melt.o simplifyPieces.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/tidyr/libs
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,*** copying figures
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (tidyr)
,* installing *source* package â€˜GGallyâ€™ ...
,** package â€˜GGallyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (GGally)
,* installing *source* package â€˜rPrefâ€™ ...
,** package â€˜rPrefâ€™ successfully unpacked and MD5 sums checked
,** libs
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c RcppExports.cpp -o RcppExports.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c bnl.cpp -o bnl.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c hasse.cpp -o hasse.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c pref-classes.cpp -o pref-classes.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par-top.cpp -o psel-par-top.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par.cpp -o psel-par.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c scalagon.cpp -o scalagon.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c topk_setting.cpp -o topk_setting.o
g++ -std=gnu++11 -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rPref.so RcppExports.o bnl.o hasse.o pref-classes.o psel-par-top.o psel-par.o scalagon.o topk_setting.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPref/libs
,** R
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜rPrefâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* installing *source* package â€˜plotlyâ€™ ...
,** package â€˜plotlyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜plotlyâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™

The downloaded source packages are in
	â€˜/tmp/RtmpivSTVC/downloaded_packagesâ€™
Warning messages:
1: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜dplyrâ€™ had non-zero exit status
2: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜rPrefâ€™ had non-zero exit status
3: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜plotlyâ€™ had non-zero exit status
#+end_example

** Modifying & Analysing the FPGA Data Set
Cloning and updating the =legup-tuner= repository:

#+NAME: update_legup_tuner
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/legup-tuner.git || (cd legup-tuner && git pull)
#+END_SRC

Export your path to =repository_dir= variable:

#+name: repository_dir
#+begin_src sh :results output :exports both
pwd | tr -d "\n"
#+end_src

** Updating & Cloning Repositories
*** GPU Autotuning Screening Experiment
#+NAME: update_screening_experiment
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/autotuning_screening_experiment.git || (cd autotuning_screening_experiment && git pull)
#+END_SRC
* 2018
** May
*** [2018-05-02 Wed]
**** Summarizing the D-Optimal + ANOVA Strategy for Steven's Experiments
1. Use ~optFederov~ to find 24 experiments for the full model:

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        vector_length + lws_y + 1 / lws_y +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

2. Use ~aov~ to fit the full model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       vector_length + lws_y + 1 / lws_y +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}

4. Identify the most significant factors from the ANOVA summary. In this
   case, they are $vector_length$ and $lws_y$.
5. Use the fitted model to predict the best $time_per_pixel$ value in the
   entire dataset
6. Prune the dataset using the predicted best values for $vector_length$ and $lws_y$
7. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
   than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

8. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}
9. Identify the most significant factors from the ANOVA summary. In this
   case, they are $y_component_number$ and $threads_number$.
10. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
11. Prune the dataset using the predicted best values for $y_component_number$ and
    $threads_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size +
        elements_number + 1 / elements_number
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size +
                       elements_number + 1 / elements_number
\end{equation}
14. Identify the most significant factors from the ANOVA summary. In this
    case, it is $elements_number$
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Prune the dataset using the predicted best values for $elements_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size
\end{equation}
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Compare the predicted $time_per_pixel$ with the global optimum
*** [2018-05-03 Thu]
**** Summarizing Experiments
Make sure you have the data:

#+NAME: update_dopt_aov_experiments
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_anova_experiments.git || (cd dopt_anova_experiments && git pull)
#+END_SRC

#+RESULTS: update_dopt_aov_experiments
: Already up to date.

This [[file:./dopt_anova_experiments/org/report.pdf][file]] contains the report.
*** [2018-05-04 Fri]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)

A = data.frame(x = 1:100,
               y = 1:100,
               z = 1:100)

A$Y = rnorm(n = 100, mean = A$x, sd = 0.4 * A$x)

big_model = lm(Y ~ x + y + z, data = A)
small_model = lm(Y ~ x, data = A)

# A_big_predict = cbind(A, predict(big_model, interval = "confidence"))
# A_small_predict = cbind(A, predict(small_model, interval = "confidence"))
#
# p_small_x <- ggplot(A_small_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_line(aes(x, fit), alpha = 0.8, color = "red1", size = 1)
#
# p_small_y <- ggplot(A_small_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_small_z <- ggplot(A_small_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_x <- ggplot(A_big_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(x, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_y <- ggplot(A_big_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_z <- ggplot(A_big_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)

par(mfrow = c(2, 5))
plot(small_model, which = c(1, 2, 3, 4, 5))
plot(big_model, which = c(1, 2, 3, 4, 5))

# grid.arrange(p_small_x, p_small_y, p_small_z,
#              p_big_x, p_big_y, p_big_z, nrow = 2)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-3203rQF/figure3203Li2.png]]
*** [2018-05-07 Mon]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)
library(AlgDesign)

read_file <- "dopt_anova_experiments/data/search_space.csv"

results <- read.csv(read_file, strip.white = T, header = T)

big_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                 vector_length + lws_y + I(1 / lws_y) +
                                 load_overlap + temporary_size +
                                 elements_number + I(1 / elements_number) +
                                 threads_number + I(1 / threads_number),
                data = results)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length,
                data = results)

bm_predict = data.frame(time_per_pixel = predict(big_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

sm_predict = data.frame(time_per_pixel = predict(small_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_min[1, ]

global_min = results[results$time_per_pixel == min(results$time_per_pixel),
                     c("time_per_pixel", "y_component_number",
                       "vector_length")]

ggplot(results) +
    aes(x = y_component_number, y = time_per_pixel) +
    theme_bw() +
    geom_point(alpha = 0.1) +
    theme(legend.position = "top") +
    geom_point(color= "green", data = bm_min, size = 3, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "red", data = sm_min, size = 4, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "blue", data = global_min, size = 5, alpha = 0.5,
               aes(x = y_component_number, y = time_per_pixel)) +
    theme(axis.text = element_text(size = 12),
          axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-27096ysq/figure27096DcA.png]]
*** [2018-05-09 Wed]
**** Plotting Predicted Values During Experiment
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 720 :height 1280
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(ggplot2)
library(gridExtra)

generate_model_plot <- function(big_model, small_model, results, full_data, metric) {
    bm_predict = data.frame(response = predict(big_model, results),
                            variable = results[metric])

    names(bm_predict)[names(bm_predict) == "response"] <- "time_per_pixel"
    names(bm_predict)[names(bm_predict) == "variable"] <- metric

    sm_predict = data.frame(response = predict(small_model, results),
                            variable = results[metric])

    names(sm_predict)[names(sm_predict) == "response"] <- "time_per_pixel"
    names(sm_predict)[names(sm_predict) == "variable"] <- metric

    bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict$time_per_pixel), ]

    sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict$time_per_pixel), ]

    sm_min = sm_min[1, ]
    bm_min = bm_min[1, ]

    global_min = full_data[full_data$time_per_pixel == min(full_data$time_per_pixel),
                           c("time_per_pixel", metric)]

    p <- ggplot() +
         scale_shape_identity() +
         geom_point(data = full_data, alpha = 0.1,
                    aes(x = full_data[metric], y = time_per_pixel,
                        color = "Search Space")) +
         geom_point(data = bm_min, size = 3, alpha = 1.0,
                    aes(x = bm_min[metric], y = time_per_pixel,
                        color = "Big Model", shape = 7)) +
         geom_point(data = sm_min, size = 3, alpha = 1.0,
                    aes(x = sm_min[metric], y = time_per_pixel,
                        color = "Small Model", shape = 8)) +
         geom_point(data = global_min, size = 3, alpha = 1.0,
                    aes(x = global_min[metric], y = time_per_pixel,
                        color = "Global Minimum", shape = 9)) +
         theme_bw() +
         theme(axis.text = element_text(size = 12),
               axis.title = element_text(size = 14, face = "bold"),
               legend.position = "top") +
         labs(y = "time_per_pixel", x = metric) +
         scale_color_manual(values = c("green", "blue", "black", "red"))

    return(p)
}

complete_data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

budget <- 120

factors = c("elements_number", "y_component_number",
            "vector_length", "temporary_size",
            "load_overlap", "threads_number",
            "lws_y")

used <- 0

data <- complete_data[, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

# Comment/Uncomment to toggle scaling

# scaled_data <- cbind(scale(select_if(data, is.numeric), center = FALSE, scale = TRUE),
#                      select_if(data, Negate(is.numeric)))
# scaled_data <- scaled_data[, names(data)]

# We are able to use the full set in this case
# sampled_data <- scaled_data[sample(nrow(data), 500), ]

# Complete model:
output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      scaled_data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

# Complete model:
regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length + lws_y + I(1 / lws_y) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                  data = federov_design)

p_vectorlength <- generate_model_plot(regression, small_model,
                                      scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

p_lwsy <- generate_model_plot(regression, small_model,
                              scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                   data = random_data)


r_lwsy <- generate_model_plot(big_random, small_random,
                              random_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

r_vectorlength <- generate_model_plot(big_random, small_random,
                                      random_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

used <- used + nrow(federov_design)

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'vector_length' and 'lws_y'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 18) {
    output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                           load_overlap + temporary_size +
                           elements_number + I(1 / elements_number) +
                           threads_number + I(1 / threads_number),
                         scaled_data,
                         nTrials = 18)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

p_ycomponentnumber <- generate_model_plot(regression, small_model,
                                          scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

p_threadsnumber <- generate_model_plot(regression, small_model,
                                       scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    threads_number + I(1 / threads_number),
                   data = random_data)


r_ycomponentnumber <- generate_model_plot(big_random, small_random,
                                          random_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

r_threadsnumber <- generate_model_plot(big_random, small_random,
                                       random_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'y_component_number' and 'threads_number'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 10) {
    output <- optFederov(~ load_overlap + temporary_size +
                            elements_number + I(1 / elements_number),
                          scaled_data,
                          nTrials = 10)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                  data = federov_design)

p_elementsnumber <- generate_model_plot(regression, small_model,
                                        scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                   data = random_data)

r_elementsnumber <- generate_model_plot(big_random, small_random,
                                        random_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

# Checking the ANOVA summary we can identify, at last, one variable
# that seem to have greater impact: 'elements_number'
# Let's fix it to their best predicted value so far,
# then fit a new model without it

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number &
                      complete_data$elements_number == predicted_best$elements_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 6) {
    output <- optFederov(~ load_overlap + temporary_size,
                          scaled_data,
                          nTrials = 6)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size,
                  data = federov_design)

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

best <- complete_data[complete_data$time_per_pixel == min(complete_data$time_per_pixel), ]
best_row <- rownames(best)

predicted_best$slowdown <- predicted_best$time_per_pixel / best$time_per_pixel
predicted_best$method <- rep("DOPTaov", nrow(predicted_best))
predicted_best$point_number <- rep(used, nrow(predicted_best))
predicted_best$vector_recompute <- rep("true", nrow(predicted_best))

predicted_best <- predicted_best[, c("elements_number", "y_component_number",
                                    "vector_length", "temporary_size", "vector_recompute",
                                    "load_overlap", "threads_number", "lws_y",
                                    "time_per_pixel", "point_number", "method",
                                    "slowdown")]

grid.arrange(p_vectorlength + ggtitle("First Step: D-Opt + aov"), p_lwsy + ggtitle(" "),
             r_vectorlength + ggtitle("First Step: Random Selection + lm"), r_lwsy + ggtitle(" "),
             p_ycomponentnumber + ggtitle("Second Step: D-Opt + aov"), p_threadsnumber + ggtitle(" "),
             r_ycomponentnumber + ggtitle("Second Step: Random Selection + lm"), r_threadsnumber + ggtitle(" "),
             p_elementsnumber + ggtitle("Third Step: D-Opt + aov"),
             r_elementsnumber + ggtitle("Third Step: Random Selection + lm"), nrow = 5)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536ZkV.png]]
*** [2018-05-14 Mon]
**** Accuracy of the DOPT+AOV Process in Steven's Case
To verify the "accuracy" of the selected metrics, I adapted the experiment
scripts to check for each removed model variable in the actual =aov= summary.
Those initial choices seem to match in most cases with the variables identified
as most relevant by the =aov= summary, as shown below.

#+HEADER: :results graphics output :session *R* :exports results
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 600 :height 500
#+BEGIN_SRC R
library(ggplot2)

accuracies_file <- "dopt_anova_experiments/data/dopt_accuracies.csv"
results <- read.csv(accuracies_file, strip.white=T, header=T)

names(results) <- c("First", "Second", "Third")
parsed_results = data.frame(names(results), t(results[1, ]))
names(parsed_results) <- c("Steps", "Accuracy")

parsed_results

ggplot(data = parsed_results, aes(x = Steps, y = Accuracy)) +
geom_bar(stat = "identity", width = 0.5) +
#geom_hline(yintercept = 1.0, color = "red", linetype = 2) +
geom_text(aes(label = Accuracy), vjust = 1.6, color = "white", size = 5)+
theme_bw() +
theme(axis.text = element_text(size = 12),
      axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536MMn.png]]

As described previously, at each step a group of variables is removed from the
model based on their "score", that is, the "Pr(>F)" value in the =aov= summary.
I selected at most two variables at each of the three steps, based on preliminary
visual analysis of the =aov= summaries.

To measure how accurate those initial selections were I checked at each step if
the $n$ selected variables were in the $n$ most relevant variables in that
step's =aov= summary. If that was the case I incremented a step-specific
counter. The counters were updated for 1000 iterations and then divided by 1000.
This value represents the accuracy of the static selection in comparison with
the values that would be selected if each individual =aov= summary was analysed.
*** [2018-05-15 Tue]
**** Writing an LM Experiment Using a Big Model
This experiment is a modification of the ``DOPTaov'' experiment that adapts the
``LM'' strategy for fitting linear models to pruned search spaces. Instead of
using small models at each step the experiments starts with large models that
are pruned as meaningful variables are identified in the =aov= summary. The
experiment used the same variables from the ``DOPTaov'' experiement at each
step.

**** Trying to Mitigate Heteroscedasticity
Using some ideas from [[https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/][this website]].

***** For a Uniformly Sampled Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

str(data)
data <- data[sample(1:nrow(data), 100, replace = FALSE), ]

regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                data = data)

summary(regression)
ncvTest(regression)

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = data)

transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$lambda) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = data)

summary(transformed_regression)
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = data)

Residuals:
       Min         1Q     Median         3Q        Max
-4.992e-09 -1.997e-09 -2.596e-10  1.126e-09  2.464e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -5.683e-09  3.382e-09  -1.680 0.096446 .
y_component_number       1.206e-09  5.251e-10   2.298 0.023956 *
I(1/y_component_number)  5.124e-09  3.369e-09   1.521 0.131863
vector_length            2.597e-10  7.338e-11   3.540 0.000643 ***
lws_y                    1.020e-11  2.951e-12   3.458 0.000841 ***
I(1/lws_y)              -2.800e-09  1.270e-09  -2.205 0.030027 *
load_overlaptrue         1.963e-10  8.042e-10   0.244 0.807695
temporary_size           1.977e-10  4.144e-10   0.477 0.634476
elements_number         -1.128e-10  1.048e-10  -1.076 0.284946
I(1/elements_number)     3.968e-09  3.270e-09   1.214 0.228123
threads_number          -2.453e-12  1.856e-12  -1.322 0.189660
I(1/threads_number)      2.043e-08  5.497e-08   0.372 0.711005
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.891e-09 on 88 degrees of freedom
Multiple R-squared:  0.3924,	Adjusted R-squared:  0.3165
F-statistic: 5.167 on 11 and 88 DF,  p-value: 3.271e-06
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 148.428    Df = 1     p = 3.824548e-34

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$lambda) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = data)

Residuals:
    Min      1Q  Median      3Q     Max
-277.65  -81.92  -10.22   79.59  220.80

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -890.45549   99.17068  -8.979 4.53e-14 ***
y_component_number        37.61691   15.39476   2.443   0.0165 *
I(1/y_component_number)  154.21569   98.77625   1.561   0.1221
vector_length             11.59971    2.15156   5.391 5.80e-07 ***
lws_y                      0.50504    0.08652   5.838 8.69e-08 ***
I(1/lws_y)              -258.56389   37.22493  -6.946 6.20e-10 ***
load_overlaptrue         -24.95474   23.57916  -1.058   0.2928
temporary_size             2.76899   12.15003   0.228   0.8203
elements_number           -6.89838    3.07364  -2.244   0.0273 *
I(1/elements_number)     125.77409   95.86892   1.312   0.1930
threads_number            -0.09253    0.05442  -1.700   0.0926 .
I(1/threads_number)     2668.72370 1611.82566   1.656   0.1013
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 114.1 on 88 degrees of freedom
Multiple R-squared:  0.7041,	Adjusted R-squared:  0.6672
F-statistic: 19.04 on 11 and 88 DF,  p-value: < 2.2e-16
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.4543362    Df = 1     p = 0.5002829
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-IVw6xY/figureecbKUS.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-zZp0Xe/figure9dCFD8.png]]

***** For a D-Optimal Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)
str(data)

output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      data = data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

str(federov_design)

# Complete model:
regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    vector_length + lws_y + I(1 / lws_y) +
                                    load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number) +
                                    threads_number + I(1 / threads_number),
                  data = federov_design)

summary(regression)
summary.aov(regression)
ncvTest(regression)

data[predict(regression, data) == min(predict(regression, data)), ]

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = federov_design)

transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$lambda) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = federov_design)

summary(transformed_regression)
summary.aov(transformed_regression)
ncvTest(transformed_regression)

data[predict(transformed_regression, data) == min(predict(transformed_regression, data)), ]
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...
'data.frame':	24 obs. of  9 variables:
 $ elements_number   : int  1 4 2 3 1 4 4 2 1 4 ...
 $ y_component_number: int  1 1 2 3 1 1 1 2 1 1 ...
 $ vector_length     : int  1 16 16 16 1 1 1 1 1 16 ...
 $ temporary_size    : int  2 2 4 4 2 4 4 2 4 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 1 2 1 1 1 1 2 2 ...
 $ threads_number    : int  256 32 32 128 256 32 128 32 32 1024 ...
 $ lws_y             : int  1 1 1 128 32 1 64 32 32 16 ...
 $ time_per_pixel    : num  2.31e-10 7.75e-10 1.70e-09 2.79e-08 7.27e-10 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = federov_design)

Residuals:
       Min         1Q     Median         3Q        Max
-8.775e-09 -4.543e-09 -1.968e-09  2.959e-09  1.856e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)              1.084e-08  1.609e-08   0.674   0.5134
y_component_number      -1.401e-09  2.450e-09  -0.572   0.5778
I(1/y_component_number) -1.300e-08  1.447e-08  -0.898   0.3867
vector_length            5.744e-10  2.591e-10   2.216   0.0467 *
lws_y                    1.121e-11  6.242e-12   1.796   0.0977 .
I(1/lws_y)              -4.183e-09  4.583e-09  -0.913   0.3793
load_overlaptrue        -1.279e-09  3.862e-09  -0.331   0.7462
temporary_size          -2.925e-11  1.926e-09  -0.015   0.9881
elements_number         -4.281e-11  3.725e-10  -0.115   0.9104
I(1/elements_number)     1.238e-08  8.077e-09   1.532   0.1514
threads_number          -3.511e-12  7.437e-12  -0.472   0.6453
I(1/threads_number)     -6.241e-08  2.281e-07  -0.274   0.7890
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 9.262e-09 on 12 degrees of freedom
Multiple R-squared:  0.5646,	Adjusted R-squared:  0.1655
F-statistic: 1.415 on 11 and 12 DF,  p-value: 0.2797
                        Df    Sum Sq   Mean Sq F value Pr(>F)
y_component_number       1 6.980e-17 6.980e-17   0.814 0.3848
I(1/y_component_number)  1 2.000e-18 2.000e-18   0.024 0.8805
vector_length            1 3.875e-16 3.875e-16   4.517 0.0550 .
lws_y                    1 5.023e-16 5.023e-16   5.856 0.0323 *
I(1/lws_y)               1 6.970e-17 6.970e-17   0.812 0.3851
load_overlap             1 6.900e-18 6.900e-18   0.080 0.7818
temporary_size           1 3.200e-18 3.200e-18   0.037 0.8499
elements_number          1 5.620e-17 5.620e-17   0.655 0.4340
I(1/elements_number)     1 2.175e-16 2.175e-16   2.536 0.1372
threads_number           1 1.340e-17 1.340e-17   0.156 0.7001
I(1/threads_number)      1 6.400e-18 6.400e-18   0.075 0.7890
Residuals               12 1.029e-15 8.580e-17
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 14.05733    Df = 1     p = 0.0001773212
      elements_number y_component_number vector_length temporary_size
18584               4                  1             1              4
      vector_recompute load_overlap threads_number lws_y time_per_pixel
18584             true         true           1024     1    3.37552e-10

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$lambda) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = federov_design)

Residuals:
     Min       1Q   Median       3Q      Max
-1.82320 -0.99051 -0.06769  0.85520  1.61215

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -30.303461   2.649147 -11.439 8.23e-08 ***
y_component_number       -0.453056   0.403302  -1.123 0.283253
I(1/y_component_number)  -3.209807   2.383153  -1.347 0.202907
vector_length             0.230701   0.042666   5.407 0.000158 ***
lws_y                     0.004914   0.001028   4.782 0.000447 ***
I(1/lws_y)               -2.779609   0.754470  -3.684 0.003125 **
load_overlaptrue         -0.193140   0.635862  -0.304 0.766525
temporary_size            0.555741   0.317136   1.752 0.105197
elements_number           0.049715   0.061324   0.811 0.433327
I(1/elements_number)      3.458007   1.329789   2.600 0.023208 *
threads_number           -0.002495   0.001224  -2.038 0.064250 .
I(1/threads_number)       6.800059  37.556377   0.181 0.859340
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.525 on 12 degrees of freedom
Multiple R-squared:  0.8911,	Adjusted R-squared:  0.7913
F-statistic:  8.93 on 11 and 12 DF,  p-value: 0.0003389
                        Df Sum Sq Mean Sq F value   Pr(>F)
y_component_number       1   5.93    5.93   2.552  0.13616
I(1/y_component_number)  1   0.06    0.06   0.027  0.87126
vector_length            1  66.84   66.84  28.746  0.00017 ***
lws_y                    1  79.03   79.03  33.992 8.08e-05 ***
I(1/lws_y)               1  30.24   30.24  13.005  0.00360 **
load_overlap             1   0.59    0.59   0.252  0.62477
temporary_size           1   5.50    5.50   2.366  0.14995
elements_number          1   0.39    0.39   0.169  0.68840
I(1/elements_number)     1  17.74   17.74   7.632  0.01720 *
threads_number           1  21.98   21.98   9.452  0.00964 **
I(1/threads_number)      1   0.08    0.08   0.033  0.85934
Residuals               12  27.90    2.33
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.09165178    Df = 1     p = 0.7620877
      elements_number y_component_number vector_length temporary_size
15927               4                  1             1              2
      vector_recompute load_overlap threads_number lws_y time_per_pixel
15927             true         true           1024     1   3.368082e-10
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figurem4Tf5r.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(regression$model$time_per_pixel, breaks = 10)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureYtLgcw.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureGF8dKO.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(transformed_regression$model["bcPower(time_per_pixel, boxcox_transform$lambda)"][[1]])
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureuPx7MI.png]]

*** [2018-05-16 Wed]
**** Power Transforms on Generated Heteroscedastic Data
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(car)
set.seed(1234)
x <- 1:1000
y <- abs(rnorm(n = 1000, mean = x, sd = 0.9 * x))

data <- data.frame(x, y)
plot(lm(y ~ x, data = data), which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5x1ltg/figureDpsnk9.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(lm(y ~ x, data = data))
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 365.3147    Df = 1     p = 1.960365e-81

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
power_transform <- powerTransform(lm(y ~ x,  data = data))#, family = "bcnPower")
coef(power_transform, round = TRUE)
transformed_regression <- lm(bcPower(y, power_transform$roundlam) ~ x, data = data)
                                      #gamma = power_transform$gamma) ~ x, data = data)

plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5x1ltg/figureKMbSqa.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 41.63823    Df = 1     p = 1.098246e-10

*** [2018-05-17 Thu]
**** Looking for Autotuning Benchmarks
Some candidates:
- Rodinia kernels (http://lava.cs.virginia.edu/Rodinia/download_links.htm)
- SPAPT (https://github.com/brnorris03/Orio/tree/master/testsuite/SPAPT)
- Dongarra's BEAST Project (http://icl.cs.utk.edu/beast/people/index.html)
*** [2018-05-22 Tue]
**** Running SPAPT Benchmarks
- Orio has a not very good documentation
- Compilation of simple examples takes +5min
**** Looking into LLVM and GCC flag autotuning
- CollectiveKnowledge has great flag space descriptions
- If we go back to compiler flag tuning we can tune
  any of the well-stablished HPC benchmarks
*** [2018-05-23 Wed]
**** Orio Setup Scripts
I've forked the Orio repository so I can freely change the code while keeping
version control.

To clone the most recent version, pick a path for the repository by editing the
source block below, using absolute paths.

#+NAME: setup_orio
#+HEADER: :results output
#+HEADER: :var ORIO_PATH="/home/phrb/code/orio"
#+BEGIN_SRC shell
git clone --depth=1 https://github.com/phrb/Orio.git $ORIO_PATH || echo "Orio already installed"
#+END_SRC

#+RESULTS: setup_orio
: Orio already installed

***** Lazy Python Databases with =Dataset=
With this it is possible to create a database from python scripts without much hassle:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
sudo pip install dataset
#+END_SRC

#+RESULTS:
: sudo pip install dataset
: Requirement already satisfied: dataset in /usr/lib/python3.6/site-packages

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import dataset
db = dataset.connect('sqlite:///mydatabase.db')
table = db['user']
table.insert(dict(name='John Doe', age=46, country='China'))
table.insert(dict(name='Jane Doe', age=37, country='France', gender='female'))
table.update(dict(name='John Doe', age=47), ['name'])
#+END_SRC

#+RESULTS:
: Python 3.6.5 (default, May 11 2018, 04:00:52)
: [GCC 8.1.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: python.el: native completion setup loaded

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
print(db.tables)
print(db['user'].columns)
print(len(db['user']))
users = db['user'].all()
print(users)

db.commit()
#+END_SRC

#+RESULTS:
: ['user']
: ['id', 'name', 'age', 'country', 'gender']
: 2
: <dataset.util.ResultIter object at 0x7f7f34e974a8>

Cleaning up:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
rm mydatabase.db
#+END_SRC

#+RESULTS:
: rm mydatabase.db
*** [2018-05-28 Mon]
**** Using AlgDesign & ANOVA from Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
from rpy2.robjects.packages import importr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
#+END_SRC

#+RESULTS:

*** [2018-05-29 Tue]
**** Implementing the D-Optimal Design + AOV Approach in Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import math
from rpy2.robjects.packages import importr
from rpy2.robjects import IntVector, StrVector, Formula, r
from rpy2.robjects.lib.dplyr import dplyr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
car       = importr("car")

def opt_federov(design_formula, data, trials):
    output = algdesign.optFederov(Formula(design_formula),
                                  data,
                                  maxIteration = 1000,
                                  nTrials = trials)
    return output

def transform_design(design, lm_formula, full_model, response):
    transform = car.powerTransform(Formula(lm_formula),
                                   data = design)

    print("Power Transform Step:")
    print(transform)
    transformed_response = car.bcPower(design.rx(response[0]), transform.rx("lambda")[0])
    design               = base.cbind(design, transformed_response)
    transform_lm_formula = "{0}".format(base.names(transformed_response)[0]) + full_model

    return design, transform_lm_formula

def anova(design, formula):
    heteroscedasticity_test = car.ncvTest(stats.lm(Formula(formula), data = design))
    print("Heteroscedasticity Test p-value:")
    print(heteroscedasticity_test.rx("p")[0][0])

    if heteroscedasticity_test.rx("p")[0][0] < 0.05:
        transform_lm_formula = transform_design(design, formula,
                                                full_model, response)
        heteroscedasticity_test = car.ncvTest(stats.lm(Formula(transform_lm_formula),
                                                        data = design))
        print("Heteroscedasticity Test p-value:")
        print(heteroscedasticity_test.rx("p")[0][0])
    else:
        print("No need to power transform")
        transform_lm_formula = lm_formula

    regression = stats.lm(Formula(formula),
                          data = design)

    summary_regression = stats.summary_aov(regression)
    print("Regression Step:")
    print(summary_regression)

    prf_values = {}

    for k, v in zip(base.rownames(summary_regression[0]), summary_regression[0][4]):
        if k.strip() != "Residuals":
            prf_values[k.strip()] = v

    return regression, prf_values

def predict_best(regression, data):
    predicted = stats.predict(regression, data)
    predicted_best = predicted.index(min(predicted))

    p_min = min(predicted)
    i = 0

    for k in range(len(predicted)):
        if math.isclose(predicted[k], p_min, rel_tol = 1e-6):
            i += 1

    print("Identical predictions (tol = 1e-17): {0}".format(i))
    return data.rx(predicted_best, True)

def prune_data(data, fixed_variables):
    print(fixed_variables)
    pruned_data = data
    for k, v in fixed_variables.items():
        pruned_data = pruned_data.rx((pruned_data.rx2(str(k)).ro == str(v)),
                                     True)

    print("Dimensions of Pruned Data: " + str(base.dim(pruned_data)))
    print("Dimensions of Full Data: " + str(base.dim(data)))
    return pruned_data

def get_fixed_variables(predicted_best, ordered_prf_keys, fixed_factors, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    fixed_variables = fixed_factors
    for v in unique_variables:
        fixed_variables[v] = predicted_best.rx2(str(v))[0]

    print("Fixed Variables: " + str(fixed_variables))
    return fixed_variables

def prune_model(factors, inverse_factors, ordered_prf_keys, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    pruned_factors = [f for f in factors if not f in unique_variables]
    pruned_inverse_factors = [f for f in inverse_factors if not f in unique_variables]

    return pruned_factors, pruned_inverse_factors

def dopt_anova_step(response, factors, inverse_factors, data, fixed_factors, budget):
    full_model     = "".join([" ~ ",
                              " + ".join(factors), " + ",
                              " + ".join(["I(1 / {0})".format(f) for f in inverse_factors])])

    design_formula = full_model
    lm_formula     = response[0] + full_model
    trials         = round(2 * (len(factors) + len(inverse_factors) + 1))

    fixed_variables = fixed_factors

    if budget - len(data[0]) < 0:
        print("Full data does not fit on budget")
        if trials < len(data[0]):
            print("Computing D-Optimal Design")
            output = opt_federov(design_formula, data, trials)
            design = output.rx("design")[0]
        else:
            print("Too few data points for a D-Optimal design")
            design = data

        used_experiments = len(design[0])
        regression, prf_values = anova(design, lm_formula)
        ordered_prf_keys       = sorted(prf_values, key = prf_values.get)
        predicted_best         = predict_best(regression, data)
        fixed_variables        = get_fixed_variables(predicted_best, ordered_prf_keys,
                                                     fixed_factors)
        pruned_data            = prune_data(data, fixed_variables)

        pruned_factors, pruned_inverse_factors = prune_model(factors, inverse_factors,
                                                            ordered_prf_keys)
    else:
        print("Full data fits on budget, picking best value")
        used_experiments = len(data[0])
        prf_values = []
        ordered_prf_keys = []
        pruned_data = []
        pruned_factors = []
        pruned_inverse_factors = []
        predicted_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                                  True)

    return {"prf_values": prf_values,
            "ordered_prf_keys": ordered_prf_keys,
            "predicted_best": predicted_best,
            "pruned_data": pruned_data,
            "pruned_factors": pruned_factors,
            "pruned_inverse_factors": pruned_inverse_factors,
            "fixed_factors": fixed_variables,
            "used_experiments": used_experiments}

def dopt_anova():
    data = utils.read_csv("dopt_anova_experiments/data/search_space.csv", header = True)

    initial_factors = ["elements_number", "y_component_number",
                       "vector_length", "temporary_size",
                       "load_overlap", "threads_number", "lws_y"]

    initial_inverse_factors = ["y_component_number", "lws_y",
                               "elements_number", "threads_number"]

    response = ["time_per_pixel"]

    data = data.rx(StrVector(initial_factors + response))
    data_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                        True)

    step_factors = initial_factors
    step_inverse_factors = initial_inverse_factors
    step_space = data

    fixed_factors = {}

    initial_budget = 58
    budget = initial_budget
    used_experiments = 0
    iterations = 3

    for i in range(iterations):
        if step_space == []:
            break

        step_data = dopt_anova_step(response,
                                    step_factors,
                                    step_inverse_factors,
                                    step_space,
                                    fixed_factors,
                                    budget)

        step_space = step_data["pruned_data"]
        step_factors = step_data["pruned_factors"]
        step_inverse_factors = step_data["pruned_inverse_factors"]
        budget -= step_data["used_experiments"]
        used_experiments += step_data["used_experiments"]
        fixed_factors = step_data["fixed_factors"]

        print("Fixed Factors: " + str(fixed_factors))
        print("Slowdown: " + str(step_data["predicted_best"].rx(response[0])[0][0] / data_best.rx(response[0])[0][0]))
        print("Budget: {0}/{1}".format(used_experiments, initial_budget))

dopt_anova()
#+END_SRC

#+RESULTS:
#+begin_example
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.000231109380007393
Power Transform Step:
Estimated transformation parameter
         Y1
-0.08680373

Heteroscedasticity Test p-value:
0.9475422563588601
Regression Step:
                        Df Sum Sq Mean Sq F value   Pr(>F)
elements_number          1  2.290   2.290  11.967 0.004723 **
y_component_number       1  0.529   0.529   2.765 0.122202
vector_length            1  3.766   3.766  19.676 0.000813 ***
temporary_size           1  0.235   0.235   1.225 0.290002
load_overlap             1  0.079   0.079   0.413 0.532456
threads_number           1  0.018   0.018   0.092 0.767416
lws_y                    1  7.271   7.271  37.987 4.85e-05 ***
I(1/y_component_number)  1  0.110   0.110   0.574 0.463168
I(1/lws_y)               1  2.861   2.861  14.949 0.002242 **
I(1/elements_number)     1  0.164   0.164   0.859 0.372333
I(1/threads_number)      1  0.432   0.432   2.259 0.158663
Residuals               12  2.297   0.191
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1}
{'lws_y': 16, 'vector_length': 1}
Dimensions of Pruned Data: [1] 576   8

Dimensions of Full Data: [1] 23120     8

Fixed Factors: {'lws_y': 16, 'vector_length': 1}
Slowdown: 11.64335230377803
Budget: 24/58
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.06519543239335164
No need to power transform
Regression Step:
                        Df    Sum Sq   Mean Sq F value   Pr(>F)
elements_number          1 6.930e-19 6.930e-19   8.120   0.0191 *
y_component_number       1 4.960e-19 4.960e-19   5.814   0.0392 *
temporary_size           1 5.100e-20 5.100e-20   0.599   0.4589
load_overlap             1 5.400e-20 5.400e-20   0.638   0.4451
threads_number           1 5.024e-18 5.024e-18  58.829 3.09e-05 ***
I(1/y_component_number)  1 2.100e-20 2.100e-20   0.251   0.6284
I(1/elements_number)     1 8.200e-20 8.200e-20   0.959   0.3530
I(1/threads_number)      1 5.726e-18 5.726e-18  67.049 1.84e-05 ***
Residuals                9 7.690e-19 8.500e-20
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
{'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Dimensions of Pruned Data: [1] 8 8

Dimensions of Full Data: [1] 576   8

Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.8225092523512674
Budget: 42/58
Full data fits on budget, picking best value
Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.265157794114108
Budget: 50/58
#+end_example
** June
*** [2018-06-04 Mon]
**** Implementing the DOPT-Anova in Orio
I've started implementing our approach in benchmark problems from SPAPT, which
is provided with Orio. I've been understanding Orio's source code implementing
the glue code, with =rpy2=, to be able to use R packages in Python.

I've reached a problem with =optFederov=. One of the benchmark applications
has ~10^{14}~ possible combinations in total. My first approach was trying to
generate a subset of this search space, as I did before, but this did not work.
The function kept finding "singular design" errors, which mean the determinant
of the candidates it tested are negative according to the documentation.

It is not clear how to fix this, so I am now trying to use the optMonteCarlo
function, which tries to generate samples of the search space as it explores
it with the Federov algorithm.
*** [2018-06-11 Mon]
**** Converting R Model Fits into Formulas
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
A <- data.frame(x1 = sample(1:100, 30, replace = T),
                x2 = sample(1:100, 30, replace = T),
                x3 = sample(1:100, 30, replace = T),
                x4 = sample(1:100, 30, replace = T),
                x5 = sample(1:100, 30, replace = T))

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5

regression <- lm(y ~ x1 + x2 + x3 + x4 + x5, data = A)
formula(regression)

new_formula <- as.formula(substituteDirect(formula(regression), as.list(coef(regression))))
new_formula

predict(regression)
predict(regression, type = "terms")
A
#+END_SRC

#+RESULTS:
#+begin_example
y ~ x1 + x2 + x3 + x4 + x5
y ~ 2.3 + 3.1 + 4.5 + 6.8 + 2.31
      1       2       3       4       5       6       7       8       9      10
1480.86  536.12 1174.93  804.95 1487.49 1427.97 1150.58 1014.75  916.79  958.22
     11      12      13      14      15      16      17      18      19      20
 953.27  438.97  710.39  709.89 1234.76 1034.89 1043.24 1622.64 1114.05  739.34
     21      22      23      24      25      26      27      28      29      30
 848.47  928.56 1320.51  744.19 1066.60  946.50 1235.58 1051.55  860.67  706.07
             x1          x2      x3          x4      x5
1    54.4333333 -129.166667  195.15  284.693333   66.99
2   -49.0666667    1.033333 -133.35 -279.706667  -11.55
3   -81.2666667 -104.366667   73.65  264.293333   13.86
4    15.3333333  125.033333 -191.85  -55.306667  -97.02
5   -39.8666667   22.733333  186.15  305.093333    4.62
6    15.3333333  131.233333 -115.35  318.693333   69.30
7    24.5333333    1.033333 -227.85  318.693333   25.41
8    98.1333333 -104.366667  100.65  -14.506667  -73.92
9    47.5333333  143.633333  -29.85 -211.706667  -41.58
10 -106.5666667  140.533333   28.65 -170.906667   57.75
11   29.1333333   44.433333  141.15 -293.306667   23.10
12   24.5333333  -23.766667 -218.85 -259.306667  -92.40
13   91.2333333  -29.966667 -119.85 -313.706667   73.92
14  -42.1666667  -14.466667   73.65 -320.506667    4.62
15   98.1333333   97.133333   55.65   46.693333  -71.61
16  -72.0666667  121.933333  186.15 -306.906667   97.02
17   -0.7666667   16.533333 -169.35  264.293333  -76.23
18   52.1333333  109.533333  163.65  203.093333   85.47
19   -0.7666667  109.533333  -38.85  155.493333 -120.12
20   10.7333333 -119.866667  186.15 -293.306667  -53.13
21   77.4333333   32.033333 -187.35   33.093333 -115.50
22  -72.0666667  -64.066667  136.65  -55.306667  -25.41
23  -30.6666667  156.033333   55.65  121.493333    9.24
24 -101.9666667 -141.566667 -223.35  243.893333  -41.58
25    1.5333333  -67.166667  -97.35  121.493333   99.33
26    1.5333333   38.233333   -7.35 -170.906667   76.23
27   36.0333333 -104.366667  -88.35  311.893333   71.61
28   75.1333333 -147.766667  114.15    5.893333   -4.62
29  -72.0666667  -95.066667  -20.85   39.893333    0.00
30  -83.5666667 -144.666667  172.65 -293.306667   46.20
attr(,"constant")
[1] 1008.76
   x1  x2 x3  x4  x5       y
1  78   8 96  95  86 1480.86
2  33  50 23  12  52  536.12
3  19  16 69  92  63 1174.93
4  61  90 10  45  15  804.95
5  37  57 94  98  59 1487.49
6  61  92 27 100  87 1427.97
7  65  50  2 100  68 1150.58
8  97  16 75  51  25 1014.75
9  75  96 46  22  39  916.79
10  8  95 59  28  82  958.22
11 67  64 84  10  67  953.27
12 65  42  4  15  17  438.97
13 94  40 26   7  89  710.39
14 36  45 69   6  59  709.89
15 97  81 65  60  26 1234.76
16 23  89 94   8  99 1034.89
17 54  55 15  92  24 1043.24
18 77  85 89  83  94 1622.64
19 54  85 44  76   5 1114.05
20 59  11 94  10  34  739.34
21 88  60 11  58   7  848.47
22 23  29 83  45  46  928.56
23 41 100 65  71  61 1320.51
24 10   4  3  89  39  744.19
25 55  28 31  71 100 1066.60
26 55  62 51  28  90  946.50
27 70  16 33  99  88 1235.58
28 87   2 78  54  55 1051.55
29 23  19 48  59  57  860.67
30 18   3 91  10  77  706.07
#+end_example
*** [2018-06-13 Wed]
**** Stepwise Regression in R
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

A <- data.frame(x1 = sample(1:100, 100, replace = T),
                x2 = sample(1:100, 100, replace = T),
                x3 = sample(1:100, 100, replace = T),
                x4 = sample(1:100, 100, replace = T),
                x5 = sample(1:100, 100, replace = T),
                x6 = sample(1:100, 100, replace = T),
                x7 = sample(1:100, 100, replace = T),
                x8 = sample(1:100, 100, replace = T)
)

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5 + 5.2 * (A$x8 * A$x2) + 3.1 * (1 / A$x7)

regression = lm(y ~ ., data = A[1:30, ])
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
A[A$y == min(A$y), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x1           1 2.566e+07 2.566e+07   1.852  0.18796
x2           1 2.007e+09 2.007e+09 144.892 6.88e-11 ***
x3           1 1.796e+05 1.796e+05   0.013  0.91044
x4           1 2.557e+08 2.557e+08  18.458  0.00032 ***
x5           1 4.264e+08 4.264e+08  30.779 1.66e-05 ***
x6           1 1.331e+08 1.331e+08   9.604  0.00544 **
x7           1 1.376e+07 1.376e+07   0.993  0.33033
x8           1 1.639e+09 1.639e+09 118.324 4.35e-10 ***
Residuals   21 2.909e+08 1.385e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
   x2 x8
14 30  1
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression = update(regression, . ~ . - x1 - x3 - x4 - x5 - x6 - x7)
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x2           1 1.953e+09 1.953e+09   127.8 9.58e-12 ***
x8           1 2.427e+09 2.427e+09   158.8 8.03e-13 ***
Residuals   27 4.126e+08 1.528e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
#+end_example
*** [2018-06-14 Thu]
**** Stepwise Regression in R
Creating two datasets, a complete one and a sample of it.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

complete_A <- expand.grid(x1 = seq(1, 10, 1),
                          x2 = seq(1, 10, 1),
                          x3 = seq(1, 10, 1),
                          x4 = seq(1, 10, 1),
                          x5 = seq(1, 10, 1),
                          x6 = seq(1, 10, 1),
                          x7 = seq(1, 10, 1)
)

complete_A$y = 2.3 * complete_A$x1 - 3.1 * complete_A$x2 + 4.5 * complete_A$x3 + 2.5 * (1 / complete_A$x2) - 6.8 * complete_A$x4 + 2.31 * (complete_A$x5 * complete_A$x1) - 4.2 * (complete_A$x2 * complete_A$x4)

A <- complete_A[sample(1:nrow(complete_A), 100, replace = F), ]
#+END_SRC

#+RESULTS:

Now we use the sample from the full set to fit the model. We then use another
sample for the predictions. In a real application this sample would not be
measured, since we want only the parameter values.

We are using the =stepAIC= function, that minimizes the [[https://en.wikipedia.org/wiki/Akaike_information_criterion][Akaike Information
Criterion]], to find the model that best fits the experiment data. We will allow
both removal and addition of model variables, up to the intial model, or "full
model". The initial or "null" model is just a constant.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
experiment_data = complete_A[sample(1:nrow(complete_A), 2 * nrow(A), replace = F), ]

null = lm(y ~ 1, data = A)
full = lm(y ~ . + I(1 / x2) + I(1 / x3), data = A)

regression = stepAIC(null, scope = list(upper = full), direction = "both", trace = 1)

summary.aov(regression)

formula(full)
formula(regression)
r_names <- attr(terms(regression), "term.labels")
r_names <- r_names[r_names %in% names(complete_A)]
r_names
#+END_SRC

#+RESULTS:
#+begin_example
Start:  AIC=964.42
y ~ 1

          Df Sum of Sq     RSS    AIC
+ x2       1    653372  859229 909.86
+ x4       1    441689 1070912 931.89
+ I(1/x2)  1    383017 1129584 937.22
+ x5       1    129020 1383581 957.50
+ x1       1     48781 1463820 963.14
+ I(1/x3)  1     46868 1465733 963.27
<none>                 1512601 964.42
+ x3       1     10526 1502075 965.72
+ x6       1      9805 1502796 965.77
+ x7       1       309 1512292 966.40

Step:  AIC=909.86
y ~ x2

          Df Sum of Sq     RSS    AIC
+ x4       1    455540  403689 836.32
+ x5       1     86545  772684 901.25
+ I(1/x3)  1     39119  820110 907.20
+ x1       1     30172  829057 908.29
+ x3       1     29944  829285 908.31
<none>                  859229 909.86
+ x7       1      6710  852519 911.08
+ x6       1      5989  853240 911.16
+ I(1/x2)  1      1708  857521 911.66
- x2       1    653372 1512601 964.42

Step:  AIC=836.32
y ~ x2 + x4

          Df Sum of Sq     RSS    AIC
+ x5       1    133553  270136 798.15
+ x1       1    109629  294061 806.64
+ I(1/x3)  1     36532  367158 828.84
+ x3       1     32372  371317 829.96
<none>                  403689 836.32
+ I(1/x2)  1      1695  401994 837.90
+ x6       1      1190  402500 838.03
+ x7       1       134  403555 838.29
- x4       1    455540  859229 909.86
- x2       1    667223 1070912 931.89

Step:  AIC=798.15
y ~ x2 + x4 + x5

          Df Sum of Sq    RSS    AIC
+ x1       1    126352 143785 737.09
+ x3       1      9771 260366 796.47
+ I(1/x3)  1      8609 261527 796.91
<none>                 270136 798.15
+ I(1/x2)  1      5190 264946 798.21
+ x6       1       246 269890 800.06
+ x7       1       148 269988 800.10
- x5       1    133553 403689 836.32
- x4       1    502548 772684 901.25
- x2       1    615682 885819 914.91

Step:  AIC=737.09
y ~ x2 + x4 + x5 + x1

          Df Sum of Sq    RSS    AIC
+ x3       1     15072 128712 728.02
+ I(1/x3)  1      9874 133911 731.98
<none>                 143785 737.09
+ I(1/x2)  1      1196 142589 738.25
+ x6       1        99 143686 739.02
+ x7       1        69 143716 739.04
- x1       1    126352 270136 798.15
- x5       1    150276 294061 806.64
- x2       1    578998 722782 896.57
- x4       1    594448 738233 898.68

Step:  AIC=728.02
y ~ x2 + x4 + x5 + x1 + x3

          Df Sum of Sq    RSS    AIC
<none>                 128712 728.02
+ I(1/x2)  1      1074 127639 729.18
+ x7       1        38 128674 729.99
+ x6       1        20 128692 730.00
+ I(1/x3)  1         0 128712 730.02
- x3       1     15072 143785 737.09
- x5       1    122550 251263 792.91
- x1       1    131653 260366 796.47
- x2       1    592170 720882 898.31
- x4       1    594763 723475 898.67
            Df Sum Sq Mean Sq F value   Pr(>F)
x2           1 653372  653372  477.17  < 2e-16 ***
x4           1 455540  455540  332.69  < 2e-16 ***
x5           1 133553  133553   97.53 3.37e-16 ***
x1           1 126352  126352   92.28 1.26e-15 ***
x3           1  15072   15072   11.01  0.00129 **
Residuals   94 128712    1369
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + I(1/x2) + I(1/x3)
y ~ x2 + x4 + x5 + x1 + x3
[1] "x2" "x4" "x5" "x1" "x3"
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
values = predict(regression, experiment_data)
complete_A[complete_A$y == min(complete_A$y), "y"] / experiment_data[values == min(values), "y"]

values = complete_A[sample(1:nrow(complete_A), nrow(A), replace = F), ]
complete_A[complete_A$y == min(complete_A$y), "y"] / values[values$y == min(values$y), "y"]
#+END_SRC

#+RESULTS:
#+begin_example
  [1] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
  [9] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [17] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [25] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [33] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [41] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [49] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [57] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [65] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [73] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [81] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [89] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [97] 1.153702 1.153702 1.153702 1.153702
  [1] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
  [9] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [17] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [25] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [33] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [41] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [49] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [57] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [65] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [73] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [81] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [89] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [97] 1.245883 1.245883 1.245883 1.245883
#+end_example
*** [2018-06-15 Fri]
**** Working with =optMonteCarlo=
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(MASS)

model_formula = formula(~ . + I(1 / (1 + x2)) + I(1 / (1 + x3)) + I(1 / (1 + x4)))
model_formula

design_data = data.frame(var = c("x1", "x2", "x3", "x4", "x5", "x6", "x7"),
                         low = c(rep(0, 7)),
                         high = c(9, 18, 18, 2, 1, 6, 40),
                         center = c(rep(0, 7)),
                         nLevels = c(10, 19, 19, 3, 2, 7, 41),
                         round = c(rep(0, 7)),
                         factor = c(rep(F, 7)),
                         mix = c(rep(F, 7)))

design_data

optMonteCarlo(model_formula, design_data, nTrials = 25)
#+END_SRC

#+RESULTS:
#+begin_example
~. + I(1/(1 + x2)) + I(1/(1 + x3)) + I(1/(1 + x4))
  var low high center nLevels round factor   mix
1  x1   0    9      0      10     0  FALSE FALSE
2  x2   0   18      0      19     0  FALSE FALSE
3  x3   0   18      0      19     0  FALSE FALSE
4  x4   0    2      0       3     0  FALSE FALSE
5  x5   0    1      0       2     0  FALSE FALSE
6  x6   0    6      0       7     0  FALSE FALSE
7  x7   0   40      0      41     0  FALSE FALSE
$D
[1] 1.248469

$A
[1] 42.83773

$Ge
[1] 0.833

$Dea
[1] 0.818

$design
   x1 x2 x3 x4 x5 x6 x7
1   1  2  6  2  0  6 23
2   1  0  6  1  0  2  5
3   7  0 14  1  0  3 30
4   4 10  0  0  0  5 35
5   2  9 18  2  1  4 32
6   1  5 15  0  0  0 32
7   9 18  0  1  0  5  0
8   8 14 16  0  1  0 12
9   8 17  4  1  0  6  0
10  3 17  4  0  0  4 35
11  3 13 18  1  1  6 11
12  9  5 13  1  0  4  3
13  1  9  7  2  0  0 24
14  2  6  3  1  1  0 23
15  2 18 17  1  1  1 27
16  9  3  4  0  1  5 27
17  4  0 14  2  1  0  9
18  7 12  0  1  1  1 20
19  8 18 12  2  1  2 38
20  2  0 17  1  0  2 31
21  5  0  0  1  1  2 10
22  9  3  6  1  1  4 31
23  9 14 16  2  0  1  3
24  2 14  2  2  1  5  4
25  1  1 15  0  1  3  7
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
set.seed(66182)
model_formula <- formula(~ T1_J + T1_K + T2_I + T2_J + T2_K + ACOPY_x + ACOPY_y + U1_I + U_I + U_K + RT_J + RT_K + SCR + VEC1 + VEC2 + I(1 / (1 + T1_J)) + I(1 / (1 + T1_K)) + I(1 / (1 + T2_I)) + I(1 / (1 + T2_J)) + I(1 / (1 + T2_K)) + I(1 / (1 + U1_I)) + I(1 / (1 + U_I)) + I(1 / (1 + U_K)) + I(1 / (1 + RT_J)) + I(1 / (1 + RT_K)))

design_data <- data.frame(var = c("T1_J", "T1_K", "T2_I", "T2_J", "T2_K", "ACOPY_x",
                                  "ACOPY_y", "U1_I", "U_I", "U_K", "RT_J", "RT_K",
                                  "SCR", "VEC1", "VEC2"),
                          low = rep(0, 15),
                          high = c(6, 6, 6, 6, 6, 1, 1, 29, 29, 29, 2, 2, 1, 1, 1),
                          center = rep(0, 15),
                          nLevels = c(7, 7, 7, 7, 7, 2, 2, 30, 30, 30, 3, 3, 2, 2, 2),
                          round = rep(0, 15),
                          factor = c(rep(F, 15)),
                          mix = c(rep(F, 15)))

design_data

optMonteCarlo(model_formula, data = design_data, nTrials = 37)
#+END_SRC

#+RESULTS:
#+begin_example
       var low high center nLevels round factor   mix
1     T1_J   0    6      0       7     0  FALSE FALSE
2     T1_K   0    6      0       7     0  FALSE FALSE
3     T2_I   0    6      0       7     0  FALSE FALSE
4     T2_J   0    6      0       7     0  FALSE FALSE
5     T2_K   0    6      0       7     0  FALSE FALSE
6  ACOPY_x   0    1      0       2     0  FALSE FALSE
7  ACOPY_y   0    1      0       2     0  FALSE FALSE
8     U1_I   0   29      0      30     0  FALSE FALSE
9      U_I   0   29      0      30     0  FALSE FALSE
10     U_K   0   29      0      30     0  FALSE FALSE
11    RT_J   0    2      0       3     0  FALSE FALSE
12    RT_K   0    2      0       3     0  FALSE FALSE
13     SCR   0    1      0       2     0  FALSE FALSE
14    VEC1   0    1      0       2     0  FALSE FALSE
15    VEC2   0    1      0       2     0  FALSE FALSE
$D
[1] 0.2969417

$A
[1] 73.70555

$Ge
[1] 0.482

$Dea
[1] 0.341

$design
   T1_J T1_K T2_I T2_J T2_K ACOPY_x ACOPY_y U1_I U_I U_K RT_J RT_K SCR VEC1
1     4    0    0    4    1       0       1   28   1   1    2    1   0    0
2     0    5    1    4    6       1       0    3  14  15    1    1   1    1
3     1    4    4    0    0       1       1   16  14   1    0    1   0    0
4     6    2    4    3    5       1       0    7  28  10    0    2   0    0
5     1    2    0    4    3       1       1   11   6   6    1    1   0    0
6     6    4    1    1    1       1       0   27   7  23    1    0   1    0
7     4    5    2    4    1       0       0    4  16  27    0    1   0    1
8     5    2    4    3    1       0       1   13   1   0    1    0   1    1
9     3    6    2    5    0       1       1    0  18  14    2    0   1    1
10    2    2    0    2    6       1       1    0   3  28    2    1   0    1
11    5    1    5    2    0       1       1   12   8   8    1    2   1    1
12    1    0    1    5    4       1       0   28   0  28    1    2   0    1
13    3    2    4    1    0       0       0    1   0   8    1    1   1    1
14    6    1    0    6    2       0       1   29  28  27    0    1   1    0
15    1    0    5    2    5       0       1   13  17  18    0    2   1    1
16    0    2    6    5    5       0       1   25   0   1    0    0   0    0
17    6    4    0    3    6       0       0   29  19  23    1    2   0    1
18    2    2    3    5    3       0       1   19   1  26    0    2   1    0
19    4    6    6    5    6       1       1   12  15  26    1    1   0    1
20    3    6    0    2    2       0       1   23  15  25    0    0   1    1
21    2    1    2    6    2       1       1   24  26   0    1    2   1    0
22    0    4    4    4    3       0       1   27   3  24    2    2   0    0
23    6    6    1    0    4       0       1    0  28   7    2    1   0    0
24    2    0    2    0    6       0       1    6  22  23    2    1   1    0
25    2    1    5    2    5       0       0    0  17  24    1    1   0    1
26    1    3    1    4    5       1       1   29  10   7    0    0   0    1
27    4    5    5    0    1       1       0   17   3   5    0    0   1    1
28    0    2    1    0    5       0       0    6  25  12    1    2   0    1
29    2    6    5    4    6       0       1    0  25  16    0    1   1    0
30    0    5    4    2    0       0       0   17  24  12    0    1   0    0
31    1    1    0    2    0       0       1   16  28  19    1    0   0    0
32    3    6    4    6    3       0       0    3   3   1    1    0   0    0
33    6    2    5    6    0       0       0   12  12   1    2    1   0    0
34    2    4    6    2    5       1       0   28  11  16    2    1   1    1
35    3    4    3    6    3       0       1   10  27   8    2    0   0    1
36    0    2    5    2    4       1       0   10  21  29    2    0   1    0
37    6    3    5    3    1       1       1   26  27   6    1    1   0    1
   VEC2
1     1
2     1
3     0
4     1
5     1
6     1
7     0
8     1
9     1
10    1
11    0
12    0
13    0
14    1
15    1
16    0
17    0
18    1
19    1
20    0
21    0
22    0
23    1
24    0
25    1
26    1
27    1
28    1
29    0
30    1
31    1
32    1
33    0
34    1
35    1
36    0
37    1
#+end_example
*** [2018-06-21 Thu]
**** A Flexible Distributed Optimization Scheme with Asynchronous, Scarse, and Sparse Communications
- Presenter: Frank Iutzeler (http://www.iutzeler.org/)
- Context: Distributed Learning
  - Global Learning Objective
  - Local data
- Proximal Gradient Algorithms
*** [2018-06-28 Thu]
**** Tweaking DLMT
***** Initial Sample for =OptFederov=
- Will take a long time to generate if we choose a large size, because
  constraint checking restricts a lot of the search space
- Will not provide enough candidates for =OptFederov= if the sample size is
  small. This can result in small D-Efficiency for the generated experiments
***** =OptFederov= Function
****** nTrials
- The simplest way to deal with runtime errors is to increase the number of
  experiments generated by =OptFederov=
- Using more trials allows for more failures before ANOVA breaks
- Should we re-generate the design after failures?
****** frml
- Using the inverse of variables in the initial model requires special care to
  remove 2-level factors that may appear due to constraints
***** Problems in Analysis
- When valid experiments in the design fail, what happens with the validity of ANOVA?
- What are the best ways to go around that?
***** Threshold for Removing Variables
- We are using an heuristic to select how many variables to remove at each step
** July
*** [2018-07-03 Tue]
**** Round Table LIG Day
- Blockchain
- HPC & Big Data Convergence
- Machine Learning
- Reproducibility
*** [2018-07-04 Wed]
**** Plotting Sampling Strategies
***** Random Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)

data = data.frame(x1 = sample(0:20, 100, replace = T),
                  x2 = sample(0:20, 100, replace = T))

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figure7g2xWl.png]]

***** LHS Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)

data <- lhs.design(nruns = 100 ,nfactors = 2, digits = 0, type = "maximin",
                   factor.names = list(x1 = c(0, 20), x2 = c(0, 20)))

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figureUvZvfi.png]]
***** D-Optimal Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~., full_factorial, nTrials = 20)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figuretQupRL.png]]

***** D-Optimal Sampling with Interactions
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + .^2, full_factorial, nTrials = 20)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figure9Cfi2s.png]]
***** D-Optimal Sampling with Quadratic Terms
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = 30)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figureXQIyzR.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
head(model.matrix(~. + I(x1 ^ 2) + I(x2 ^ 2), full_factorial))
#+END_SRC

#+RESULTS:
:   (Intercept)  x1  x2 I(x1^2) I(x2^2)
: 1           1 -19 -19     361     361
: 2           1 -17 -19     289     361
: 3           1 -15 -19     225     361
: 4           1 -13 -19     169     361
: 5           1 -11 -19     121     361
: 6           1  -9 -19      81     361

***** D-Optimal Sampling with Cubic Terms
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + cubic(.), full_factorial, nTrials = 30)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figureTljFYC.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
head(model.matrix(~ . + cubic(.), full_factorial))
#+END_SRC

#+RESULTS:
:   (Intercept)  x1  x2 I(x1^2) I(x2^2) I(x1^3) I(x2^3) x1:x2
: 1           1 -19 -19     361     361   -6859   -6859   361
: 2           1 -17 -19     289     361   -4913   -6859   323
: 3           1 -15 -19     225     361   -3375   -6859   285
: 4           1 -13 -19     169     361   -2197   -6859   247
: 5           1 -11 -19     121     361   -1331   -6859   209
: 6           1  -9 -19      81     361    -729   -6859   171
***** I-Optimal Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~., full_factorial, criterion = "I", nTrials = 60)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figurecZgDnj.png]]
**** Experiments with the ATAX application
I've been using the smallest input size for the atax application. So far I am
not sure if the larger sizes even work properly, since some of then do not use
the validation code. Some of the also do not have the same parameters.

Experiments with DLMT using all factors and their inverses, when applicable, is
not giving results much different than random search. The model might be more or
less complex. The D-Optimality of the designs is usually low.

I am also not sure of the impact of sample sizes in the two steps where they are
used: the sample for the Fedorov algorithm and the sample for predicting the
best result. These samples are not evaluated, but must be validated using the
constraints provided by the application.

Next steps:
- Run DLMT experiments with a simpler model
- Run DLMT experiments using interactions in the model
- Compare results for other applications and input sizes
- Compare results with other search algorithms
- Address issues in [[Tweaking DLMT][Tweaking DLMT]]
*** [2018-07-05 Thu]
**** Building D-Optimal Designs with Interactions
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(1000),
                   x2 = runif(1000),
                   x3 = runif(1000),
                   x4 = runif(1000),
                   x5 = runif(1000),
                   x6 = runif(1000))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:          x1         x2        x3        x4        x5        x6        y
: 1 0.3145343 0.22122070 0.5791998 0.6329657 0.9590482 0.5580798 6.236130
: 2 0.4204504 0.52465529 0.9820209 0.4381713 0.8702131 0.8412574 7.397551
: 3 0.9683900 0.07251716 0.8357594 0.3334859 0.1510384 0.7234758 5.482052
: 4 0.1489611 0.84208199 0.3208823 0.8697806 0.5734586 0.3794286 6.219130
: 5 0.1562686 0.01527451 0.8520008 0.9050570 0.7896101 0.8236433 6.724943
: 6 0.8644607 0.33800877 0.4537771 0.9145758 0.8455194 0.3449356 7.638418

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
output <- optFederov(~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, data, nTrials = 14)
output

design <- output$design
#+END_SRC

#+RESULTS:
#+begin_example
$D
[1] 0.1539217

$A
[1] 11.50989

$Ge
[1] 0.8

$Dea
[1] 0.779

$design
            x1         x2          x3          x4          x5          x6
5   0.15626861 0.01527451 0.852000780 0.905057031 0.789610110 0.823643283
60  0.14111468 0.06047899 0.005816296 0.122148020 0.241837807 0.204990197
82  0.83102789 0.24063190 0.066638218 0.835985614 0.938030672 0.274673609
167 0.01083552 0.86262930 0.919079563 0.994813522 0.326322796 0.348092876
188 0.99944823 0.48202436 0.184919251 0.034891482 0.945193022 0.029429601
431 0.65156391 0.85404254 0.997202041 0.039830894 0.868841200 0.898136727
479 0.07126425 0.02491834 0.072766728 0.091690738 0.506438140 0.968787553
488 0.90546459 0.08732170 0.985465054 0.616232353 0.107418757 0.003533801
489 0.15103343 0.58518849 0.999827211 0.002674163 0.934751620 0.217067372
566 0.77890381 0.88071788 0.096931037 0.982260734 0.169843512 0.991753875
852 0.03508049 0.98718423 0.066761902 0.912848081 0.375775029 0.103496414
909 0.42360652 0.94109188 0.988913755 0.840630643 0.956933183 0.841327438
915 0.74483130 0.82045704 0.042712603 0.007349035 0.022841742 0.893580211
968 0.99884604 0.02018211 0.894035780 0.503326149 0.001980576 0.860871009
            y
5    6.724943
60   1.487574
82   6.523872
167  7.935617
188  4.651014
431  6.727184
479  3.111935
488  5.092613
489  4.243379
566  7.433653
852  4.648488
909 10.527218
915  3.944372
968  5.940554

$rows
 [1]   5  60  82 167 188 431 479 488 489 566 852 909 915 968
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression <- aov(y ~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, design)
summary(regression)
#+END_SRC

#+RESULTS:
#+begin_example
            Df Sum Sq Mean Sq   F value Pr(>F)
x1           1  1.365   1.365 3.407e+30 <2e-16 ***
x2           1 14.548  14.548 3.630e+31 <2e-16 ***
x3           1 17.450  17.450 4.355e+31 <2e-16 ***
x4           1 19.636  19.636 4.900e+31 <2e-16 ***
x5           1  4.612   4.612 1.151e+31 <2e-16 ***
x6           1  5.692   5.692 1.420e+31 <2e-16 ***
x2:x3:x4     1  3.020   3.020 7.537e+30 <2e-16 ***
Residuals    6  0.000   0.000
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#+end_example
*** [2018-07-09 Mon]
**** Paper Review: Assessing Time Predictability Features of ARM big.LITTLE Multicores
This paper presents qualitative and quantitative assessments of some
characteristics of the ARM big.LITTLE architecture. The objective of the paper
is to measure the applicability of this architecture to critical real-time
embedded systems. The paper is well organized, and it was easy to read and
understand.

The qualitative assessment uses the technical reference manuals of the ARM
Cortex-A53 processor and the ARM Juno R2 board. The quantitative assessment uses
a simple stress benchmark application and measures the cycles per memory access
in different scenarios.

The quantitative assessment is based on three experiments: a scenario without
contention, or isolated, a scenario with contention, and a scenario with
contention and no-ops between memory accesses. The results in Figures 3 and 4
show the median of 1000 measurements for each vector size. The Figures do not
show the standard deviations of the measurements.

It is not discussed whether the median is the best choice to represent the
measurements, or whether the variability of measurements was relevant. Without
knowing the variability of measurements it is hard to know if the differences
observed in each experimental scenario are meaningful. I suggest to include the
standard deviation of each measurement in the data presented. Also, unless there
is a strong reason for using the median, I suggest using the mean of the 1000
measurements. I believe this will provide better support for the conclusions of
the paper.

I also believe that measuring the cycles per access in a more comprehensive
benchmark suite would help support the arguments presented in the paper, as well
as using real applications on critical real-time embedded systems. My
recommendation is that this paper should be submitted again after addressing the
issues with data presentation and analysis, and possibly with more experimental
evaluations using different benchmark applications, potentially using
measurements in real applications.
*** [2018-07-10 Tue]
**** Building Models with Interactions
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:           x1        x2        x3        x4         x5        x6        y
: 1 0.18076715 0.4140549 0.4354110 0.8747773 0.44056661 0.2059766 5.257260
: 2 0.04511744 0.3389018 0.6758916 0.7470131 0.95302942 0.7220186 6.740219
: 3 0.64351052 0.8803408 0.3407263 0.4162235 0.78808724 0.6384063 6.607299
: 4 0.13375844 0.4154362 0.5740708 0.9881421 0.22562579 0.0736479 5.206433
: 5 0.70947263 0.9798590 0.9904583 0.8542580 0.13675687 0.7153690 9.525387
: 6 0.68455712 0.3255436 0.9922359 0.2413945 0.05896366 0.5179531 4.732390

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)         x1        x2        x3        x4         x5        x6
1           1 0.18076715 0.4140549 0.4354110 0.8747773 0.44056661 0.2059766
2           1 0.04511744 0.3389018 0.6758916 0.7470131 0.95302942 0.7220186
3           1 0.64351052 0.8803408 0.3407263 0.4162235 0.78808724 0.6384063
4           1 0.13375844 0.4154362 0.5740708 0.9881421 0.22562579 0.0736479
5           1 0.70947263 0.9798590 0.9904583 0.8542580 0.13675687 0.7153690
6           1 0.68455712 0.3255436 0.9922359 0.2413945 0.05896366 0.5179531
    x2:x3:x4
1 0.15770839
2 0.17111149
3 0.12484840
4 0.23566183
5 0.82906553
6 0.07797429
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
model_matrix <- as.data.frame(model_matrix)
removed_interactions <- names(Filter(function(x)(length(unique(x)) == 1), model_matrix))
removed_interactions
#+END_SRC

#+RESULTS:
: [1] "(Intercept)"
*** [2018-07-13 Fri]
**** Interesting Paper using SPAPT
- [[file:~/Dropbox/papers/autotuning/2017/ogilvie2017minimizing.pdf][Minimizing the Cost of Iterative Compilation with Active Learning]]
**** Building Models with Quadratic Terms
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:           x1        x2        x3         x4        x5        x6        y
: 1 0.03468630 0.5173048 0.1087729 0.08684336 0.6359173 0.9587616 3.802525
: 2 0.98720044 0.4519941 0.9221820 0.28453285 0.9660307 0.4632033 7.232514
: 3 0.31504150 0.9030804 0.4090276 0.21641727 0.5562343 0.5831133 4.864308
: 4 0.48529389 0.7517897 0.3670082 0.37152128 0.9808670 0.4651149 6.086757
: 5 0.07717325 0.1913457 0.3366301 0.82366125 0.7304363 0.9233006 6.068915
: 6 0.29731807 0.5504496 0.2815305 0.21052078 0.8033675 0.7820747 4.992076

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ I(x2 ^ 2) + x1 + x2 + x3 + x4 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)    I(x2^2)         x1        x2        x3         x4        x5
1           1 0.26760424 0.03468630 0.5173048 0.1087729 0.08684336 0.6359173
2           1 0.20429871 0.98720044 0.4519941 0.9221820 0.28453285 0.9660307
3           1 0.81555417 0.31504150 0.9030804 0.4090276 0.21641727 0.5562343
4           1 0.56518779 0.48529389 0.7517897 0.3670082 0.37152128 0.9808670
5           1 0.03661316 0.07717325 0.1913457 0.3366301 0.82366125 0.7304363
6           1 0.30299480 0.29731807 0.5504496 0.2815305 0.21052078 0.8033675
         x6
1 0.9587616
2 0.4632033
3 0.5831133
4 0.4651149
5 0.9233006
6 0.7820747
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- as.data.frame(model_matrix)
removed_interactions <- names(Filter(function(x)(length(unique(x)) == 1), model_matrix))
removed_interactions
#+END_SRC

#+RESULTS:
: [1] "(Intercept)"
*** [2018-07-16 Mon]
**** Temporary Parameters for DLMT
- Using quadratic terms yielded better results: Importance of the model
- Comparing with O2: The same was done in this [[Interesting Paper using SPAPT]]
- Removed binary variables: Caused runtime errors; Removed in other works too
- Less iterations: Trying to minimize executions
**** Plotting Orio Experiments Data (atax, O2, quadratic terms, Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output
#+BEGIN_SRC shell
cd orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/ && ./db2csv.py
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/results.csv")
str(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup_O3))
data_median <- ddply(data, .(technique), summarize, median = median(speedup_O3))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup_O3),
                   err = 2 * sd(speedup_O3) / sqrt(length(speedup_O3)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup_O3))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  18 variables:
 $ id        : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT_K      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ T1_I      : int  1 16 1 16 16 1 16 16 32 1 ...
 $ T1_J      : int  16 16 128 16 256 256 128 256 16 128 ...
 $ T1_K      : int  32 64 512 512 1 512 256 1 64 128 ...
 $ U_K       : int  1 26 28 1 11 1 1 21 26 12 ...
 $ U_J       : int  18 28 19 11 1 22 8 10 26 1 ...
 $ U_I       : int  10 1 1 17 8 11 8 1 1 12 ...
 $ technique : Factor w/ 2 levels "DLMT","RS": 2 2 2 2 2 2 2 2 2 2 ...
 $ U1_I      : int  27 14 20 14 23 21 9 1 19 3 ...
 $ speedup_O3: num  1.68 1.67 1.66 1.71 1.68 ...
 $ T2_K      : int  2048 256 1 1024 256 1024 512 128 1 256 ...
 $ T2_J      : int  2048 256 2048 256 256 1 128 256 128 1024 ...
 $ T2_I      : int  1 1024 1 256 2048 1024 64 512 512 1 ...
 $ points    : int  100 100 100 100 100 100 100 100 100 100 ...
 $ RT_I      : int  1 8 8 8 1 1 1 8 8 1 ...
 $ cost_mean : num  0.109 0.109 0.11 0.107 0.109 ...
 $ RT_J      : int  8 1 1 1 8 8 1 1 1 8 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup_O3), #, y = 0.1 * ..density..),
    binwidth = 0.05) +
    labs(y = "Frequency", x = "Speedup vs O2") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 100)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 30, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 30, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "", breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AiL4Pb/figureG457Cf.png]]
**** Plotting Orio Experiments Data (atax, O2, quadratic terms, Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output
#+BEGIN_SRC shell
cd orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O2_quad/ && ./db2csv.py
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O2_quad/results.csv")
str(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup_O3))
data_median <- ddply(data, .(technique), summarize, median = median(speedup_O3))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup_O3),
                   err = 2 * sd(speedup_O3) / sqrt(length(speedup_O3)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup_O3))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  18 variables:
 $ id        : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT_K      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ T1_I      : int  512 512 1 32 64 128 16 512 32 512 ...
 $ T1_J      : int  16 512 1 1 1 512 1 512 16 128 ...
 $ T1_K      : int  128 1 512 1 1 1 128 512 1 512 ...
 $ U_K       : int  1 1 1 30 1 1 1 12 1 17 ...
 $ U_J       : int  16 21 24 21 22 15 17 1 9 21 ...
 $ U_I       : int  22 15 6 1 10 15 18 19 21 1 ...
 $ technique : Factor w/ 2 levels "DLMT","RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ U1_I      : int  28 30 5 16 14 4 9 2 16 15 ...
 $ speedup_O3: num  1.51 1.53 1.5 1.52 1.52 ...
 $ T2_K      : int  256 1 1 1 1 2048 256 2048 2048 1 ...
 $ T2_J      : int  2048 1024 128 1024 512 1 1 1 2048 2048 ...
 $ T2_I      : int  2048 1 2048 128 1 1 256 2048 2048 2048 ...
 $ points    : int  105 105 162 136 139 114 126 115 123 118 ...
 $ RT_I      : int  8 8 1 8 8 8 8 8 8 8 ...
 $ cost_mean : num  0.0938 0.093 0.0945 0.0935 0.0934 ...
 $ RT_J      : int  1 1 8 1 1 1 1 1 1 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup_O3), #, y = 0.01 * ..density..),
    binwidth = 0.05) +
    labs(y = "Frequency", x = "Speedup vs O2") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 100)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 30, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 30, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AyGCXu/figureKUTrtW.png]]
**** Building Models with Quadratic Terms II
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:          x1         x2        x3        x4         x5          x6        y
: 1 0.6778492 0.92349391 0.1576724 0.9815172 0.77496216 0.282501746 7.442764
: 2 0.7088311 0.51079715 0.2335002 0.5522585 0.22558065 0.005148194 4.308371
: 3 0.5280327 0.03529519 0.5537724 0.2926394 0.67773583 0.236637721 4.250867
: 4 0.4423366 0.93826929 0.4526374 0.7090073 0.84701227 0.157596670 6.940557
: 5 0.2282255 0.37932004 0.5559482 0.7280456 0.04624094 0.923890930 5.526287
: 6 0.7575107 0.69316315 0.1938151 0.7458463 0.29209639 0.149164932 5.523634

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ I(x2 ^ 2) + I(x1 ^ 2) + I(x3 ^ 2) + I(x4 ^ 2) + (x1 + x2 + x3 + x4) ^ 2 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)     I(x2^2)   I(x1^2)    I(x3^2)   I(x4^2)        x1         x2
1           1 0.852840999 0.4594796 0.02486059 0.9633761 0.6778492 0.92349391
2           1 0.260913733 0.5024415 0.05452236 0.3049895 0.7088311 0.51079715
3           1 0.001245751 0.2788185 0.30666389 0.0856378 0.5280327 0.03529519
4           1 0.880349268 0.1956617 0.20488059 0.5026914 0.4423366 0.93826929
5           1 0.143883695 0.0520869 0.30907835 0.5300504 0.2282255 0.37932004
6           1 0.480475149 0.5738224 0.03756428 0.5562866 0.7575107 0.69316315
         x3        x4         x5          x6      x1:x2     x1:x3     x1:x4
1 0.1576724 0.9815172 0.77496216 0.282501746 0.62598964 0.1068781 0.6653207
2 0.2335002 0.5522585 0.22558065 0.005148194 0.36206889 0.1655122 0.3914580
3 0.5537724 0.2926394 0.67773583 0.236637721 0.01863702 0.2924099 0.1545232
4 0.4526374 0.7090073 0.84701227 0.157596670 0.41503088 0.2002181 0.3136199
5 0.5559482 0.7280456 0.04624094 0.923890930 0.08657052 0.1268816 0.1661586
6 0.1938151 0.7458463 0.29209639 0.149164932 0.52507848 0.1468170 0.5649865
      x2:x3      x2:x4     x3:x4
1 0.1456095 0.90642520 0.1547582
2 0.1192713 0.28209208 0.1289525
3 0.0195455 0.01032876 0.1620556
4 0.4246957 0.66523981 0.3209232
5 0.2108823 0.27616229 0.4047556
6 0.1343455 0.51699314 0.1445562
#+end_example
*** [2018-07-17 Tue]
**** Plotting atax0 O2 Uniform + D-Optimal Sampling (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/search_space.csv")

data <- data[data$correct_result == "True", ]
str(data)

O2_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_K == 1, ]
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	20999 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.105 0.129 0.162 0.213 0.106 ...
 $ RT_K                        : int  8 1 1 1 8 1 8 32 8 1 ...
 $ T1_I                        : int  1 32 32 16 32 128 64 256 128 128 ...
 $ T1_J                        : int  128 64 64 64 32 512 32 32 32 32 ...
 $ T1_K                        : int  128 1 64 64 64 64 512 64 256 64 ...
 $ U_K                         : int  1 28 1 5 1 30 6 26 1 1 ...
 $ U_J                         : int  13 1 28 7 10 1 1 5 10 11 ...
 $ U_I                         : int  21 7 23 1 26 20 25 1 5 13 ...
 $ U1_I                        : int  5 23 27 26 5 15 9 2 25 24 ...
 $ T2_K                        : int  512 512 256 1024 2048 512 2048 2048 1024 2048 ...
 $ T2_J                        : int  1024 256 128 512 128 1024 1 512 256 1024 ...
 $ T2_I                        : int  128 1 512 256 1 128 128 2048 128 512 ...
 $ mean_confidence_interval_inf: num  0.105 0.128 0.161 0.211 0.105 ...
 $ cost_std                    : num  0.000172 0.001385 0.001781 0.002961 0.000509 ...
 $ RT_I                        : int  8 1 1 32 8 1 1 1 8 1 ...
 $ cost_mean                   : num  0.105 0.128 0.161 0.212 0.106 ...
 $ RT_J                        : int  1 8 8 1 1 32 1 1 1 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean)) +
    geom_vline(aes(xintercept = cost_mean), O2_baseline, color = "black", linetype = 2) +
    geom_text(aes(x = cost_mean + 0.03, y = 6000, label = "-O2 baseline"), data = O2_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dLTdMe/figurej8zchR.png]]
*** [2018-07-18 Wed]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$ACOPY_x == "False" & data$ACOPY_y == "False" &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1600   26
'data.frame':	1443 obs. of  26 variables:
 $ id                          : int  1 2 4 5 6 7 8 9 11 12 ...
 $ T2_K                        : int  2048 64 2048 256 1 128 64 512 256 1 ...
 $ T2_J                        : int  128 128 2048 256 1 256 1 2048 1 512 ...
 $ T2_I                        : int  512 1 256 512 1024 128 128 256 1 2048 ...
 $ mean_confidence_interval_inf: num  0.183 0.101 0.218 0.113 0.183 ...
 $ RT_K                        : int  1 1 32 8 8 8 1 1 1 1 ...
 $ T1_I                        : int  512 512 64 256 256 32 16 64 128 256 ...
 $ T1_J                        : int  128 32 16 16 64 128 64 128 256 32 ...
 $ T1_K                        : int  512 16 64 16 256 128 16 16 1 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 2 1 2 2 1 1 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 1 2 1 1 1 2 2 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 1 1 2 1 1 2 1 2 ...
 $ ACOPY_x                     : Factor w/ 2 levels "False","True": 1 1 1 1 1 2 2 1 1 1 ...
 $ ACOPY_y                     : Factor w/ 2 levels "False","True": 2 1 1 1 2 2 2 2 1 1 ...
 $ U1_I                        : int  4 2 11 2 23 11 10 22 15 2 ...
 $ RT_J                        : int  32 1 1 8 8 1 1 1 1 8 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.183 0.104 0.224 0.117 0.189 ...
 $ cost_std                    : num  0.000204 0.00477 0.008259 0.006777 0.008412 ...
 $ cost_mean                   : num  0.183 0.102 0.221 0.115 0.186 ...
 $ U_J                         : int  8 1 1 1 22 1 1 20 8 4 ...
 $ U_I                         : int  22 18 28 7 1 10 28 1 1 18 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ U_K                         : int  1 28 11 1 16 25 14 24 9 1 ...
 $ RT_I                        : int  1 8 1 1 1 8 8 8 32 8 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, 0.6), ylim = c(0, 180)) +
    geom_text(aes(x = cost_mean + 0.04, y = 170, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dwa8la/figure2iRWvN.png]]
*** [2018-07-19 Thu]
**** Plotting bigckernel O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/bicgkernel/xeon_e5_2630_v2/src1_O3_binary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 &
                    data$T2_I == 1 & data$T2_J == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$RT_I == 1 & data$RT_J == 1 &
                    data$SCR == "False" & data$OMP == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 3001   21
'data.frame':	2980 obs. of  21 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ mean_confidence_interval_sup: num  0.0847 0.0308 0.086 0.3751 0.2981 ...
 $ mean_confidence_interval_inf: num  0.0819 0.0278 0.0798 0.3741 0.2933 ...
 $ T1_I                        : int  16 1 1 1 128 32 64 256 1 1 ...
 $ T1_J                        : int  128 1 256 1 128 32 128 64 64 128 ...
 $ cost_mean                   : num  0.0833 0.0293 0.0829 0.3746 0.2957 ...
 $ U_J                         : int  1 1 1 4 6 1 1 7 4 24 ...
 $ U_I                         : int  5 16 24 1 1 14 12 1 1 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ T2_I                        : int  2048 128 256 2048 2048 256 1 2048 2048 256 ...
 $ cost_std                    : num  0.00432 0.00452 0.00938 0.00145 0.00713 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ T2_J                        : int  1 1 512 128 128 128 1024 2048 1024 2048 ...
 $ U1_I                        : int  5 28 18 16 5 5 30 26 6 2 ...
 $ OMP                         : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 2 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 2 1 2 2 1 1 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 2 2 2 1 1 ...
 $ RT_I                        : int  8 8 1 32 1 32 32 32 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 1 1 1 1 2 2 1 1 ...
 $ RT_J                        : int  1 1 32 1 32 1 1 1 32 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 300)) +
    geom_text(aes(x = cost_mean + 0.04, y = 100, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-s376G4/figureHIEfVL.png]]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E3 1505M v6)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1505M_v6/atax1_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$ACOPY_x == "False" & data$ACOPY_y == "False" &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 211  26
'data.frame':	189 obs. of  26 variables:
 $ id                          : int  1 2 3 5 6 7 8 9 11 12 ...
 $ T2_K                        : int  1 512 1 512 128 256 1024 1024 128 1 ...
 $ T2_J                        : int  128 128 2048 2048 2048 1 128 1024 1 1024 ...
 $ T2_I                        : int  64 2048 256 64 1024 128 512 512 512 1024 ...
 $ mean_confidence_interval_inf: num  0.787 0.735 0.254 0.402 0.614 ...
 $ RT_K                        : int  1 8 8 1 1 8 32 1 1 1 ...
 $ T1_I                        : int  16 256 128 16 1 16 128 64 128 1 ...
 $ T1_J                        : int  1 16 32 512 512 128 32 512 1 256 ...
 $ T1_K                        : int  32 128 256 128 16 64 16 16 1 16 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 2 2 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 2 1 2 1 2 1 1 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 1 2 2 ...
 $ ACOPY_x                     : Factor w/ 2 levels "False","True": 2 1 1 1 1 2 2 1 2 1 ...
 $ ACOPY_y                     : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 1 2 1 ...
 $ U1_I                        : int  7 28 9 27 13 29 19 12 7 12 ...
 $ RT_J                        : int  1 8 1 1 32 8 1 8 1 32 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.807 0.744 0.255 0.403 0.628 ...
 $ cost_std                    : num  0.02843 0.0131 0.00178 0.00149 0.02003 ...
 $ cost_mean                   : num  0.797 0.74 0.255 0.403 0.621 ...
 $ U_J                         : int  12 16 1 17 20 1 21 1 7 2 ...
 $ U_I                         : int  1 1 11 15 24 17 1 22 1 1 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ U_K                         : int  9 11 30 1 1 28 19 18 12 3 ...
 $ RT_I                        : int  1 1 8 8 1 1 1 1 32 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 12)) +
    geom_text(aes(x = cost_mean + 0.1, y = 12, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-OIl2Ho/figureW5OgZc.png]]
**** Paper Review: GASPOW: An Autotunable System for Accelerating General Sliding-Window Streaming Operators on GPU
***** Overall merit
 1. Reject
 2. Weak reject
 * 3. Weak accept
 4. Accept
 5. Strong accept
***** Reviewer expertise
 1. No familiarity
 * 2. Some familiarity
 3. Knowledgeable
 4. Expert
***** Paper short summary
The paper presents a system for offloading stream processing workloads to GPUs.
The system applies the concept of distinct sliding windows that can be run in
parallel in GPUs. It uses online learning and a raindrop algorithm to optimize
the concurrency level and the batch length that should be used in each
application. Experiments with two queries are presented, showing significant
improvements in comparison with execution on CPUs.
***** Comments for PC (hidden from authors) - optional
***** Novelty
 1. Published before or openly commercialized
 * 2. Incremental improvement
 3. New contribution
 4. Surprisingly new contribution
***** Strengths
- Presentation: Clear tables and figures
- Application of the parallel windows inside sliding batches approach on GPUs
- Provides open-source code
***** Weaknesses
- Text could be made clearer
- Evaluation based on only 2 applications and 1 architecture
- Proposed autotuning strategy was not compared with other strategies
***** Well situated regarding previous work
 1. Low
 * 2. Medium
 3. High
***** Detailed comments for author:
****** Disclaimer
Since this was a double blind review, it was not possible to access the source
code used in the experiments or some of the references to previous work.
****** Section 5: Evaluation
It would be interesting to present more explanation on the selection of the
queries for the evaluation, and why only two different queries where selected.
Evaluation on more than a single GPU and CPU would also help support the
conclusions of the paper.
******* Section 5.2: Mechanism Evaluation
The autotuning search space for this problem has two parameters and 150 possible
combinations. This is a very small search space in relation to common autotuning
problems in other domains. The entire search spaces of two scenarios of query 1
are represented in the heatmaps of Figure 11.

As far as I understood there is no diference in result throughput between points
in and out of the Pareto frontier, at least with respect to the number of
replicas. If this is the case, it would be interesting to compare the results of
the active learning approach with much simpler autotuning approaches such as
random uniform sampling or latin hypersquare sampling. If most query search
spaces are like the ones presented in Figure 11, such simple approaches should
have interesting results while consuming few resources.
******* Section 5.3: Autotuning Evaluation
In the last paragraph of this subsection, it is claimed that "[in] almost all
cases the raindrop-based strategy ends up in an optimal configuration on the
Pareto frontier. Only in few cases it picks a point not on the frontier but
still very close to it.".

According to the table, only in 5 out of the 12 scenarios the configuration
found by the raindrop strategy was at distance zero from the Pareto Frontier.
Since the space of configurations is discrete, it may be the case that distances
smaller than 1 could be as close as possible, but if that is the case more
clarification on the meaning of this distance should be provided.
****** Text Clarity
The paper could be made clearer by further text revision. An example, which
appears in the last paragraph of subsubsection "Maximum sustainable input rate"
of subsection 5.1, is the following:

Original:   "The CPU version has a latency lower of some orders of magnitude [..]"
Suggestion: "The CPU version has latency lower by 3 orders of magnitude [...]"

****** Style and Formatting
The following are some style and formatting suggestions:
- Abbreviations such as "a.k.a", used in the first page, might not be
  immediately understood by non-native english speakers, and can easily be
  replaced by clearer expressions. In the first page, for example: "[...]" has
  been applied to sliding-window queries [8] (a.k.a. windowed queries) that
  [...]" could be rewritten as "[...] has been applied to sliding-window, or
  windowed, queries [8] that [...]".
- The intent of other abbreviations such as "e.g." and "i.e.", widely used in
  the text, can be more clearly conveyed by replacing them by "for example" and
  "that is".
*** [2018-07-20 Fri]
**** Plotting stencil3d O3 No Binary Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3_nobinary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 &
                    data$RT1_K == 1, ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 840  20
'data.frame':	838 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 12 ...
 $ RT1_K                       : int  1 8 1 8 8 1 1 32 1 1 ...
 $ RT1_J                       : int  8 1 1 1 8 1 8 1 8 1 ...
 $ T1_Ja                       : int  64 256 1024 1024 1024 128 1 64 2048 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ mean_confidence_interval_inf: num  0.1117 0.1941 0.0965 0.2412 0.1994 ...
 $ T1_I                        : int  1 512 16 256 512 1 128 256 1 32 ...
 $ T1_J                        : int  1 32 512 512 1 1 32 32 32 64 ...
 $ T1_Ia                       : int  64 2048 128 2048 512 256 512 256 512 128 ...
 $ cost_mean                   : num  0.1144 0.1959 0.0996 0.2464 0.1999 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ T1_K                        : int  32 32 64 64 16 1 1 512 128 128 ...
 $ T1_Ka                       : int  128 256 512 64 256 2048 256 512 2048 2048 ...
 $ U1_K                        : int  28 7 1 23 7 22 19 9 3 22 ...
 $ U1_J                        : int  2 1 13 1 16 29 20 1 1 1 ...
 $ U1_I                        : int  1 1 27 19 1 1 1 17 13 18 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ RT1_I                       : int  1 8 1 8 1 32 8 1 8 32 ...
 $ mean_confidence_interval_sup: num  0.117 0.198 0.103 0.252 0.201 ...
 $ cost_std                    : num  0.00828 0.00525 0.00948 0.01567 0.00174 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 230)) +
    geom_text(aes(x = cost_mean + 0.02, y = 230, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dwa8la/figure9grgau.png]]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O3_nobinary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1, ]

str(O3_baseline$cost_mean)

best <- data[data$cost_mean == min(data$cost_mean), ]
str(best$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 63289    20
'data.frame':	63009 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.225 0.11 0.188 0.251 0.153 ...
 $ RT_K                        : int  8 1 1 1 32 8 1 8 1 8 ...
 $ T1_I                        : int  64 128 16 64 1 1 512 32 1 512 ...
 $ T1_J                        : int  1 64 16 16 512 32 32 32 64 1 ...
 $ T1_K                        : int  32 1 16 64 512 128 256 512 16 1 ...
 $ U_K                         : int  30 1 29 1 5 1 13 30 17 1 ...
 $ U_J                         : int  1 22 1 14 16 28 1 4 21 28 ...
 $ U_I                         : int  30 11 22 16 1 23 30 1 1 30 ...
 $ U1_I                        : int  6 14 4 29 25 30 18 29 2 29 ...
 $ T2_K                        : int  256 1024 2048 64 1 1 1024 1 1 1 ...
 $ T2_J                        : int  1024 512 128 1 1 2048 1 256 2048 1 ...
 $ T2_I                        : int  64 256 2048 1 128 64 1 512 512 512 ...
 $ mean_confidence_interval_inf: num  0.224 0.109 0.188 0.251 0.153 ...
 $ cost_std                    : num  1.36e-03 1.35e-04 6.58e-05 4.17e-04 5.96e-05 ...
 $ RT_I                        : int  1 1 1 32 1 8 8 8 32 1 ...
 $ cost_mean                   : num  0.225 0.11 0.188 0.251 0.153 ...
 $ RT_J                        : int  8 1 8 1 1 1 8 1 1 8 ...
 num 0.142
 num 0.0802
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 380)) +
    geom_text(aes(x = cost_mean + 0.1, y = 12, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-InAPVd/figureKGd8Rb.png]]
**** Plotting dgem1 O3 Random Uniform Sample (Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e3_1230_v2/dgemv1/search_space/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

str(O3_baseline$cost_mean)
best <- data[data$cost_mean == min(data$cost_mean), ]

str(best$cost_mean)
#+END_SRC

#+RESULTS:
: [1] 3001   57
:  num 0.0929
:  num 0.0651

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 620)) +
    geom_text(aes(x = cost_mean - 0.02, y = 620, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-uAeVrZ/figureqVYiYt.png]]
*** [2018-07-23 Mon]
**** Plotting stencil3d O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 2458   37
'data.frame':	2454 obs. of  37 variables:
 $ id                          : int  2 3 4 5 6 7 8 9 10 11 ...
 $ RT1_K                       : int  8 1 32 1 1 1 1 8 1 32 ...
 $ RT1_J                       : int  1 1 1 32 8 1 1 1 32 1 ...
 $ T1_Ja                       : int  1 1024 1024 64 1024 128 1024 1024 256 2048 ...
 $ RT1_I                       : int  1 32 1 1 8 8 32 1 1 1 ...
 $ T2_K                        : int  16 512 1 64 512 64 256 128 16 64 ...
 $ T2_J                        : int  16 16 64 64 64 512 32 512 512 256 ...
 $ T2_I                        : int  16 32 16 256 32 64 64 512 32 1 ...
 $ T2_Ja                       : int  128 2048 256 256 256 2048 1024 2048 1 256 ...
 $ U2_I                        : int  1 1 1 23 19 28 3 30 27 21 ...
 $ mean_confidence_interval_inf: num  0.0778 0.3177 0.1435 0.3858 0.2973 ...
 $ T1_I                        : int  32 512 1 128 64 1 64 64 32 128 ...
 $ T1_J                        : int  64 512 64 32 64 16 16 64 1 128 ...
 $ T1_K                        : int  128 256 16 128 1 512 16 256 16 64 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ RT2_J                       : int  1 32 32 8 32 1 1 32 1 8 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 2 2 1 2 2 2 2 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 2 1 1 1 1 1 2 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 2 2 1 2 1 1 1 1 ...
 $ T1_Ia                       : int  1 1 1024 1 128 1024 256 256 1024 1 ...
 $ T1_Ka                       : int  1024 1024 1024 128 1 1024 1024 1 512 128 ...
 $ OMP1                        : Factor w/ 2 levels "False","True": 2 2 2 2 1 2 2 1 2 1 ...
 $ U1_K                        : int  1 21 1 1 2 3 1 15 18 15 ...
 $ U1_J                        : int  15 20 1 28 30 1 2 1 30 1 ...
 $ U1_I                        : int  28 1 3 9 1 24 21 29 1 13 ...
 $ T2_Ka                       : int  1 1 1 64 1 1024 256 2048 2048 128 ...
 $ RT2_I                       : int  1 1 1 1 1 1 1 1 8 8 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ T2_Ia                       : int  128 128 128 1024 128 512 512 1 2048 256 ...
 $ U2_J                        : int  7 18 23 16 25 1 1 1 27 17 ...
 $ U2_K                        : int  26 15 8 1 1 22 3 29 1 1 ...
 $ cost_mean                   : num  0.0799 0.3211 0.1464 0.3899 0.3016 ...
 $ RT2_K                       : int  32 1 1 8 1 1 32 1 8 1 ...
 $ OMP2                        : Factor w/ 2 levels "False","True": 2 1 1 2 1 1 2 2 2 2 ...
 $ cost_std                    : num  0.0064 0.01032 0.00867 0.01229 0.01317 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ mean_confidence_interval_sup: num  0.082 0.325 0.149 0.394 0.306 ...
 num 0.0736
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 120)) +
    geom_text(aes(x = cost_mean + 0.05, y = 123, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dwa8la/figureRaW8Tf.png]]
*** [2018-07-24 Tue]
**** Plotting dgem1 O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/dgemv3_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

str(O3_baseline$cost_mean)
best <- data[data$cost_mean == min(data$cost_mean), ]

str(best$cost_mean)
str(best)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 3915   57
 num 0.114
 num 0.0728
'data.frame':	1 obs. of  57 variables:
 $ id                          : int 887
 $ T5_I                        : int 64
 $ T1_Ja                       : int 2048
 $ technique                   : Factor w/ 2 levels "DLMT","RS": 1
 $ RT1_J                       : int 32
 $ RT7_I                       : int 8
 $ VEC10                       : Factor w/ 2 levels "False","True": 2
 $ RT7_J                       : int 1
 $ T5_J                        : int 512
 $ VEC9                        : Factor w/ 2 levels "False","True": 1
 $ T3_J                        : int 128
 $ T3_I                        : int 512
 $ RT1_I                       : int 1
 $ VEC6                        : Factor w/ 2 levels "False","True": 1
 $ mean_confidence_interval_inf: num 0.0728
 $ T1_I                        : int 32
 $ T1_J                        : int 64
 $ T5_Ia                       : int 2048
 $ T3_Ja                       : int 128
 $ VEC7                        : Factor w/ 2 levels "False","True": 2
 $ VEC4                        : Factor w/ 2 levels "False","True": 1
 $ VEC5                        : Factor w/ 2 levels "False","True": 2
 $ T7_J                        : int 1
 $ VEC3                        : Factor w/ 2 levels "False","True": 1
 $ T7_I                        : int 1
 $ VEC2                        : Factor w/ 2 levels "False","True": 1
 $ T7_Ia                       : int 2048
 $ VEC8                        : Factor w/ 2 levels "False","True": 2
 $ U10_I                       : int 16
 $ U6_I                        : int 16
 $ T1_Ia                       : int 1
 $ VEC1                        : Factor w/ 2 levels "False","True": 1
 $ U5_J                        : int 1
 $ U5_I                        : int 4
 $ U1_J                        : int 1
 $ U1_I                        : int 4
 $ U8_I                        : int 16
 $ U7_I                        : int 1
 $ U7_J                        : int 1
 $ cost_std                    : num 8.57e-05
 $ runs                        : int 35
 $ mean_confidence_interval_sup: num 0.0728
 $ U2_I                        : int 4
 $ cost_mean                   : num 0.0728
 $ U9_I                        : int 16
 $ RT3_I                       : int 8
 $ RT3_J                       : int 8
 $ correct_result              : Factor w/ 1 level "True": 1
 $ T7_Ja                       : int 256
 $ T5_Ja                       : int 512
 $ U4_I                        : int 16
 $ T3_Ia                       : int 1
 $ SCR                         : Factor w/ 2 levels "False","True": 2
 $ U3_I                        : int 1
 $ RT5_J                       : int 1
 $ RT5_I                       : int 1
 $ U3_J                        : int 1
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data[data$technique == "RS", ]) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 1050)) +
    geom_text(aes(x = cost_mean - 0.02, y = 200, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AiL4Pb/figured8ABQO.png]]
*** [2018-07-25 Wed]
**** Plotting gemver O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/gemver/xeon_e5_2630_v2/gemver_O3/search_space.csv")

data <- data[data$correct_result == "True", ]

str(data)

O3_baseline <- data[data$T2_I == 1 & data$T2_J == 1 & data$T2_Ia == 1 &
                    data$T2_Ja == 1 & data$T4_I == 1 & data$T4_J == 1 &
                    data$T4_Ia == 1 & data$T4_Ja == 1 &
                    data$U1_I == 1 & data$U2_I == 1 & data$U2_J == 1 &
                    data$U3_I == 1 & data$U4_I == 1 & data$U4_J == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT4_I == 1 &
                    data$RT4_J == 1 &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$SCR == "False" &
                    data$OMP == "False", ]

str(O3_baseline$cost_mean)
best <- data[data$cost_mean == min(data$cost_mean), ]

str(best$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	1917 obs. of  32 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  256 64 256 512 1 64 1 256 512 32 ...
 $ T2_I                        : int  512 512 16 32 256 128 16 1 32 32 ...
 $ T4_I                        : int  1 128 1 1 64 512 32 256 32 128 ...
 $ T4_J                        : int  64 128 256 16 512 32 512 32 16 256 ...
 $ mean_confidence_interval_inf: num  0.363 0.49 0.255 0.117 1.229 ...
 $ T2_Ja                       : int  256 256 256 1024 1 1024 64 1024 1024 1 ...
 $ T2_Ia                       : int  1024 1 512 512 1 1024 128 256 1 128 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ T4_Ia                       : int  256 128 512 64 1 512 1 2048 1 1024 ...
 $ VEC4                        : Factor w/ 2 levels "False","True": 2 2 2 2 1 1 2 2 1 2 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 2 2 1 2 2 1 1 2 1 1 ...
 $ VEC3                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 1 2 1 2 2 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 1 2 1 2 1 1 1 1 2 ...
 $ U4_J                        : int  1 15 1 2 3 16 5 12 23 1 ...
 $ U2_J                        : int  20 24 9 13 24 11 2 19 9 5 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 2 2 1 2 1 2 1 2 ...
 $ U4_I                        : int  3 1 22 1 1 1 1 1 1 9 ...
 $ U1_I                        : int  16 23 2 22 22 7 9 22 10 7 ...
 $ RT2_I                       : int  8 1 32 1 32 1 1 8 8 8 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ U2_I                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ RT2_J                       : int  8 32 1 8 1 8 8 8 1 8 ...
 $ cost_mean                   : num  0.364 0.491 0.255 0.122 1.23 ...
 $ cost_std                    : num  0.00371 0.00271 0.00244 0.01616 0.00498 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ OMP                         : Factor w/ 2 levels "False","True": 1 1 1 2 1 2 1 2 1 1 ...
 $ RT4_J                       : int  8 8 8 1 8 8 1 1 1 8 ...
 $ RT4_I                       : int  1 1 8 8 1 8 32 32 8 1 ...
 $ T4_Ja                       : int  2048 512 512 1024 1024 2048 2048 1 1 2048 ...
 $ mean_confidence_interval_sup: num  0.365 0.492 0.256 0.128 1.232 ...
 $ U3_I                        : int  13 21 15 20 1 21 17 7 3 7 ...
 num 0.356
 num 0.108
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.1) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 500)) +
    geom_text(aes(x = cost_mean + 0.1, y = 130, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-s376G4/figureIcyjLa.png]]
**** Plotting stencil3d O3 Random Uniform Sample (Xeon E5 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e3_1230_v2/stencil_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 &
                    data$RT1_K == 1, ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1925   37
'data.frame':	1919 obs. of  37 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT1_K                       : int  32 1 1 1 1 32 1 32 8 1 ...
 $ RT1_J                       : int  1 1 8 1 1 1 8 1 8 1 ...
 $ T1_Ja                       : int  1 128 2048 1024 2048 256 1 1024 64 256 ...
 $ RT1_I                       : int  1 8 8 32 1 1 1 1 1 32 ...
 $ T2_K                        : int  1 128 32 128 128 256 256 32 32 16 ...
 $ T2_J                        : int  16 32 16 16 1 1 32 1 256 16 ...
 $ T2_I                        : int  32 1 1 256 16 256 64 1 64 64 ...
 $ T2_Ja                       : int  2048 512 128 128 2048 1024 2048 256 1 512 ...
 $ U2_I                        : int  6 2 1 21 23 1 8 1 6 1 ...
 $ mean_confidence_interval_inf: num  0.1 0.1999 0.1828 0.0819 0.1435 ...
 $ T1_I                        : int  32 128 64 16 64 32 128 512 128 1 ...
 $ T1_J                        : int  128 128 64 1 512 64 512 16 32 64 ...
 $ T1_K                        : int  32 512 32 128 128 512 64 16 16 256 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ RT2_J                       : int  1 8 8 32 8 1 8 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 2 1 2 2 2 1 1 2 1 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 2 1 1 2 2 1 1 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 1 2 1 2 2 2 2 1 2 2 ...
 $ T1_Ia                       : int  128 512 64 1024 256 512 256 1 1024 64 ...
 $ T1_Ka                       : int  64 2048 64 2048 256 2048 1024 256 1 2048 ...
 $ OMP1                        : Factor w/ 2 levels "False","True": 1 1 1 2 1 1 2 1 1 1 ...
 $ U1_K                        : int  1 18 6 2 10 6 20 1 1 27 ...
 $ U1_J                        : int  5 1 16 1 5 1 24 28 23 16 ...
 $ U1_I                        : int  17 26 1 17 1 6 1 16 22 1 ...
 $ T2_Ka                       : int  64 512 128 1 1 512 1 128 512 2048 ...
 $ RT2_I                       : int  1 1 1 1 8 8 8 1 8 32 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ T2_Ia                       : int  64 2048 1 512 64 256 2048 256 1024 512 ...
 $ U2_J                        : int  18 15 26 8 1 5 1 3 13 1 ...
 $ U2_K                        : int  1 1 19 1 8 22 8 15 1 28 ...
 $ cost_mean                   : num  0.1025 0.2018 0.1842 0.0825 0.146 ...
 $ RT2_K                       : int  32 1 1 1 1 1 1 8 1 1 ...
 $ OMP2                        : Factor w/ 2 levels "False","True": 1 2 1 2 1 2 2 1 1 2 ...
 $ cost_std                    : num  0.00741 0.00585 0.00402 0.00197 0.00774 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ mean_confidence_interval_sup: num  0.1049 0.2038 0.1855 0.0832 0.1486 ...
 num 0.0698
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 150)) +
    geom_text(aes(x = cost_mean + 0.05, y = 150, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-C2GnlH/figurexUeHjC.png]]
*** [2018-07-26 Thu]
**** Running DLMT for stencil3d O3 (Xeon E5 1230 v2)
- Started running DLMT for stencil3d. Added a timeout of 20 minutes in the program
  running command because some applications take 1h+ to compile and still don't
  finish. Timeout length should be exposed as a configuration parameter.

- The initial model used is linear for all 2-level binary factors and for the second
  and third levels of the parameters controlling loop unrolling, tiling, etc.

- The search space for this problem has "few" valid configurations, so sampling takes
  a very long time. I opted to add more experiments and use a smaller sample
  size. This seemed to improve the D-Efficiency of the designs, but this can
  simply be a result of having more experiments.
*** [2018-07-30 Mon]
**** Plotting Orio Experiments Data (dgemv3, O3, Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e3_1230_v2/dgemv1/experiments/results.csv")

data[data$technique == "DLMT" & data$points <= 70, "experiment"] = "DLMT_small"
data[data$technique == "RS" & data$points <= 70, "experiment"] = "RS_small"
data[data$technique == "DLMT" & data$points > 70, "experiment"] = "DLMT_large"
data[data$technique == "RS" & data$points > 70, "experiment"] = "RS_large"

data_mean <- ddply(data, .(experiment), summarize, mean = mean(speedup))
data_median <- ddply(data, .(experiment), summarize, median = median(speedup))
data_error <- ddply(data, .(experiment), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(experiment), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(experiment ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 4)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 3, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 3, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AiL4Pb/figureHQnKL8.png]]
** August
*** [2018-08-22 Wed]
**** Paper Review for WSCAD: Performance and Energy Efficiency Evaluation for HPC
***** Grades
- Originality: 2
- Relevance: 4
- Technical Merit: 2
- Text: 2
- References:
- Evaluation: 2
- Verdict: 2
- Reviewer Expertise: 4

***** Comments to Authors
****** Paper Summary
The paper presents an evaluation of performance and power consumption of five
experiments, two of which consist of real-world applications, in different
architecture setups. With the results, the paper suggests an architecture setup
for each experiment depending on the optimization objective.

****** Section 4: Results
Assuming that the data presented consists of means of several repetitions, the
number of repetitions of each experiment and the variance of repeated executions
are not discussed in the paper. Other values such as the standard error and
standard deviation are also not discusssed. Without this crucial information
about the experiments and results it is difficult to support any conclusion
based on the data presented.

I recommend that the relevant statistics are added to the plots, and that a
discussion regarding their values is added to the paper. This information will
help sustain the conclusions of the paper and complete the presentation of the
results.
****** Writing Comments and Suggestions
The text would benefit from a revision in all sections. The current text is hard
to read and to understand at certain points. Some sentences to improve and
improvement suggestions are listed below, although I still recommend a thorough
revision.

******* Abstract

1. Original sentence:

"Those experiments, executed on a range of heterogeneous architectures and
programming models, contributes by studying how such applications execute using
metrics that take into account not just performance, but also the power
consumption for an energy efficient HPC system."

Suggestion:

Here, "contributes" should agree with "Those experiments". It is also hard
to extract the meaning of the sentence, perhaps because it is trying to convey
too much information. A possible rewrite is:

"Our results enable analysing the performance and power consumption of the
selected applications, and help to achieve energy efficiency in HPC systems."

******* Introduction
1. Original:

"Scientific computing usually requires huge processing power resources' to
perform large scale experiments and run simulations in a reasonable time."

Suggestion:

Remove the typo "'".

2. Original:

"These demands have been addressed by High Performance Computing (HPC), which
makes them invaluable tool for modern scientific research."

Suggestion:

It is not clear which are those demands, or how they are made an invaluable
tool. Perhaps more explanations could be given before, in separate sentences.

3. Original:

"To this end, it is important to understand the relationships among performance
(in this work we consider as time to solution and scalability), power
consumption, and the characteristics of each scientific application."

Suggestion:

Replace "among" with "between". It is not clear what is the "end" or "objective"
which is pursued. An introductory sentence to the paragraph could be added.

******* Related Work
1. Original:

"A lot of works are focused on alternative and heterogeneous architectures [...]"

Suggestion:

Avoid using "a lot of", use instead expressions such as "many", or "multiple".

2. Original:

"The work [Okina et al. 2016] discusses [...]"
"In [Castro et al. 2016] is presented [...]"
(and others)

Suggestion:

Avoid using the reference marker as part of the text, as shown in the examples
above. This style choice does not work in all reference systems, and the
sentence will lose its meaning if the reference system changes to a superscript,
for example.

3. Original:

"[...] particularly those representing the behavior of physical phenomenon such
as seismic activity."

Suggestion:

"[...] particularly those representing the behavior of physical phenomena such
as seismic activity."

or

"[...] particularly those representing the behavior of a physical phenomenon such
as seismic activity."

******* Experimental Setup (3.3)
1. Original:

"The form to collect the data from the internal sensors and the hardware param-
eters, for each architecture, differ from each other."

Suggestion:

This sentence is hard to read. Changing the ordering of terms can help, such as:

"Collecting data from sensors and hardware parameters is done in a different way
in each architecture."

2. Original:

"For this reason, it was developed a [..] tool [...]"
"[...] it was developed a module [...]"

Suggestion:

Replace "it was developed a X" by "X was developed", or "we developed X".

3.Original:

"These power and the execution time are used [..]"

Suggestion:

Replace "These" by "The", for example.

******* Results
1. Original:

"The hatched area displays the EC of the application while the solid color of
the system."

Suggestion:

"The hatched and solid areas displays the EC of the application and the system, respectively."

2. Original:

"This is in stark contrast when [...]"

Suggestion:

"This is in stark contrast to when [..]"
"This is in stark contrast to the case when [..]"

Or, to avoid imprecise expressions:

"This is the opposite of when [...]"
"This is the opposite of the case when [...]"

******* Final Considerations and Future Works
1. Original:

"[...] the execution of scientific applications on a myriad of environments [...]"
"[...] executed on a range of heterogeneous architectures [...]"

Suggestion:

Avoid imprecise or grandiose terms such as "range" and "myriad". Always use an
exact number when possible. When an exact number can't be obtained, use "many"
or "multiple".
*** [2018-08-23 Thu]
**** Topics for Meeting with Arnaud
***** Evaluation letter for CAPES and visa Extension
- Alfredo, Brice and Jean-Marc should sign
***** Recent Autotuning "Review" Paper :ATTACH:
:PROPERTIES:
:Attachments: balaprakash2018autotuning.pdf
:ID:       b9173855-f4a8-42df-8ce4-ce4cba2d9935
:END:
- Balaprakash (Orio, SPAPT), Dongarra, Gamblin, Hall, Hollingsworth,
  Norris (Orio, SPAPT), Vuduc
***** Current SPAPT Experiments
- Tests with other search techniques?
****** Uniform Random Samples of Search Spaces
- Sampled spaces for some problems do not seem good targets for
  our approach, since most configurations are better than the default
****** Original SPAPT Paper
- Uses mean of 35 executions in evaluations
- I was not able to reproduce the "densities" of runtime
  for the problems in the paper
****** O3 Usage
- My initial assumption was that they used O3 with all parameters set to zero
  as the initial point, and then kept using O3 for all other points measured
- I also assumed that removing O3 and only using code transformations would
  always decrease performance
- Additionally, initial experiments with O2, O1 and O0 showed similar sample
  distributions as in the O3 case, but this was not tested for all problems
****** Using Designs for Autotuning
******* Choosing the Initial Model
- Implemented a way to specify model terms in the Orio problem specification
- It is possible to add interactions cubic and quadratic terms
- In my experiments I am only using linear and quadratic terms so far, without interactions
- How to select the best model for each application?
- Adding more terms decreases D-Efficiency and increases the number of experiments
******* Pruning the Model
- In some cases, points sampled during the design ended up
  having better performance than the best predicted point after fitting
- When that happened, I fixed the most "relevant" factors to the
  value of the best point on the design, instead of to the value of the best
  predicted point
- Removed the second sampling for the prediction using the fitted model,
  the prediction is now made on the initial uniformly sampled set
******* Finding Good Points
- Usually the first evaluated design has good results, which is usually not improved
  upon too much by further iterations
- We are more interested in finding a good point than in finding the
  perfect model for the problem, so good solutions should be prioritized
  against the solutions predicted by the model
***** IPDPS Submission
- http://www.ipdps.org/ipdps2019/2019-organization.html
- http://iwapt.org/2019/
- Invite Steven
- I believe we can write a paper for IPDPS 19, or to
  the autotuning workshop that usually happens in it
- As a strong motivation, present results obtained in Steven's problem
- Finish experiments with Orio and SPAPT
- Discuss the performance of our approach in different problems
*** [2018-08-24 Fri]
**** Plotting Orio Experiments Data (dgemv3, O3, Xeon E5 2630 v2, Fixing Factors with Design Best)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/results.csv")

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 2)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dLTdMe/figureFUVqNF.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/search_space.csv")

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[2, "technique"] <- "RS"
best <- data[data$cost_mean == min(data$cost_mean), ]
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 1050)) +
    geom_text(aes(x = cost_mean - 0.02, y = 200, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dLTdMe/figurekxWgId.png]]
*** [2018-08-27 Mon]
**** Plotting Orio Experiments Data (dgemv3, O3, Xeon E5 2630 v2, Fixing Factors with Predicted Best)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/results.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_predicted/results.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 2)) +
    #geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max + 0.05, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    #geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-JtRBod/figure24NU56.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/search_space.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_predicted/search_space.csv")
data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

dim(data)
data <- data[data$correct_result == "True", ]
dim(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[2, "technique"] <- "RS"
best <- data[data$cost_mean == min(data$cost_mean), ]
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: [1] 6163   57
: [1] 6163   57

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 1400)) +
    geom_text(aes(x = cost_mean + 0.02, y = 700, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-JtRBod/figurefiVktR.png]]
**** Interesting Papers on Autotuner Performance Comparison :ATTACH:
:PROPERTIES:
:Attachments: knijnenburg2003combined.pdf seymour2008comparison.pdf
:ID:       b4e4b662-3cd1-4c9e-ad70-68bf9a907e1b
:END:

Both papers present performance comparisons of different autotuning strategies
based on search, such as simulated annealing and particle swarm optimization.
The conclusion of both studies is that, in general, those techniques are
marginally better than a random search in the problems tested.
*** [2018-08-28 Tue]
**** Experimental Settings on the Xeon E5 2630 v2
The tables in this section are for keeping track of what would be interesting to
measure and what was already measured.

***** Uniform Random Sampling
|----+--------------------+---------|
|    | Application        | Sampled |
|----+--------------------+---------|
|  1 | stencil3d          | DONE    |
|  2 | dgemv3             | DONE    |
|  3 | atax               | DONE    |
|  4 | gemver             | DONE    |
|  5 | bicgkernel         | DONE    |
|  6 | trmm               |         |
|  7 | adi                | *       |
|  8 | gesummv            |         |
|  9 | mvt                |         |
| 10 | fdtd               | *       |
| 11 | correlation        |         |
| 12 | lu                 |         |
| 13 | tensor-contraction |         |
| 14 | mm                 |         |
| 15 | jacobi             |         |
| 16 | covariance         |         |
| 17 | hessian            |         |
| 18 | seidel             |         |
|----+--------------------+---------|
***** Setting 0
- "Large" (8) number of ANOVA steps
- Model with linear terms
- Using complete model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     | 5/10 |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 1
- "Large" (8) number of ANOVA steps
- Model with linear and quadratic terms
- Using complete model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     | 3/10 |
|  2 | dgemv3             | DONE |     |     |      |      |     | DONE |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 2
- "Large" (8) number of ANOVA steps
- Model with linear and quadratic terms
- Using only relevant variables on model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     |      |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 3
- "Small" (?) number of ANOVA steps
- Model with linear and quadratic terms
- Using complete model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     |      |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 4
- "Small" (?) number of ANOVA steps
- Model with linear and quadratic terms
- Using only relevant variables on model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     |      |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
*** [2018-08-29 Wed]
**** Plotting stencil3d (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/results.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted/results.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
5: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
6: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    #geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max + 0.05, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    #geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-rB9l61/figurewLjn6V.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/search_space.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted/search_space.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)


data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
:  num [1:2] 0.0736 0.0739

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 120)) +
    geom_text(aes(x = cost_mean + 0.05, y = 123, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-rB9l61/figure0XDFiY.png]]
*** [2018-08-30 Thu]
**** Plotting stencil3d (Linear, Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/results.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted_linear/results.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
5: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
6: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    #geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max + 0.05, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    #geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figuregtIPAU.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/search_space.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted_linear/search_space.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)


data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
:  num [1:2] 0.0736 0.0707

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 160)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figure07533Y.png]]
*** [2018-08-31 Fri]
**** Learning About Grid5000 and Setting up Experiments
Basic information on how to allocate resources and deploy images:

- https://www.grid5000.fr/mediawiki/index.php/Getting_Started

Creating permanent images after configuration:

- https://www.grid5000.fr/mediawiki/index.php/Advanced_Kadeploy

Visualizing status of all clusters:

- https://api.grid5000.fr/3.0/ui/visualizations/status.html

Hardware:

- List of all hardware: https://www.grid5000.fr/mediawiki/index.php/Hardware
- Targetting the Xeon E5 2630 v3 because it is the most numerous

- Target server: Nancy
  - Target clusters: grisou, grimoire, graoully

- Target server: Rennes
  - Target clusters: parasilo, paravance

Setting up the SSH Agent and Agent Forwarding:

- https://www.grid5000.fr/mediawiki/index.php/SSH#Using_the_SSH_agent

GitHub repository with configuration files for the Grid5000 deploy:

- https://github.com/phrb/orio_experiments_g5kjob
** September
*** [2018-09-03 Mon]
**** DOE + Autotuning Summaries (Collected from previous entries)
I aggregated all past entries that contain summaries and explanations on DoE and
Autotuning. I repeat them here because the performance of the other journal file
is very bad.

***** Some References on Optimal Multi-Level Supersaturated Designs
I was looking into works that cited that Addelman and Kempthorne paper and found
that one way to name those kinds of designs is "Optimal Multi-Level
Supersaturated".

Searching by that term I found some more recent references which I believe will
take some time to digest, but that present some ways to construct those designs:

https://projecteuclid.org/download/pdfview_1/euclid.aos/1140191674
https://projecteuclid.org/download/pdfview_1/euclid.ssu/1244555797

Those works also seem to present ways to measure optimality for those designs,
but I still haven't read them.

There is also a reference which seems interesting by its abstract, but I was not
able to find it yet:

https://www.sciencedirect.com/science/article/pii/S0378375812003084

I also found this book on factorial designs which I did not know of, but that
does not seem to discuss Addelman Kempthorne designs directly. It could be
useful for me to study more about DoE in general.

https://www.springer.com/gp/book/9780387319919
***** Reading About Design of Experiments, Factorial Designs, Supersaturated Designs
- [[file:~/Dropbox/papers/design-of-experiments/georgiou2014supersaturated.pdf][Georgiou's paper on Supersaturated Designs]]
- [[file:~/Dropbox/papers/design-of-experiments/montgomery2013design.pdf][Montgomery's Book]]
***** [[file:~/Dropbox/papers/design-of-experiments/mukerjee2006modern.pdf][Mukerjee's Book on Factorial Designs]]
- The introduction points to [[file:~/Dropbox/papers/design-of-experiments/wu2000experiments.pdf][Wu's Book on Design of Experiments]], which seems very nice, especially:
  - Chapter 7: Other Design and Analysis Techniques for Experiments at More than
    Two Levels
  - Chapter 8: Nonregular Designs: Construction and Properties
***** Generating Good/Optimal Mixed-Level Designs
Reading [[file:~/Dropbox/papers/design-of-experiments/mukerjee2006modern.pdf][Mukerjee's book]] on Factorial Designs pointed to [[file:~/Dropbox/papers/design-of-experiments/wu2000experiments.pdf][Wu's book]] on Design of
Experiments, which has two chapters (7 & 8) on the design and analysis of
multi-level experimental designs.

Search for more specific terms such as "multi-" or "mixed-level experimental
designs" led to finding [[file:~/Dropbox/papers/design-of-experiments/fontana2013algebraic.pdf][Fontana's paper]] on algebraic generation of minimum
orthogonal designs. A [[file:~/Dropbox/papers/design-of-experiments/fontana2017generalized.pdf][later paper]] from the same author generalized these
constructions to mixed-level orthogonal arrays, which can be used to generate
mixed-level optimal designs. Fontana [[file:~/Dropbox/papers/design-of-experiments/fontana2013minimum.pdf][implemented his algorithm]] using the
proprietary software SAS.

Fontana later collaborated with Ulrike, from the ~DoE~ R packages, to implement
this construction in the ~DoE.MIParray~ R [[https://cran.r-project.org/web/packages/DoE.MIParray/DoE.MIParray.pdf][package]]. The [[file:~/Dropbox/papers/design-of-experiments/ulrike2018algorithm.pdf][description]] of the
implementation mentions that proprietary solvers such as ~gurobi~ where used in
the solution.

We now have a way of generating optimal mixed-level orthogonal experimental
designs using R, albeit using proprietary software. Would it be worth evaluating
the difficulty of implementing this using open-source software?

***** Tuning FPGAs using Experimental Design
Looking for works that cited Fontana's paper I came across a 2018 JPDC paper on
[[file:~/Dropbox/papers/design-of-experiments/tuzov2018tuning.pdf][tuning synthesis flags on FPGAs]]. This work uses fractional factorial
experimental designs to explore a subset of flags that control synthesis in
FPGAs, targeting a specific architecture.

The 9 optimization flags they tested in this paper were selected because they
were the most common flags across multiple FPGA synthesis tools. The authors
give no other reason to select those flags. The total number of flags and
optimizations in the targeted system was 72.

Of those 9 flags, it is not entirely clear which are multi-level factors and
which are only boolean or two-level factors. It is also not clear whether the
other 63 flags on the system are multi-level, integers, or other. It seems they
converted some flags to string selections, but this is not explained further.

They used a 2^{9 - 4}_{IV} fractional factorial design with 32 runs to measure
the impact of the 9 flags. The entire search space in this case has 512
different configurations. They use a set of 24 response variables to measure the
effectiveness of each configuration, and attempt to fit linear models using the
9 factors for each response variable, separately.

One particularly weird choice of this study was to use the fitted linear models,
that used 32 configurations, to compute the values of the response variables for
the remaining 480 configurations. This is strange because the using ANOVA
attempts to measure whether each factor impacts the response variables, but not
necessarily gives a correct model, since factor levels are mapped to [0, 1] in
this case.

They did not re-evaluate the predicted points to check for accuracy and did not
show where in the search space the 32 actually measured points were.
***** Reading [[file:~/Dropbox/papers/design-of-experiments/mukerjee2006modern.pdf][Mukerjee's Book]] on Factorial Designs
Chapter 2 provides fundamental definitions of concepts related to factorial
designs. Its definition of contrasts is a little bit different from [[https://en.wikipedia.org/wiki/Contrast_(statistics)][this one]].
The [[https://en.wikipedia.org/wiki/Kronecker_product][Kronecker product]] is also a useful concept presented in this chapter.

This book is much more mathematically denser than any other DoE book I have read
so far. It seems to be a good mathematical foundation for the concepts presented
in books such as [[file:~/Dropbox/papers/design-of-experiments/montgomery2013design.pdf][Montgomery's]], but does not offer much in terms of practical
algorithms or implementations.

A more practical reference is [[file:~/Dropbox/papers/design-of-experiments/hedayat1999orthogonal.pdf][Hedayat's]] Orthogonal Arrays: Theory and
Applications. That book gives an algorithm for the expansive and contractive
replacement techniques for constructing orthogonal arrays from existing
orthogonal arrays.

***** Expansive & Contractive Replacement Method from [[file:~/Dropbox/papers/design-of-experiments/hedayat1999orthogonal.pdf][Hedayat's Orthogonal Arrays]]
Section 9.3 of this book presents the expansive and contractive replacement
methods, two apparently simple strategies to construct mixed-level designs from
other designs, or from parts from the same design.

The expansive replacement method can be used to insert runs from designs with
factors with less levels into designs with factors with more levels, by
replacing selected columns. This seems straightforward because it "encodes" a
set of columns with more levels to a column with less levels.

The contractive replacement method seems more directly applicable in our case.
This method is what the AutoDesign system [[http://www.functionbay.co.kr/documentation/onlinehelp/default.htm#!Documents/plackettandburmandesign.htm][claims to use]] to generate its
mixed-level Plackett-Burman designs. The method requires finding a set of
columns in the original design that fulfill certain properties. Those columns
are used to encode a new factor with more levels, in a process similar to the
one discussed with Arnaud. The main advantages of this method are the guarantee
of not losing orthogonality of the design and not changing the number of runs.

****** The Contractive Replacement Method
One notation for orthogonal arrays is $OA(N, s_1, \dots, s_k, t)$, where
$N$ is the number of rows, $s_1, \dots, s_k$ are the factors, where each $s_i$
represents the number of levels, and $t$ is the strength.

To use the contractive method on an array $A = OA(N, s_1, \dots, s_k, 2)$, we
need to find an OA $B = OA(N, s_u, \forall u \in U, 2)$, constructed from a
subset $U$ of columns of $A$. $B$ must have the following properties:

1. $B$ is composed of $N / L$ repetitions of the same rows, where $L$ denotes
   the number of levels of the new factor
2. $L = 1 - u + \sum\limits_{u \in U}{s_u}$

If we can find $B$, we then encode each unique row as $0, 1, \dots, L - 1$ and
replace the corresponding rows of $A$, in columns in $U$, with this new
encoding. This effectively "contracts" the columns in $U$ into a new column
represeting a factor with L levels.

***** Thoughts on the Contractive Replacement Method
It seems that Plackett-Burman designs are not very well suited
for using this method, since no groups of columns seem to have
the necessary properties. Maybe there is something wrong with my
implementation.

Future steps could be:

1. Re-implement this checking in R using FrF2's ~pb~
2. Try this with different classes of OAs (full factorial?)
3. Debug the checking implementation

***** ~DoE.MIParray~
This technique does not seem to be the best approach for our problem,
since the construction involves solving a mixed integer problem that
dependes on the number of levels. From a note in the documentation:

#+BEGIN_QUOTE
The package is not meant for situations, for which a full factorial design would
be huge; the mixed integer problem to be solved has at least prod(nlevels)
binary or general integer variables and will likely be untractable, if this
number is too large. (For extending an existing designs, since some variables
are fixed, the limit moves out a bit.)
#+END_QUOTE

Therefore, attempting to construct a design for more than a 100 factors with
multiple levels each would not be feasible.

***** Summarizing Strategies for Multi-Level Designs

****** 2-Level Screening with Random Sampling of Levels

In this strategy factors with more than two unordered levels are sampled at two
random levels. This enables the usage of a small design such as the
Plackett-Burman screening design.

Advantages are the small design size and good estimation capability for main
effects. Disadvantages are the incapability of estimating interactions, but
mainly the lack of information regarding the response for levels not selected in
the initial screening.

****** Level Projection, Factor Merging or Contractive Replacement

An initial 2-Level design is used to generated mixed-level designs by reencoding
some columns into a new single column represeting a multi-level factor. The
contractive replacement of Addelman-Kempthorne is a strategy of this kind.

Advantages are also small design size and good estimation capability of main
effects. Additionally, the contractive replacement technique is keeps
orthogonality of designs. Disadvantages are the requirements on the initial
designs. Not all 2-Level designs can be contracted with those methods if we want
to keep orthogonality.

****** Direct Generation

The work from [[file:~/Dropbox/papers/design-of-experiments/ulrike2018algorithm.pdf][GrÃ¶mping and Fontana]] enables the generation of multi-level
designs with the Generalized Minimum Aberration optimality criterion by
solving mixed integer problems.

Advantages are the direct generation of multi-level designs and the optimality
criteria. Disadvantages are the use of proprietary MIP solvers and the
limitations on the size and shape of the designs that can be generated.

****** Optimal Designs

This strategy consists of generating optimal multi-level designs. I still did not
find any algorithms or papers related to this.

***** Reading Triefenbach's Bachelor Thesis on D-Optimal Designs
The [[http://www8.cs.umu.se/education/examina/Rapporter/FabianTtriefenbach.pdf][thesis]] presents some of the theory behind D-Optimal Designs in a simple way.
It discusses a modification to the model matrix that enables constructing less
model-dependent D-Optimal designs, but does not details why this "Bayesian
modification" helps.

Constructing D-Optimal designs seems to be the most straightforward way of
generating large multi-level designs I have found so far. In theory, it is
possible to generate designs of any number of runs, with any number of factors
with any number of different levels. At first, it seems that this is the best
candidate so far.

In practice, even though the tools from ~AlgDesign~ generate designs for
arbitrary numbers of factors and levels, that implementation does not work for
even slightly larger designs. I believe this happens due to implementation
choices and not "theoretical" or algorithmic limitations.

****** Constructing D-Optimal Designs
A D-Optimal design is a selection from a set of possible experiments that
minimizes some optimality criterion.

For example, imagine we have a full factorial design. We decide that we want
to analyse this design using a "simple linear model", such as =~.=. Therefore,
the *set of possible experiments*, or *model matrix* for this experiment is
obtained by adding a column of ones to the original full factorial design. If we
had interations, quadratic terms, and so on, we would simply compute the
correspondent columns using original columns in the original design.

With the model matrix defined, the criterion of our D-Optimal design could be
minimizing the determinant of the *information matrix*. This new matrix is
defined by multiplying the previously generated model matrix by its inverse.

Now, the hard part is finding a selection of experiments from the *model matrix*
that minimizes the determinant of the *information matrix*. Common algorithms
for this task seem to be pretty straightforward, relying on swapping columns of
an existing matrix with columns in the possible runs and measuring changes in
the determinant.
***** Understanding D-Efficiency
I've found conflicting definitions for the D-Efficiency criteria.

****** In the ~AlgDesign~ Package
The first definition is from the ~AlgDesign~ [[https://cran.r-project.org/web/packages/AlgDesign/AlgDesign.pdf][R package]]:

\begin{equation}
  D_{elb} = exp(1 - 1 / G_e)
\end{equation}

Where $G_e$ is given by:

\begin{equation}
  G_e = k / max(x\prime{}(X\prime{}X)x, \forall x \in X))
\end{equation}

Where $k$ is the number of of column in the model matrix $X$, and each $x$ is a
row in $X$ that describes an experiment.

I was not able to find a paper or book describing or explaining this criteria.

****** From DETMAX, SAS, and Castillo's Book                    :ATTACH:
:PROPERTIES:
:Attachments: castillo2007process.pdf
:ID:       21aa2891-9cc8-4e49-8f75-ad668b71ea93
:END:

Mitchell's [[https://www.tandfonline.com/doi/abs/10.1080/00401706.1974.10489176][second paper on the DETMAX algorithm]] and Castillo's book (Process
Optimization: A Statistical Approach, annexed to this section) present a
different criteria for D-Efficiency:

\begin{equation}
  D_{elb} = det(X\prime{}X)^(1/k) / N
\end{equation}

Where $X$ is the model matrix, $k$ is its number of columns and $N$ its number
of rows.

Castillo mentions that this is a lower bound on the D-Efficiency of a design,
more specifically, from section 5.7.2, page 152 of Castillo's book, this version
of D-Efficiency "[...] is a measure of D-optimality with respect to a
hypothetical orthogonal design (which would be D-optimal)[...]".

****** From Fedorov's Theory of Optimal Designs                 :ATTACH:
:PROPERTIES:
:Attachments: fedorov1972theory.pdf
:ID:       1c941f7d-bd93-40ab-a1a5-4033eab52841
:END:

***** Understanding D-Efficiency: The Box and Draper Encoding
:PROPERTIES:
:Attachments: box2007response.pdf
:ID:       c8cb30f5-a404-4ea8-bb9e-90bd726c00a0
:END:

Box and Draper's 2007 edition of "Response Surfaces, Mixtures and Ridge
Analyses" is a very clear book which explains the encoding used in the
computation of the D-Efficiency metric used in the DETMAX paper and Castillo's
book (See attachment in [[From DETMAX, SAS, and Castillo's Book]]). I am currently
reading Chapter 14, Variance Optimal Designs, where the book describes D-Optimal
designs and the encoding used. The intuition is that this metric compares
***** Simple Terminology for Design of Experiments
****** Basic Concepts
- An *experimental design* is a plan for running a set of *experiments*,
  can be displayed as a matrix
- *Factors* are the columns of an experimental design matrix, and
  represent experimental variables with at least two possible values, or *levels*
- *Categorical factors* have unordered levels, that is, there is no sense of progression
  from one level to another
- *Numerical factors* can be *continuous*, such as floating-point values, or *discrete*,
  such as integer values
- The rows of an experimental design matrix are individual
  *treatments*, *experiments*, or *runs*
- Each *run* provides a measurement of a *dependent variable*, or
  *response*, subject to a specific assignment of *factor levels*
- Running all *experiments* in a design gives some information that allows
  studying the *effects* of *factor levels* in the *response variable*
- Two *effects* are *aliased* or *confounded* when it is not possible to
  distinguish them from the data obtained from running a given design
- The *resolution* of a design indicates which *effects* are independently
  *estimated* in a design. There is less *aliasing* or *confounding* between
  *effects* as the *resolution* increases
- A *full-factorial design* is an experimental design containing all
  possible combinations of *factor levels*
- A *fractional-factorial design* is a subset of the rows of a *full-factorial
  design*
- An experimental design is *balanced* when each *level* occurs equally often
  within every *factor*
- An experimental design is *orthogonal* when every *pair of levels* occurs
  equally often in all *pairs of factors*. A design can also be *orthogonal*
  when the occurence frequencies of *pairs of levels* are proportional
****** Modelling and Analysing
- A *model* specifies which *factors* and *factor relationships* will be considered in
  a posterior *linear regression* or *analysis of variance*, using the measured
  *responses*
- A *model matrix* or *design matrix* is constructed with an original
  *experimental design* and according to a *model*. Each column, or set of
  columns for categorical factors, in a *model matrix* represents one *effect*
- *Contrasts* can be used to encode *categorical factors* in a *model matrix*
  using extra columns. *Contrast matrices* specify the encoding for each level
  of a *categorical factor*
- An *information matrix* is defined as the multiplication of the transpose of
  a *model matrix* and the *model matrix* itself. The inverse of the *information matrix*
  is the *dispersion matrix*, or *covariance matrix*
****** Design Efficiency
- In a *least-squares linear regression*, considering a *linear relationship* between
  variables, *parameter estimates* are proportional to a design's *covariance matrix*
- The *eigenvalues* of the *covariance matrix* of a design represent its "size" in a certain sense
- The *A-Efficiency* of a design is inversely proportional to the *arithmetic mean* of the *eigenvalues*
  of the design's *covariance matrix*
- The *D-Efficiency* of a design is inversely proportional to the *geometric mean* of the *eigenvalues*
  of the design's *covariance matrix*
- For a given set of *factors*, a hypothetical *balanced* an *orthogonal* design has optimal *A-Efficiency* and
  *D-Efficiency*
- A design with $N$ rows is *balanced and orthogonal* when the *dispersion matrix* is diagonal
  and equals to $\dfrac{1}{N}\mathbf{I}$
  - The following designs are all *balanced and orthogonal*:
    #+HEADER: :results output :session *R*
    #+BEGIN_SRC R
    library(AlgDesign)
    library(FrF2)

    design <- gen.factorial(c(2, 2, 2, 2))
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    eval.design(~., design)

    design <- pb(16)
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    design <- FrF2(nruns = 8, nfactors = 4)
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)
    #+END_SRC

    #+RESULTS:
    #+begin_example
       X1 X2 X3 X4
    1  -1 -1 -1 -1
    2   1 -1 -1 -1
    3  -1  1 -1 -1
    4   1  1 -1 -1
    5  -1 -1  1 -1
    6   1 -1  1 -1
    7  -1  1  1 -1
    8   1  1  1 -1
    9  -1 -1 -1  1
    10  1 -1 -1  1
    11 -1  1 -1  1
    12  1  1 -1  1
    13 -1 -1  1  1
    14  1 -1  1  1
    15 -1  1  1  1
    16  1  1  1  1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
       (Intercept) X1 X2 X3 X4
    1            1 -1 -1 -1 -1
    2            1  1 -1 -1 -1
    3            1 -1  1 -1 -1
    4            1  1  1 -1 -1
    5            1 -1 -1  1 -1
    6            1  1 -1  1 -1
    7            1 -1  1  1 -1
    8            1  1  1  1 -1
    9            1 -1 -1 -1  1
    10           1  1 -1 -1  1
    11           1 -1  1 -1  1
    12           1  1  1 -1  1
    13           1 -1 -1  1  1
    14           1  1 -1  1  1
    15           1 -1  1  1  1
    16           1  1  1  1  1
    attr(,"assign")
    [1] 0 1 2 3 4
                (Intercept) X1 X2 X3 X4
    (Intercept)           1  0  0  0  0
    X1                    0  1  0  0  0
    X2                    0  0  1  0  0
    X3                    0  0  0  1  0
    X4                    0  0  0  0  1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
    Screening 15 factors in 16 runs involves perfect aliasing of
     pairwise interactions of the first six factors with the last factor.
        A  B  C  D  E  F  G  H  J  K  L  M  N  O  P
    1  -1  1 -1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1
    2   1  1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1 -1  1
    3   1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1
    4  -1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1  1 -1
    5   1 -1 -1  1  1 -1  1 -1 -1  1 -1  1 -1  1 -1
    6  -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1  1  1
    7  -1 -1  1  1  1  1 -1 -1 -1 -1  1  1 -1 -1  1
    8   1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1
    9  -1  1 -1  1  1 -1 -1  1 -1  1  1 -1  1 -1 -1
    10  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
    11 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1  1
    12 -1 -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1
    13  1 -1  1 -1 -1  1 -1  1 -1  1 -1  1  1 -1 -1
    14 -1  1  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1
    15  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1
    16  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1 -1 -1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
       (Intercept)  A  B  C  D  E  F  G  H  J  K  L  M  N  O  P
    1            1 -1  1 -1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1
    2            1  1  1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1 -1  1
    3            1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1
    4            1 -1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1  1 -1
    5            1  1 -1 -1  1  1 -1  1 -1 -1  1 -1  1 -1  1 -1
    6            1 -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1  1  1
    7            1 -1 -1  1  1  1  1 -1 -1 -1 -1  1  1 -1 -1  1
    8            1  1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1
    9            1 -1  1 -1  1  1 -1 -1  1 -1  1  1 -1  1 -1 -1
    10           1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
    11           1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1  1
    12           1 -1 -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1
    13           1  1 -1  1 -1 -1  1 -1  1 -1  1 -1  1  1 -1 -1
    14           1 -1  1  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1
    15           1  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1
    16           1  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1 -1 -1
    attr(,"assign")
     [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
                (Intercept) A B C D E F G H J K L M N O P
    (Intercept)           1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
    A                     0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
    B                     0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
    C                     0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
    D                     0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
    E                     0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
    F                     0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
    G                     0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
    H                     0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
    J                     0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
    K                     0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
    L                     0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0
    M                     0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
    N                     0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
    O                     0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
    P                     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
       A  B  C  D
    1  1 -1 -1  1
    2 -1  1  1 -1
    3  1 -1  1 -1
    4 -1 -1  1  1
    5  1  1 -1 -1
    6  1  1  1  1
    7 -1 -1 -1 -1
    8 -1  1 -1  1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
      (Intercept)  A  B  C  D
    1           1  1 -1 -1  1
    2           1 -1  1  1 -1
    3           1  1 -1  1 -1
    4           1 -1 -1  1  1
    5           1  1  1 -1 -1
    6           1  1  1  1  1
    7           1 -1 -1 -1 -1
    8           1 -1  1 -1  1
    attr(,"assign")
    [1] 0 1 2 3 4
                (Intercept) A B C D
    (Intercept)           1 0 0 0 0
    A                     0 1 0 0 0
    B                     0 0 1 0 0
    C                     0 0 0 1 0
    D                     0 0 0 0 1
    #+end_example
- A design is *orthogonal* when the *dispersion matrix* is diagonal, excluding the row and column
  correspondent to the *intercept*, that is, there can be non-zero elements in the *intercept* row
  and column
  - The following designs are all *orthogonal*, but not *balanced*:
    #+HEADER: :results output :session *R*
    #+BEGIN_SRC R
    library(AlgDesign)
    library(FrF2)

    design <- oa.design(nfactors = 6, nlevels = 3)
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    t(model_matrix) %*% model_matrix

    design <- oa.design(nlevels=c(4,3,3,2))
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)
    #+END_SRC

    #+RESULTS:
    #+begin_example
    The columns of the array have been used in order of appearance.
    For designs with relatively few columns,
    the properties can sometimes be substantially improved
    using option columns with min3 or even min34.
       A B C D E F
    1  1 3 2 3 2 1
    2  3 3 1 1 2 2
    3  2 2 2 2 2 2
    4  3 2 1 2 1 3
    5  3 2 2 1 3 1
    6  2 3 1 2 3 1
    7  2 1 1 3 2 3
    8  3 1 2 3 1 2
    9  2 2 3 3 1 1
    10 3 3 3 3 3 3
    11 1 2 3 1 2 3
    12 2 1 3 1 3 2
    13 2 3 2 1 1 3
    14 1 2 1 3 3 2
    15 1 1 2 2 3 3
    16 3 1 3 2 2 1
    17 1 3 3 2 1 2
    18 1 1 1 1 1 1
    $determinant
    [1] 0.7064227

    $A
    [1] 6.571429

    $diagonality
    [1] 0.261

    $gmean.variances
    [1] 1.5
       (Intercept) A B C D E F
    1            1 1 3 2 3 2 1
    2            1 3 3 1 1 2 2
    3            1 2 2 2 2 2 2
    4            1 3 2 1 2 1 3
    5            1 3 2 2 1 3 1
    6            1 2 3 1 2 3 1
    7            1 2 1 1 3 2 3
    8            1 3 1 2 3 1 2
    9            1 2 2 3 3 1 1
    10           1 3 3 3 3 3 3
    11           1 1 2 3 1 2 3
    12           1 2 1 3 1 3 2
    13           1 2 3 2 1 1 3
    14           1 1 2 1 3 3 2
    15           1 1 1 2 2 3 3
    16           1 3 1 3 2 2 1
    17           1 1 3 3 2 1 2
    18           1 1 1 1 1 1 1
    attr(,"assign")
    [1] 0 1 2 3 4 5 6
                (Intercept)    A    B    C    D    E    F
    (Intercept)          37 -3.0 -3.0 -3.0 -3.0 -3.0 -3.0
    A                    -3  1.5  0.0  0.0  0.0  0.0  0.0
    B                    -3  0.0  1.5  0.0  0.0  0.0  0.0
    C                    -3  0.0  0.0  1.5  0.0  0.0  0.0
    D                    -3  0.0  0.0  0.0  1.5  0.0  0.0
    E                    -3  0.0  0.0  0.0  0.0  1.5  0.0
    F                    -3  0.0  0.0  0.0  0.0  0.0  1.5
                (Intercept)  A  B  C  D  E  F
    (Intercept)          18 36 36 36 36 36 36
    A                    36 84 72 72 72 72 72
    B                    36 72 84 72 72 72 72
    C                    36 72 72 84 72 72 72
    D                    36 72 72 72 84 72 72
    E                    36 72 72 72 72 84 72
    F                    36 72 72 72 72 72 84
    creating full factorial with 72 runs ...
       A B C D
    1  1 3 2 1
    2  2 1 1 2
    3  2 3 1 1
    4  3 3 3 1
    5  2 1 3 1
    6  2 1 3 2
    7  1 1 2 2
    8  2 2 1 1
    9  3 1 3 1
    10 1 3 3 1
    11 4 1 2 1
    12 4 2 2 2
    13 2 2 2 1
    14 4 1 2 2
    15 4 3 1 2
    16 2 2 2 2
    17 3 2 3 1
    18 3 2 2 2
    19 4 1 1 1
    20 1 3 1 1
    21 3 2 1 1
    22 2 3 2 1
    23 3 3 1 1
    24 3 2 1 2
    25 4 1 3 1
    26 2 1 2 2
    27 1 1 3 1
    28 4 1 1 2
    29 1 1 1 1
    30 1 2 2 2
    31 2 1 1 1
    32 1 3 3 2
    33 4 2 1 1
    34 3 3 2 1
    35 4 3 2 2
    36 1 3 2 2
    37 1 1 1 2
    38 2 2 1 2
    39 3 1 2 2
    40 3 2 2 1
    41 1 3 1 2
    42 2 2 3 1
    43 4 2 3 1
    44 3 1 3 2
    45 4 2 3 2
    46 1 2 3 1
    47 2 2 3 2
    48 2 3 1 2
    49 3 1 2 1
    50 1 1 2 1
    51 3 1 1 1
    52 3 3 2 2
    53 4 2 2 1
    54 4 3 3 2
    55 4 2 1 2
    56 4 3 2 1
    57 2 3 3 2
    58 4 1 3 2
    59 3 3 1 2
    60 2 1 2 1
    61 1 2 1 1
    62 3 3 3 2
    63 3 2 3 2
    64 1 2 3 2
    65 1 2 1 2
    66 2 3 3 1
    67 3 1 1 2
    68 4 3 1 1
    69 1 2 2 1
    70 1 1 3 2
    71 4 3 3 1
    72 2 3 2 2
    $determinant
    [1] 0.6738039

    $A
    [1] 6.96

    $diagonality
    [1] 0.31

    $gmean.variances
    [1] 1.638073
       (Intercept) A B C D
    1            1 1 3 2 1
    2            1 2 1 1 2
    3            1 2 3 1 1
    4            1 3 3 3 1
    5            1 2 1 3 1
    6            1 2 1 3 2
    7            1 1 1 2 2
    8            1 2 2 1 1
    9            1 3 1 3 1
    10           1 1 3 3 1
    11           1 4 1 2 1
    12           1 4 2 2 2
    13           1 2 2 2 1
    14           1 4 1 2 2
    15           1 4 3 1 2
    16           1 2 2 2 2
    17           1 3 2 3 1
    18           1 3 2 2 2
    19           1 4 1 1 1
    20           1 1 3 1 1
    21           1 3 2 1 1
    22           1 2 3 2 1
    23           1 3 3 1 1
    24           1 3 2 1 2
    25           1 4 1 3 1
    26           1 2 1 2 2
    27           1 1 1 3 1
    28           1 4 1 1 2
    29           1 1 1 1 1
    30           1 1 2 2 2
    31           1 2 1 1 1
    32           1 1 3 3 2
    33           1 4 2 1 1
    34           1 3 3 2 1
    35           1 4 3 2 2
    36           1 1 3 2 2
    37           1 1 1 1 2
    38           1 2 2 1 2
    39           1 3 1 2 2
    40           1 3 2 2 1
    41           1 1 3 1 2
    42           1 2 2 3 1
    43           1 4 2 3 1
    44           1 3 1 3 2
    45           1 4 2 3 2
    46           1 1 2 3 1
    47           1 2 2 3 2
    48           1 2 3 1 2
    49           1 3 1 2 1
    50           1 1 1 2 1
    51           1 3 1 1 1
    52           1 3 3 2 2
    53           1 4 2 2 1
    54           1 4 3 3 2
    55           1 4 2 1 2
    56           1 4 3 2 1
    57           1 2 3 3 2
    58           1 4 1 3 2
    59           1 3 3 1 2
    60           1 2 1 2 1
    61           1 1 2 1 1
    62           1 3 3 3 2
    63           1 3 2 3 2
    64           1 1 2 3 2
    65           1 1 2 1 2
    66           1 2 3 3 1
    67           1 3 1 1 2
    68           1 4 3 1 1
    69           1 1 2 2 1
    70           1 1 1 3 2
    71           1 4 3 3 1
    72           1 2 3 2 2
    attr(,"assign")
    [1] 0 1 2 3 4
                (Intercept)             A    B    C  D
    (Intercept)          27 -2.000000e+00 -3.0 -3.0 -6
    A                    -2  8.000000e-01  0.0  0.0  0
    B                    -3  0.000000e+00  1.5  0.0  0
    C                    -3  0.000000e+00  0.0  1.5  0
    D                    -6  4.440892e-16  0.0  0.0  4
    #+end_example
- A design is *balanced* when the elements of the *intercept* row and column are all zero, except
  for the element in the diagonal, that is, there can be non-zero elements in every other row and
  column
  - The following designs are all *balanced* but not *orthogonal*:
    #+HEADER: :results output :session *R*
    #+BEGIN_SRC R
    library(AlgDesign)
    library(FrF2)

    design <- data.frame(x1 = c(1, -1, 1, -1, 1, -1),
                         x2 = c(1, 1, 1, -1, -1, -1),
                         x3 = c(-1, 1, 1, 1, -1, -1))
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    eval.design(~., design)
    #+END_SRC

    #+RESULTS:
    #+begin_example
      x1 x2 x3
    1  1  1 -1
    2 -1  1  1
    3  1  1  1
    4 -1 -1  1
    5  1 -1 -1
    6 -1 -1 -1
    $determinant
    [1] 0.8773827

    $A
    [1] 1.375

    $diagonality
    [1] 0.84

    $gmean.variances
    [1] 1.5
      (Intercept) x1 x2 x3
    1           1  1  1 -1
    2           1 -1  1  1
    3           1  1  1  1
    4           1 -1 -1  1
    5           1  1 -1 -1
    6           1 -1 -1 -1
    attr(,"assign")
    [1] 0 1 2 3
                (Intercept)    x1    x2    x3
    (Intercept)           1  0.00  0.00  0.00
    x1                    0  1.50 -0.75  0.75
    x2                    0 -0.75  1.50 -0.75
    x3                    0  0.75 -0.75  1.50
    $determinant
    [1] 0.8773827

    $A
    [1] 1.375

    $diagonality
    [1] 0.84

    $gmean.variances
    [1] 1.5
    #+end_example
**** Application: stencil3d (Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/stencil3d/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)
data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])

str(data)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
[1] 10 34
[1] 10 34
[1] 300
[1] 235.3
'data.frame':	20 obs. of  34 variables:
 $ id       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ RT1_K    : int  1 1 32 1 1 32 1 8 1 1 ...
 $ RT1_J    : int  1 1 1 32 32 1 32 1 1 1 ...
 $ T1_Ja    : int  1 2048 256 256 64 64 128 2048 1024 1 ...
 $ RT1_I    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ T2_K     : int  512 16 512 1 128 512 1 512 16 256 ...
 $ T2_J     : int  256 128 16 1 16 512 16 32 16 16 ...
 $ T2_I     : int  512 1 1 32 64 16 32 16 256 64 ...
 $ T2_Ja    : int  256 512 1 1024 64 1 1024 1 1024 1 ...
 $ U2_I     : int  11 19 9 13 24 30 13 30 13 8 ...
 $ T1_I     : int  1 16 64 32 512 16 128 1 32 1 ...
 $ T1_J     : int  128 512 1 128 16 1 16 32 256 1 ...
 $ T1_K     : int  256 1 1 32 1 128 128 512 512 1 ...
 $ technique: chr  "RS" "RS" "DLMT" "RS" ...
 $ speedup  : num  0.871 1.002 0.668 1.371 0.907 ...
 $ RT2_J    : int  1 1 32 8 1 1 1 1 1 32 ...
 $ VEC2     : chr  "True" "True" "True" "True" ...
 $ VEC1     : chr  "True" "False" "False" "False" ...
 $ SCR      : chr  "True" "True" "True" "False" ...
 $ T1_Ia    : int  1 2048 64 1 1 1 1024 1 128 64 ...
 $ T1_Ka    : int  1 256 1 2048 256 256 1024 1 512 128 ...
 $ OMP1     : chr  "True" "False" "False" "True" ...
 $ U1_K     : int  7 1 1 29 2 4 18 1 5 1 ...
 $ U1_J     : int  1 30 6 1 12 1 13 5 1 1 ...
 $ U1_I     : int  14 28 17 6 1 4 1 16 26 17 ...
 $ T2_Ka    : int  1 256 1 1024 1024 2048 2048 2048 512 1 ...
 $ RT2_I    : int  8 1 1 1 8 1 1 1 8 1 ...
 $ T2_Ia    : int  1 512 1 1 1024 1 512 1 256 1 ...
 $ U2_J     : int  19 15 1 28 9 1 21 1 1 5 ...
 $ U2_K     : int  1 1 27 1 1 29 1 1 25 1 ...
 $ cost_mean: num  0.0596 0.0528 0.0772 0.0384 0.0585 ...
 $ RT2_K    : int  1 1 1 8 1 32 32 32 8 1 ...
 $ OMP2     : chr  "True" "False" "False" "True" ...
 $ points   : int  300 300 198 300 300 237 300 240 300 247 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figurecXzenc.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    geom_jitter() +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figurefptjRn.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(plyr)
library(ggplot2)
library(RColorBrewer)
library(colorRamps)

read.csv.id <- function(filename) {
    data <- read.csv(filename)
    data$file <- rep(filename, nrow(data))
    return(data)
}

target_dir <-"dlmt_spapt_experiments/data/stencil3d/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)
data <- lapply(csv_files, read.csv.id)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_Ia, U1_I, T2_I, T2_Ia, U1_I, RT1_I, U2_I, RT2_I,
                 SCR, OMP1, OMP2, .keep_all = TRUE)

#+END_SRC

#+RESULTS:
: There were 19 warnings (use warnings() to see them)

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
colourCount <- length(unique(data$file))
getPalette <- colorRampPalette(brewer.pal(9, "Spectral"))

ggplot(data, aes(fill = file)) +
    scale_fill_manual(values = colorRampPalette(brewer.pal(12, "Spectral"))(colourCount)) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 250)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18) +
    theme(legend.position="none")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-SrcUCi/figurel2BbOo.png]]
*** [2018-09-04 Tue]
**** Application: atax (8 Steps, Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/atax/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])
#+END_SRC

#+RESULTS:
: There were 31 warnings (use warnings() to see them)
: [1] 10 18
: [1] 10 18
: [1] 131
: [1] 115.7

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figure903s2q.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-YBOcEp/figureybvd4R.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)

target_dir <-"dlmt_spapt_experiments/data/atax/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" & data$VEC1 == "False" &
                    data$VEC2 == "False", ]


O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_J, T1_K, T2_I, T2_J, T2_K,
                 U1_I, U_I, U_J, U_K, RT_I, RT_J, RT_K, SCR,
                 VEC1, VEC2, .keep_all = TRUE)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
Warning message:
Trying to compute distinct() for variables not found in the data:
- `SCR`, `VEC1`, `VEC2`
This is an error, but only a warning is raised for compatibility reasons.
The following variables will be used:
- T1_I
- T1_J
- T1_K
- T2_I
- T2_J
- (8 more)
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 220)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figureTd8uqU.png]]
**** Application: dgemv3 (Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/dgemv3/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])
#+END_SRC

#+RESULTS:
: There were 50 or more warnings (use warnings() to see the first 50)
: [1] 10 54
: [1] 10 54
: [1] 360
: [1] 333.9

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figure87avL8.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figure8P1fGd.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)

target_dir <-"dlmt_spapt_experiments/data/dgemv3/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" & data$VEC1 == "False" &
                    data$VEC2 == "False", ]


O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_J, T1_K, T2_I, T2_J, T2_K,
                 U1_I, U_I, U_J, U_K, RT_I, RT_J, RT_K, SCR,
                 VEC1, VEC2, .keep_all = TRUE)
#+END_SRC

#+RESULTS:
#+begin_example
There were 21 warnings (use warnings() to see them)
Warning message:
Trying to compute distinct() for variables not found in the data:
- `T1_K`, `T2_I`, `T2_J`, `T2_K`, `U_I`, ...
This is an error, but only a warning is raised for compatibility reasons.
The following variables will be used:
- T1_I
- T1_J
- U1_I
- SCR
- VEC1
- VEC2
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 220)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figureVBQEIr.png]]
**** Experimental Settings on the Xeon E5 2630 v2
The tables in this section are for keeping track of what would be interesting to
measure and what was already measured.
***** Tracking Table Grid5000 Xeon E5 2630 v3
|----+--------------------+-------+-------+-------+-------+-------|
|    | Application        | RS    | RSL   | SIMA  | SIMP  | DLMT  |
|----+--------------------+-------+-------+-------+-------+-------|
|  1 | stencil3d          | DONE  |       |       |       | DONE  |
|  2 | dgemv3             | DONE  |       |       |       | DONE  |
|  3 | atax               | DONE  | DONE  | DONE  | DONE  | DONE  |
|  4 | gemver             |       |       |       |       |       |
|  5 | bicgkernel         |       |       |       |       |       |
|  6 | trmm               |       |       |       |       |       |
|  7 | adi                | DOING | DOING | DOING | DOING | DOING |
|  8 | gesummv            |       |       |       |       |       |
|  9 | mvt                |       |       |       |       |       |
| 10 | fdtd               |       |       |       |       |       |
| 11 | correlation        |       |       |       |       |       |
| 12 | lu                 |       |       |       |       |       |
| 13 | tensor-contraction |       |       |       |       |       |
| 14 | mm                 |       |       |       |       |       |
| 15 | jacobi             |       |       |       |       |       |
| 16 | covariance         |       |       |       |       |       |
| 17 | hessian            |       |       |       |       |       |
| 18 | seidel             |       |       |       |       |       |
|----+--------------------+-------+-------+-------+-------+-------|

****** DLMT:
- "Large" (8) number of ANOVA steps
- Model with linear terms
- Using complete model for prediction

******* Properties to measure:
Experiment size:
- "Large" (8) number of ANOVA steps
- "Small" (?) number of ANOVA steps

Model:
- Model with linear terms
- More complex models (?)

Prediction:
- Using complete model for prediction
- Using only relevant variables on model for prediction

******* Other concerns
- Size of initial sampled space (multipliers)
- Sampling again for prediction?
- Extra experiments due to runtime failures
*** [2018-09-05 Wed]
**** Application: atax (Latest, Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/atax/latest/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
dim(data[data$technique == "SIMA", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])
mean(data[data$technique == "SIMA", "points"])
data_mean
#+END_SRC

#+RESULTS:
#+begin_example
[1] 0 0
Warning message:
Unknown or uninitialised column: 'technique'.
[1] 0 0
Warning message:
Unknown or uninitialised column: 'technique'.
[1] 0 0
Warning message:
Unknown or uninitialised column: 'technique'.
Error: Column `points` not found
Error: Column `points` not found
Error: Column `points` not found
# A tibble: 0 x 0
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figuredKoeQd.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figure8pDX68.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)

target_dir <-"dlmt_spapt_experiments/data/atax/latest/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1, ]


O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_J, T1_K, T2_I, T2_J, T2_K,
                 U1_I, U_I, U_J, U_K, RT_I, RT_J, RT_K, .keep_all = TRUE)
#+END_SRC

#+RESULTS:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 500)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figurev1dId1.png]]
*** [2018-09-06 Thu]
**** Plotting DOPT Sampling Strategies
***** Generate Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)

sample_size <- 10
pre_sample_size <- 30 * sample_size

rs_data <- data.frame(x1 = sample(0:20, sample_size, replace = T),
                      x2 = sample(0:20, sample_size, replace = T))
rs_data$name <- rep("RS", nrow(rs_data))
data <- rs_data

lhs_data <- lhs.design(nruns = sample_size ,nfactors = 2, digits = 0, type = "maximin",
                       factor.names = list(x1 = c(0, 20), x2 = c(0, 20)))
lhs_data$name <- rep("LHS", nrow(lhs_data))
data <- bind_rows(data, lhs_data)

full_factorial <- gen.factorial(c(20, 20), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~., full_factorial, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("DOPT Linear", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(20, 20), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ I(1 / (10e-6 + x1)) + I(1 / (10e-6 + x2)), full_factorial, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("DOPT Inverse", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(20, 20), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ .^2, full_factorial, nTrials = sample_size)
dopti_data <- output$design
dopti_data$name <- rep("DOPT Interactions", nrow(dopti_data))
data <- bind_rows(data, dopti_data)

full_factorial <- gen.factorial(c(20, 20), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
#output <- optFederov(~ . + quad(.), full_factorial, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("DOPT Linear & Quadratic", nrow(doptq_data))
data <- bind_rows(data, doptq_data)

full_factorial <- gen.factorial(c(20, 20), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ . + I(x1 ^ 3) + I(x2 ^ 3), full_factorial, nTrials = sample_size)
#output <- optFederov(~ . + cubic(.), full_factorial, nTrials = sample_size)
doptc_data <- output$design
doptc_data$name <- rep("DOPT Linear & Cubic", nrow(doptc_data))
data <- bind_rows(data, doptc_data)

drs_data <- data.frame(x1 = sample(0:20, pre_sample_size, replace = T),
                       x2 = sample(0:20, pre_sample_size, replace = T))
output <- optFederov(~., drs_data, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("Large RS + DOPT Linear", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

drs_data <- data.frame(x1 = sample(0:20, pre_sample_size, replace = T),
                       x2 = sample(0:20, pre_sample_size, replace = T))
output <- optFederov(~ I(1 / (10e-6 + x1)) + I(1 / (10e-6 + x2)), drs_data, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("Large RS + DOPT Inverse", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

drs_data <- data.frame(x1 = sample(0:20, pre_sample_size, replace = T),
                       x2 = sample(0:20, pre_sample_size, replace = T))
output <- optFederov(~ . + .^2, drs_data, nTrials = sample_size)
dopti_data <- output$design
dopti_data$name <- rep("Large RS + DOPT Interactions", nrow(dopti_data))
data <- bind_rows(data, dopti_data)

drs_data <- data.frame(x1 = sample(0:20, pre_sample_size, replace = T),
                       x2 = sample(0:20, pre_sample_size, replace = T))
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), drs_data, nTrials = sample_size)
#output <- optFederov(~ . + quad(.), full_factorial, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("Large RS + DOPT Linear & Quadratic", nrow(doptq_data))
data <- bind_rows(data, doptq_data)

drs_data <- data.frame(x1 = sample(0:20, pre_sample_size, replace = T),
                       x2 = sample(0:20, pre_sample_size, replace = T))
output <- optFederov(~ . + I(x1 ^ 3) + I(x2 ^ 3), drs_data, nTrials = sample_size)
#output <- optFederov(~ . + cubic(.), full_factorial, nTrials = sample_size)
doptc_data <- output$design
doptc_data$name <- rep("Large RS + DOPT Linear & Cubic", nrow(doptc_data))
data <- bind_rows(data, doptc_data)

data$facet <- factor(data$name, levels = c("RS", "LHS", "DOPT Linear", "DOPT Interactions",
                                           "DOPT Inverse", "DOPT Linear & Quadratic",
                                           "DOPT Linear & Cubic", "Large RS + DOPT Linear",
                                           "Large RS + DOPT Interactions", "Large RS + DOPT Inverse",
                                           "Large RS + DOPT Linear & Quadratic",
                                           "Large RS + DOPT Linear & Cubic"))
#+END_SRC

#+RESULTS:

***** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = x1, y = x2)) +
    facet_wrap(facet ~ ., ncol = 4) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figurem6zZEk.png]]
*** [2018-09-10 Mon]
**** Tests with Expanding Designs
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

full_factorial <- gen.factorial(c(2,2,3,2,3), center = FALSE)
full_factorial$Y <- (2.3 * full_factorial$X1 ^ 2) + (1.4 * full_factorial$X2) +
                    (3.4 * full_factorial$X3 * full_factorial$X4) +
                    (2.1 * full_factorial$X5)

str(full_factorial)

output <- optFederov(~ I(X1 ^ 2) + X2 + X3 + X4 + X5, full_factorial, nTrials = 6)
output$design

design <- output$design
design[sample(seq(1, nrow(design)), 1), "Y"] <- NA
design

regression <- lm(Y ~ I(X1 ^ 2) + X2 + X3 + X4 + X5, data = design)
summary.aov(regression)

design <- design[complete.cases(design), ]
design

regression <- lm(Y ~ I(X1 ^ 2) + X2 + X3 + X4 + X5, data = design)
summary.aov(regression)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	72 obs. of  6 variables:
 $ X1: num  1 2 1 2 1 2 1 2 1 2 ...
 $ X2: num  1 1 2 2 1 1 2 2 1 1 ...
 $ X3: num  1 1 1 1 2 2 2 2 3 3 ...
 $ X4: num  1 1 1 1 1 1 1 1 1 1 ...
 $ X5: num  1 1 1 1 1 1 1 1 1 1 ...
 $ Y : num  9.2 16.1 10.6 17.5 12.6 19.5 14 20.9 16 22.9 ...
   X1 X2 X3 X4 X5    Y
4   2  2  1  1  1 17.5
9   1  1  3  1  1 16.0
13  1  1  1  2  1 12.6
49  1  1  1  1  3 13.4
70  2  1  3  2  3 37.3
71  1  2  3  2  3 31.8
   X1 X2 X3 X4 X5    Y
4   2  2  1  1  1 17.5
9   1  1  3  1  1 16.0
13  1  1  1  2  1 12.6
49  1  1  1  1  3 13.4
70  2  1  3  2  3   NA
71  1  2  3  2  3 31.8
            Df Sum Sq Mean Sq
I(X1^2)      1   0.72    0.72
X2           1 237.63  237.63
X3           1   6.00    6.00
X4           1   0.32    0.32
1 observation deleted due to missingness
   X1 X2 X3 X4 X5    Y
4   2  2  1  1  1 17.5
9   1  1  3  1  1 16.0
13  1  1  1  2  1 12.6
49  1  1  1  1  3 13.4
71  1  2  3  2  3 31.8
            Df Sum Sq Mean Sq
I(X1^2)      1   0.72    0.72
X2           1 237.63  237.63
X3           1   6.00    6.00
X4           1   0.32    0.32
#+end_example
*** [2018-09-12 Wed]
**** Plotting Applications (Xeon E5 2630 v3, Grid5000)
***** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
***** Atax Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/atax"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 80)

#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 48 x 5
# Groups:   application [?]
   application                                     technique points_max
   <chr>                                           <chr>          <dbl>
 1 atax_1_step_4x_lin_bin_quad_others              DLMT              21
 2 atax_1_step_4x_lin_bin_quad_others              RS                22
 3 atax_1_step_4x_lin_cub_quad                     DLMT              59
 4 atax_1_step_4x_lin_quad                         DLMT              42
 5 atax_1_step_4x_lin_quad                         RS                43
 6 atax_1_step_4x_lin_quad_no_extra                DLMT              33
 7 atax_1_step_4x_lin_quad_no_extra                RS                33
 8 atax_1_step_4x_lin_quad_no_extra_no_constraints DLMT              33
 9 atax_1_step_4x_lin_quad_no_extra_no_constraints RS                33
10 atax_1_step_4x_lin_quad_small                   DLMT              36
11 atax_12_steps_small_model                       DLMT             539
12 atax_12_steps_small_model_large_prf             DLMT             254
13 atax_12_steps_small_model_min_prf               DLMT             565
14 atax_4x_lin                                     DLMT             101
15 atax_4x_lin                                     RS               101
16 atax_4x_lin_cub_quad                            DLMT             164
17 atax_4x_lin_cub_quad                            RS               201
18 atax_expanded_lin                               DLMT             480
19 atax_expanded_lin                               RS               151
20 atax_expanded_lin                               RSL              150
21 atax_expanded_lin                               SIMA             151
22 atax_expanded_lin                               SIMP             150
23 atax_expanded_lin_quad_cub                      DLMT             213
24 atax_expanded_lin_quad_cub                      RS               151
25 atax_expanded_lin_quad_cub                      RSL              150
26 atax_expanded_lin_quad_cub                      SIMA             151
27 atax_expanded_lin_quad_cub                      SIMP             150
28 atax_lin_quad                                   DLMT             231
29 atax_lin_quad                                   RS               301
30 atax_lin_quad_8_steps                           DLMT             233
31 atax_lin_quad_cub                               DLMT              96
32 atax_lin_quad_cub                               RS               101
33 atax_lin_quad_cub                               RSL              100
34 atax_lin_quad_cub                               SIMA             101
35 atax_lin_quad_cub                               SIMP             101
36 atax_no_binary_cubic_quad                       DLMT             297
37 atax_no_binary_cubic_quad                       RS               301
38 atax_no_binary_cubic_quad                       SIMA             301
39 atax_no_binary_lin                              DLMT             168
40 atax_no_binary_lin                              RS               171
41 atax_no_binary_lin                              RSL              170
42 atax_no_binary_lin                              SIMA             171
43 atax_no_binary_lin                              SIMP             171
44 atax_no_binary_no_dlmt                          RS               171
45 atax_no_binary_no_dlmt                          RSL              170
46 atax_no_binary_no_dlmt                          SIMA             171
47 atax_no_binary_no_dlmt                          SIMP             171
48 atax_small_model_quad                           DLMT             120
   max_speedup experiments
         <dbl>       <int>
 1        2.06          10
 2        1.97          10
 3        2.09           5
 4        2.18          10
 5        2.17          10
 6        2.24          20
 7        2.20          20
 8        2.06          12
 9        2.18          10
10        2.19          10
11        2.04           8
12        2.04           9
13        2.05          10
14        2.20          10
15        2.20          10
16        2.22          10
17        2.22          10
18        2.05           8
19        2.03          10
20        2.17          10
21        2.13          10
22        2.02          10
23        2.14           9
24        2.03          10
25        2.17          10
26        2.13          10
27        2.02          10
28        2.07           4
29        2.04           6
30        2.05          10
31        2.10          30
32        2.05          30
33        2.09          30
34        2.11          30
35        2.05          30
36        1.91          18
37        1.89          10
38        1.71          10
39        2.05          50
40        1.89          40
41        1.87          30
42        1.66          41
43        1.88          30
44        1.74          10
45        1.86          10
46        1.64          10
47        1.89          10
48        2.08          27
#+end_example

******* Plotting
******** Using all variables for prediction
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_no_binary_no_dlmt",
                                          "atax_no_binary_lin",
                                          "atax_no_binary_cubic_quad",
                                          "atax_lin_quad",
                                          "atax_lin_quad_8_steps",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 3) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-7lYTEt/figurezA610X.png]]

******** Using only relevant variables for prediction
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_small_model_quad",
                                          "atax_12_steps_small_model",
                                          "atax_12_steps_small_model_large_prf",
                                          "atax_12_steps_small_model_min_prf",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 3) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureKcGzbd.png]]

******** Expanding the search space and problem size
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_expanded_lin",
                                          "atax_expanded_lin_quad_cub",
                                          "atax_4x_lin",
                                          "atax_4x_lin_cub_quad",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 3) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureicD4sn.png]]

******** Making a single sampling step
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_1_step_4x_lin_bin_quad_others",
                                          "atax_1_step_4x_lin_cub_quad",
                                          "atax_1_step_4x_lin_quad",
                                          "atax_1_step_4x_lin_quad_small",
                                          "atax_1_step_4x_lin_quad_no_extra",
                                          #"atax_1_step_4x_lin_quad_no_extra_no_constraints",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureNma9V0.png]]

******** All Experiments
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    #geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 5) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figurevloe9p.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/atax"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 48 x 6
# Groups:   application [?]
   application                                     technique baseline
   <chr>                                           <chr>        <dbl>
 1 atax_1_step_4x_lin_bin_quad_others              DLMT         3.08
 2 atax_1_step_4x_lin_bin_quad_others              RS           3.08
 3 atax_1_step_4x_lin_cub_quad                     DLMT         3.08
 4 atax_1_step_4x_lin_quad                         DLMT         3.07
 5 atax_1_step_4x_lin_quad                         RS           3.07
 6 atax_1_step_4x_lin_quad_no_extra                DLMT         3.09
 7 atax_1_step_4x_lin_quad_no_extra                RS           3.09
 8 atax_1_step_4x_lin_quad_no_extra_no_constraints DLMT         3.08
 9 atax_1_step_4x_lin_quad_no_extra_no_constraints RS           3.08
10 atax_1_step_4x_lin_quad_small                   DLMT         3.08
11 atax_12_steps_small_model                       DLMT         0.177
12 atax_12_steps_small_model_large_prf             DLMT         0.177
13 atax_12_steps_small_model_min_prf               DLMT         0.177
14 atax_4x_lin                                     DLMT         3.08
15 atax_4x_lin                                     RS           3.08
16 atax_4x_lin_cub_quad                            DLMT         3.08
17 atax_4x_lin_cub_quad                            RS           3.08
18 atax_expanded_lin                               DLMT         0.179
19 atax_expanded_lin                               RS           0.179
20 atax_expanded_lin                               RSL          0.179
21 atax_expanded_lin                               SIMA         0.179
22 atax_expanded_lin                               SIMP         0.179
23 atax_expanded_lin_quad_cub                      DLMT         0.178
24 atax_expanded_lin_quad_cub                      RS           0.178
25 atax_expanded_lin_quad_cub                      RSL          0.178
26 atax_expanded_lin_quad_cub                      SIMA         0.178
27 atax_expanded_lin_quad_cub                      SIMP         0.178
28 atax_lin_quad                                   DLMT         0.178
29 atax_lin_quad                                   RS           0.178
30 atax_lin_quad_8_steps                           DLMT         0.178
31 atax_lin_quad_cub                               DLMT         0.178
32 atax_lin_quad_cub                               RS           0.178
33 atax_lin_quad_cub                               RSL          0.178
34 atax_lin_quad_cub                               SIMA         0.178
35 atax_lin_quad_cub                               SIMP         0.178
36 atax_no_binary_cubic_quad                       DLMT         0.178
37 atax_no_binary_cubic_quad                       RS           0.178
38 atax_no_binary_cubic_quad                       SIMA         0.178
39 atax_no_binary_lin                              DLMT        NA
40 atax_no_binary_lin                              RS          NA
   max_cost_mean min_cost_mean experiments
           <dbl>         <dbl>       <int>
 1         8.73         1.50           176
 2        11.4          1.56           197
 3         8.59         1.47           259
 4        11.5          1.42           378
 5         9.12         1.42           366
 6        12.5          1.38           584
 7        11.0          1.40           567
 8        10.0          1.50           327
 9         9.88         1.41           285
10         9.04         1.41           322
11         0.505        0.0875        2560
12         0.510        0.0866        1513
13         0.483        0.0866        4314
14        13.2          1.40           865
15        13.5          1.41           868
16        12.9          1.39          1322
17        11.8          1.39          1767
18         0.710        0.0866        2867
19         0.727        0.0871        1245
20         0.783        0.0819        1269
21         0.682        0.0835        1433
22         0.889        0.0874        1371
23         0.675        0.0832        1519
24         0.727        0.0871        1245
25         0.783        0.0819        1269
26         0.682        0.0835        1433
27         0.889        0.0874        1371
28         0.434        0.0861         760
29         0.566        0.0865        1599
30         0.422        0.0874        1665
31         0.454        0.0848        2296
32         0.454        0.0863        2685
33         0.483        0.0852        2676
34         0.500        0.0846        2853
35         0.519        0.0868        2846
36         0.461        0.0930        3962
37         0.472        0.0942        2996
38         0.442        0.104         3009
39         0.556        0.0872        6212
40         0.491        0.0940        6412
# ... with 8 more rows
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 5) +
    geom_histogram(aes(cost_mean, fill = technique), binwidth = 0.05) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureJ8S6TN.png]]

***** Stencil3d Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/stencil3d"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)

#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 15 x 7
# Groups:   application [?]
   application                    technique points_mean points_min points_max
   <chr>                          <chr>           <dbl>      <dbl>      <dbl>
 1 stencil3d_3x_1_step_lin_quad   DLMT              55          55         55
 2 stencil3d_3x_1_step_lin_quad   RS                56          56         56
 3 stencil3d_3x_4_steps_lin       DLMT             133.        119        144
 4 stencil3d_3x_4_steps_lin       RS               201         201        201
 5 stencil3d_3x_lin_quad_cub      DLMT             276.        218        316
 6 stencil3d_3x_lin_quad_cub      RS               201         101        301
 7 stencil3d_8_steps_large_sample DLMT             332         304        383
 8 stencil3d_8_steps_small_sample DLMT             293.        176        344
 9 stencil3d_all_cubic_quad       DLMT             439.        339        517
10 stencil3d_all_cubic_quad       RS               300         300        300
11 stencil3d_lin_quad             DLMT             218.        194        266
12 stencil3d_lin_quad_8_steps     DLMT             349.        276        408
13 stencil3d_linear               DLMT             235.        198        265
14 stencil3d_linear               RS               300         300        300
15 stencil3d_small_model_min_prf  DLMT             546         544        548
   max_speedup experiments
         <dbl>       <int>
 1        2.31          10
 2        2.10          10
 3        2.45           7
 4        2.37          10
 5        2.43           9
 6        2.43          10
 7        1.18           6
 8        1.71          11
 9        2.62           9
10        1.37          10
11        1.35           5
12        3.25          11
13        2.85          10
14        1.37          10
15        1.12           2
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(0.5, 3.0)) +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figurel5jVrt.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/stencil3d"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 15 x 6
# Groups:   application [?]
   application                    technique baseline max_cost_mean min_cost_mean
   <chr>                          <chr>        <dbl>         <dbl>         <dbl>
 1 stencil3d_3x_1_step_lin_quad   DLMT        3.36          28.1          1.45
 2 stencil3d_3x_1_step_lin_quad   RS          3.36          33.1          1.60
 3 stencil3d_3x_4_steps_lin       DLMT        3.37          32.7          1.37
 4 stencil3d_3x_4_steps_lin       RS          3.37          32.7          1.42
 5 stencil3d_3x_lin_quad_cub      DLMT        3.35          33.5          1.38
 6 stencil3d_3x_lin_quad_cub      RS          3.35          33.2          1.38
 7 stencil3d_8_steps_large_sample DLMT        0.0525         0.381        0.0444
 8 stencil3d_8_steps_small_sample DLMT        0.0521         0.392        0.0301
 9 stencil3d_all_cubic_quad       DLMT       NA              0.412        0.0198
10 stencil3d_all_cubic_quad       RS         NA              0.437        0.0384
11 stencil3d_lin_quad             DLMT        0.0526         0.412        0.0385
12 stencil3d_lin_quad_8_steps     DLMT        0.0522         0.458        0.0161
13 stencil3d_linear               DLMT       NA              0.376        0.0183
14 stencil3d_linear               RS         NA              0.437        0.0384
15 stencil3d_small_model_min_prf  DLMT        0.0531         0.384        0.0473
   experiments
         <int>
 1         549
 2         555
 3         917
 4        2003
 5        2433
 6        1999
 7        1966
 8        3196
 9        3910
10        2965
11        1080
12        3792
13        2436
14        2965
15        1083
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 3) +
    geom_histogram(aes(cost_mean, fill = technique)) +#, binwidth = 0.05) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figure4vPMxv.png]]

***** Dgemv3 Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/dgemv3"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)

#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
5: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
6: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
7: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
8: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
9: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
# A tibble: 1 x 7
# Groups:   application [?]
  application                  technique points_mean points_min points_max
  <chr>                        <fct>           <dbl>      <dbl>      <dbl>
1 dgemv3_4_steps_lin_quad_true DLMT             228.        210        245
  max_speedup experiments
        <dbl>       <int>
1        1.72           2
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(0.5, 3.0)) +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figuretaoVFm.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/dgemv3"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
: # A tibble: 1 x 6
: # Groups:   application [?]
:   application                  technique baseline max_cost_mean min_cost_mean
:   <chr>                        <fct>        <dbl>         <dbl>         <dbl>
: 1 dgemv3_4_steps_lin_quad_true DLMT          2.59          6.55          1.51
:   experiments
:         <int>
: 1         455

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 3) +
    geom_histogram(aes(cost_mean, fill = technique)) +#, binwidth = 0.05) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figure0Uy7o3.png]]

***** Adi Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/adi"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)

#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 4 x 7
# Groups:   application [?]
  application                 technique points_mean points_min points_max
  <chr>                       <chr>           <dbl>      <dbl>      <dbl>
1 adi_3x_cubic_quad_big_model DLMT             420         420        420
2 adi1_cubic_quad_big_model   DLMT             320.        197        393
3 adi1_cubic_quad_big_model   RS               201         201        201
4 adi1_cubic_quad_big_model   SIMA             201         201        201
  max_speedup experiments
        <dbl>       <int>
1        1.01           1
2        1.01           5
3        1.00          10
4        1.01           5
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip(ylim = c(1.0, 3.0)) +
    coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureVrhR51.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/adi"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 4 x 6
# Groups:   application [?]
  application                 technique baseline max_cost_mean min_cost_mean
  <chr>                       <chr>        <dbl>         <dbl>         <dbl>
1 adi_3x_cubic_quad_big_model DLMT         4.41         15.8           4.36
2 adi1_cubic_quad_big_model   DLMT         0.273         0.825         0.271
3 adi1_cubic_quad_big_model   RS           0.273         1.31          0.271
4 adi1_cubic_quad_big_model   SIMA         0.273         1.23          0.271
  experiments
        <int>
1         281
2         660
3         879
4         464
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 3) +
    geom_histogram(aes(cost_mean, fill = technique)) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figurePa68p0.png]]

***** All Applications
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 8 x 7
# Groups:   application [?]
  application technique points_mean points_min points_max max_speedup
  <chr>       <chr>           <dbl>      <dbl>      <dbl>       <dbl>
1 adi4        DLMT             185.        104        295        1.01
2 atax        DLMT             176          89        213        2.14
3 atax        RS               151         151        151        2.03
4 atax        RSL              150         150        150        2.17
5 atax        SIMA             151         151        151        2.13
6 atax        SIMP             150         150        150        2.02
7 dgemv3      DLMT             334.        287        371        1.59
8 dgemv3      RS               360         360        360        1.64
  experiments
        <int>
1          10
2           9
3          10
4          10
5          10
6          10
7          10
8          10
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_y", ncol = 2) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureHrvTfV.png]]

****** Histogram of Explored Search Spaces
******* Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 8 x 6
# Groups:   application [?]
  application technique baseline max_cost_mean min_cost_mean experiments
  <chr>       <chr>        <dbl>         <dbl>         <dbl>       <int>
1 adi4        DLMT         1.10          1.90         1.09          1852
2 atax        DLMT         0.178         0.675        0.0832        1519
3 atax        RS           0.178         0.727        0.0871        1245
4 atax        RSL          0.178         0.783        0.0819        1269
5 atax        SIMA         0.178         0.682        0.0835        1433
6 atax        SIMP         0.178         0.889        0.0874        1371
7 dgemv3      DLMT        NA             0.340        0.0775        3428
8 dgemv3      RS          NA             0.344        0.0752        3610
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 2) +
    geom_histogram(aes(cost_mean, fill = technique), binwidth = 0.01) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figure5bja46.png]]
*** [2018-09-19 Wed]
**** Meeting with Arnaud and Brice
***** Topics on this Journal
-[[ DOE + Autotuning Summaries (Collected from previous entries)]]
- [[Plotting DOPT Sampling Strategies]]
- [[Interesting Papers on Autotuner Performance Comparison]]
- [[Plotting Applications (Xeon E5 2630 v3, Grid5000)]]
*** [2018-09-20 Thu]
**** Final Experimental Settings on the Xeon E5 2630 v2
After preliminary studies with some of the applications, I decided to fix some
of the parameters to explore and run all experiments at least once with fixed
settings. The settings for all experiments are the following:

***** Repetitions
I'll start with 10 repetitions for RS and DLMT, then complete for other search
algorithms.

***** DLMT Configuration
- 4 ANOVA steps
- ANOVA Prf threshold set to 0.05
- Use a new random sample for prediction
- Use only the variables identified in ANOVA in model for prediction
- Pick best point from failed regressions and skip model fitting
- Model for all applications: =Y ~ . + I(. ^ 2) + I(. ^ 3)=
- Quadratic and cubic terms exclude binary variables

***** SPAPT Configuration
- Add missing powers of two in all parameters
- Maximum available problem size
- Cost of a configuration is the mean of 10 runs
- Keep provided constraints

***** Budget
The DLMT budget is variable within the 4 ANOVA steps. This happens because
each regression is done on new data samples, and different samples could
be used to fix different numbers of variables at different times.

The RS budget was initially fixed to 300 points. Additional experiments
might have to be done depending on how many points DLMT uses for each
application.

***** Tracking Table Grid5000 Xeon E5 2630 v3
|----+--------------------+--------------+-----+------+------+--------------|
|    | Application        | RS           | RSL | SIMA | SIMP | DLMT         |
|----+--------------------+--------------+-----+------+------+--------------|
|  1 | stencil3d          | DONE         |     |      |      | DONE         |
|  2 | dgemv3             | DONE         |     |      |      | DONE         |
|  3 | atax               | DONE         |     |      |      | DONE         |
|  4 | gemver             | DONE         |     |      |      | DONE         |
|  5 | bicgkernel         | DONE         |     |      |      | DONE         |
|  6 | trmm               | DONE         |     |      |      | DONE         |
|  7 | adi                | DOING (0/10) |     |      |      | DONE         |
|  8 | gesummv            | DONE         |     |      |      | DONE         |
|  9 | mvt                | DONE         |     |      |      | DONE         |
| 10 | fdtd               | PREP         |     |      |      | PREP         |
| 11 | correlation        | DONE         |     |      |      | DONE         |
| 12 | lu                 | DOING (0/10) |     |      |      | DONE         |
| 13 | tensor-contraction | DONE         |     |      |      | DOING (0/10) |
| 14 | mm                 | DONE         |     |      |      | DONE         |
| 15 | jacobi             | DONE         |     |      |      | DONE         |
| 16 | covariance         | FAIL         |     |      |      | FAIL         |
| 17 | hessian            | DONE         |     |      |      | DONE         |
| 18 | seidel             | DOING        |     |      |      | DOING (6/10) |
|----+--------------------+--------------+-----+------+------+--------------|

**** Plotting Applications (Xeon E5 2630 v3, Grid5000)
***** Speedup
This entry contains plots of the speedups achieved by DLMT
and other algorithms.

****** Loading Data
Run this before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]

        target_data <- lapply(csv_files, read.csv)
        target_data <- bind_rows(target_data)

        target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
        target_data$application <- rep(target_dir, nrow(target_data))
        target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))
        target_data$mean_points <- rep(mean(target_data$points), nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:plyrâ€™:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 28 x 7
# Groups:   application [?]
   application technique points_mean points_min points_max max_speedup
   <chr>       <chr>           <dbl>      <dbl>      <dbl>       <dbl>
 1 adi         DLMT            377.         340        417        1.42
 2 atax        DLMT            186.         115        221        2.18
 3 atax        RS              301          301        301        2.22
 4 bicgkernel  DLMT            120.         102        139       13.5
 5 bicgkernel  RS              301          301        301       12.7
 6 correlation DLMT            205.         168        235        1.01
 7 correlation RS              301          301        301        1.02
 8 dgemv3      DLMT            301.         219        414        2.15
 9 dgemv3      RS              391          301        401        2.21
10 gemver      DLMT            150.          76        225        3.83
11 gemver      RS              301          301        301        3.85
12 gesummv     DLMT            133          133        133        2.83
13 gesummv     RS              301          301        301        3.07
14 hessian     DLMT            105          105        105        1.07
15 hessian     RS              301          301        301        1.07
16 jacobi      DLMT            102           74        135        2.35
17 jacobi      RS              301          301        301        1.87
18 lu          DLMT             43           43         43       12.7
19 mm          DLMT            101.          73        126       20.4
20 mm          RS              294.         226        301       19.4
21 mvt         DLMT             83.1         51        105       11.2
22 mvt         RS              301          301        301       10.0
23 seidel      DLMT            149.          68        188       50.9
24 stencil3d   DLMT            233.         153        292        2.40
25 stencil3d   RS              251          251        251        2.45
26 tensor      RS              301          301        301       25.9
27 trmm        DLMT            215.          81        285       15.1
28 trmm        RS              301          301        301       14.4
   experiments
         <int>
 1          10
 2          10
 3          10
 4          10
 5          10
 6          11
 7          10
 8          11
 9          10
10          10
11          10
12          10
13          10
14          10
15          10
16          18
17          18
18          10
19          10
20          10
21          10
22          10
23           6
24          10
25          10
26          10
27          10
28          10
#+end_example

****** Boxplots
I chose to represent the speedups of each of the 10 runs for DLMT and the other
algorithms using boxplots. The vertical dotted lines mark the best speedups
achieved overall for each application.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1920 :height 1080
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip(ylim = c(1.0, 16.0)) +
    coord_flip() +
    #geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 35)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-WbWcC3/figureMdBZJL.png]]

****** Speedup & Budget
******* Setup
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(reshape2)

plot_data <- data[data$technique == "DLMT", c("points", "speedup", "application")]
plot_data <- plot_data %>%
             group_by(application) %>%
             summarize(mean_points = mean(points),
                       mean_speedup = mean(speedup))

random_data <- data[data$technique == "RS", c("points", "speedup", "application")]
random_data <- random_data %>%
               group_by(application) %>%
               summarize(mean_points = mean(points),
                         mean_speedup = mean(speedup))

plot_data <- plot_data[plot_data$application %in% random_data$application, ]
random_data <- random_data[random_data$application %in% plot_data$application, ]

plot_data$mean_speedup_ratio_RS <- plot_data$mean_speedup / random_data$mean_speedup
plot_data$mean_budget_ratio_RS <- plot_data$mean_points / random_data$mean_points

plot_data <- plot_data[, c("application", "mean_speedup_ratio_RS", "mean_budget_ratio_RS")]
plot_data <- melt(plot_data)
plot_data
#+END_SRC

#+RESULTS:
#+begin_example
Using application as id variables
   application              variable     value
1         atax mean_speedup_ratio_RS 0.9612893
2   bicgkernel mean_speedup_ratio_RS 1.0988750
3  correlation mean_speedup_ratio_RS 0.9997396
4       dgemv3 mean_speedup_ratio_RS 1.0106460
5       gemver mean_speedup_ratio_RS 1.0205835
6      gesummv mean_speedup_ratio_RS 0.9778234
7      hessian mean_speedup_ratio_RS 0.9981345
8       jacobi mean_speedup_ratio_RS 1.0215033
9           mm mean_speedup_ratio_RS 1.0030311
10         mvt mean_speedup_ratio_RS 0.9787649
11   stencil3d mean_speedup_ratio_RS 1.0115290
12        trmm mean_speedup_ratio_RS 1.1383463
13        atax  mean_budget_ratio_RS 0.6186047
14  bicgkernel  mean_budget_ratio_RS 0.3980066
15 correlation  mean_budget_ratio_RS 0.6822712
16      dgemv3  mean_budget_ratio_RS 0.7709835
17      gemver  mean_budget_ratio_RS 0.4990033
18     gesummv  mean_budget_ratio_RS 0.4418605
19     hessian  mean_budget_ratio_RS 0.3488372
20      jacobi  mean_budget_ratio_RS 0.3388704
21          mm  mean_budget_ratio_RS 0.3444634
22         mvt  mean_budget_ratio_RS 0.2760797
23   stencil3d  mean_budget_ratio_RS 0.9294821
24        trmm  mean_budget_ratio_RS 0.7139535
#+end_example
******* Plot
This plot shows the ratio between the maximum number of points
used by the DLMT and RS, and the ration between the maximum speedups
achieved by DLMT and RS, for all applications.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1920 :height 1080
#+BEGIN_SRC R
ggplot(plot_data, aes(application, value, fill = variable)) +
    #facet_grid(variable ~ .) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_hline(aes(yintercept = 1.0), color = "black", linetype = 8) +
    scale_y_continuous(breaks = seq(0, 2, by = 0.2)) +
    #coord_flip() +
    ggtitle("") +
    theme_bw(base_size = 38) +
    theme(legend.position = "top", legend.direction = "horizontal")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-WbWcC3/figuredy8F5n.png]]

***** Search Spaces
This entry contains histograms of the search spaces explored by each
search algorithm.

****** Load Data
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]

        target_data <- lapply(csv_files, read.csv)
        target_data <- bind_rows(target_data)

        target_data <- target_data[target_data$correct_result == "True", ]
        target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

        target_data <- target_data[, c("cost_mean", "technique", "baseline")]

        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 28 x 6
# Groups:   application [?]
   application technique baseline max_cost_mean min_cost_mean experiments
   <chr>       <chr>        <dbl>         <dbl>         <dbl>       <int>
 1 adi         DLMT         4.41         22.7          3.11          3758
 2 atax        DLMT         3.08         11.5          1.41          1674
 3 atax        RS           3.08         12.5          1.39          2603
 4 bicgkernel  DLMT         5.91         11.0          0.437         1173
 5 bicgkernel  RS           5.91         12.3          0.465         2993
 6 correlation DLMT         7.02          7.18         6.94          2259
 7 correlation RS           7.02          7.20         6.94          3010
 8 dgemv3      DLMT         2.59          8.20         1.33          3286
 9 dgemv3      RS           2.59         11.1          1.39          3875
10 gemver      DLMT         6.43         29.3          1.68          1426
11 gemver      RS           6.43         31.9          1.67          3000
12 gesummv     DLMT         2.52          8.90         0.892          524
13 gesummv     RS           2.52          9.00         0.818         1162
14 hessian     DLMT         0.204         1.38         0.190          532
15 hessian     RS           0.204         3.31         0.192         1479
16 jacobi      DLMT         0.204         1.51         0.0409        1355
17 jacobi      RS           0.204         3.31         0.0515        4035
18 lu          DLMT         1.93         76.1          0.152          237
19 mm          DLMT         1.47          6.94         0.0720        1011
20 mm          RS           1.47          7.20         0.0759        2935
21 mvt         DLMT         0.134         0.428        0.0138         831
22 mvt         RS           0.134         0.505        0.0134        3010
23 seidel      DLMT         2.90         20.4          0.0569         822
24 stencil3d   DLMT         3.37         31.8          1.40          2307
25 stencil3d   RS           3.37         32.8          1.37          2500
26 tensor      RS           4.76         19.9          0.184         3010
27 trmm        DLMT         1.42         13.9          0.0942        1932
28 trmm        RS           1.42         14.7          0.0985        2722
#+end_example

****** Histograms
The plots in this section show the mean over 10 executions of the cost of all
configurations evaluated during search. The vertical dotted line marks the mean
cost over 10 executions of the -O3 baseline. Search algorithms are coded by
color.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1920 :height 1080
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ application, scales = "free") +
    #geom_histogram(aes(y = ..density.., x = cost_mean, fill = technique)) +
    geom_histogram(aes(cost_mean, fill = technique)) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 25) +
    theme(legend.position = "none")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-WbWcC3/figuremZIT9y.png]]
***** Speedup for Half of the Random Samples
This entry contains plots of the speedups achieved by DLMT
and other algorithms.

****** Load Data
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.parse <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE)

    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <-  data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))

    if (data[1, "technique"] == "RS") {
        data <- data[seq(1, nrow(data) / 1.5), ]
    }

    data <- data[data$baseline == "False", ]

    data$points <- rep(nrow(data), nrow(data))

    data <- data[data$correct_result == "True", ]

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]

        target_data <- lapply(csv_files, read.csv.parse)
        target_data <- bind_rows(target_data)

        target_data <- target_data[, c("cost_mean", "technique", "cost_baseline", "speedup", "max_run_speedup", "points")]

        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

plot_data <- data[data$technique == "DLMT", c("points", "max_run_speedup", "application")]
plot_data <- plot_data %>%
             group_by(application) %>%
             summarize(mean_points = mean(points),
                       mean_speedup = mean(max_run_speedup))

random_data <- data[data$technique == "RS", c("points", "max_run_speedup", "application")]
random_data <- random_data %>%
               group_by(application) %>%
               summarize(mean_points = mean(points),
                         mean_speedup = mean(max_run_speedup))

plot_data <- plot_data[plot_data$application %in% random_data$application, ]
random_data <- random_data[random_data$application %in% plot_data$application, ]

plot_data$mean_speedup_ratio_RS <- plot_data$mean_speedup / random_data$mean_speedup
plot_data$mean_budget_ratio_RS <- plot_data$mean_points / random_data$mean_points

plot_data <- plot_data[, c("application", "mean_speedup_ratio_RS", "mean_budget_ratio_RS")]
plot_data <- melt(plot_data)
plot_data

#print(summarise(baseline = max(cost_baseline), mean_speedup = mean(max_run_speedup),
#                min_cost_mean = min(cost_mean), experiments = n(),
#                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
Using application as id variables
   application              variable     value
1         atax mean_speedup_ratio_RS 0.9635847
2   bicgkernel mean_speedup_ratio_RS 1.0994281
3  correlation mean_speedup_ratio_RS 0.9996541
4       dgemv3 mean_speedup_ratio_RS 1.0125722
5       gemver mean_speedup_ratio_RS 1.0279472
6      gesummv mean_speedup_ratio_RS 0.9736536
7      hessian mean_speedup_ratio_RS 0.9974503
8       jacobi mean_speedup_ratio_RS 1.0170016
9           mm mean_speedup_ratio_RS 1.0105306
10         mvt mean_speedup_ratio_RS 0.9750377
11   stencil3d mean_speedup_ratio_RS 1.0094657
12        trmm mean_speedup_ratio_RS 1.1493333
13        atax  mean_budget_ratio_RS 0.9622656
14  bicgkernel  mean_budget_ratio_RS 0.5987919
15 correlation  mean_budget_ratio_RS 1.0360854
16      dgemv3  mean_budget_ratio_RS 1.1973344
17      gemver  mean_budget_ratio_RS 0.8153001
18     gesummv  mean_budget_ratio_RS 0.6600000
19     hessian  mean_budget_ratio_RS 0.5200000
20      jacobi  mean_budget_ratio_RS 0.5126776
21          mm  mean_budget_ratio_RS 0.5259027
22         mvt  mean_budget_ratio_RS 0.4298234
23   stencil3d  mean_budget_ratio_RS 1.4431581
24        trmm  mean_budget_ratio_RS 1.1431556
#+end_example

****** Speedup & Budget
******* Plot
This plot shows the ratio between the maximum number of points
used by the DLMT and RS, and the ration between the maximum speedups
achieved by DLMT and RS, for all applications.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(plot_data, aes(application, value, fill = variable)) +
    #facet_grid(variable ~ .) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_hline(aes(yintercept = 1.0), color = "black", linetype = 8) +
    scale_y_continuous(breaks = seq(0, 2, by = 0.2)) +
    #coord_flip() +
    ggtitle("") +
    theme_bw(base_size = 18) +
    theme(legend.position = "top", legend.direction = "horizontal")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-WbWcC3/figurelKGbCc.png]]
