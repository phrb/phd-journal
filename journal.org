# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro H R Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[margin=2cm]{geometry}
#+LATEX_HEADER: \usepackage{sourcecodepro}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
# #+LATEX_HEADER:   escapeinside={\%*}{*)},
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   language=R,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

* Setup
** Julia
#+NAME: install_julia_deps
#+HEADER: :results output :session *julia*
#+BEGIN_SRC julia
Pkg.add("Plots")
Pkg.add("Lint")
Pkg.add("Gadfly")
Pkg.add("ProfileView")
Pkg.add("CSV")
Pkg.add("StatsBase")
Pkg.add("StatsModels")
Pkg.add("GLM")
Pkg.add("RDatasets")
Pkg.add("IterTools")
Pkg.add("Missings")
Pkg.add("RCall")
Pkg.add("DataFrames")
#+END_SRC

#+RESULTS: install_julia_deps
#+begin_example
INFO: Package Plots is already installed
INFO: Package Lint is already installed
INFO: Package Gadfly is already installed
INFO: Cloning cache of Gtk from https://github.com/JuliaGraphics/Gtk.jl.git
INFO: Cloning cache of GtkReactive from https://github.com/JuliaGizmos/GtkReactive.jl.git
INFO: Cloning cache of IntervalSets from https://github.com/JuliaMath/IntervalSets.jl.git
INFO: Cloning cache of ProfileView from https://github.com/timholy/ProfileView.jl.git
INFO: Cloning cache of Reactive from https://github.com/JuliaGizmos/Reactive.jl.git
INFO: Cloning cache of RoundingIntegers from https://github.com/JuliaMath/RoundingIntegers.jl.git
INFO: Installing Gtk v0.13.1
INFO: Installing GtkReactive v0.4.0
INFO: Installing IntervalSets v0.1.1
INFO: Installing ProfileView v0.3.0
INFO: Installing Reactive v0.6.0
INFO: Installing RoundingIntegers v0.0.3
INFO: Building Cairo
INFO: Building Gtk
INFO: Package database updated
INFO: Package CSV is already installed
INFO: Package StatsBase is already installed
INFO: Package StatsModels is already installed
INFO: Package GLM is already installed
INFO: Package RDatasets is already installed
#+end_example

#+NAME: update_julia_pkg
#+HEADER:  :results output :session *julia*
#+BEGIN_SRC julia
Pkg.update()
#+END_SRC

#+RESULTS: update_julia_pkg
: INFO: Updating METADATA...
: WARNING: Package ASTInterpreter: skipping update (dirty)...
: INFO: Updating Gallium master...
: INFO: Computing changes...
: INFO: No packages to install, update or remove

*** NODAL Development                                          :Code:NODAL:
**** Installing NODAL in Julia Nightly
[[https://github.com/phrb/NODAL.jl][NODAL]] is the autotuning library I am developing in the [[https://julialang.org][Julia]]
language. The idea is to provide tools for the implementation of
parallel and distributed autotuners for various problem domains.
***** Download Julia Nightly
****** [[https://julialang.org/downloads][Download Generic Binary]]
****** Downloading from the CLI
You can run the following to install the latest *Julia* version:
#+BEGIN_SRC bash
cd ~ && mkdir .bin && cd .bin
wget https://julialangnightlies-s3.julialang.org/bin/linux/x64/julia-latest-linux64.tar.gz
tar xvf julia-latest-linux64.tar.gz
mv julia-* julia
rm julia-latest-linux64.tar.gz
#+END_SRC
This will put the *Julia* binary at =~/.bin/julia/bin/julia=.
You can use it like that or add an =alias= to your shell.
***** Installing the unregistered version
This will not be needed after registering NODAL to METADATA.
****** [[https://docs.julialang.org/en/latest/manual/packages/#Installing-Unregistered-Packages-1][Documentation]]
****** Julia Commands
#+BEGIN_SRC julia
Pkg.clone("https://github.com/phrb/NODAL.jl")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
***** Installing from the Julia package manager
****** Julia commands
#+BEGIN_SRC julia
Pkg.add("NODAL")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
**** Setting up a new Release
***** Using Attobot
[[https://github.com/attobot][Attobot]] integrates with *GitHub* to automatically register a new package
or a package version to *Julia*'s =METADATA= package repository.  Attobot
only needs a new *GitHub* release to work.
***** Using *Julia*'s =PkgDev=
Check the [[https://docs.julialang.org/en/latest/manual/packages/#Tagging-and-Publishing-Your-Package-1][documentation]] to learn how to register and publish user
packages to =METADATA=.
**** Development Workflow
The process of fixing an [[https://github.com/phrb/NODAL.jl/issues][issue]] or submitting a new
feature is:
0. Fork [[https://github.com/phrb/NODAL.jl][NODAL on GitHub]]

   You will need a GitHub account for this.

1. Make sure you have the latest version
   #+BEGIN_SRC bash
git checkout master
git fetch
   #+END_SRC

   New branches must be made from the =dev= branch:
   #+BEGIN_SRC bash
git checkout dev
   #+END_SRC
2. Checkout a new branch
   #+BEGIN_SRC bash
git checkout -b fix-or-feature
   #+END_SRC
3. Write code and commit to your new branch

   Make sure you write short and descriptive commit
   messages. Something similar to [[https://udacity.github.io/git-styleguide/][Udacity's guidelines]] is preferred
   but not strictly necessary.

4. Open a [[https://github.com/phrb/NODAL.jl/pulls][pull request]] to the =dev= bran

** R
Installing *R* dependencies:
#+NAME: install_r_deps
#+HEADER: :results output :exports both :session *R*
#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",
                 "plotly", "rPref", "pracma", "FrF2", "AlgDesign",
                 "quantreg"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

#+RESULTS: install_r_deps
#+begin_example
Installing packages into â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™
(as â€˜libâ€™ is unspecified)
trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/ggplot2_2.2.1.tar.gz'
Content type 'application/x-gzip' length 2213308 bytes (2.1 MB)
==================================================
downloaded 2.1 MB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/dplyr_0.7.4.tar.gz'
Content type 'application/x-gzip' length 808054 bytes (789 KB)
==================================================
downloaded 789 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/tidyr_0.8.0.tar.gz'
Content type 'application/x-gzip' length 377417 bytes (368 KB)
==================================================
downloaded 368 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rjson_0.2.18.tar.gz'
Content type 'application/x-gzip' length 99478 bytes (97 KB)
==================================================
downloaded 97 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/GGally_1.3.2.tar.gz'
Content type 'application/x-gzip' length 1031885 bytes (1007 KB)
==================================================
downloaded 1007 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/plotly_4.7.1.tar.gz'
Content type 'application/x-gzip' length 1034951 bytes (1010 KB)
==================================================
downloaded 1010 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rPref_1.2.tar.gz'
Content type 'application/x-gzip' length 99297 bytes (96 KB)
==================================================
downloaded 96 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/pracma_2.1.4.tar.gz'
Content type 'application/x-gzip' length 382113 bytes (373 KB)
==================================================
downloaded 373 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/FrF2_1.7-2.tar.gz'
Content type 'application/x-gzip' length 282582 bytes (275 KB)
==================================================
downloaded 275 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/AlgDesign_1.1-7.3.tar.gz'
Content type 'application/x-gzip' length 514391 bytes (502 KB)
==================================================
downloaded 502 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/quantreg_5.35.tar.gz'
Content type 'application/x-gzip' length 1640297 bytes (1.6 MB)
==================================================
downloaded 1.6 MB

,* installing *source* package â€˜ggplot2â€™ ...
,** package â€˜ggplot2â€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (ggplot2)
ERROR: failed to lock directory â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™ for modifying
Try removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/00LOCK-dplyrâ€™
,* installing *source* package â€˜rjsonâ€™ ...
,** package â€˜rjsonâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c dump.cpp -o dump.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c parser.c -o parser.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c register.c -o register.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rjson.so dump.o parser.o register.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rjson/libs
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (rjson)
,* installing *source* package â€˜pracmaâ€™ ...
,** package â€˜pracmaâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** byte-compile and prepare package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (pracma)
,* installing *source* package â€˜FrF2â€™ ...
,** package â€˜FrF2â€™ successfully unpacked and MD5 sums checked
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (FrF2)
,* installing *source* package â€˜AlgDesignâ€™ ...
,** package â€˜AlgDesignâ€™ successfully unpacked and MD5 sums checked
,** libs
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c FederovOpt.c -o FederovOpt.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c OptBlock.c -o OptBlock.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c Utility.c -o Utility.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o AlgDesign.so FederovOpt.o OptBlock.o Utility.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/AlgDesign/libs
,** R
,** data
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (AlgDesign)
,* installing *source* package â€˜quantregâ€™ ...
,** package â€˜quantregâ€™ successfully unpacked and MD5 sums checked
,** libs
gfortran   -fpic  -g -O2  -c akj.f -o akj.o
gfortran   -fpic  -g -O2  -c boot.f -o boot.o
gfortran   -fpic  -g -O2  -c bound.f -o bound.o
gfortran   -fpic  -g -O2  -c boundc.f -o boundc.o
gfortran   -fpic  -g -O2  -c brute.f -o brute.o
gfortran   -fpic  -g -O2  -c chlfct.f -o chlfct.o
gfortran   -fpic  -g -O2  -c cholesky.f -o cholesky.o
gfortran   -fpic  -g -O2  -c combos.f -o combos.o
gfortran   -fpic  -g -O2  -c crq.f -o crq.o
gfortran   -fpic  -g -O2  -c crqfnb.f -o crqfnb.o
gfortran   -fpic  -g -O2  -c dsel05.f -o dsel05.o
gfortran   -fpic  -g -O2  -c etime.f -o etime.o
gfortran   -fpic  -g -O2  -c extract.f -o extract.o
gfortran   -fpic  -g -O2  -c idmin.f -o idmin.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c init.c -o init.o
gfortran   -fpic  -g -O2  -c iswap.f -o iswap.o
gfortran   -fpic  -g -O2  -c kuantile.f -o kuantile.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c mcmb.c -o mcmb.o
gfortran   -fpic  -g -O2  -c penalty.f -o penalty.o
gfortran   -fpic  -g -O2  -c powell.f -o powell.o
gfortran   -fpic  -g -O2  -c rls.f -o rls.o
gfortran   -fpic  -g -O2  -c rq0.f -o rq0.o
gfortran   -fpic  -g -O2  -c rq1.f -o rq1.o
gfortran   -fpic  -g -O2  -c rqbr.f -o rqbr.o
gfortran   -fpic  -g -O2  -c rqfn.f -o rqfn.o
gfortran   -fpic  -g -O2  -c rqfnb.f -o rqfnb.o
gfortran   -fpic  -g -O2  -c rqfnc.f -o rqfnc.o
gfortran   -fpic  -g -O2  -c rqs.f -o rqs.o
gfortran   -fpic  -g -O2  -c sparskit2.f -o sparskit2.o
gfortran   -fpic  -g -O2  -c srqfn.f -o srqfn.o
gfortran   -fpic  -g -O2  -c srqfnc.f -o srqfnc.o
gfortran   -fpic  -g -O2  -c srtpai.f -o srtpai.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o quantreg.so akj.o boot.o bound.o boundc.o brute.o chlfct.o cholesky.o combos.o crq.o crqfnb.o dsel05.o etime.o extract.o idmin.o init.o iswap.o kuantile.o mcmb.o penalty.o powell.o rls.o rq0.o rq1.o rqbr.o rqfn.o rqfnb.o rqfnc.o rqs.o sparskit2.o srqfn.o srqfnc.o srtpai.o -llapack -lblas -lgfortran -lm -lquadmath -lgfortran -lm -lquadmath -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/quantreg/libs
,** R
,** data
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (quantreg)
,* installing *source* package â€˜tidyrâ€™ ...
,** package â€˜tidyrâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c RcppExports.cpp -o RcppExports.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c fill.cpp -o fill.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c melt.cpp -o melt.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c simplifyPieces.cpp -o simplifyPieces.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o tidyr.so RcppExports.o fill.o melt.o simplifyPieces.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/tidyr/libs
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,*** copying figures
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (tidyr)
,* installing *source* package â€˜GGallyâ€™ ...
,** package â€˜GGallyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (GGally)
,* installing *source* package â€˜rPrefâ€™ ...
,** package â€˜rPrefâ€™ successfully unpacked and MD5 sums checked
,** libs
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c RcppExports.cpp -o RcppExports.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c bnl.cpp -o bnl.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c hasse.cpp -o hasse.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c pref-classes.cpp -o pref-classes.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par-top.cpp -o psel-par-top.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par.cpp -o psel-par.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c scalagon.cpp -o scalagon.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c topk_setting.cpp -o topk_setting.o
g++ -std=gnu++11 -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rPref.so RcppExports.o bnl.o hasse.o pref-classes.o psel-par-top.o psel-par.o scalagon.o topk_setting.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPref/libs
,** R
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜rPrefâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* installing *source* package â€˜plotlyâ€™ ...
,** package â€˜plotlyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜plotlyâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™

The downloaded source packages are in
	â€˜/tmp/RtmpivSTVC/downloaded_packagesâ€™
Warning messages:
1: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜dplyrâ€™ had non-zero exit status
2: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜rPrefâ€™ had non-zero exit status
3: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜plotlyâ€™ had non-zero exit status
#+end_example

** Modifying & Analysing the FPGA Data Set
Cloning and updating the =legup-tuner= repository:

#+NAME: update_legup_tuner
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/legup-tuner.git || (cd legup-tuner && git pull)
#+END_SRC

Export your path to =repository_dir= variable:

#+name: repository_dir
#+begin_src sh :results output :exports both
pwd | tr -d "\n"
#+end_src

** Updating & Cloning Repositories
*** GPU Autotuning Screening Experiment
#+NAME: update_screening_experiment
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/autotuning_screening_experiment.git || (cd autotuning_screening_experiment && git pull)
#+END_SRC
* 2018
** May
*** [2018-05-02 Wed]
**** Summarizing the D-Optimal + ANOVA Strategy for Steven's Experiments
1. Use ~optFederov~ to find 24 experiments for the full model:

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        vector_length + lws_y + 1 / lws_y +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

2. Use ~aov~ to fit the full model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       vector_length + lws_y + 1 / lws_y +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}

4. Identify the most significant factors from the ANOVA summary. In this
   case, they are $vector_length$ and $lws_y$.
5. Use the fitted model to predict the best $time_per_pixel$ value in the
   entire dataset
6. Prune the dataset using the predicted best values for $vector_length$ and $lws_y$
7. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
   than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

8. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}
9. Identify the most significant factors from the ANOVA summary. In this
   case, they are $y_component_number$ and $threads_number$.
10. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
11. Prune the dataset using the predicted best values for $y_component_number$ and
    $threads_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size +
        elements_number + 1 / elements_number
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size +
                       elements_number + 1 / elements_number
\end{equation}
14. Identify the most significant factors from the ANOVA summary. In this
    case, it is $elements_number$
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Prune the dataset using the predicted best values for $elements_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size
\end{equation}
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Compare the predicted $time_per_pixel$ with the global optimum
*** [2018-05-03 Thu]
**** Summarizing Experiments
Make sure you have the data:

#+NAME: update_dopt_aov_experiments
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_anova_experiments.git || (cd dopt_anova_experiments && git pull)
#+END_SRC

#+RESULTS: update_dopt_aov_experiments
: Already up to date.

This [[file:./dopt_anova_experiments/org/report.pdf][file]] contains the report.
*** [2018-05-04 Fri]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)

A = data.frame(x = 1:100,
               y = 1:100,
               z = 1:100)

A$Y = rnorm(n = 100, mean = A$x, sd = 0.4 * A$x)

big_model = lm(Y ~ x + y + z, data = A)
small_model = lm(Y ~ x, data = A)

# A_big_predict = cbind(A, predict(big_model, interval = "confidence"))
# A_small_predict = cbind(A, predict(small_model, interval = "confidence"))
#
# p_small_x <- ggplot(A_small_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_line(aes(x, fit), alpha = 0.8, color = "red1", size = 1)
#
# p_small_y <- ggplot(A_small_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_small_z <- ggplot(A_small_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_x <- ggplot(A_big_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(x, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_y <- ggplot(A_big_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_z <- ggplot(A_big_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)

par(mfrow = c(2, 5))
plot(small_model, which = c(1, 2, 3, 4, 5))
plot(big_model, which = c(1, 2, 3, 4, 5))

# grid.arrange(p_small_x, p_small_y, p_small_z,
#              p_big_x, p_big_y, p_big_z, nrow = 2)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-3203rQF/figure3203Li2.png]]
*** [2018-05-07 Mon]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)
library(AlgDesign)

read_file <- "dopt_anova_experiments/data/search_space.csv"

results <- read.csv(read_file, strip.white = T, header = T)

big_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                 vector_length + lws_y + I(1 / lws_y) +
                                 load_overlap + temporary_size +
                                 elements_number + I(1 / elements_number) +
                                 threads_number + I(1 / threads_number),
                data = results)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length,
                data = results)

bm_predict = data.frame(time_per_pixel = predict(big_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

sm_predict = data.frame(time_per_pixel = predict(small_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_min[1, ]

global_min = results[results$time_per_pixel == min(results$time_per_pixel),
                     c("time_per_pixel", "y_component_number",
                       "vector_length")]

ggplot(results) +
    aes(x = y_component_number, y = time_per_pixel) +
    theme_bw() +
    geom_point(alpha = 0.1) +
    theme(legend.position = "top") +
    geom_point(color= "green", data = bm_min, size = 3, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "red", data = sm_min, size = 4, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "blue", data = global_min, size = 5, alpha = 0.5,
               aes(x = y_component_number, y = time_per_pixel)) +
    theme(axis.text = element_text(size = 12),
          axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-27096ysq/figure27096DcA.png]]
*** [2018-05-09 Wed]
**** Plotting Predicted Values During Experiment
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 720 :height 1280
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(ggplot2)
library(gridExtra)

generate_model_plot <- function(big_model, small_model, results, full_data, metric) {
    bm_predict = data.frame(response = predict(big_model, results),
                            variable = results[metric])

    names(bm_predict)[names(bm_predict) == "response"] <- "time_per_pixel"
    names(bm_predict)[names(bm_predict) == "variable"] <- metric

    sm_predict = data.frame(response = predict(small_model, results),
                            variable = results[metric])

    names(sm_predict)[names(sm_predict) == "response"] <- "time_per_pixel"
    names(sm_predict)[names(sm_predict) == "variable"] <- metric

    bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict$time_per_pixel), ]

    sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict$time_per_pixel), ]

    sm_min = sm_min[1, ]
    bm_min = bm_min[1, ]

    global_min = full_data[full_data$time_per_pixel == min(full_data$time_per_pixel),
                           c("time_per_pixel", metric)]

    p <- ggplot() +
         scale_shape_identity() +
         geom_point(data = full_data, alpha = 0.1,
                    aes(x = full_data[metric], y = time_per_pixel,
                        color = "Search Space")) +
         geom_point(data = bm_min, size = 3, alpha = 1.0,
                    aes(x = bm_min[metric], y = time_per_pixel,
                        color = "Big Model", shape = 7)) +
         geom_point(data = sm_min, size = 3, alpha = 1.0,
                    aes(x = sm_min[metric], y = time_per_pixel,
                        color = "Small Model", shape = 8)) +
         geom_point(data = global_min, size = 3, alpha = 1.0,
                    aes(x = global_min[metric], y = time_per_pixel,
                        color = "Global Minimum", shape = 9)) +
         theme_bw() +
         theme(axis.text = element_text(size = 12),
               axis.title = element_text(size = 14, face = "bold"),
               legend.position = "top") +
         labs(y = "time_per_pixel", x = metric) +
         scale_color_manual(values = c("green", "blue", "black", "red"))

    return(p)
}

complete_data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

budget <- 120

factors = c("elements_number", "y_component_number",
            "vector_length", "temporary_size",
            "load_overlap", "threads_number",
            "lws_y")

used <- 0

data <- complete_data[, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

# Comment/Uncomment to toggle scaling

# scaled_data <- cbind(scale(select_if(data, is.numeric), center = FALSE, scale = TRUE),
#                      select_if(data, Negate(is.numeric)))
# scaled_data <- scaled_data[, names(data)]

# We are able to use the full set in this case
# sampled_data <- scaled_data[sample(nrow(data), 500), ]

# Complete model:
output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      scaled_data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

# Complete model:
regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length + lws_y + I(1 / lws_y) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                  data = federov_design)

p_vectorlength <- generate_model_plot(regression, small_model,
                                      scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

p_lwsy <- generate_model_plot(regression, small_model,
                              scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                   data = random_data)


r_lwsy <- generate_model_plot(big_random, small_random,
                              random_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

r_vectorlength <- generate_model_plot(big_random, small_random,
                                      random_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

used <- used + nrow(federov_design)

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'vector_length' and 'lws_y'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 18) {
    output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                           load_overlap + temporary_size +
                           elements_number + I(1 / elements_number) +
                           threads_number + I(1 / threads_number),
                         scaled_data,
                         nTrials = 18)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

p_ycomponentnumber <- generate_model_plot(regression, small_model,
                                          scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

p_threadsnumber <- generate_model_plot(regression, small_model,
                                       scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    threads_number + I(1 / threads_number),
                   data = random_data)


r_ycomponentnumber <- generate_model_plot(big_random, small_random,
                                          random_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

r_threadsnumber <- generate_model_plot(big_random, small_random,
                                       random_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'y_component_number' and 'threads_number'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 10) {
    output <- optFederov(~ load_overlap + temporary_size +
                            elements_number + I(1 / elements_number),
                          scaled_data,
                          nTrials = 10)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                  data = federov_design)

p_elementsnumber <- generate_model_plot(regression, small_model,
                                        scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                   data = random_data)

r_elementsnumber <- generate_model_plot(big_random, small_random,
                                        random_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

# Checking the ANOVA summary we can identify, at last, one variable
# that seem to have greater impact: 'elements_number'
# Let's fix it to their best predicted value so far,
# then fit a new model without it

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number &
                      complete_data$elements_number == predicted_best$elements_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 6) {
    output <- optFederov(~ load_overlap + temporary_size,
                          scaled_data,
                          nTrials = 6)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size,
                  data = federov_design)

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

best <- complete_data[complete_data$time_per_pixel == min(complete_data$time_per_pixel), ]
best_row <- rownames(best)

predicted_best$slowdown <- predicted_best$time_per_pixel / best$time_per_pixel
predicted_best$method <- rep("DOPTaov", nrow(predicted_best))
predicted_best$point_number <- rep(used, nrow(predicted_best))
predicted_best$vector_recompute <- rep("true", nrow(predicted_best))

predicted_best <- predicted_best[, c("elements_number", "y_component_number",
                                    "vector_length", "temporary_size", "vector_recompute",
                                    "load_overlap", "threads_number", "lws_y",
                                    "time_per_pixel", "point_number", "method",
                                    "slowdown")]

grid.arrange(p_vectorlength + ggtitle("First Step: D-Opt + aov"), p_lwsy + ggtitle(" "),
             r_vectorlength + ggtitle("First Step: Random Selection + lm"), r_lwsy + ggtitle(" "),
             p_ycomponentnumber + ggtitle("Second Step: D-Opt + aov"), p_threadsnumber + ggtitle(" "),
             r_ycomponentnumber + ggtitle("Second Step: Random Selection + lm"), r_threadsnumber + ggtitle(" "),
             p_elementsnumber + ggtitle("Third Step: D-Opt + aov"),
             r_elementsnumber + ggtitle("Third Step: Random Selection + lm"), nrow = 5)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536ZkV.png]]
*** [2018-05-14 Mon]
**** Accuracy of the DOPT+AOV Process in Steven's Case
To verify the "accuracy" of the selected metrics, I adapted the experiment
scripts to check for each removed model variable in the actual =aov= summary.
Those initial choices seem to match in most cases with the variables identified
as most relevant by the =aov= summary, as shown below.

#+HEADER: :results graphics output :session *R* :exports results
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 600 :height 500
#+BEGIN_SRC R
library(ggplot2)

accuracies_file <- "dopt_anova_experiments/data/dopt_accuracies.csv"
results <- read.csv(accuracies_file, strip.white=T, header=T)

names(results) <- c("First", "Second", "Third")
parsed_results = data.frame(names(results), t(results[1, ]))
names(parsed_results) <- c("Steps", "Accuracy")

parsed_results

ggplot(data = parsed_results, aes(x = Steps, y = Accuracy)) +
geom_bar(stat = "identity", width = 0.5) +
#geom_hline(yintercept = 1.0, color = "red", linetype = 2) +
geom_text(aes(label = Accuracy), vjust = 1.6, color = "white", size = 5)+
theme_bw() +
theme(axis.text = element_text(size = 12),
      axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536MMn.png]]

As described previously, at each step a group of variables is removed from the
model based on their "score", that is, the "Pr(>F)" value in the =aov= summary.
I selected at most two variables at each of the three steps, based on preliminary
visual analysis of the =aov= summaries.

To measure how accurate those initial selections were I checked at each step if
the $n$ selected variables were in the $n$ most relevant variables in that
step's =aov= summary. If that was the case I incremented a step-specific
counter. The counters were updated for 1000 iterations and then divided by 1000.
This value represents the accuracy of the static selection in comparison with
the values that would be selected if each individual =aov= summary was analysed.
*** [2018-05-15 Tue]
**** Writing an LM Experiment Using a Big Model
This experiment is a modification of the fo``DOPTaov'' experiment that adapts the
``LM'' strategy for fitting linear models to pruned search spaces. Instead of
using small models at each step the experiments starts with large models that
are pruned as meaningful variables are identified in the =aov= summary. The
experiment used the same variables from the ``DOPTaov'' experiement at each
step.

**** Trying to Mitigate Heteroscedasticity
Using some ideas from [[https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/][this website]].

***** For a Uniformly Sampled Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

str(data)
data <- data[sample(1:nrow(data), 100, replace = FALSE), ]

regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                data = data)

summary(regression)
ncvTest(regression)

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = data)

transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$lambda) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = data)

summary(transformed_regression)
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = data)

Residuals:
       Min         1Q     Median         3Q        Max
-4.992e-09 -1.997e-09 -2.596e-10  1.126e-09  2.464e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -5.683e-09  3.382e-09  -1.680 0.096446 .
y_component_number       1.206e-09  5.251e-10   2.298 0.023956 *
I(1/y_component_number)  5.124e-09  3.369e-09   1.521 0.131863
vector_length            2.597e-10  7.338e-11   3.540 0.000643 ***
lws_y                    1.020e-11  2.951e-12   3.458 0.000841 ***
I(1/lws_y)              -2.800e-09  1.270e-09  -2.205 0.030027 *
load_overlaptrue         1.963e-10  8.042e-10   0.244 0.807695
temporary_size           1.977e-10  4.144e-10   0.477 0.634476
elements_number         -1.128e-10  1.048e-10  -1.076 0.284946
I(1/elements_number)     3.968e-09  3.270e-09   1.214 0.228123
threads_number          -2.453e-12  1.856e-12  -1.322 0.189660
I(1/threads_number)      2.043e-08  5.497e-08   0.372 0.711005
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.891e-09 on 88 degrees of freedom
Multiple R-squared:  0.3924,	Adjusted R-squared:  0.3165
F-statistic: 5.167 on 11 and 88 DF,  p-value: 3.271e-06
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 148.428    Df = 1     p = 3.824548e-34

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$lambda) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = data)

Residuals:
    Min      1Q  Median      3Q     Max
-277.65  -81.92  -10.22   79.59  220.80

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -890.45549   99.17068  -8.979 4.53e-14 ***
y_component_number        37.61691   15.39476   2.443   0.0165 *
I(1/y_component_number)  154.21569   98.77625   1.561   0.1221
vector_length             11.59971    2.15156   5.391 5.80e-07 ***
lws_y                      0.50504    0.08652   5.838 8.69e-08 ***
I(1/lws_y)              -258.56389   37.22493  -6.946 6.20e-10 ***
load_overlaptrue         -24.95474   23.57916  -1.058   0.2928
temporary_size             2.76899   12.15003   0.228   0.8203
elements_number           -6.89838    3.07364  -2.244   0.0273 *
I(1/elements_number)     125.77409   95.86892   1.312   0.1930
threads_number            -0.09253    0.05442  -1.700   0.0926 .
I(1/threads_number)     2668.72370 1611.82566   1.656   0.1013
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 114.1 on 88 degrees of freedom
Multiple R-squared:  0.7041,	Adjusted R-squared:  0.6672
F-statistic: 19.04 on 11 and 88 DF,  p-value: < 2.2e-16
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.4543362    Df = 1     p = 0.5002829
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-IVw6xY/figureecbKUS.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-zZp0Xe/figure9dCFD8.png]]

***** For a D-Optimal Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)
str(data)

output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      data = data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

str(federov_design)

# Complete model:
regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    vector_length + lws_y + I(1 / lws_y) +
                                    load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number) +
                                    threads_number + I(1 / threads_number),
                  data = federov_design)

summary(regression)
summary.aov(regression)
ncvTest(regression)

data[predict(regression, data) == min(predict(regression, data)), ]

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = federov_design)

transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$lambda) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = federov_design)

summary(transformed_regression)
summary.aov(transformed_regression)
ncvTest(transformed_regression)

data[predict(transformed_regression, data) == min(predict(transformed_regression, data)), ]
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...
'data.frame':	24 obs. of  9 variables:
 $ elements_number   : int  1 4 2 3 1 4 4 2 1 4 ...
 $ y_component_number: int  1 1 2 3 1 1 1 2 1 1 ...
 $ vector_length     : int  1 16 16 16 1 1 1 1 1 16 ...
 $ temporary_size    : int  2 2 4 4 2 4 4 2 4 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 1 2 1 1 1 1 2 2 ...
 $ threads_number    : int  256 32 32 128 256 32 128 32 32 1024 ...
 $ lws_y             : int  1 1 1 128 32 1 64 32 32 16 ...
 $ time_per_pixel    : num  2.31e-10 7.75e-10 1.70e-09 2.79e-08 7.27e-10 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = federov_design)

Residuals:
       Min         1Q     Median         3Q        Max
-8.775e-09 -4.543e-09 -1.968e-09  2.959e-09  1.856e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)              1.084e-08  1.609e-08   0.674   0.5134
y_component_number      -1.401e-09  2.450e-09  -0.572   0.5778
I(1/y_component_number) -1.300e-08  1.447e-08  -0.898   0.3867
vector_length            5.744e-10  2.591e-10   2.216   0.0467 *
lws_y                    1.121e-11  6.242e-12   1.796   0.0977 .
I(1/lws_y)              -4.183e-09  4.583e-09  -0.913   0.3793
load_overlaptrue        -1.279e-09  3.862e-09  -0.331   0.7462
temporary_size          -2.925e-11  1.926e-09  -0.015   0.9881
elements_number         -4.281e-11  3.725e-10  -0.115   0.9104
I(1/elements_number)     1.238e-08  8.077e-09   1.532   0.1514
threads_number          -3.511e-12  7.437e-12  -0.472   0.6453
I(1/threads_number)     -6.241e-08  2.281e-07  -0.274   0.7890
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 9.262e-09 on 12 degrees of freedom
Multiple R-squared:  0.5646,	Adjusted R-squared:  0.1655
F-statistic: 1.415 on 11 and 12 DF,  p-value: 0.2797
                        Df    Sum Sq   Mean Sq F value Pr(>F)
y_component_number       1 6.980e-17 6.980e-17   0.814 0.3848
I(1/y_component_number)  1 2.000e-18 2.000e-18   0.024 0.8805
vector_length            1 3.875e-16 3.875e-16   4.517 0.0550 .
lws_y                    1 5.023e-16 5.023e-16   5.856 0.0323 *
I(1/lws_y)               1 6.970e-17 6.970e-17   0.812 0.3851
load_overlap             1 6.900e-18 6.900e-18   0.080 0.7818
temporary_size           1 3.200e-18 3.200e-18   0.037 0.8499
elements_number          1 5.620e-17 5.620e-17   0.655 0.4340
I(1/elements_number)     1 2.175e-16 2.175e-16   2.536 0.1372
threads_number           1 1.340e-17 1.340e-17   0.156 0.7001
I(1/threads_number)      1 6.400e-18 6.400e-18   0.075 0.7890
Residuals               12 1.029e-15 8.580e-17
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 14.05733    Df = 1     p = 0.0001773212
      elements_number y_component_number vector_length temporary_size
18584               4                  1             1              4
      vector_recompute load_overlap threads_number lws_y time_per_pixel
18584             true         true           1024     1    3.37552e-10

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$lambda) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = federov_design)

Residuals:
     Min       1Q   Median       3Q      Max
-1.82320 -0.99051 -0.06769  0.85520  1.61215

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -30.303461   2.649147 -11.439 8.23e-08 ***
y_component_number       -0.453056   0.403302  -1.123 0.283253
I(1/y_component_number)  -3.209807   2.383153  -1.347 0.202907
vector_length             0.230701   0.042666   5.407 0.000158 ***
lws_y                     0.004914   0.001028   4.782 0.000447 ***
I(1/lws_y)               -2.779609   0.754470  -3.684 0.003125 **
load_overlaptrue         -0.193140   0.635862  -0.304 0.766525
temporary_size            0.555741   0.317136   1.752 0.105197
elements_number           0.049715   0.061324   0.811 0.433327
I(1/elements_number)      3.458007   1.329789   2.600 0.023208 *
threads_number           -0.002495   0.001224  -2.038 0.064250 .
I(1/threads_number)       6.800059  37.556377   0.181 0.859340
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.525 on 12 degrees of freedom
Multiple R-squared:  0.8911,	Adjusted R-squared:  0.7913
F-statistic:  8.93 on 11 and 12 DF,  p-value: 0.0003389
                        Df Sum Sq Mean Sq F value   Pr(>F)
y_component_number       1   5.93    5.93   2.552  0.13616
I(1/y_component_number)  1   0.06    0.06   0.027  0.87126
vector_length            1  66.84   66.84  28.746  0.00017 ***
lws_y                    1  79.03   79.03  33.992 8.08e-05 ***
I(1/lws_y)               1  30.24   30.24  13.005  0.00360 **
load_overlap             1   0.59    0.59   0.252  0.62477
temporary_size           1   5.50    5.50   2.366  0.14995
elements_number          1   0.39    0.39   0.169  0.68840
I(1/elements_number)     1  17.74   17.74   7.632  0.01720 *
threads_number           1  21.98   21.98   9.452  0.00964 **
I(1/threads_number)      1   0.08    0.08   0.033  0.85934
Residuals               12  27.90    2.33
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.09165178    Df = 1     p = 0.7620877
      elements_number y_component_number vector_length temporary_size
15927               4                  1             1              2
      vector_recompute load_overlap threads_number lws_y time_per_pixel
15927             true         true           1024     1   3.368082e-10
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figurem4Tf5r.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(regression$model$time_per_pixel, breaks = 10)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureYtLgcw.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureGF8dKO.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(transformed_regression$model["bcPower(time_per_pixel, boxcox_transform$lambda)"][[1]])
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureuPx7MI.png]]

*** [2018-05-16 Wed]
**** Power Transforms on Generated Heteroscedastic Data
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(car)
set.seed(1234)
x <- 1:1000
y <- abs(rnorm(n = 1000, mean = x, sd = 0.9 * x))

data <- data.frame(x, y)
plot(lm(y ~ x, data = data), which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5x1ltg/figureDpsnk9.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(lm(y ~ x, data = data))
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 365.3147    Df = 1     p = 1.960365e-81

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
power_transform <- powerTransform(lm(y ~ x,  data = data))#, family = "bcnPower")
coef(power_transform, round = TRUE)
transformed_regression <- lm(bcPower(y, power_transform$roundlam) ~ x, data = data)
                                      #gamma = power_transform$gamma) ~ x, data = data)

plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5x1ltg/figureKMbSqa.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 41.63823    Df = 1     p = 1.098246e-10

*** [2018-05-17 Thu]
**** Looking for Autotuning Benchmarks
Some candidates:
- Rodinia kernels (http://lava.cs.virginia.edu/Rodinia/download_links.htm)
- SPAPT (https://github.com/brnorris03/Orio/tree/master/testsuite/SPAPT)
- Dongarra's BEAST Project (http://icl.cs.utk.edu/beast/people/index.html)
*** [2018-05-22 Tue]
**** Running SPAPT Benchmarks
- Orio has a not very good documentation
- Compilation of simple examples takes +5min
**** Looking into LLVM and GCC flag autotuning
- CollectiveKnowledge has great flag space descriptions
- If we go back to compiler flag tuning we can tune
  any of the well-stablished HPC benchmarks
*** [2018-05-23 Wed]
**** Orio Setup Scripts
I've forked the Orio repository so I can freely change the code while keeping
version control.

To clone the most recent version, pick a path for the repository by editing the
source block below, using absolute paths.

#+NAME: setup_orio
#+HEADER: :results output
#+HEADER: :var ORIO_PATH="/home/phrb/code/orio"
#+BEGIN_SRC shell
git clone --depth=1 https://github.com/phrb/Orio.git $ORIO_PATH || echo "Orio already installed"
#+END_SRC

#+RESULTS: setup_orio
: Orio already installed

***** Lazy Python Databases with =Dataset=
With this it is possible to create a database from python scripts without much hassle:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
sudo pip install dataset
#+END_SRC

#+RESULTS:
: sudo pip install dataset
: Requirement already satisfied: dataset in /usr/lib/python3.6/site-packages

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import dataset
db = dataset.connect('sqlite:///mydatabase.db')
table = db['user']
table.insert(dict(name='John Doe', age=46, country='China'))
table.insert(dict(name='Jane Doe', age=37, country='France', gender='female'))
table.update(dict(name='John Doe', age=47), ['name'])
#+END_SRC

#+RESULTS:
: Python 3.6.5 (default, May 11 2018, 04:00:52)
: [GCC 8.1.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: python.el: native completion setup loaded

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
print(db.tables)
print(db['user'].columns)
print(len(db['user']))
users = db['user'].all()
print(users)

db.commit()
#+END_SRC

#+RESULTS:
: ['user']
: ['id', 'name', 'age', 'country', 'gender']
: 2
: <dataset.util.ResultIter object at 0x7f7f34e974a8>

Cleaning up:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
rm mydatabase.db
#+END_SRC

#+RESULTS:
: rm mydatabase.db
*** [2018-05-28 Mon]
**** Using AlgDesign & ANOVA from Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
from rpy2.robjects.packages import importr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
#+END_SRC

#+RESULTS:

*** [2018-05-29 Tue]
**** Implementing the D-Optimal Design + AOV Approach in Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import math
from rpy2.robjects.packages import importr
from rpy2.robjects import IntVector, StrVector, Formula, r
from rpy2.robjects.lib.dplyr import dplyr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
car       = importr("car")

def opt_federov(design_formula, data, trials):
    output = algdesign.optFederov(Formula(design_formula),
                                  data,
                                  maxIteration = 1000,
                                  nTrials = trials)
    return output

def transform_design(design, lm_formula, full_model, response):
    transform = car.powerTransform(Formula(lm_formula),
                                   data = design)

    print("Power Transform Step:")
    print(transform)
    transformed_response = car.bcPower(design.rx(response[0]), transform.rx("lambda")[0])
    design               = base.cbind(design, transformed_response)
    transform_lm_formula = "{0}".format(base.names(transformed_response)[0]) + full_model

    return design, transform_lm_formula

def anova(design, formula):
    heteroscedasticity_test = car.ncvTest(stats.lm(Formula(formula), data = design))
    print("Heteroscedasticity Test p-value:")
    print(heteroscedasticity_test.rx("p")[0][0])

    if heteroscedasticity_test.rx("p")[0][0] < 0.05:
        transform_lm_formula = transform_design(design, formula,
                                                full_model, response)
        heteroscedasticity_test = car.ncvTest(stats.lm(Formula(transform_lm_formula),
                                                        data = design))
        print("Heteroscedasticity Test p-value:")
        print(heteroscedasticity_test.rx("p")[0][0])
    else:
        print("No need to power transform")
        transform_lm_formula = lm_formula

    regression = stats.lm(Formula(formula),
                          data = design)

    summary_regression = stats.summary_aov(regression)
    print("Regression Step:")
    print(summary_regression)

    prf_values = {}

    for k, v in zip(base.rownames(summary_regression[0]), summary_regression[0][4]):
        if k.strip() != "Residuals":
            prf_values[k.strip()] = v

    return regression, prf_values

def predict_best(regression, data):
    predicted = stats.predict(regression, data)
    predicted_best = predicted.index(min(predicted))

    p_min = min(predicted)
    i = 0

    for k in range(len(predicted)):
        if math.isclose(predicted[k], p_min, rel_tol = 1e-6):
            i += 1

    print("Identical predictions (tol = 1e-17): {0}".format(i))
    return data.rx(predicted_best, True)

def prune_data(data, fixed_variables):
    print(fixed_variables)
    pruned_data = data
    for k, v in fixed_variables.items():
        pruned_data = pruned_data.rx((pruned_data.rx2(str(k)).ro == str(v)),
                                     True)

    print("Dimensions of Pruned Data: " + str(base.dim(pruned_data)))
    print("Dimensions of Full Data: " + str(base.dim(data)))
    return pruned_data

def get_fixed_variables(predicted_best, ordered_prf_keys, fixed_factors, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    fixed_variables = fixed_factors
    for v in unique_variables:
        fixed_variables[v] = predicted_best.rx2(str(v))[0]

    print("Fixed Variables: " + str(fixed_variables))
    return fixed_variables

def prune_model(factors, inverse_factors, ordered_prf_keys, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    pruned_factors = [f for f in factors if not f in unique_variables]
    pruned_inverse_factors = [f for f in inverse_factors if not f in unique_variables]

    return pruned_factors, pruned_inverse_factors

def dopt_anova_step(response, factors, inverse_factors, data, fixed_factors, budget):
    full_model     = "".join([" ~ ",
                              " + ".join(factors), " + ",
                              " + ".join(["I(1 / {0})".format(f) for f in inverse_factors])])

    design_formula = full_model
    lm_formula     = response[0] + full_model
    trials         = round(2 * (len(factors) + len(inverse_factors) + 1))

    fixed_variables = fixed_factors

    if budget - len(data[0]) < 0:
        print("Full data does not fit on budget")
        if trials < len(data[0]):
            print("Computing D-Optimal Design")
            output = opt_federov(design_formula, data, trials)
            design = output.rx("design")[0]
        else:
            print("Too few data points for a D-Optimal design")
            design = data

        used_experiments = len(design[0])
        regression, prf_values = anova(design, lm_formula)
        ordered_prf_keys       = sorted(prf_values, key = prf_values.get)
        predicted_best         = predict_best(regression, data)
        fixed_variables        = get_fixed_variables(predicted_best, ordered_prf_keys,
                                                     fixed_factors)
        pruned_data            = prune_data(data, fixed_variables)

        pruned_factors, pruned_inverse_factors = prune_model(factors, inverse_factors,
                                                            ordered_prf_keys)
    else:
        print("Full data fits on budget, picking best value")
        used_experiments = len(data[0])
        prf_values = []
        ordered_prf_keys = []
        pruned_data = []
        pruned_factors = []
        pruned_inverse_factors = []
        predicted_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                                  True)

    return {"prf_values": prf_values,
            "ordered_prf_keys": ordered_prf_keys,
            "predicted_best": predicted_best,
            "pruned_data": pruned_data,
            "pruned_factors": pruned_factors,
            "pruned_inverse_factors": pruned_inverse_factors,
            "fixed_factors": fixed_variables,
            "used_experiments": used_experiments}

def dopt_anova():
    data = utils.read_csv("dopt_anova_experiments/data/search_space.csv", header = True)

    initial_factors = ["elements_number", "y_component_number",
                       "vector_length", "temporary_size",
                       "load_overlap", "threads_number", "lws_y"]

    initial_inverse_factors = ["y_component_number", "lws_y",
                               "elements_number", "threads_number"]

    response = ["time_per_pixel"]

    data = data.rx(StrVector(initial_factors + response))
    data_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                        True)

    step_factors = initial_factors
    step_inverse_factors = initial_inverse_factors
    step_space = data

    fixed_factors = {}

    initial_budget = 58
    budget = initial_budget
    used_experiments = 0
    iterations = 3

    for i in range(iterations):
        if step_space == []:
            break

        step_data = dopt_anova_step(response,
                                    step_factors,
                                    step_inverse_factors,
                                    step_space,
                                    fixed_factors,
                                    budget)

        step_space = step_data["pruned_data"]
        step_factors = step_data["pruned_factors"]
        step_inverse_factors = step_data["pruned_inverse_factors"]
        budget -= step_data["used_experiments"]
        used_experiments += step_data["used_experiments"]
        fixed_factors = step_data["fixed_factors"]

        print("Fixed Factors: " + str(fixed_factors))
        print("Slowdown: " + str(step_data["predicted_best"].rx(response[0])[0][0] / data_best.rx(response[0])[0][0]))
        print("Budget: {0}/{1}".format(used_experiments, initial_budget))

dopt_anova()
#+END_SRC

#+RESULTS:
#+begin_example
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.000231109380007393
Power Transform Step:
Estimated transformation parameter
         Y1
-0.08680373

Heteroscedasticity Test p-value:
0.9475422563588601
Regression Step:
                        Df Sum Sq Mean Sq F value   Pr(>F)
elements_number          1  2.290   2.290  11.967 0.004723 **
y_component_number       1  0.529   0.529   2.765 0.122202
vector_length            1  3.766   3.766  19.676 0.000813 ***
temporary_size           1  0.235   0.235   1.225 0.290002
load_overlap             1  0.079   0.079   0.413 0.532456
threads_number           1  0.018   0.018   0.092 0.767416
lws_y                    1  7.271   7.271  37.987 4.85e-05 ***
I(1/y_component_number)  1  0.110   0.110   0.574 0.463168
I(1/lws_y)               1  2.861   2.861  14.949 0.002242 **
I(1/elements_number)     1  0.164   0.164   0.859 0.372333
I(1/threads_number)      1  0.432   0.432   2.259 0.158663
Residuals               12  2.297   0.191
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1}
{'lws_y': 16, 'vector_length': 1}
Dimensions of Pruned Data: [1] 576   8

Dimensions of Full Data: [1] 23120     8

Fixed Factors: {'lws_y': 16, 'vector_length': 1}
Slowdown: 11.64335230377803
Budget: 24/58
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.06519543239335164
No need to power transform
Regression Step:
                        Df    Sum Sq   Mean Sq F value   Pr(>F)
elements_number          1 6.930e-19 6.930e-19   8.120   0.0191 *
y_component_number       1 4.960e-19 4.960e-19   5.814   0.0392 *
temporary_size           1 5.100e-20 5.100e-20   0.599   0.4589
load_overlap             1 5.400e-20 5.400e-20   0.638   0.4451
threads_number           1 5.024e-18 5.024e-18  58.829 3.09e-05 ***
I(1/y_component_number)  1 2.100e-20 2.100e-20   0.251   0.6284
I(1/elements_number)     1 8.200e-20 8.200e-20   0.959   0.3530
I(1/threads_number)      1 5.726e-18 5.726e-18  67.049 1.84e-05 ***
Residuals                9 7.690e-19 8.500e-20
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
{'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Dimensions of Pruned Data: [1] 8 8

Dimensions of Full Data: [1] 576   8

Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.8225092523512674
Budget: 42/58
Full data fits on budget, picking best value
Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.265157794114108
Budget: 50/58
#+end_example
** June
*** [2018-06-04 Mon]
**** Implementing the DOPT-Anova in Orio
I've started implementing our approach in benchmark problems from SPAPT, which
is provided with Orio. I've been understanding Orio's source code implementing
the glue code, with =rpy2=, to be able to use R packages in Python.

I've reached a problem with =optFederov=. One of the benchmark applications
has ~10^{14}~ possible combinations in total. My first approach was trying to
generate a subset of this search space, as I did before, but this did not work.
The function kept finding "singular design" errors, which mean the determinant
of the candidates it tested are negative according to the documentation.

It is not clear how to fix this, so I am now trying to use the optMonteCarlo
function, which tries to generate samples of the search space as it explores
it with the Federov algorithm.
*** [2018-06-11 Mon]
**** Converting R Model Fits into Formulas
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
A <- data.frame(x1 = sample(1:100, 30, replace = T),
                x2 = sample(1:100, 30, replace = T),
                x3 = sample(1:100, 30, replace = T),
                x4 = sample(1:100, 30, replace = T),
                x5 = sample(1:100, 30, replace = T))

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5

regression <- lm(y ~ x1 + x2 + x3 + x4 + x5, data = A)
formula(regression)

new_formula <- as.formula(substituteDirect(formula(regression), as.list(coef(regression))))
new_formula

predict(regression)
predict(regression, type = "terms")
A
#+END_SRC

#+RESULTS:
#+begin_example
y ~ x1 + x2 + x3 + x4 + x5
y ~ 2.3 + 3.1 + 4.5 + 6.8 + 2.31
      1       2       3       4       5       6       7       8       9      10
1480.86  536.12 1174.93  804.95 1487.49 1427.97 1150.58 1014.75  916.79  958.22
     11      12      13      14      15      16      17      18      19      20
 953.27  438.97  710.39  709.89 1234.76 1034.89 1043.24 1622.64 1114.05  739.34
     21      22      23      24      25      26      27      28      29      30
 848.47  928.56 1320.51  744.19 1066.60  946.50 1235.58 1051.55  860.67  706.07
             x1          x2      x3          x4      x5
1    54.4333333 -129.166667  195.15  284.693333   66.99
2   -49.0666667    1.033333 -133.35 -279.706667  -11.55
3   -81.2666667 -104.366667   73.65  264.293333   13.86
4    15.3333333  125.033333 -191.85  -55.306667  -97.02
5   -39.8666667   22.733333  186.15  305.093333    4.62
6    15.3333333  131.233333 -115.35  318.693333   69.30
7    24.5333333    1.033333 -227.85  318.693333   25.41
8    98.1333333 -104.366667  100.65  -14.506667  -73.92
9    47.5333333  143.633333  -29.85 -211.706667  -41.58
10 -106.5666667  140.533333   28.65 -170.906667   57.75
11   29.1333333   44.433333  141.15 -293.306667   23.10
12   24.5333333  -23.766667 -218.85 -259.306667  -92.40
13   91.2333333  -29.966667 -119.85 -313.706667   73.92
14  -42.1666667  -14.466667   73.65 -320.506667    4.62
15   98.1333333   97.133333   55.65   46.693333  -71.61
16  -72.0666667  121.933333  186.15 -306.906667   97.02
17   -0.7666667   16.533333 -169.35  264.293333  -76.23
18   52.1333333  109.533333  163.65  203.093333   85.47
19   -0.7666667  109.533333  -38.85  155.493333 -120.12
20   10.7333333 -119.866667  186.15 -293.306667  -53.13
21   77.4333333   32.033333 -187.35   33.093333 -115.50
22  -72.0666667  -64.066667  136.65  -55.306667  -25.41
23  -30.6666667  156.033333   55.65  121.493333    9.24
24 -101.9666667 -141.566667 -223.35  243.893333  -41.58
25    1.5333333  -67.166667  -97.35  121.493333   99.33
26    1.5333333   38.233333   -7.35 -170.906667   76.23
27   36.0333333 -104.366667  -88.35  311.893333   71.61
28   75.1333333 -147.766667  114.15    5.893333   -4.62
29  -72.0666667  -95.066667  -20.85   39.893333    0.00
30  -83.5666667 -144.666667  172.65 -293.306667   46.20
attr(,"constant")
[1] 1008.76
   x1  x2 x3  x4  x5       y
1  78   8 96  95  86 1480.86
2  33  50 23  12  52  536.12
3  19  16 69  92  63 1174.93
4  61  90 10  45  15  804.95
5  37  57 94  98  59 1487.49
6  61  92 27 100  87 1427.97
7  65  50  2 100  68 1150.58
8  97  16 75  51  25 1014.75
9  75  96 46  22  39  916.79
10  8  95 59  28  82  958.22
11 67  64 84  10  67  953.27
12 65  42  4  15  17  438.97
13 94  40 26   7  89  710.39
14 36  45 69   6  59  709.89
15 97  81 65  60  26 1234.76
16 23  89 94   8  99 1034.89
17 54  55 15  92  24 1043.24
18 77  85 89  83  94 1622.64
19 54  85 44  76   5 1114.05
20 59  11 94  10  34  739.34
21 88  60 11  58   7  848.47
22 23  29 83  45  46  928.56
23 41 100 65  71  61 1320.51
24 10   4  3  89  39  744.19
25 55  28 31  71 100 1066.60
26 55  62 51  28  90  946.50
27 70  16 33  99  88 1235.58
28 87   2 78  54  55 1051.55
29 23  19 48  59  57  860.67
30 18   3 91  10  77  706.07
#+end_example
*** [2018-06-13 Wed]
**** Stepwise Regression in R
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

A <- data.frame(x1 = sample(1:100, 100, replace = T),
                x2 = sample(1:100, 100, replace = T),
                x3 = sample(1:100, 100, replace = T),
                x4 = sample(1:100, 100, replace = T),
                x5 = sample(1:100, 100, replace = T),
                x6 = sample(1:100, 100, replace = T),
                x7 = sample(1:100, 100, replace = T),
                x8 = sample(1:100, 100, replace = T)
)

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5 + 5.2 * (A$x8 * A$x2) + 3.1 * (1 / A$x7)

regression = lm(y ~ ., data = A[1:30, ])
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
A[A$y == min(A$y), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x1           1 2.566e+07 2.566e+07   1.852  0.18796
x2           1 2.007e+09 2.007e+09 144.892 6.88e-11 ***
x3           1 1.796e+05 1.796e+05   0.013  0.91044
x4           1 2.557e+08 2.557e+08  18.458  0.00032 ***
x5           1 4.264e+08 4.264e+08  30.779 1.66e-05 ***
x6           1 1.331e+08 1.331e+08   9.604  0.00544 **
x7           1 1.376e+07 1.376e+07   0.993  0.33033
x8           1 1.639e+09 1.639e+09 118.324 4.35e-10 ***
Residuals   21 2.909e+08 1.385e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
   x2 x8
14 30  1
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression = update(regression, . ~ . - x1 - x3 - x4 - x5 - x6 - x7)
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x2           1 1.953e+09 1.953e+09   127.8 9.58e-12 ***
x8           1 2.427e+09 2.427e+09   158.8 8.03e-13 ***
Residuals   27 4.126e+08 1.528e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
#+end_example
*** [2018-06-14 Thu]
**** Stepwise Regression in R
Creating two datasets, a complete one and a sample of it.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

complete_A <- expand.grid(x1 = seq(1, 10, 1),
                          x2 = seq(1, 10, 1),
                          x3 = seq(1, 10, 1),
                          x4 = seq(1, 10, 1),
                          x5 = seq(1, 10, 1),
                          x6 = seq(1, 10, 1),
                          x7 = seq(1, 10, 1)
)

complete_A$y = 2.3 * complete_A$x1 - 3.1 * complete_A$x2 + 4.5 * complete_A$x3 + 2.5 * (1 / complete_A$x2) - 6.8 * complete_A$x4 + 2.31 * (complete_A$x5 * complete_A$x1) - 4.2 * (complete_A$x2 * complete_A$x4)

A <- complete_A[sample(1:nrow(complete_A), 100, replace = F), ]
#+END_SRC

#+RESULTS:

Now we use the sample from the full set to fit the model. We then use another
sample for the predictions. In a real application this sample would not be
measured, since we want only the parameter values.

We are using the =stepAIC= function, that minimizes the [[https://en.wikipedia.org/wiki/Akaike_information_criterion][Akaike Information
Criterion]], to find the model that best fits the experiment data. We will allow
both removal and addition of model variables, up to the intial model, or "full
model". The initial or "null" model is just a constant.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
experiment_data = complete_A[sample(1:nrow(complete_A), 2 * nrow(A), replace = F), ]

null = lm(y ~ 1, data = A)
full = lm(y ~ . + I(1 / x2) + I(1 / x3), data = A)

regression = stepAIC(null, scope = list(upper = full), direction = "both", trace = 1)

summary.aov(regression)

formula(full)
formula(regression)
r_names <- attr(terms(regression), "term.labels")
r_names <- r_names[r_names %in% names(complete_A)]
r_names
#+END_SRC

#+RESULTS:
#+begin_example
Start:  AIC=964.42
y ~ 1

          Df Sum of Sq     RSS    AIC
+ x2       1    653372  859229 909.86
+ x4       1    441689 1070912 931.89
+ I(1/x2)  1    383017 1129584 937.22
+ x5       1    129020 1383581 957.50
+ x1       1     48781 1463820 963.14
+ I(1/x3)  1     46868 1465733 963.27
<none>                 1512601 964.42
+ x3       1     10526 1502075 965.72
+ x6       1      9805 1502796 965.77
+ x7       1       309 1512292 966.40

Step:  AIC=909.86
y ~ x2

          Df Sum of Sq     RSS    AIC
+ x4       1    455540  403689 836.32
+ x5       1     86545  772684 901.25
+ I(1/x3)  1     39119  820110 907.20
+ x1       1     30172  829057 908.29
+ x3       1     29944  829285 908.31
<none>                  859229 909.86
+ x7       1      6710  852519 911.08
+ x6       1      5989  853240 911.16
+ I(1/x2)  1      1708  857521 911.66
- x2       1    653372 1512601 964.42

Step:  AIC=836.32
y ~ x2 + x4

          Df Sum of Sq     RSS    AIC
+ x5       1    133553  270136 798.15
+ x1       1    109629  294061 806.64
+ I(1/x3)  1     36532  367158 828.84
+ x3       1     32372  371317 829.96
<none>                  403689 836.32
+ I(1/x2)  1      1695  401994 837.90
+ x6       1      1190  402500 838.03
+ x7       1       134  403555 838.29
- x4       1    455540  859229 909.86
- x2       1    667223 1070912 931.89

Step:  AIC=798.15
y ~ x2 + x4 + x5

          Df Sum of Sq    RSS    AIC
+ x1       1    126352 143785 737.09
+ x3       1      9771 260366 796.47
+ I(1/x3)  1      8609 261527 796.91
<none>                 270136 798.15
+ I(1/x2)  1      5190 264946 798.21
+ x6       1       246 269890 800.06
+ x7       1       148 269988 800.10
- x5       1    133553 403689 836.32
- x4       1    502548 772684 901.25
- x2       1    615682 885819 914.91

Step:  AIC=737.09
y ~ x2 + x4 + x5 + x1

          Df Sum of Sq    RSS    AIC
+ x3       1     15072 128712 728.02
+ I(1/x3)  1      9874 133911 731.98
<none>                 143785 737.09
+ I(1/x2)  1      1196 142589 738.25
+ x6       1        99 143686 739.02
+ x7       1        69 143716 739.04
- x1       1    126352 270136 798.15
- x5       1    150276 294061 806.64
- x2       1    578998 722782 896.57
- x4       1    594448 738233 898.68

Step:  AIC=728.02
y ~ x2 + x4 + x5 + x1 + x3

          Df Sum of Sq    RSS    AIC
<none>                 128712 728.02
+ I(1/x2)  1      1074 127639 729.18
+ x7       1        38 128674 729.99
+ x6       1        20 128692 730.00
+ I(1/x3)  1         0 128712 730.02
- x3       1     15072 143785 737.09
- x5       1    122550 251263 792.91
- x1       1    131653 260366 796.47
- x2       1    592170 720882 898.31
- x4       1    594763 723475 898.67
            Df Sum Sq Mean Sq F value   Pr(>F)
x2           1 653372  653372  477.17  < 2e-16 ***
x4           1 455540  455540  332.69  < 2e-16 ***
x5           1 133553  133553   97.53 3.37e-16 ***
x1           1 126352  126352   92.28 1.26e-15 ***
x3           1  15072   15072   11.01  0.00129 **
Residuals   94 128712    1369
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + I(1/x2) + I(1/x3)
y ~ x2 + x4 + x5 + x1 + x3
[1] "x2" "x4" "x5" "x1" "x3"
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
values = predict(regression, experiment_data)
complete_A[complete_A$y == min(complete_A$y), "y"] / experiment_data[values == min(values), "y"]

values = complete_A[sample(1:nrow(complete_A), nrow(A), replace = F), ]
complete_A[complete_A$y == min(complete_A$y), "y"] / values[values$y == min(values$y), "y"]
#+END_SRC

#+RESULTS:
#+begin_example
  [1] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
  [9] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [17] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [25] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [33] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [41] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [49] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [57] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [65] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [73] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [81] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [89] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [97] 1.153702 1.153702 1.153702 1.153702
  [1] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
  [9] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [17] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [25] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [33] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [41] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [49] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [57] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [65] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [73] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [81] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [89] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [97] 1.245883 1.245883 1.245883 1.245883
#+end_example
*** [2018-06-15 Fri]
**** Working with =optMonteCarlo=
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(MASS)

model_formula = formula(~ . + I(1 / (1 + x2)) + I(1 / (1 + x3)) + I(1 / (1 + x4)))
model_formula

design_data = data.frame(var = c("x1", "x2", "x3", "x4", "x5", "x6", "x7"),
                         low = c(rep(0, 7)),
                         high = c(9, 18, 18, 2, 1, 6, 40),
                         center = c(rep(0, 7)),
                         nLevels = c(10, 19, 19, 3, 2, 7, 41),
                         round = c(rep(0, 7)),
                         factor = c(rep(F, 7)),
                         mix = c(rep(F, 7)))

design_data

optMonteCarlo(model_formula, design_data, nTrials = 25)
#+END_SRC

#+RESULTS:
#+begin_example
~. + I(1/(1 + x2)) + I(1/(1 + x3)) + I(1/(1 + x4))
  var low high center nLevels round factor   mix
1  x1   0    9      0      10     0  FALSE FALSE
2  x2   0   18      0      19     0  FALSE FALSE
3  x3   0   18      0      19     0  FALSE FALSE
4  x4   0    2      0       3     0  FALSE FALSE
5  x5   0    1      0       2     0  FALSE FALSE
6  x6   0    6      0       7     0  FALSE FALSE
7  x7   0   40      0      41     0  FALSE FALSE
$D
[1] 1.248469

$A
[1] 42.83773

$Ge
[1] 0.833

$Dea
[1] 0.818

$design
   x1 x2 x3 x4 x5 x6 x7
1   1  2  6  2  0  6 23
2   1  0  6  1  0  2  5
3   7  0 14  1  0  3 30
4   4 10  0  0  0  5 35
5   2  9 18  2  1  4 32
6   1  5 15  0  0  0 32
7   9 18  0  1  0  5  0
8   8 14 16  0  1  0 12
9   8 17  4  1  0  6  0
10  3 17  4  0  0  4 35
11  3 13 18  1  1  6 11
12  9  5 13  1  0  4  3
13  1  9  7  2  0  0 24
14  2  6  3  1  1  0 23
15  2 18 17  1  1  1 27
16  9  3  4  0  1  5 27
17  4  0 14  2  1  0  9
18  7 12  0  1  1  1 20
19  8 18 12  2  1  2 38
20  2  0 17  1  0  2 31
21  5  0  0  1  1  2 10
22  9  3  6  1  1  4 31
23  9 14 16  2  0  1  3
24  2 14  2  2  1  5  4
25  1  1 15  0  1  3  7
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
set.seed(66182)
model_formula <- formula(~ T1_J + T1_K + T2_I + T2_J + T2_K + ACOPY_x + ACOPY_y + U1_I + U_I + U_K + RT_J + RT_K + SCR + VEC1 + VEC2 + I(1 / (1 + T1_J)) + I(1 / (1 + T1_K)) + I(1 / (1 + T2_I)) + I(1 / (1 + T2_J)) + I(1 / (1 + T2_K)) + I(1 / (1 + U1_I)) + I(1 / (1 + U_I)) + I(1 / (1 + U_K)) + I(1 / (1 + RT_J)) + I(1 / (1 + RT_K)))

design_data <- data.frame(var = c("T1_J", "T1_K", "T2_I", "T2_J", "T2_K", "ACOPY_x",
                                  "ACOPY_y", "U1_I", "U_I", "U_K", "RT_J", "RT_K",
                                  "SCR", "VEC1", "VEC2"),
                          low = rep(0, 15),
                          high = c(6, 6, 6, 6, 6, 1, 1, 29, 29, 29, 2, 2, 1, 1, 1),
                          center = rep(0, 15),
                          nLevels = c(7, 7, 7, 7, 7, 2, 2, 30, 30, 30, 3, 3, 2, 2, 2),
                          round = rep(0, 15),
                          factor = c(rep(F, 15)),
                          mix = c(rep(F, 15)))

design_data

optMonteCarlo(model_formula, data = design_data, nTrials = 37)
#+END_SRC

#+RESULTS:
#+begin_example
       var low high center nLevels round factor   mix
1     T1_J   0    6      0       7     0  FALSE FALSE
2     T1_K   0    6      0       7     0  FALSE FALSE
3     T2_I   0    6      0       7     0  FALSE FALSE
4     T2_J   0    6      0       7     0  FALSE FALSE
5     T2_K   0    6      0       7     0  FALSE FALSE
6  ACOPY_x   0    1      0       2     0  FALSE FALSE
7  ACOPY_y   0    1      0       2     0  FALSE FALSE
8     U1_I   0   29      0      30     0  FALSE FALSE
9      U_I   0   29      0      30     0  FALSE FALSE
10     U_K   0   29      0      30     0  FALSE FALSE
11    RT_J   0    2      0       3     0  FALSE FALSE
12    RT_K   0    2      0       3     0  FALSE FALSE
13     SCR   0    1      0       2     0  FALSE FALSE
14    VEC1   0    1      0       2     0  FALSE FALSE
15    VEC2   0    1      0       2     0  FALSE FALSE
$D
[1] 0.2969417

$A
[1] 73.70555

$Ge
[1] 0.482

$Dea
[1] 0.341

$design
   T1_J T1_K T2_I T2_J T2_K ACOPY_x ACOPY_y U1_I U_I U_K RT_J RT_K SCR VEC1
1     4    0    0    4    1       0       1   28   1   1    2    1   0    0
2     0    5    1    4    6       1       0    3  14  15    1    1   1    1
3     1    4    4    0    0       1       1   16  14   1    0    1   0    0
4     6    2    4    3    5       1       0    7  28  10    0    2   0    0
5     1    2    0    4    3       1       1   11   6   6    1    1   0    0
6     6    4    1    1    1       1       0   27   7  23    1    0   1    0
7     4    5    2    4    1       0       0    4  16  27    0    1   0    1
8     5    2    4    3    1       0       1   13   1   0    1    0   1    1
9     3    6    2    5    0       1       1    0  18  14    2    0   1    1
10    2    2    0    2    6       1       1    0   3  28    2    1   0    1
11    5    1    5    2    0       1       1   12   8   8    1    2   1    1
12    1    0    1    5    4       1       0   28   0  28    1    2   0    1
13    3    2    4    1    0       0       0    1   0   8    1    1   1    1
14    6    1    0    6    2       0       1   29  28  27    0    1   1    0
15    1    0    5    2    5       0       1   13  17  18    0    2   1    1
16    0    2    6    5    5       0       1   25   0   1    0    0   0    0
17    6    4    0    3    6       0       0   29  19  23    1    2   0    1
18    2    2    3    5    3       0       1   19   1  26    0    2   1    0
19    4    6    6    5    6       1       1   12  15  26    1    1   0    1
20    3    6    0    2    2       0       1   23  15  25    0    0   1    1
21    2    1    2    6    2       1       1   24  26   0    1    2   1    0
22    0    4    4    4    3       0       1   27   3  24    2    2   0    0
23    6    6    1    0    4       0       1    0  28   7    2    1   0    0
24    2    0    2    0    6       0       1    6  22  23    2    1   1    0
25    2    1    5    2    5       0       0    0  17  24    1    1   0    1
26    1    3    1    4    5       1       1   29  10   7    0    0   0    1
27    4    5    5    0    1       1       0   17   3   5    0    0   1    1
28    0    2    1    0    5       0       0    6  25  12    1    2   0    1
29    2    6    5    4    6       0       1    0  25  16    0    1   1    0
30    0    5    4    2    0       0       0   17  24  12    0    1   0    0
31    1    1    0    2    0       0       1   16  28  19    1    0   0    0
32    3    6    4    6    3       0       0    3   3   1    1    0   0    0
33    6    2    5    6    0       0       0   12  12   1    2    1   0    0
34    2    4    6    2    5       1       0   28  11  16    2    1   1    1
35    3    4    3    6    3       0       1   10  27   8    2    0   0    1
36    0    2    5    2    4       1       0   10  21  29    2    0   1    0
37    6    3    5    3    1       1       1   26  27   6    1    1   0    1
   VEC2
1     1
2     1
3     0
4     1
5     1
6     1
7     0
8     1
9     1
10    1
11    0
12    0
13    0
14    1
15    1
16    0
17    0
18    1
19    1
20    0
21    0
22    0
23    1
24    0
25    1
26    1
27    1
28    1
29    0
30    1
31    1
32    1
33    0
34    1
35    1
36    0
37    1
#+end_example
*** [2018-06-21 Thu]
**** A Flexible Distributed Optimization Scheme with Asynchronous, Scarse, and Sparse Communications
- Presenter: Frank Iutzeler (http://www.iutzeler.org/)
- Context: Distributed Learning
  - Global Learning Objective
  - Local data
- Proximal Gradient Algorithms
*** [2018-06-28 Thu]
**** Tweaking DLMT
***** Initial Sample for =OptFederov=
- Will take a long time to generate if we choose a large size, because
  constraint checking restricts a lot of the search space
- Will not provide enough candidates for =OptFederov= if the sample size is
  small. This can result in small D-Efficiency for the generated experiments
***** =OptFederov= Function
****** nTrials
- The simplest way to deal with runtime errors is to increase the number of
  experiments generated by =OptFederov=
- Using more trials allows for more failures before ANOVA breaks
- Should we re-generate the design after failures?
****** frml
- Using the inverse of variables in the initial model requires special care to
  remove 2-level factors that may appear due to constraints
***** Problems in Analysis
- When valid experiments in the design fail, what happens with the validity of ANOVA?
- What are the best ways to go around that?
***** Threshold for Removing Variables
- We are using an heuristic to select how many variables to remove at each step
** July
*** [2018-07-03 Tue]
**** Round Table LIG Day
- Blockchain
- HPC & Big Data Convergence
- Machine Learning
- Reproducibility
*** [2018-07-04 Wed]
**** Plotting Sampling Strategies
***** Random Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)

data = data.frame(x1 = sample(0:20, 100, replace = T),
                  x2 = sample(0:20, 100, replace = T))

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figure7g2xWl.png]]

***** LHS Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)

data <- lhs.design(nruns = 100 ,nfactors = 2, digits = 0, type = "maximin",
                   factor.names = list(x1 = c(0, 20), x2 = c(0, 20)))

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figureUvZvfi.png]]
***** D-Optimal Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~., full_factorial, nTrials = 20)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figureCgGwUG.png]]

***** D-Optimal Sampling with Interactions
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + .^2, full_factorial, nTrials = 20)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figure9Cfi2s.png]]
***** D-Optimal Sampling with Quadratic Terms
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = 30)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figureXQIyzR.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
head(model.matrix(~. + I(x1 ^ 2) + I(x2 ^ 2), full_factorial))
#+END_SRC

#+RESULTS:
:   (Intercept)  x1  x2 I(x1^2) I(x2^2)
: 1           1 -19 -19     361     361
: 2           1 -17 -19     289     361
: 3           1 -15 -19     225     361
: 4           1 -13 -19     169     361
: 5           1 -11 -19     121     361
: 6           1  -9 -19      81     361

***** D-Optimal Sampling with Cubic Terms
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + cubic(.), full_factorial, nTrials = 30)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figureTljFYC.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
head(model.matrix(~ . + cubic(.), full_factorial))
#+END_SRC

#+RESULTS:
:   (Intercept)  x1  x2 I(x1^2) I(x2^2) I(x1^3) I(x2^3) x1:x2
: 1           1 -19 -19     361     361   -6859   -6859   361
: 2           1 -17 -19     289     361   -4913   -6859   323
: 3           1 -15 -19     225     361   -3375   -6859   285
: 4           1 -13 -19     169     361   -2197   -6859   247
: 5           1 -11 -19     121     361   -1331   -6859   209
: 6           1  -9 -19      81     361    -729   -6859   171
***** I-Optimal Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~., full_factorial, criterion = "I", nTrials = 60)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-pKA0GA/figurecZgDnj.png]]
**** Experiments with the ATAX application
I've been using the smallest input size for the atax application. So far I am
not sure if the larger sizes even work properly, since some of then do not use
the validation code. Some of the also do not have the same parameters.

Experiments with DLMT using all factors and their inverses, when applicable, is
not giving results much different than random search. The model might be more or
less complex. The D-Optimality of the designs is usually low.

I am also not sure of the impact of sample sizes in the two steps where they are
used: the sample for the Fedorov algorithm and the sample for predicting the
best result. These samples are not evaluated, but must be validated using the
constraints provided by the application.

Next steps:
- Run DLMT experiments with a simpler model
- Run DLMT experiments using interactions in the model
- Compare results for other applications and input sizes
- Compare results with other search algorithms
- Address issues in [[Tweaking DLMT][Tweaking DLMT]]
*** [2018-07-05 Thu]
**** Building D-Optimal Designs with Interactions
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(1000),
                   x2 = runif(1000),
                   x3 = runif(1000),
                   x4 = runif(1000),
                   x5 = runif(1000),
                   x6 = runif(1000))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:          x1         x2        x3        x4        x5        x6        y
: 1 0.3145343 0.22122070 0.5791998 0.6329657 0.9590482 0.5580798 6.236130
: 2 0.4204504 0.52465529 0.9820209 0.4381713 0.8702131 0.8412574 7.397551
: 3 0.9683900 0.07251716 0.8357594 0.3334859 0.1510384 0.7234758 5.482052
: 4 0.1489611 0.84208199 0.3208823 0.8697806 0.5734586 0.3794286 6.219130
: 5 0.1562686 0.01527451 0.8520008 0.9050570 0.7896101 0.8236433 6.724943
: 6 0.8644607 0.33800877 0.4537771 0.9145758 0.8455194 0.3449356 7.638418

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
output <- optFederov(~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, data, nTrials = 14)
output

design <- output$design
#+END_SRC

#+RESULTS:
#+begin_example
$D
[1] 0.1539217

$A
[1] 11.50989

$Ge
[1] 0.8

$Dea
[1] 0.779

$design
            x1         x2          x3          x4          x5          x6
5   0.15626861 0.01527451 0.852000780 0.905057031 0.789610110 0.823643283
60  0.14111468 0.06047899 0.005816296 0.122148020 0.241837807 0.204990197
82  0.83102789 0.24063190 0.066638218 0.835985614 0.938030672 0.274673609
167 0.01083552 0.86262930 0.919079563 0.994813522 0.326322796 0.348092876
188 0.99944823 0.48202436 0.184919251 0.034891482 0.945193022 0.029429601
431 0.65156391 0.85404254 0.997202041 0.039830894 0.868841200 0.898136727
479 0.07126425 0.02491834 0.072766728 0.091690738 0.506438140 0.968787553
488 0.90546459 0.08732170 0.985465054 0.616232353 0.107418757 0.003533801
489 0.15103343 0.58518849 0.999827211 0.002674163 0.934751620 0.217067372
566 0.77890381 0.88071788 0.096931037 0.982260734 0.169843512 0.991753875
852 0.03508049 0.98718423 0.066761902 0.912848081 0.375775029 0.103496414
909 0.42360652 0.94109188 0.988913755 0.840630643 0.956933183 0.841327438
915 0.74483130 0.82045704 0.042712603 0.007349035 0.022841742 0.893580211
968 0.99884604 0.02018211 0.894035780 0.503326149 0.001980576 0.860871009
            y
5    6.724943
60   1.487574
82   6.523872
167  7.935617
188  4.651014
431  6.727184
479  3.111935
488  5.092613
489  4.243379
566  7.433653
852  4.648488
909 10.527218
915  3.944372
968  5.940554

$rows
 [1]   5  60  82 167 188 431 479 488 489 566 852 909 915 968
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression <- aov(y ~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, design)
summary(regression)
#+END_SRC

#+RESULTS:
#+begin_example
            Df Sum Sq Mean Sq   F value Pr(>F)
x1           1  1.365   1.365 3.407e+30 <2e-16 ***
x2           1 14.548  14.548 3.630e+31 <2e-16 ***
x3           1 17.450  17.450 4.355e+31 <2e-16 ***
x4           1 19.636  19.636 4.900e+31 <2e-16 ***
x5           1  4.612   4.612 1.151e+31 <2e-16 ***
x6           1  5.692   5.692 1.420e+31 <2e-16 ***
x2:x3:x4     1  3.020   3.020 7.537e+30 <2e-16 ***
Residuals    6  0.000   0.000
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#+end_example
*** [2018-07-09 Mon]
**** Paper Review: Assessing Time Predictability Features of ARM big.LITTLE Multicores
This paper presents qualitative and quantitative assessments of some
characteristics of the ARM big.LITTLE architecture. The objective of the paper
is to measure the applicability of this architecture to critical real-time
embedded systems. The paper is well organized, and it was easy to read and
understand.

The qualitative assessment uses the technical reference manuals of the ARM
Cortex-A53 processor and the ARM Juno R2 board. The quantitative assessment uses
a simple stress benchmark application and measures the cycles per memory access
in different scenarios.

The quantitative assessment is based on three experiments: a scenario without
contention, or isolated, a scenario with contention, and a scenario with
contention and no-ops between memory accesses. The results in Figures 3 and 4
show the median of 1000 measurements for each vector size. The Figures do not
show the standard deviations of the measurements.

It is not discussed whether the median is the best choice to represent the
measurements, or whether the variability of measurements was relevant. Without
knowing the variability of measurements it is hard to know if the differences
observed in each experimental scenario are meaningful. I suggest to include the
standard deviation of each measurement in the data presented. Also, unless there
is a strong reason for using the median, I suggest using the mean of the 1000
measurements. I believe this will provide better support for the conclusions of
the paper.

I also believe that measuring the cycles per access in a more comprehensive
benchmark suite would help support the arguments presented in the paper, as well
as using real applications on critical real-time embedded systems. My
recommendation is that this paper should be submitted again after addressing the
issues with data presentation and analysis, and possibly with more experimental
evaluations using different benchmark applications, potentially using
measurements in real applications.
*** [2018-07-10 Tue]
**** Building Models with Interactions
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:           x1        x2        x3        x4         x5        x6        y
: 1 0.18076715 0.4140549 0.4354110 0.8747773 0.44056661 0.2059766 5.257260
: 2 0.04511744 0.3389018 0.6758916 0.7470131 0.95302942 0.7220186 6.740219
: 3 0.64351052 0.8803408 0.3407263 0.4162235 0.78808724 0.6384063 6.607299
: 4 0.13375844 0.4154362 0.5740708 0.9881421 0.22562579 0.0736479 5.206433
: 5 0.70947263 0.9798590 0.9904583 0.8542580 0.13675687 0.7153690 9.525387
: 6 0.68455712 0.3255436 0.9922359 0.2413945 0.05896366 0.5179531 4.732390

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)         x1        x2        x3        x4         x5        x6
1           1 0.18076715 0.4140549 0.4354110 0.8747773 0.44056661 0.2059766
2           1 0.04511744 0.3389018 0.6758916 0.7470131 0.95302942 0.7220186
3           1 0.64351052 0.8803408 0.3407263 0.4162235 0.78808724 0.6384063
4           1 0.13375844 0.4154362 0.5740708 0.9881421 0.22562579 0.0736479
5           1 0.70947263 0.9798590 0.9904583 0.8542580 0.13675687 0.7153690
6           1 0.68455712 0.3255436 0.9922359 0.2413945 0.05896366 0.5179531
    x2:x3:x4
1 0.15770839
2 0.17111149
3 0.12484840
4 0.23566183
5 0.82906553
6 0.07797429
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
model_matrix <- as.data.frame(model_matrix)
removed_interactions <- names(Filter(function(x)(length(unique(x)) == 1), model_matrix))
removed_interactions
#+END_SRC

#+RESULTS:
: [1] "(Intercept)"
*** [2018-07-13 Fri]
**** Interesting Paper using SPAPT
- [[file:~/Dropbox/papers/autotuning/2017/ogilvie2017minimizing.pdf][Minimizing the Cost of Iterative Compilation with Active Learning]]
**** Building Models with Quadratic Terms
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:           x1        x2        x3         x4        x5        x6        y
: 1 0.03468630 0.5173048 0.1087729 0.08684336 0.6359173 0.9587616 3.802525
: 2 0.98720044 0.4519941 0.9221820 0.28453285 0.9660307 0.4632033 7.232514
: 3 0.31504150 0.9030804 0.4090276 0.21641727 0.5562343 0.5831133 4.864308
: 4 0.48529389 0.7517897 0.3670082 0.37152128 0.9808670 0.4651149 6.086757
: 5 0.07717325 0.1913457 0.3366301 0.82366125 0.7304363 0.9233006 6.068915
: 6 0.29731807 0.5504496 0.2815305 0.21052078 0.8033675 0.7820747 4.992076

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ I(x2 ^ 2) + x1 + x2 + x3 + x4 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)    I(x2^2)         x1        x2        x3         x4        x5
1           1 0.26760424 0.03468630 0.5173048 0.1087729 0.08684336 0.6359173
2           1 0.20429871 0.98720044 0.4519941 0.9221820 0.28453285 0.9660307
3           1 0.81555417 0.31504150 0.9030804 0.4090276 0.21641727 0.5562343
4           1 0.56518779 0.48529389 0.7517897 0.3670082 0.37152128 0.9808670
5           1 0.03661316 0.07717325 0.1913457 0.3366301 0.82366125 0.7304363
6           1 0.30299480 0.29731807 0.5504496 0.2815305 0.21052078 0.8033675
         x6
1 0.9587616
2 0.4632033
3 0.5831133
4 0.4651149
5 0.9233006
6 0.7820747
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- as.data.frame(model_matrix)
removed_interactions <- names(Filter(function(x)(length(unique(x)) == 1), model_matrix))
removed_interactions
#+END_SRC

#+RESULTS:
: [1] "(Intercept)"
*** [2018-07-16 Mon]
**** Temporary Parameters for DLMT
- Using quadratic terms yielded better results: Importance of the model
- Comparing with O2: The same was done in this [[Interesting Paper using SPAPT]]
- Removed binary variables: Caused runtime errors; Removed in other works too
- Less iterations: Trying to minimize executions
**** Plotting Orio Experiments Data (atax, O2, quadratic terms, Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output
#+BEGIN_SRC shell
cd orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/ && ./db2csv.py
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/results.csv")
str(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup_O3))
data_median <- ddply(data, .(technique), summarize, median = median(speedup_O3))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup_O3),
                   err = 2 * sd(speedup_O3) / sqrt(length(speedup_O3)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup_O3))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  18 variables:
 $ id        : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT_K      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ T1_I      : int  1 16 1 16 16 1 16 16 32 1 ...
 $ T1_J      : int  16 16 128 16 256 256 128 256 16 128 ...
 $ T1_K      : int  32 64 512 512 1 512 256 1 64 128 ...
 $ U_K       : int  1 26 28 1 11 1 1 21 26 12 ...
 $ U_J       : int  18 28 19 11 1 22 8 10 26 1 ...
 $ U_I       : int  10 1 1 17 8 11 8 1 1 12 ...
 $ technique : Factor w/ 2 levels "DLMT","RS": 2 2 2 2 2 2 2 2 2 2 ...
 $ U1_I      : int  27 14 20 14 23 21 9 1 19 3 ...
 $ speedup_O3: num  1.68 1.67 1.66 1.71 1.68 ...
 $ T2_K      : int  2048 256 1 1024 256 1024 512 128 1 256 ...
 $ T2_J      : int  2048 256 2048 256 256 1 128 256 128 1024 ...
 $ T2_I      : int  1 1024 1 256 2048 1024 64 512 512 1 ...
 $ points    : int  100 100 100 100 100 100 100 100 100 100 ...
 $ RT_I      : int  1 8 8 8 1 1 1 8 8 1 ...
 $ cost_mean : num  0.109 0.109 0.11 0.107 0.109 ...
 $ RT_J      : int  8 1 1 1 8 8 1 1 1 8 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup_O3), #, y = 0.1 * ..density..),
    binwidth = 0.05) +
    labs(y = "Frequency", x = "Speedup vs O2") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 100)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 30, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 30, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "", breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AiL4Pb/figureG457Cf.png]]
**** Plotting Orio Experiments Data (atax, O2, quadratic terms, Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output
#+BEGIN_SRC shell
cd orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O2_quad/ && ./db2csv.py
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O2_quad/results.csv")
str(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup_O3))
data_median <- ddply(data, .(technique), summarize, median = median(speedup_O3))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup_O3),
                   err = 2 * sd(speedup_O3) / sqrt(length(speedup_O3)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup_O3))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  18 variables:
 $ id        : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT_K      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ T1_I      : int  512 512 1 32 64 128 16 512 32 512 ...
 $ T1_J      : int  16 512 1 1 1 512 1 512 16 128 ...
 $ T1_K      : int  128 1 512 1 1 1 128 512 1 512 ...
 $ U_K       : int  1 1 1 30 1 1 1 12 1 17 ...
 $ U_J       : int  16 21 24 21 22 15 17 1 9 21 ...
 $ U_I       : int  22 15 6 1 10 15 18 19 21 1 ...
 $ technique : Factor w/ 2 levels "DLMT","RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ U1_I      : int  28 30 5 16 14 4 9 2 16 15 ...
 $ speedup_O3: num  1.51 1.53 1.5 1.52 1.52 ...
 $ T2_K      : int  256 1 1 1 1 2048 256 2048 2048 1 ...
 $ T2_J      : int  2048 1024 128 1024 512 1 1 1 2048 2048 ...
 $ T2_I      : int  2048 1 2048 128 1 1 256 2048 2048 2048 ...
 $ points    : int  105 105 162 136 139 114 126 115 123 118 ...
 $ RT_I      : int  8 8 1 8 8 8 8 8 8 8 ...
 $ cost_mean : num  0.0938 0.093 0.0945 0.0935 0.0934 ...
 $ RT_J      : int  1 1 8 1 1 1 1 1 1 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup_O3), #, y = 0.01 * ..density..),
    binwidth = 0.05) +
    labs(y = "Frequency", x = "Speedup vs O2") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 100)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 30, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 30, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AyGCXu/figureKUTrtW.png]]
**** Building Models with Quadratic Terms II
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:          x1         x2        x3        x4         x5          x6        y
: 1 0.6778492 0.92349391 0.1576724 0.9815172 0.77496216 0.282501746 7.442764
: 2 0.7088311 0.51079715 0.2335002 0.5522585 0.22558065 0.005148194 4.308371
: 3 0.5280327 0.03529519 0.5537724 0.2926394 0.67773583 0.236637721 4.250867
: 4 0.4423366 0.93826929 0.4526374 0.7090073 0.84701227 0.157596670 6.940557
: 5 0.2282255 0.37932004 0.5559482 0.7280456 0.04624094 0.923890930 5.526287
: 6 0.7575107 0.69316315 0.1938151 0.7458463 0.29209639 0.149164932 5.523634

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ I(x2 ^ 2) + I(x1 ^ 2) + I(x3 ^ 2) + I(x4 ^ 2) + (x1 + x2 + x3 + x4) ^ 2 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)     I(x2^2)   I(x1^2)    I(x3^2)   I(x4^2)        x1         x2
1           1 0.852840999 0.4594796 0.02486059 0.9633761 0.6778492 0.92349391
2           1 0.260913733 0.5024415 0.05452236 0.3049895 0.7088311 0.51079715
3           1 0.001245751 0.2788185 0.30666389 0.0856378 0.5280327 0.03529519
4           1 0.880349268 0.1956617 0.20488059 0.5026914 0.4423366 0.93826929
5           1 0.143883695 0.0520869 0.30907835 0.5300504 0.2282255 0.37932004
6           1 0.480475149 0.5738224 0.03756428 0.5562866 0.7575107 0.69316315
         x3        x4         x5          x6      x1:x2     x1:x3     x1:x4
1 0.1576724 0.9815172 0.77496216 0.282501746 0.62598964 0.1068781 0.6653207
2 0.2335002 0.5522585 0.22558065 0.005148194 0.36206889 0.1655122 0.3914580
3 0.5537724 0.2926394 0.67773583 0.236637721 0.01863702 0.2924099 0.1545232
4 0.4526374 0.7090073 0.84701227 0.157596670 0.41503088 0.2002181 0.3136199
5 0.5559482 0.7280456 0.04624094 0.923890930 0.08657052 0.1268816 0.1661586
6 0.1938151 0.7458463 0.29209639 0.149164932 0.52507848 0.1468170 0.5649865
      x2:x3      x2:x4     x3:x4
1 0.1456095 0.90642520 0.1547582
2 0.1192713 0.28209208 0.1289525
3 0.0195455 0.01032876 0.1620556
4 0.4246957 0.66523981 0.3209232
5 0.2108823 0.27616229 0.4047556
6 0.1343455 0.51699314 0.1445562
#+end_example
*** [2018-07-17 Tue]
**** Plotting atax0 O2 Uniform + D-Optimal Sampling (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/search_space.csv")

data <- data[data$correct_result == "True", ]
str(data)

O2_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_K == 1, ]
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	20999 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.105 0.129 0.162 0.213 0.106 ...
 $ RT_K                        : int  8 1 1 1 8 1 8 32 8 1 ...
 $ T1_I                        : int  1 32 32 16 32 128 64 256 128 128 ...
 $ T1_J                        : int  128 64 64 64 32 512 32 32 32 32 ...
 $ T1_K                        : int  128 1 64 64 64 64 512 64 256 64 ...
 $ U_K                         : int  1 28 1 5 1 30 6 26 1 1 ...
 $ U_J                         : int  13 1 28 7 10 1 1 5 10 11 ...
 $ U_I                         : int  21 7 23 1 26 20 25 1 5 13 ...
 $ U1_I                        : int  5 23 27 26 5 15 9 2 25 24 ...
 $ T2_K                        : int  512 512 256 1024 2048 512 2048 2048 1024 2048 ...
 $ T2_J                        : int  1024 256 128 512 128 1024 1 512 256 1024 ...
 $ T2_I                        : int  128 1 512 256 1 128 128 2048 128 512 ...
 $ mean_confidence_interval_inf: num  0.105 0.128 0.161 0.211 0.105 ...
 $ cost_std                    : num  0.000172 0.001385 0.001781 0.002961 0.000509 ...
 $ RT_I                        : int  8 1 1 32 8 1 1 1 8 1 ...
 $ cost_mean                   : num  0.105 0.128 0.161 0.212 0.106 ...
 $ RT_J                        : int  1 8 8 1 1 32 1 1 1 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean)) +
    geom_vline(aes(xintercept = cost_mean), O2_baseline, color = "black", linetype = 2) +
    geom_text(aes(x = cost_mean + 0.03, y = 6000, label = "-O2 baseline"), data = O2_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dLTdMe/figurej8zchR.png]]
*** [2018-07-18 Wed]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$ACOPY_x == "False" & data$ACOPY_y == "False" &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1600   26
'data.frame':	1443 obs. of  26 variables:
 $ id                          : int  1 2 4 5 6 7 8 9 11 12 ...
 $ T2_K                        : int  2048 64 2048 256 1 128 64 512 256 1 ...
 $ T2_J                        : int  128 128 2048 256 1 256 1 2048 1 512 ...
 $ T2_I                        : int  512 1 256 512 1024 128 128 256 1 2048 ...
 $ mean_confidence_interval_inf: num  0.183 0.101 0.218 0.113 0.183 ...
 $ RT_K                        : int  1 1 32 8 8 8 1 1 1 1 ...
 $ T1_I                        : int  512 512 64 256 256 32 16 64 128 256 ...
 $ T1_J                        : int  128 32 16 16 64 128 64 128 256 32 ...
 $ T1_K                        : int  512 16 64 16 256 128 16 16 1 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 2 1 2 2 1 1 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 1 2 1 1 1 2 2 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 1 1 2 1 1 2 1 2 ...
 $ ACOPY_x                     : Factor w/ 2 levels "False","True": 1 1 1 1 1 2 2 1 1 1 ...
 $ ACOPY_y                     : Factor w/ 2 levels "False","True": 2 1 1 1 2 2 2 2 1 1 ...
 $ U1_I                        : int  4 2 11 2 23 11 10 22 15 2 ...
 $ RT_J                        : int  32 1 1 8 8 1 1 1 1 8 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.183 0.104 0.224 0.117 0.189 ...
 $ cost_std                    : num  0.000204 0.00477 0.008259 0.006777 0.008412 ...
 $ cost_mean                   : num  0.183 0.102 0.221 0.115 0.186 ...
 $ U_J                         : int  8 1 1 1 22 1 1 20 8 4 ...
 $ U_I                         : int  22 18 28 7 1 10 28 1 1 18 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ U_K                         : int  1 28 11 1 16 25 14 24 9 1 ...
 $ RT_I                        : int  1 8 1 1 1 8 8 8 32 8 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, 0.6), ylim = c(0, 180)) +
    geom_text(aes(x = cost_mean + 0.04, y = 170, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dwa8la/figure2iRWvN.png]]
*** [2018-07-19 Thu]
**** Plotting bigckernel O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/bicgkernel/xeon_e5_2630_v2/src1_O3_binary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 &
                    data$T2_I == 1 & data$T2_J == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$RT_I == 1 & data$RT_J == 1 &
                    data$SCR == "False" & data$OMP == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 3001   21
'data.frame':	2980 obs. of  21 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ mean_confidence_interval_sup: num  0.0847 0.0308 0.086 0.3751 0.2981 ...
 $ mean_confidence_interval_inf: num  0.0819 0.0278 0.0798 0.3741 0.2933 ...
 $ T1_I                        : int  16 1 1 1 128 32 64 256 1 1 ...
 $ T1_J                        : int  128 1 256 1 128 32 128 64 64 128 ...
 $ cost_mean                   : num  0.0833 0.0293 0.0829 0.3746 0.2957 ...
 $ U_J                         : int  1 1 1 4 6 1 1 7 4 24 ...
 $ U_I                         : int  5 16 24 1 1 14 12 1 1 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ T2_I                        : int  2048 128 256 2048 2048 256 1 2048 2048 256 ...
 $ cost_std                    : num  0.00432 0.00452 0.00938 0.00145 0.00713 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ T2_J                        : int  1 1 512 128 128 128 1024 2048 1024 2048 ...
 $ U1_I                        : int  5 28 18 16 5 5 30 26 6 2 ...
 $ OMP                         : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 2 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 2 1 2 2 1 1 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 2 2 2 1 1 ...
 $ RT_I                        : int  8 8 1 32 1 32 32 32 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 1 1 1 1 2 2 1 1 ...
 $ RT_J                        : int  1 1 32 1 32 1 1 1 32 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 300)) +
    geom_text(aes(x = cost_mean + 0.04, y = 100, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-s376G4/figureHIEfVL.png]]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E3 1505M v6)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1505M_v6/atax1_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$ACOPY_x == "False" & data$ACOPY_y == "False" &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 211  26
'data.frame':	189 obs. of  26 variables:
 $ id                          : int  1 2 3 5 6 7 8 9 11 12 ...
 $ T2_K                        : int  1 512 1 512 128 256 1024 1024 128 1 ...
 $ T2_J                        : int  128 128 2048 2048 2048 1 128 1024 1 1024 ...
 $ T2_I                        : int  64 2048 256 64 1024 128 512 512 512 1024 ...
 $ mean_confidence_interval_inf: num  0.787 0.735 0.254 0.402 0.614 ...
 $ RT_K                        : int  1 8 8 1 1 8 32 1 1 1 ...
 $ T1_I                        : int  16 256 128 16 1 16 128 64 128 1 ...
 $ T1_J                        : int  1 16 32 512 512 128 32 512 1 256 ...
 $ T1_K                        : int  32 128 256 128 16 64 16 16 1 16 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 2 2 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 2 1 2 1 2 1 1 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 1 2 2 ...
 $ ACOPY_x                     : Factor w/ 2 levels "False","True": 2 1 1 1 1 2 2 1 2 1 ...
 $ ACOPY_y                     : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 1 2 1 ...
 $ U1_I                        : int  7 28 9 27 13 29 19 12 7 12 ...
 $ RT_J                        : int  1 8 1 1 32 8 1 8 1 32 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.807 0.744 0.255 0.403 0.628 ...
 $ cost_std                    : num  0.02843 0.0131 0.00178 0.00149 0.02003 ...
 $ cost_mean                   : num  0.797 0.74 0.255 0.403 0.621 ...
 $ U_J                         : int  12 16 1 17 20 1 21 1 7 2 ...
 $ U_I                         : int  1 1 11 15 24 17 1 22 1 1 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ U_K                         : int  9 11 30 1 1 28 19 18 12 3 ...
 $ RT_I                        : int  1 1 8 8 1 1 1 1 32 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 12)) +
    geom_text(aes(x = cost_mean + 0.1, y = 12, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-OIl2Ho/figureW5OgZc.png]]
**** Paper Review: GASPOW: An Autotunable System for Accelerating General Sliding-Window Streaming Operators on GPU
***** Overall merit
 1. Reject
 2. Weak reject
 * 3. Weak accept
 4. Accept
 5. Strong accept
***** Reviewer expertise
 1. No familiarity
 * 2. Some familiarity
 3. Knowledgeable
 4. Expert
***** Paper short summary
The paper presents a system for offloading stream processing workloads to GPUs.
The system applies the concept of distinct sliding windows that can be run in
parallel in GPUs. It uses online learning and a raindrop algorithm to optimize
the concurrency level and the batch length that should be used in each
application. Experiments with two queries are presented, showing significant
improvements in comparison with execution on CPUs.
***** Comments for PC (hidden from authors) - optional
***** Novelty
 1. Published before or openly commercialized
 * 2. Incremental improvement
 3. New contribution
 4. Surprisingly new contribution
***** Strengths
- Presentation: Clear tables and figures
- Application of the parallel windows inside sliding batches approach on GPUs
- Provides open-source code
***** Weaknesses
- Text could be made clearer
- Evaluation based on only 2 applications and 1 architecture
- Proposed autotuning strategy was not compared with other strategies
***** Well situated regarding previous work
 1. Low
 * 2. Medium
 3. High
***** Detailed comments for author:
****** Disclaimer
Since this was a double blind review, it was not possible to access the source
code used in the experiments or some of the references to previous work.
****** Section 5: Evaluation
It would be interesting to present more explanation on the selection of the
queries for the evaluation, and why only two different queries where selected.
Evaluation on more than a single GPU and CPU would also help support the
conclusions of the paper.
******* Section 5.2: Mechanism Evaluation
The autotuning search space for this problem has two parameters and 150 possible
combinations. This is a very small search space in relation to common autotuning
problems in other domains. The entire search spaces of two scenarios of query 1
are represented in the heatmaps of Figure 11.

As far as I understood there is no diference in result throughput between points
in and out of the Pareto frontier, at least with respect to the number of
replicas. If this is the case, it would be interesting to compare the results of
the active learning approach with much simpler autotuning approaches such as
random uniform sampling or latin hypersquare sampling. If most query search
spaces are like the ones presented in Figure 11, such simple approaches should
have interesting results while consuming few resources.
******* Section 5.3: Autotuning Evaluation
In the last paragraph of this subsection, it is claimed that "[in] almost all
cases the raindrop-based strategy ends up in an optimal configuration on the
Pareto frontier. Only in few cases it picks a point not on the frontier but
still very close to it.".

According to the table, only in 5 out of the 12 scenarios the configuration
found by the raindrop strategy was at distance zero from the Pareto Frontier.
Since the space of configurations is discrete, it may be the case that distances
smaller than 1 could be as close as possible, but if that is the case more
clarification on the meaning of this distance should be provided.
****** Text Clarity
The paper could be made clearer by further text revision. An example, which
appears in the last paragraph of subsubsection "Maximum sustainable input rate"
of subsection 5.1, is the following:

Original:   "The CPU version has a latency lower of some orders of magnitude [..]"
Suggestion: "The CPU version has latency lower by 3 orders of magnitude [...]"

****** Style and Formatting
The following are some style and formatting suggestions:
- Abbreviations such as "a.k.a", used in the first page, might not be
  immediately understood by non-native english speakers, and can easily be
  replaced by clearer expressions. In the first page, for example: "[...]" has
  been applied to sliding-window queries [8] (a.k.a. windowed queries) that
  [...]" could be rewritten as "[...] has been applied to sliding-window, or
  windowed, queries [8] that [...]".
- The intent of other abbreviations such as "e.g." and "i.e.", widely used in
  the text, can be more clearly conveyed by replacing them by "for example" and
  "that is".
*** [2018-07-20 Fri]
**** Plotting stencil3d O3 No Binary Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3_nobinary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 &
                    data$RT1_K == 1, ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 840  20
'data.frame':	838 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 12 ...
 $ RT1_K                       : int  1 8 1 8 8 1 1 32 1 1 ...
 $ RT1_J                       : int  8 1 1 1 8 1 8 1 8 1 ...
 $ T1_Ja                       : int  64 256 1024 1024 1024 128 1 64 2048 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ mean_confidence_interval_inf: num  0.1117 0.1941 0.0965 0.2412 0.1994 ...
 $ T1_I                        : int  1 512 16 256 512 1 128 256 1 32 ...
 $ T1_J                        : int  1 32 512 512 1 1 32 32 32 64 ...
 $ T1_Ia                       : int  64 2048 128 2048 512 256 512 256 512 128 ...
 $ cost_mean                   : num  0.1144 0.1959 0.0996 0.2464 0.1999 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ T1_K                        : int  32 32 64 64 16 1 1 512 128 128 ...
 $ T1_Ka                       : int  128 256 512 64 256 2048 256 512 2048 2048 ...
 $ U1_K                        : int  28 7 1 23 7 22 19 9 3 22 ...
 $ U1_J                        : int  2 1 13 1 16 29 20 1 1 1 ...
 $ U1_I                        : int  1 1 27 19 1 1 1 17 13 18 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ RT1_I                       : int  1 8 1 8 1 32 8 1 8 32 ...
 $ mean_confidence_interval_sup: num  0.117 0.198 0.103 0.252 0.201 ...
 $ cost_std                    : num  0.00828 0.00525 0.00948 0.01567 0.00174 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 230)) +
    geom_text(aes(x = cost_mean + 0.02, y = 230, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dwa8la/figure9grgau.png]]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O3_nobinary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1, ]

str(O3_baseline$cost_mean)

best <- data[data$cost_mean == min(data$cost_mean), ]
str(best$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 63289    20
'data.frame':	63009 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.225 0.11 0.188 0.251 0.153 ...
 $ RT_K                        : int  8 1 1 1 32 8 1 8 1 8 ...
 $ T1_I                        : int  64 128 16 64 1 1 512 32 1 512 ...
 $ T1_J                        : int  1 64 16 16 512 32 32 32 64 1 ...
 $ T1_K                        : int  32 1 16 64 512 128 256 512 16 1 ...
 $ U_K                         : int  30 1 29 1 5 1 13 30 17 1 ...
 $ U_J                         : int  1 22 1 14 16 28 1 4 21 28 ...
 $ U_I                         : int  30 11 22 16 1 23 30 1 1 30 ...
 $ U1_I                        : int  6 14 4 29 25 30 18 29 2 29 ...
 $ T2_K                        : int  256 1024 2048 64 1 1 1024 1 1 1 ...
 $ T2_J                        : int  1024 512 128 1 1 2048 1 256 2048 1 ...
 $ T2_I                        : int  64 256 2048 1 128 64 1 512 512 512 ...
 $ mean_confidence_interval_inf: num  0.224 0.109 0.188 0.251 0.153 ...
 $ cost_std                    : num  1.36e-03 1.35e-04 6.58e-05 4.17e-04 5.96e-05 ...
 $ RT_I                        : int  1 1 1 32 1 8 8 8 32 1 ...
 $ cost_mean                   : num  0.225 0.11 0.188 0.251 0.153 ...
 $ RT_J                        : int  8 1 8 1 1 1 8 1 1 8 ...
 num 0.142
 num 0.0802
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 380)) +
    geom_text(aes(x = cost_mean + 0.1, y = 12, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-InAPVd/figureKGd8Rb.png]]
**** Plotting dgem1 O3 Random Uniform Sample (Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e3_1230_v2/dgemv1/search_space/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

str(O3_baseline$cost_mean)
best <- data[data$cost_mean == min(data$cost_mean), ]

str(best$cost_mean)
#+END_SRC

#+RESULTS:
: [1] 3001   57
:  num 0.0929
:  num 0.0651

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 620)) +
    geom_text(aes(x = cost_mean - 0.02, y = 620, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-uAeVrZ/figureqVYiYt.png]]
*** [2018-07-23 Mon]
**** Plotting stencil3d O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 2458   37
'data.frame':	2454 obs. of  37 variables:
 $ id                          : int  2 3 4 5 6 7 8 9 10 11 ...
 $ RT1_K                       : int  8 1 32 1 1 1 1 8 1 32 ...
 $ RT1_J                       : int  1 1 1 32 8 1 1 1 32 1 ...
 $ T1_Ja                       : int  1 1024 1024 64 1024 128 1024 1024 256 2048 ...
 $ RT1_I                       : int  1 32 1 1 8 8 32 1 1 1 ...
 $ T2_K                        : int  16 512 1 64 512 64 256 128 16 64 ...
 $ T2_J                        : int  16 16 64 64 64 512 32 512 512 256 ...
 $ T2_I                        : int  16 32 16 256 32 64 64 512 32 1 ...
 $ T2_Ja                       : int  128 2048 256 256 256 2048 1024 2048 1 256 ...
 $ U2_I                        : int  1 1 1 23 19 28 3 30 27 21 ...
 $ mean_confidence_interval_inf: num  0.0778 0.3177 0.1435 0.3858 0.2973 ...
 $ T1_I                        : int  32 512 1 128 64 1 64 64 32 128 ...
 $ T1_J                        : int  64 512 64 32 64 16 16 64 1 128 ...
 $ T1_K                        : int  128 256 16 128 1 512 16 256 16 64 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ RT2_J                       : int  1 32 32 8 32 1 1 32 1 8 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 2 2 1 2 2 2 2 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 2 1 1 1 1 1 2 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 2 2 1 2 1 1 1 1 ...
 $ T1_Ia                       : int  1 1 1024 1 128 1024 256 256 1024 1 ...
 $ T1_Ka                       : int  1024 1024 1024 128 1 1024 1024 1 512 128 ...
 $ OMP1                        : Factor w/ 2 levels "False","True": 2 2 2 2 1 2 2 1 2 1 ...
 $ U1_K                        : int  1 21 1 1 2 3 1 15 18 15 ...
 $ U1_J                        : int  15 20 1 28 30 1 2 1 30 1 ...
 $ U1_I                        : int  28 1 3 9 1 24 21 29 1 13 ...
 $ T2_Ka                       : int  1 1 1 64 1 1024 256 2048 2048 128 ...
 $ RT2_I                       : int  1 1 1 1 1 1 1 1 8 8 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ T2_Ia                       : int  128 128 128 1024 128 512 512 1 2048 256 ...
 $ U2_J                        : int  7 18 23 16 25 1 1 1 27 17 ...
 $ U2_K                        : int  26 15 8 1 1 22 3 29 1 1 ...
 $ cost_mean                   : num  0.0799 0.3211 0.1464 0.3899 0.3016 ...
 $ RT2_K                       : int  32 1 1 8 1 1 32 1 8 1 ...
 $ OMP2                        : Factor w/ 2 levels "False","True": 2 1 1 2 1 1 2 2 2 2 ...
 $ cost_std                    : num  0.0064 0.01032 0.00867 0.01229 0.01317 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ mean_confidence_interval_sup: num  0.082 0.325 0.149 0.394 0.306 ...
 num 0.0736
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 120)) +
    geom_text(aes(x = cost_mean + 0.05, y = 123, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dwa8la/figureRaW8Tf.png]]
*** [2018-07-24 Tue]
**** Plotting dgem1 O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/dgemv3_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

str(O3_baseline$cost_mean)
best <- data[data$cost_mean == min(data$cost_mean), ]

str(best$cost_mean)
str(best)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 3915   57
 num 0.114
 num 0.0728
'data.frame':	1 obs. of  57 variables:
 $ id                          : int 887
 $ T5_I                        : int 64
 $ T1_Ja                       : int 2048
 $ technique                   : Factor w/ 2 levels "DLMT","RS": 1
 $ RT1_J                       : int 32
 $ RT7_I                       : int 8
 $ VEC10                       : Factor w/ 2 levels "False","True": 2
 $ RT7_J                       : int 1
 $ T5_J                        : int 512
 $ VEC9                        : Factor w/ 2 levels "False","True": 1
 $ T3_J                        : int 128
 $ T3_I                        : int 512
 $ RT1_I                       : int 1
 $ VEC6                        : Factor w/ 2 levels "False","True": 1
 $ mean_confidence_interval_inf: num 0.0728
 $ T1_I                        : int 32
 $ T1_J                        : int 64
 $ T5_Ia                       : int 2048
 $ T3_Ja                       : int 128
 $ VEC7                        : Factor w/ 2 levels "False","True": 2
 $ VEC4                        : Factor w/ 2 levels "False","True": 1
 $ VEC5                        : Factor w/ 2 levels "False","True": 2
 $ T7_J                        : int 1
 $ VEC3                        : Factor w/ 2 levels "False","True": 1
 $ T7_I                        : int 1
 $ VEC2                        : Factor w/ 2 levels "False","True": 1
 $ T7_Ia                       : int 2048
 $ VEC8                        : Factor w/ 2 levels "False","True": 2
 $ U10_I                       : int 16
 $ U6_I                        : int 16
 $ T1_Ia                       : int 1
 $ VEC1                        : Factor w/ 2 levels "False","True": 1
 $ U5_J                        : int 1
 $ U5_I                        : int 4
 $ U1_J                        : int 1
 $ U1_I                        : int 4
 $ U8_I                        : int 16
 $ U7_I                        : int 1
 $ U7_J                        : int 1
 $ cost_std                    : num 8.57e-05
 $ runs                        : int 35
 $ mean_confidence_interval_sup: num 0.0728
 $ U2_I                        : int 4
 $ cost_mean                   : num 0.0728
 $ U9_I                        : int 16
 $ RT3_I                       : int 8
 $ RT3_J                       : int 8
 $ correct_result              : Factor w/ 1 level "True": 1
 $ T7_Ja                       : int 256
 $ T5_Ja                       : int 512
 $ U4_I                        : int 16
 $ T3_Ia                       : int 1
 $ SCR                         : Factor w/ 2 levels "False","True": 2
 $ U3_I                        : int 1
 $ RT5_J                       : int 1
 $ RT5_I                       : int 1
 $ U3_J                        : int 1
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data[data$technique == "RS", ]) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 1050)) +
    geom_text(aes(x = cost_mean - 0.02, y = 200, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AiL4Pb/figured8ABQO.png]]
*** [2018-07-25 Wed]
**** Plotting gemver O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/gemver/xeon_e5_2630_v2/gemver_O3/search_space.csv")

data <- data[data$correct_result == "True", ]

str(data)

O3_baseline <- data[data$T2_I == 1 & data$T2_J == 1 & data$T2_Ia == 1 &
                    data$T2_Ja == 1 & data$T4_I == 1 & data$T4_J == 1 &
                    data$T4_Ia == 1 & data$T4_Ja == 1 &
                    data$U1_I == 1 & data$U2_I == 1 & data$U2_J == 1 &
                    data$U3_I == 1 & data$U4_I == 1 & data$U4_J == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT4_I == 1 &
                    data$RT4_J == 1 &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$SCR == "False" &
                    data$OMP == "False", ]

str(O3_baseline$cost_mean)
best <- data[data$cost_mean == min(data$cost_mean), ]

str(best$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	1917 obs. of  32 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  256 64 256 512 1 64 1 256 512 32 ...
 $ T2_I                        : int  512 512 16 32 256 128 16 1 32 32 ...
 $ T4_I                        : int  1 128 1 1 64 512 32 256 32 128 ...
 $ T4_J                        : int  64 128 256 16 512 32 512 32 16 256 ...
 $ mean_confidence_interval_inf: num  0.363 0.49 0.255 0.117 1.229 ...
 $ T2_Ja                       : int  256 256 256 1024 1 1024 64 1024 1024 1 ...
 $ T2_Ia                       : int  1024 1 512 512 1 1024 128 256 1 128 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ T4_Ia                       : int  256 128 512 64 1 512 1 2048 1 1024 ...
 $ VEC4                        : Factor w/ 2 levels "False","True": 2 2 2 2 1 1 2 2 1 2 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 2 2 1 2 2 1 1 2 1 1 ...
 $ VEC3                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 1 2 1 2 2 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 1 2 1 2 1 1 1 1 2 ...
 $ U4_J                        : int  1 15 1 2 3 16 5 12 23 1 ...
 $ U2_J                        : int  20 24 9 13 24 11 2 19 9 5 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 2 2 1 2 1 2 1 2 ...
 $ U4_I                        : int  3 1 22 1 1 1 1 1 1 9 ...
 $ U1_I                        : int  16 23 2 22 22 7 9 22 10 7 ...
 $ RT2_I                       : int  8 1 32 1 32 1 1 8 8 8 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ U2_I                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ RT2_J                       : int  8 32 1 8 1 8 8 8 1 8 ...
 $ cost_mean                   : num  0.364 0.491 0.255 0.122 1.23 ...
 $ cost_std                    : num  0.00371 0.00271 0.00244 0.01616 0.00498 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ OMP                         : Factor w/ 2 levels "False","True": 1 1 1 2 1 2 1 2 1 1 ...
 $ RT4_J                       : int  8 8 8 1 8 8 1 1 1 8 ...
 $ RT4_I                       : int  1 1 8 8 1 8 32 32 8 1 ...
 $ T4_Ja                       : int  2048 512 512 1024 1024 2048 2048 1 1 2048 ...
 $ mean_confidence_interval_sup: num  0.365 0.492 0.256 0.128 1.232 ...
 $ U3_I                        : int  13 21 15 20 1 21 17 7 3 7 ...
 num 0.356
 num 0.108
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.1) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 500)) +
    geom_text(aes(x = cost_mean + 0.1, y = 130, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-s376G4/figureIcyjLa.png]]
**** Plotting stencil3d O3 Random Uniform Sample (Xeon E5 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e3_1230_v2/stencil_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 &
                    data$RT1_K == 1, ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1925   37
'data.frame':	1919 obs. of  37 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT1_K                       : int  32 1 1 1 1 32 1 32 8 1 ...
 $ RT1_J                       : int  1 1 8 1 1 1 8 1 8 1 ...
 $ T1_Ja                       : int  1 128 2048 1024 2048 256 1 1024 64 256 ...
 $ RT1_I                       : int  1 8 8 32 1 1 1 1 1 32 ...
 $ T2_K                        : int  1 128 32 128 128 256 256 32 32 16 ...
 $ T2_J                        : int  16 32 16 16 1 1 32 1 256 16 ...
 $ T2_I                        : int  32 1 1 256 16 256 64 1 64 64 ...
 $ T2_Ja                       : int  2048 512 128 128 2048 1024 2048 256 1 512 ...
 $ U2_I                        : int  6 2 1 21 23 1 8 1 6 1 ...
 $ mean_confidence_interval_inf: num  0.1 0.1999 0.1828 0.0819 0.1435 ...
 $ T1_I                        : int  32 128 64 16 64 32 128 512 128 1 ...
 $ T1_J                        : int  128 128 64 1 512 64 512 16 32 64 ...
 $ T1_K                        : int  32 512 32 128 128 512 64 16 16 256 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ RT2_J                       : int  1 8 8 32 8 1 8 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 2 1 2 2 2 1 1 2 1 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 2 1 1 2 2 1 1 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 1 2 1 2 2 2 2 1 2 2 ...
 $ T1_Ia                       : int  128 512 64 1024 256 512 256 1 1024 64 ...
 $ T1_Ka                       : int  64 2048 64 2048 256 2048 1024 256 1 2048 ...
 $ OMP1                        : Factor w/ 2 levels "False","True": 1 1 1 2 1 1 2 1 1 1 ...
 $ U1_K                        : int  1 18 6 2 10 6 20 1 1 27 ...
 $ U1_J                        : int  5 1 16 1 5 1 24 28 23 16 ...
 $ U1_I                        : int  17 26 1 17 1 6 1 16 22 1 ...
 $ T2_Ka                       : int  64 512 128 1 1 512 1 128 512 2048 ...
 $ RT2_I                       : int  1 1 1 1 8 8 8 1 8 32 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ T2_Ia                       : int  64 2048 1 512 64 256 2048 256 1024 512 ...
 $ U2_J                        : int  18 15 26 8 1 5 1 3 13 1 ...
 $ U2_K                        : int  1 1 19 1 8 22 8 15 1 28 ...
 $ cost_mean                   : num  0.1025 0.2018 0.1842 0.0825 0.146 ...
 $ RT2_K                       : int  32 1 1 1 1 1 1 8 1 1 ...
 $ OMP2                        : Factor w/ 2 levels "False","True": 1 2 1 2 1 2 2 1 1 2 ...
 $ cost_std                    : num  0.00741 0.00585 0.00402 0.00197 0.00774 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ mean_confidence_interval_sup: num  0.1049 0.2038 0.1855 0.0832 0.1486 ...
 num 0.0698
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 150)) +
    geom_text(aes(x = cost_mean + 0.05, y = 150, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-C2GnlH/figurexUeHjC.png]]
*** [2018-07-26 Thu]
**** Running DLMT for stencil3d O3 (Xeon E5 1230 v2)
- Started running DLMT for stencil3d. Added a timeout of 20 minutes in the program
  running command because some applications take 1h+ to compile and still don't
  finish. Timeout length should be exposed as a configuration parameter.

- The initial model used is linear for all 2-level binary factors and for the second
  and third levels of the parameters controlling loop unrolling, tiling, etc.

- The search space for this problem has "few" valid configurations, so sampling takes
  a very long time. I opted to add more experiments and use a smaller sample
  size. This seemed to improve the D-Efficiency of the designs, but this can
  simply be a result of having more experiments.
*** [2018-07-30 Mon]
**** Plotting Orio Experiments Data (dgemv3, O3, Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e3_1230_v2/dgemv1/experiments/results.csv")

data[data$technique == "DLMT" & data$points <= 70, "experiment"] = "DLMT_small"
data[data$technique == "RS" & data$points <= 70, "experiment"] = "RS_small"
data[data$technique == "DLMT" & data$points > 70, "experiment"] = "DLMT_large"
data[data$technique == "RS" & data$points > 70, "experiment"] = "RS_large"

data_mean <- ddply(data, .(experiment), summarize, mean = mean(speedup))
data_median <- ddply(data, .(experiment), summarize, median = median(speedup))
data_error <- ddply(data, .(experiment), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(experiment), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(experiment ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 4)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 3, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 3, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AiL4Pb/figureHQnKL8.png]]
** August
*** [2018-08-22 Wed]
**** Paper Review for WSCAD: Performance and Energy Efficiency Evaluation for HPC
***** Grades
- Originality: 2
- Relevance: 4
- Technical Merit: 2
- Text: 2
- References:
- Evaluation: 2
- Verdict: 2
- Reviewer Expertise: 4

***** Comments to Authors
****** Paper Summary
The paper presents an evaluation of performance and power consumption of five
experiments, two of which consist of real-world applications, in different
architecture setups. With the results, the paper suggests an architecture setup
for each experiment depending on the optimization objective.

****** Section 4: Results
Assuming that the data presented consists of means of several repetitions, the
number of repetitions of each experiment and the variance of repeated executions
are not discussed in the paper. Other values such as the standard error and
standard deviation are also not discusssed. Without this crucial information
about the experiments and results it is difficult to support any conclusion
based on the data presented.

I recommend that the relevant statistics are added to the plots, and that a
discussion regarding their values is added to the paper. This information will
help sustain the conclusions of the paper and complete the presentation of the
results.
****** Writing Comments and Suggestions
The text would benefit from a revision in all sections. The current text is hard
to read and to understand at certain points. Some sentences to improve and
improvement suggestions are listed below, although I still recommend a thorough
revision.

******* Abstract

1. Original sentence:

"Those experiments, executed on a range of heterogeneous architectures and
programming models, contributes by studying how such applications execute using
metrics that take into account not just performance, but also the power
consumption for an energy efficient HPC system."

Suggestion:

Here, "contributes" should agree with "Those experiments". It is also hard
to extract the meaning of the sentence, perhaps because it is trying to convey
too much information. A possible rewrite is:

"Our results enable analysing the performance and power consumption of the
selected applications, and help to achieve energy efficiency in HPC systems."

******* Introduction
1. Original:

"Scientific computing usually requires huge processing power resources' to
perform large scale experiments and run simulations in a reasonable time."

Suggestion:

Remove the typo "'".

2. Original:

"These demands have been addressed by High Performance Computing (HPC), which
makes them invaluable tool for modern scientific research."

Suggestion:

It is not clear which are those demands, or how they are made an invaluable
tool. Perhaps more explanations could be given before, in separate sentences.

3. Original:

"To this end, it is important to understand the relationships among performance
(in this work we consider as time to solution and scalability), power
consumption, and the characteristics of each scientific application."

Suggestion:

Replace "among" with "between". It is not clear what is the "end" or "objective"
which is pursued. An introductory sentence to the paragraph could be added.

******* Related Work
1. Original:

"A lot of works are focused on alternative and heterogeneous architectures [...]"

Suggestion:

Avoid using "a lot of", use instead expressions such as "many", or "multiple".

2. Original:

"The work [Okina et al. 2016] discusses [...]"
"In [Castro et al. 2016] is presented [...]"
(and others)

Suggestion:

Avoid using the reference marker as part of the text, as shown in the examples
above. This style choice does not work in all reference systems, and the
sentence will lose its meaning if the reference system changes to a superscript,
for example.

3. Original:

"[...] particularly those representing the behavior of physical phenomenon such
as seismic activity."

Suggestion:

"[...] particularly those representing the behavior of physical phenomena such
as seismic activity."

or

"[...] particularly those representing the behavior of a physical phenomenon such
as seismic activity."

******* Experimental Setup (3.3)
1. Original:

"The form to collect the data from the internal sensors and the hardware param-
eters, for each architecture, differ from each other."

Suggestion:

This sentence is hard to read. Changing the ordering of terms can help, such as:

"Collecting data from sensors and hardware parameters is done in a different way
in each architecture."

2. Original:

"For this reason, it was developed a [..] tool [...]"
"[...] it was developed a module [...]"

Suggestion:

Replace "it was developed a X" by "X was developed", or "we developed X".

3.Original:

"These power and the execution time are used [..]"

Suggestion:

Replace "These" by "The", for example.

******* Results
1. Original:

"The hatched area displays the EC of the application while the solid color of
the system."

Suggestion:

"The hatched and solid areas displays the EC of the application and the system, respectively."

2. Original:

"This is in stark contrast when [...]"

Suggestion:

"This is in stark contrast to when [..]"
"This is in stark contrast to the case when [..]"

Or, to avoid imprecise expressions:

"This is the opposite of when [...]"
"This is the opposite of the case when [...]"

******* Final Considerations and Future Works
1. Original:

"[...] the execution of scientific applications on a myriad of environments [...]"
"[...] executed on a range of heterogeneous architectures [...]"

Suggestion:

Avoid imprecise or grandiose terms such as "range" and "myriad". Always use an
exact number when possible. When an exact number can't be obtained, use "many"
or "multiple".
*** [2018-08-23 Thu]
**** Topics for Meeting with Arnaud
***** Evaluation letter for CAPES and visa Extension
- Alfredo, Brice and Jean-Marc should sign
***** Recent Autotuning "Review" Paper :ATTACH:
:PROPERTIES:
:Attachments: balaprakash2018autotuning.pdf
:ID:       b9173855-f4a8-42df-8ce4-ce4cba2d9935
:END:
- Balaprakash (Orio, SPAPT), Dongarra, Gamblin, Hall, Hollingsworth,
  Norris (Orio, SPAPT), Vuduc
***** Current SPAPT Experiments
- Tests with other search techniques?
****** Uniform Random Samples of Search Spaces
- Sampled spaces for some problems do not seem good targets for
  our approach, since most configurations are better than the default
****** Original SPAPT Paper
- Uses mean of 35 executions in evaluations
- I was not able to reproduce the "densities" of runtime
  for the problems in the paper
****** O3 Usage
- My initial assumption was that they used O3 with all parameters set to zero
  as the initial point, and then kept using O3 for all other points measured
- I also assumed that removing O3 and only using code transformations would
  always decrease performance
- Additionally, initial experiments with O2, O1 and O0 showed similar sample
  distributions as in the O3 case, but this was not tested for all problems
****** Using Designs for Autotuning
******* Choosing the Initial Model
- Implemented a way to specify model terms in the Orio problem specification
- It is possible to add interactions cubic and quadratic terms
- In my experiments I am only using linear and quadratic terms so far, without interactions
- How to select the best model for each application?
- Adding more terms decreases D-Efficiency and increases the number of experiments
******* Pruning the Model
- In some cases, points sampled during the design ended up
  having better performance than the best predicted point after fitting
- When that happened, I fixed the most "relevant" factors to the
  value of the best point on the design, instead of to the value of the best
  predicted point
- Removed the second sampling for the prediction using the fitted model,
  the prediction is now made on the initial uniformly sampled set
******* Finding Good Points
- Usually the first evaluated design has good results, which is usually not improved
  upon too much by further iterations
- We are more interested in finding a good point than in finding the
  perfect model for the problem, so good solutions should be prioritized
  against the solutions predicted by the model
***** IPDPS Submission
- http://www.ipdps.org/ipdps2019/2019-organization.html
- http://iwapt.org/2019/
- Invite Steven
- I believe we can write a paper for IPDPS 19, or to
  the autotuning workshop that usually happens in it
- As a strong motivation, present results obtained in Steven's problem
- Finish experiments with Orio and SPAPT
- Discuss the performance of our approach in different problems
*** [2018-08-24 Fri]
**** Plotting Orio Experiments Data (dgemv3, O3, Xeon E5 2630 v2, Fixing Factors with Design Best)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/results.csv")

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 2)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dLTdMe/figureFUVqNF.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/search_space.csv")

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[2, "technique"] <- "RS"
best <- data[data$cost_mean == min(data$cost_mean), ]
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 1050)) +
    geom_text(aes(x = cost_mean - 0.02, y = 200, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dLTdMe/figurekxWgId.png]]
*** [2018-08-27 Mon]
**** Plotting Orio Experiments Data (dgemv3, O3, Xeon E5 2630 v2, Fixing Factors with Predicted Best)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/results.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_predicted/results.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 2)) +
    #geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max + 0.05, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    #geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-JtRBod/figure24NU56.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_best/search_space.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/dgemv3/xeon_e5_2630_v2/full_model_fixed_predicted/search_space.csv")
data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

dim(data)
data <- data[data$correct_result == "True", ]
dim(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_Ia == 1 &
                    data$T1_Ja == 1 & data$T3_I == 1 & data$T3_J == 1 &
                    data$T3_Ia == 1 & data$T3_Ja == 1 & data$T5_I == 1 &
                    data$T5_J == 1 & data$T5_Ia == 1 & data$T5_Ja == 1 &
                    data$T7_I == 1 & data$T7_J == 1 & data$T7_Ia == 1 &
                    data$T7_Ja == 1 & data$U1_I == 1 & data$U1_J == 1 &
                    data$U2_I == 1 & data$U3_I == 1 & data$U3_J == 1 &
                    data$U4_I == 1 & data$U5_I == 1 & data$U5_J == 1 &
                    data$U6_I == 1 & data$U7_I == 1 & data$U7_J == 1 &
                    data$U8_I == 1 & data$U9_I == 1 & data$U10_I == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT3_I == 1 &
                    data$RT3_J == 1 & data$RT5_I == 1 & data$RT5_J == 1 &
                    data$RT7_I == 1 & data$RT7_J == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2  == "False" & data$VEC3 == "False" &
                    data$VEC4 == "False" & data$VEC5 == "False" & data$VEC6 == "False" &
                    data$VEC7 == "False" & data$VEC8 == "False" & data$VEC9 == "False" &
                    data$VEC10 == "False", ]

O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[2, "technique"] <- "RS"
best <- data[data$cost_mean == min(data$cost_mean), ]
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: [1] 6163   57
: [1] 6163   57

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 1400)) +
    geom_text(aes(x = cost_mean + 0.02, y = 700, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-JtRBod/figurefiVktR.png]]
**** Interesting Papers on Autotuner Performance Comparison :ATTACH:
:PROPERTIES:
:Attachments: knijnenburg2003combined.pdf seymour2008comparison.pdf
:ID:       b4e4b662-3cd1-4c9e-ad70-68bf9a907e1b
:END:

Both papers present performance comparisons of different autotuning strategies
based on search, such as simulated annealing and particle swarm optimization.
The conclusion of both studies is that, in general, those techniques are
marginally better than a random search in the problems tested.
*** [2018-08-28 Tue]
**** Experimental Settings on the Xeon E5 2630 v2
The tables in this section are for keeping track of what would be interesting to
measure and what was already measured.

***** Uniform Random Sampling
|----+--------------------+---------|
|    | Application        | Sampled |
|----+--------------------+---------|
|  1 | stencil3d          | DONE    |
|  2 | dgemv3             | DONE    |
|  3 | atax               | DONE    |
|  4 | gemver             | DONE    |
|  5 | bicgkernel         | DONE    |
|  6 | trmm               |         |
|  7 | adi                | *       |
|  8 | gesummv            |         |
|  9 | mvt                |         |
| 10 | fdtd               | *       |
| 11 | correlation        |         |
| 12 | lu                 |         |
| 13 | tensor-contraction |         |
| 14 | mm                 |         |
| 15 | jacobi             |         |
| 16 | covariance         |         |
| 17 | hessian            |         |
| 18 | seidel             |         |
|----+--------------------+---------|
***** Setting 0
- "Large" (8) number of ANOVA steps
- Model with linear terms
- Using complete model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     | 5/10 |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 1
- "Large" (8) number of ANOVA steps
- Model with linear and quadratic terms
- Using complete model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     | 3/10 |
|  2 | dgemv3             | DONE |     |     |      |      |     | DONE |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 2
- "Large" (8) number of ANOVA steps
- Model with linear and quadratic terms
- Using only relevant variables on model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     |      |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 3
- "Small" (?) number of ANOVA steps
- Model with linear and quadratic terms
- Using complete model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     |      |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
***** Setting 4
- "Small" (?) number of ANOVA steps
- Model with linear and quadratic terms
- Using only relevant variables on model for prediction

|----+--------------------+------+-----+-----+------+------+-----+------|
|    | Application        | RS   | RSL | EXH | SIMA | SIMP | MLS | DLMT |
|----+--------------------+------+-----+-----+------+------+-----+------|
|  1 | stencil3d          | DONE |     |     |      |      |     |      |
|  2 | dgemv3             | DONE |     |     |      |      |     |      |
|  3 | atax               | DONE |     |     |      |      |     |      |
|  4 | gemver             | DONE |     |     |      |      |     |      |
|  5 | bicgkernel         | DONE |     |     |      |      |     |      |
|  6 | trmm               |      |     |     |      |      |     |      |
|  7 | adi                |      |     |     |      |      |     |      |
|  8 | gesummv            |      |     |     |      |      |     |      |
|  9 | mvt                |      |     |     |      |      |     |      |
| 10 | fdtd               |      |     |     |      |      |     |      |
| 11 | correlation        |      |     |     |      |      |     |      |
| 12 | lu                 |      |     |     |      |      |     |      |
| 13 | tensor-contraction |      |     |     |      |      |     |      |
| 14 | mm                 |      |     |     |      |      |     |      |
| 15 | jacobi             |      |     |     |      |      |     |      |
| 16 | covariance         |      |     |     |      |      |     |      |
| 17 | hessian            |      |     |     |      |      |     |      |
| 18 | seidel             |      |     |     |      |      |     |      |
|----+--------------------+------+-----+-----+------+------+-----+------|
*** [2018-08-29 Wed]
**** Plotting stencil3d (Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/results.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted/results.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
5: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
6: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    #geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max + 0.05, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    #geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-rB9l61/figurewLjn6V.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/search_space.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted/search_space.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)


data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
:  num [1:2] 0.0736 0.0739

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 120)) +
    geom_text(aes(x = cost_mean + 0.05, y = 123, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-rB9l61/figure0XDFiY.png]]
*** [2018-08-30 Thu]
**** Plotting stencil3d (Linear, Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/results.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted_linear/results.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))
#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
5: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
6: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    #geom_curve(data = data_max, aes(x = max + 0.1, y = 2, xend = max + 0.05, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    #geom_text(aes(x = max + 0.12, y = 2, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figuregtIPAU.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
random_sample_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/old_results/stencil_O3/search_space.csv")
experiment_data <- read.csv("orio_experiments/testsuite/SPAPT/stencil3d/xeon_e5_2630_v2/full_model_fixed_predicted_linear/search_space.csv")

data <- bind_rows(random_sample_data[random_sample_data$technique == "RS", ], experiment_data)


data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

str(O3_baseline$cost_mean)
#+END_SRC

#+RESULTS:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
:  num [1:2] 0.0736 0.0707

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 160)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figure07533Y.png]]
*** [2018-08-31 Fri]
**** Learning About Grid5000 and Setting up Experiments
Basic information on how to allocate resources and deploy images:

- https://www.grid5000.fr/mediawiki/index.php/Getting_Started

Creating permanent images after configuration:

- https://www.grid5000.fr/mediawiki/index.php/Advanced_Kadeploy

Visualizing status of all clusters:

- https://api.grid5000.fr/3.0/ui/visualizations/status.html

Hardware:

- List of all hardware: https://www.grid5000.fr/mediawiki/index.php/Hardware
- Targetting the Xeon E5 2630 v3 because it is the most numerous

- Target server: Nancy
  - Target clusters: grisou, grimoire, graoully

- Target server: Rennes
  - Target clusters: parasilo, paravance

Setting up the SSH Agent and Agent Forwarding:

- https://www.grid5000.fr/mediawiki/index.php/SSH#Using_the_SSH_agent

GitHub repository with configuration files for the Grid5000 deploy:

- https://github.com/phrb/orio_experiments_g5kjob
** September
*** [2018-09-03 Mon]
**** DOE + Autotuning Summaries (Collected from previous entries)
I aggregated all past entries that contain summaries and explanations on DoE and
Autotuning. I repeat them here because the performance of the other journal file
is very bad.

***** Some References on Optimal Multi-Level Supersaturated Designs
I was looking into works that cited that Addelman and Kempthorne paper and found
that one way to name those kinds of designs is "Optimal Multi-Level
Supersaturated".

Searching by that term I found some more recent references which I believe will
take some time to digest, but that present some ways to construct those designs:

https://projecteuclid.org/download/pdfview_1/euclid.aos/1140191674
https://projecteuclid.org/download/pdfview_1/euclid.ssu/1244555797

Those works also seem to present ways to measure optimality for those designs,
but I still haven't read them.

There is also a reference which seems interesting by its abstract, but I was not
able to find it yet:

https://www.sciencedirect.com/science/article/pii/S0378375812003084

I also found this book on factorial designs which I did not know of, but that
does not seem to discuss Addelman Kempthorne designs directly. It could be
useful for me to study more about DoE in general.

https://www.springer.com/gp/book/9780387319919
***** Reading About Design of Experiments, Factorial Designs, Supersaturated Designs
- [[file:~/Dropbox/papers/design-of-experiments/georgiou2014supersaturated.pdf][Georgiou's paper on Supersaturated Designs]]
- [[file:~/Dropbox/papers/design-of-experiments/montgomery2013design.pdf][Montgomery's Book]]
***** [[file:~/Dropbox/papers/design-of-experiments/mukerjee2006modern.pdf][Mukerjee's Book on Factorial Designs]]
- The introduction points to [[file:~/Dropbox/papers/design-of-experiments/wu2000experiments.pdf][Wu's Book on Design of Experiments]], which seems very nice, especially:
  - Chapter 7: Other Design and Analysis Techniques for Experiments at More than
    Two Levels
  - Chapter 8: Nonregular Designs: Construction and Properties
***** Generating Good/Optimal Mixed-Level Designs
Reading [[file:~/Dropbox/papers/design-of-experiments/mukerjee2006modern.pdf][Mukerjee's book]] on Factorial Designs pointed to [[file:~/Dropbox/papers/design-of-experiments/wu2000experiments.pdf][Wu's book]] on Design of
Experiments, which has two chapters (7 & 8) on the design and analysis of
multi-level experimental designs.

Search for more specific terms such as "multi-" or "mixed-level experimental
designs" led to finding [[file:~/Dropbox/papers/design-of-experiments/fontana2013algebraic.pdf][Fontana's paper]] on algebraic generation of minimum
orthogonal designs. A [[file:~/Dropbox/papers/design-of-experiments/fontana2017generalized.pdf][later paper]] from the same author generalized these
constructions to mixed-level orthogonal arrays, which can be used to generate
mixed-level optimal designs. Fontana [[file:~/Dropbox/papers/design-of-experiments/fontana2013minimum.pdf][implemented his algorithm]] using the
proprietary software SAS.

Fontana later collaborated with Ulrike, from the ~DoE~ R packages, to implement
this construction in the ~DoE.MIParray~ R [[https://cran.r-project.org/web/packages/DoE.MIParray/DoE.MIParray.pdf][package]]. The [[file:~/Dropbox/papers/design-of-experiments/ulrike2018algorithm.pdf][description]] of the
implementation mentions that proprietary solvers such as ~gurobi~ where used in
the solution.

We now have a way of generating optimal mixed-level orthogonal experimental
designs using R, albeit using proprietary software. Would it be worth evaluating
the difficulty of implementing this using open-source software?

***** Tuning FPGAs using Experimental Design
Looking for works that cited Fontana's paper I came across a 2018 JPDC paper on
[[file:~/Dropbox/papers/design-of-experiments/tuzov2018tuning.pdf][tuning synthesis flags on FPGAs]]. This work uses fractional factorial
experimental designs to explore a subset of flags that control synthesis in
FPGAs, targeting a specific architecture.

The 9 optimization flags they tested in this paper were selected because they
were the most common flags across multiple FPGA synthesis tools. The authors
give no other reason to select those flags. The total number of flags and
optimizations in the targeted system was 72.

Of those 9 flags, it is not entirely clear which are multi-level factors and
which are only boolean or two-level factors. It is also not clear whether the
other 63 flags on the system are multi-level, integers, or other. It seems they
converted some flags to string selections, but this is not explained further.

They used a 2^{9 - 4}_{IV} fractional factorial design with 32 runs to measure
the impact of the 9 flags. The entire search space in this case has 512
different configurations. They use a set of 24 response variables to measure the
effectiveness of each configuration, and attempt to fit linear models using the
9 factors for each response variable, separately.

One particularly weird choice of this study was to use the fitted linear models,
that used 32 configurations, to compute the values of the response variables for
the remaining 480 configurations. This is strange because the using ANOVA
attempts to measure whether each factor impacts the response variables, but not
necessarily gives a correct model, since factor levels are mapped to [0, 1] in
this case.

They did not re-evaluate the predicted points to check for accuracy and did not
show where in the search space the 32 actually measured points were.
***** Reading [[file:~/Dropbox/papers/design-of-experiments/mukerjee2006modern.pdf][Mukerjee's Book]] on Factorial Designs
Chapter 2 provides fundamental definitions of concepts related to factorial
designs. Its definition of contrasts is a little bit different from [[https://en.wikipedia.org/wiki/Contrast_(statistics)][this one]].
The [[https://en.wikipedia.org/wiki/Kronecker_product][Kronecker product]] is also a useful concept presented in this chapter.

This book is much more mathematically denser than any other DoE book I have read
so far. It seems to be a good mathematical foundation for the concepts presented
in books such as [[file:~/Dropbox/papers/design-of-experiments/montgomery2013design.pdf][Montgomery's]], but does not offer much in terms of practical
algorithms or implementations.

A more practical reference is [[file:~/Dropbox/papers/design-of-experiments/hedayat1999orthogonal.pdf][Hedayat's]] Orthogonal Arrays: Theory and
Applications. That book gives an algorithm for the expansive and contractive
replacement techniques for constructing orthogonal arrays from existing
orthogonal arrays.

***** Expansive & Contractive Replacement Method from [[file:~/Dropbox/papers/design-of-experiments/hedayat1999orthogonal.pdf][Hedayat's Orthogonal Arrays]]
Section 9.3 of this book presents the expansive and contractive replacement
methods, two apparently simple strategies to construct mixed-level designs from
other designs, or from parts from the same design.

The expansive replacement method can be used to insert runs from designs with
factors with less levels into designs with factors with more levels, by
replacing selected columns. This seems straightforward because it "encodes" a
set of columns with more levels to a column with less levels.

The contractive replacement method seems more directly applicable in our case.
This method is what the AutoDesign system [[http://www.functionbay.co.kr/documentation/onlinehelp/default.htm#!Documents/plackettandburmandesign.htm][claims to use]] to generate its
mixed-level Plackett-Burman designs. The method requires finding a set of
columns in the original design that fulfill certain properties. Those columns
are used to encode a new factor with more levels, in a process similar to the
one discussed with Arnaud. The main advantages of this method are the guarantee
of not losing orthogonality of the design and not changing the number of runs.

****** The Contractive Replacement Method
One notation for orthogonal arrays is $OA(N, s_1, \dots, s_k, t)$, where
$N$ is the number of rows, $s_1, \dots, s_k$ are the factors, where each $s_i$
represents the number of levels, and $t$ is the strength.

To use the contractive method on an array $A = OA(N, s_1, \dots, s_k, 2)$, we
need to find an OA $B = OA(N, s_u, \forall u \in U, 2)$, constructed from a
subset $U$ of columns of $A$. $B$ must have the following properties:

1. $B$ is composed of $N / L$ repetitions of the same rows, where $L$ denotes
   the number of levels of the new factor
2. $L = 1 - u + \sum\limits_{u \in U}{s_u}$

If we can find $B$, we then encode each unique row as $0, 1, \dots, L - 1$ and
replace the corresponding rows of $A$, in columns in $U$, with this new
encoding. This effectively "contracts" the columns in $U$ into a new column
represeting a factor with L levels.

***** Thoughts on the Contractive Replacement Method
It seems that Plackett-Burman designs are not very well suited
for using this method, since no groups of columns seem to have
the necessary properties. Maybe there is something wrong with my
implementation.

Future steps could be:

1. Re-implement this checking in R using FrF2's ~pb~
2. Try this with different classes of OAs (full factorial?)
3. Debug the checking implementation

***** ~DoE.MIParray~
This technique does not seem to be the best approach for our problem,
since the construction involves solving a mixed integer problem that
dependes on the number of levels. From a note in the documentation:

#+BEGIN_QUOTE
The package is not meant for situations, for which a full factorial design would
be huge; the mixed integer problem to be solved has at least prod(nlevels)
binary or general integer variables and will likely be untractable, if this
number is too large. (For extending an existing designs, since some variables
are fixed, the limit moves out a bit.)
#+END_QUOTE

Therefore, attempting to construct a design for more than a 100 factors with
multiple levels each would not be feasible.

***** Summarizing Strategies for Multi-Level Designs

****** 2-Level Screening with Random Sampling of Levels

In this strategy factors with more than two unordered levels are sampled at two
random levels. This enables the usage of a small design such as the
Plackett-Burman screening design.

Advantages are the small design size and good estimation capability for main
effects. Disadvantages are the incapability of estimating interactions, but
mainly the lack of information regarding the response for levels not selected in
the initial screening.

****** Level Projection, Factor Merging or Contractive Replacement

An initial 2-Level design is used to generated mixed-level designs by reencoding
some columns into a new single column represeting a multi-level factor. The
contractive replacement of Addelman-Kempthorne is a strategy of this kind.

Advantages are also small design size and good estimation capability of main
effects. Additionally, the contractive replacement technique is keeps
orthogonality of designs. Disadvantages are the requirements on the initial
designs. Not all 2-Level designs can be contracted with those methods if we want
to keep orthogonality.

****** Direct Generation

The work from [[file:~/Dropbox/papers/design-of-experiments/ulrike2018algorithm.pdf][GrÃ¶mping and Fontana]] enables the generation of multi-level
designs with the Generalized Minimum Aberration optimality criterion by
solving mixed integer problems.

Advantages are the direct generation of multi-level designs and the optimality
criteria. Disadvantages are the use of proprietary MIP solvers and the
limitations on the size and shape of the designs that can be generated.

****** Optimal Designs

This strategy consists of generating optimal multi-level designs. I still did not
find any algorithms or papers related to this.

***** Reading Triefenbach's Bachelor Thesis on D-Optimal Designs
The [[http://www8.cs.umu.se/education/examina/Rapporter/FabianTtriefenbach.pdf][thesis]] presents some of the theory behind D-Optimal Designs in a simple way.
It discusses a modification to the model matrix that enables constructing less
model-dependent D-Optimal designs, but does not details why this "Bayesian
modification" helps.

Constructing D-Optimal designs seems to be the most straightforward way of
generating large multi-level designs I have found so far. In theory, it is
possible to generate designs of any number of runs, with any number of factors
with any number of different levels. At first, it seems that this is the best
candidate so far.

In practice, even though the tools from ~AlgDesign~ generate designs for
arbitrary numbers of factors and levels, that implementation does not work for
even slightly larger designs. I believe this happens due to implementation
choices and not "theoretical" or algorithmic limitations.

****** Constructing D-Optimal Designs
A D-Optimal design is a selection from a set of possible experiments that
minimizes some optimality criterion.

For example, imagine we have a full factorial design. We decide that we want
to analyse this design using a "simple linear model", such as =~.=. Therefore,
the *set of possible experiments*, or *model matrix* for this experiment is
obtained by adding a column of ones to the original full factorial design. If we
had interations, quadratic terms, and so on, we would simply compute the
correspondent columns using original columns in the original design.

With the model matrix defined, the criterion of our D-Optimal design could be
minimizing the determinant of the *information matrix*. This new matrix is
defined by multiplying the previously generated model matrix by its inverse.

Now, the hard part is finding a selection of experiments from the *model matrix*
that minimizes the determinant of the *information matrix*. Common algorithms
for this task seem to be pretty straightforward, relying on swapping columns of
an existing matrix with columns in the possible runs and measuring changes in
the determinant.
***** Understanding D-Efficiency
I've found conflicting definitions for the D-Efficiency criteria.

****** In the ~AlgDesign~ Package
The first definition is from the ~AlgDesign~ [[https://cran.r-project.org/web/packages/AlgDesign/AlgDesign.pdf][R package]]:

\begin{equation}
  D_{elb} = exp(1 - 1 / G_e)
\end{equation}

Where $G_e$ is given by:

\begin{equation}
  G_e = k / max(x\prime{}(X\prime{}X)x, \forall x \in X))
\end{equation}

Where $k$ is the number of of column in the model matrix $X$, and each $x$ is a
row in $X$ that describes an experiment.

I was not able to find a paper or book describing or explaining this criteria.

****** From DETMAX, SAS, and Castillo's Book                    :ATTACH:
:PROPERTIES:
:Attachments: castillo2007process.pdf
:ID:       21aa2891-9cc8-4e49-8f75-ad668b71ea93
:END:

Mitchell's [[https://www.tandfonline.com/doi/abs/10.1080/00401706.1974.10489176][second paper on the DETMAX algorithm]] and Castillo's book (Process
Optimization: A Statistical Approach, annexed to this section) present a
different criteria for D-Efficiency:

\begin{equation}
  D_{elb} = det(X\prime{}X)^(1/k) / N
\end{equation}

Where $X$ is the model matrix, $k$ is its number of columns and $N$ its number
of rows.

Castillo mentions that this is a lower bound on the D-Efficiency of a design,
more specifically, from section 5.7.2, page 152 of Castillo's book, this version
of D-Efficiency "[...] is a measure of D-optimality with respect to a
hypothetical orthogonal design (which would be D-optimal)[...]".

****** From Fedorov's Theory of Optimal Designs                 :ATTACH:
:PROPERTIES:
:Attachments: fedorov1972theory.pdf
:ID:       1c941f7d-bd93-40ab-a1a5-4033eab52841
:END:

***** Understanding D-Efficiency: The Box and Draper Encoding
:PROPERTIES:
:Attachments: box2007response.pdf
:ID:       c8cb30f5-a404-4ea8-bb9e-90bd726c00a0
:END:

Box and Draper's 2007 edition of "Response Surfaces, Mixtures and Ridge
Analyses" is a very clear book which explains the encoding used in the
computation of the D-Efficiency metric used in the DETMAX paper and Castillo's
book (See attachment in [[From DETMAX, SAS, and Castillo's Book]]). I am currently
reading Chapter 14, Variance Optimal Designs, where the book describes D-Optimal
designs and the encoding used. The intuition is that this metric compares
***** Simple Terminology for Design of Experiments
****** Basic Concepts
- An *experimental design* is a plan for running a set of *experiments*,
  can be displayed as a matrix
- *Factors* are the columns of an experimental design matrix, and
  represent experimental variables with at least two possible values, or *levels*
- *Categorical factors* have unordered levels, that is, there is no sense of progression
  from one level to another
- *Numerical factors* can be *continuous*, such as floating-point values, or *discrete*,
  such as integer values
- The rows of an experimental design matrix are individual
  *treatments*, *experiments*, or *runs*
- Each *run* provides a measurement of a *dependent variable*, or
  *response*, subject to a specific assignment of *factor levels*
- Running all *experiments* in a design gives some information that allows
  studying the *effects* of *factor levels* in the *response variable*
- Two *effects* are *aliased* or *confounded* when it is not possible to
  distinguish them from the data obtained from running a given design
- The *resolution* of a design indicates which *effects* are independently
  *estimated* in a design. There is less *aliasing* or *confounding* between
  *effects* as the *resolution* increases
- A *full-factorial design* is an experimental design containing all
  possible combinations of *factor levels*
- A *fractional-factorial design* is a subset of the rows of a *full-factorial
  design*
- An experimental design is *balanced* when each *level* occurs equally often
  within every *factor*
- An experimental design is *orthogonal* when every *pair of levels* occurs
  equally often in all *pairs of factors*. A design can also be *orthogonal*
  when the occurence frequencies of *pairs of levels* are proportional
****** Modelling and Analysing
- A *model* specifies which *factors* and *factor relationships* will be considered in
  a posterior *linear regression* or *analysis of variance*, using the measured
  *responses*
- A *model matrix* or *design matrix* is constructed with an original
  *experimental design* and according to a *model*. Each column, or set of
  columns for categorical factors, in a *model matrix* represents one *effect*
- *Contrasts* can be used to encode *categorical factors* in a *model matrix*
  using extra columns. *Contrast matrices* specify the encoding for each level
  of a *categorical factor*
- An *information matrix* is defined as the multiplication of the transpose of
  a *model matrix* and the *model matrix* itself. The inverse of the *information matrix*
  is the *dispersion matrix*, or *covariance matrix*
****** Design Efficiency
- In a *least-squares linear regression*, considering a *linear relationship* between
  variables, *parameter estimates* are proportional to a design's *covariance matrix*
- The *eigenvalues* of the *covariance matrix* of a design represent its "size" in a certain sense
- The *A-Efficiency* of a design is inversely proportional to the *arithmetic mean* of the *eigenvalues*
  of the design's *covariance matrix*
- The *D-Efficiency* of a design is inversely proportional to the *geometric mean* of the *eigenvalues*
  of the design's *covariance matrix*
- For a given set of *factors*, a hypothetical *balanced* an *orthogonal* design has optimal *A-Efficiency* and
  *D-Efficiency*
- A design with $N$ rows is *balanced and orthogonal* when the *dispersion matrix* is diagonal
  and equals to $\dfrac{1}{N}\mathbf{I}$
  - The following designs are all *balanced and orthogonal*:
    #+HEADER: :results output :session *R*
    #+BEGIN_SRC R
    library(AlgDesign)
    library(FrF2)

    design <- gen.factorial(c(2, 2, 2, 2))
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    eval.design(~., design)

    design <- pb(16)
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    design <- FrF2(nruns = 8, nfactors = 4)
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)
    #+END_SRC

    #+RESULTS:
    #+begin_example
       X1 X2 X3 X4
    1  -1 -1 -1 -1
    2   1 -1 -1 -1
    3  -1  1 -1 -1
    4   1  1 -1 -1
    5  -1 -1  1 -1
    6   1 -1  1 -1
    7  -1  1  1 -1
    8   1  1  1 -1
    9  -1 -1 -1  1
    10  1 -1 -1  1
    11 -1  1 -1  1
    12  1  1 -1  1
    13 -1 -1  1  1
    14  1 -1  1  1
    15 -1  1  1  1
    16  1  1  1  1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
       (Intercept) X1 X2 X3 X4
    1            1 -1 -1 -1 -1
    2            1  1 -1 -1 -1
    3            1 -1  1 -1 -1
    4            1  1  1 -1 -1
    5            1 -1 -1  1 -1
    6            1  1 -1  1 -1
    7            1 -1  1  1 -1
    8            1  1  1  1 -1
    9            1 -1 -1 -1  1
    10           1  1 -1 -1  1
    11           1 -1  1 -1  1
    12           1  1  1 -1  1
    13           1 -1 -1  1  1
    14           1  1 -1  1  1
    15           1 -1  1  1  1
    16           1  1  1  1  1
    attr(,"assign")
    [1] 0 1 2 3 4
                (Intercept) X1 X2 X3 X4
    (Intercept)           1  0  0  0  0
    X1                    0  1  0  0  0
    X2                    0  0  1  0  0
    X3                    0  0  0  1  0
    X4                    0  0  0  0  1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
    Screening 15 factors in 16 runs involves perfect aliasing of
     pairwise interactions of the first six factors with the last factor.
        A  B  C  D  E  F  G  H  J  K  L  M  N  O  P
    1  -1  1 -1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1
    2   1  1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1 -1  1
    3   1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1
    4  -1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1  1 -1
    5   1 -1 -1  1  1 -1  1 -1 -1  1 -1  1 -1  1 -1
    6  -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1  1  1
    7  -1 -1  1  1  1  1 -1 -1 -1 -1  1  1 -1 -1  1
    8   1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1
    9  -1  1 -1  1  1 -1 -1  1 -1  1  1 -1  1 -1 -1
    10  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
    11 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1  1
    12 -1 -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1
    13  1 -1  1 -1 -1  1 -1  1 -1  1 -1  1  1 -1 -1
    14 -1  1  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1
    15  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1
    16  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1 -1 -1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
       (Intercept)  A  B  C  D  E  F  G  H  J  K  L  M  N  O  P
    1            1 -1  1 -1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1
    2            1  1  1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1 -1  1
    3            1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1
    4            1 -1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1  1 -1
    5            1  1 -1 -1  1  1 -1  1 -1 -1  1 -1  1 -1  1 -1
    6            1 -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1  1  1
    7            1 -1 -1  1  1  1  1 -1 -1 -1 -1  1  1 -1 -1  1
    8            1  1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1
    9            1 -1  1 -1  1  1 -1 -1  1 -1  1  1 -1  1 -1 -1
    10           1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
    11           1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1  1
    12           1 -1 -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1
    13           1  1 -1  1 -1 -1  1 -1  1 -1  1 -1  1  1 -1 -1
    14           1 -1  1  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1
    15           1  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1
    16           1  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1 -1 -1
    attr(,"assign")
     [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
                (Intercept) A B C D E F G H J K L M N O P
    (Intercept)           1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
    A                     0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
    B                     0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
    C                     0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
    D                     0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
    E                     0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
    F                     0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
    G                     0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
    H                     0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
    J                     0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
    K                     0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
    L                     0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0
    M                     0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
    N                     0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
    O                     0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
    P                     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
       A  B  C  D
    1  1 -1 -1  1
    2 -1  1  1 -1
    3  1 -1  1 -1
    4 -1 -1  1  1
    5  1  1 -1 -1
    6  1  1  1  1
    7 -1 -1 -1 -1
    8 -1  1 -1  1
    $determinant
    [1] 1

    $A
    [1] 1

    $diagonality
    [1] 1

    $gmean.variances
    [1] 1
      (Intercept)  A  B  C  D
    1           1  1 -1 -1  1
    2           1 -1  1  1 -1
    3           1  1 -1  1 -1
    4           1 -1 -1  1  1
    5           1  1  1 -1 -1
    6           1  1  1  1  1
    7           1 -1 -1 -1 -1
    8           1 -1  1 -1  1
    attr(,"assign")
    [1] 0 1 2 3 4
                (Intercept) A B C D
    (Intercept)           1 0 0 0 0
    A                     0 1 0 0 0
    B                     0 0 1 0 0
    C                     0 0 0 1 0
    D                     0 0 0 0 1
    #+end_example
- A design is *orthogonal* when the *dispersion matrix* is diagonal, excluding the row and column
  correspondent to the *intercept*, that is, there can be non-zero elements in the *intercept* row
  and column
  - The following designs are all *orthogonal*, but not *balanced*:
    #+HEADER: :results output :session *R*
    #+BEGIN_SRC R
    library(AlgDesign)
    library(FrF2)

    design <- oa.design(nfactors = 6, nlevels = 3)
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    t(model_matrix) %*% model_matrix

    design <- oa.design(nlevels=c(4,3,3,2))
    design <- data.frame(design)
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)
    #+END_SRC

    #+RESULTS:
    #+begin_example
    The columns of the array have been used in order of appearance.
    For designs with relatively few columns,
    the properties can sometimes be substantially improved
    using option columns with min3 or even min34.
       A B C D E F
    1  1 3 2 3 2 1
    2  3 3 1 1 2 2
    3  2 2 2 2 2 2
    4  3 2 1 2 1 3
    5  3 2 2 1 3 1
    6  2 3 1 2 3 1
    7  2 1 1 3 2 3
    8  3 1 2 3 1 2
    9  2 2 3 3 1 1
    10 3 3 3 3 3 3
    11 1 2 3 1 2 3
    12 2 1 3 1 3 2
    13 2 3 2 1 1 3
    14 1 2 1 3 3 2
    15 1 1 2 2 3 3
    16 3 1 3 2 2 1
    17 1 3 3 2 1 2
    18 1 1 1 1 1 1
    $determinant
    [1] 0.7064227

    $A
    [1] 6.571429

    $diagonality
    [1] 0.261

    $gmean.variances
    [1] 1.5
       (Intercept) A B C D E F
    1            1 1 3 2 3 2 1
    2            1 3 3 1 1 2 2
    3            1 2 2 2 2 2 2
    4            1 3 2 1 2 1 3
    5            1 3 2 2 1 3 1
    6            1 2 3 1 2 3 1
    7            1 2 1 1 3 2 3
    8            1 3 1 2 3 1 2
    9            1 2 2 3 3 1 1
    10           1 3 3 3 3 3 3
    11           1 1 2 3 1 2 3
    12           1 2 1 3 1 3 2
    13           1 2 3 2 1 1 3
    14           1 1 2 1 3 3 2
    15           1 1 1 2 2 3 3
    16           1 3 1 3 2 2 1
    17           1 1 3 3 2 1 2
    18           1 1 1 1 1 1 1
    attr(,"assign")
    [1] 0 1 2 3 4 5 6
                (Intercept)    A    B    C    D    E    F
    (Intercept)          37 -3.0 -3.0 -3.0 -3.0 -3.0 -3.0
    A                    -3  1.5  0.0  0.0  0.0  0.0  0.0
    B                    -3  0.0  1.5  0.0  0.0  0.0  0.0
    C                    -3  0.0  0.0  1.5  0.0  0.0  0.0
    D                    -3  0.0  0.0  0.0  1.5  0.0  0.0
    E                    -3  0.0  0.0  0.0  0.0  1.5  0.0
    F                    -3  0.0  0.0  0.0  0.0  0.0  1.5
                (Intercept)  A  B  C  D  E  F
    (Intercept)          18 36 36 36 36 36 36
    A                    36 84 72 72 72 72 72
    B                    36 72 84 72 72 72 72
    C                    36 72 72 84 72 72 72
    D                    36 72 72 72 84 72 72
    E                    36 72 72 72 72 84 72
    F                    36 72 72 72 72 72 84
    creating full factorial with 72 runs ...
       A B C D
    1  1 3 2 1
    2  2 1 1 2
    3  2 3 1 1
    4  3 3 3 1
    5  2 1 3 1
    6  2 1 3 2
    7  1 1 2 2
    8  2 2 1 1
    9  3 1 3 1
    10 1 3 3 1
    11 4 1 2 1
    12 4 2 2 2
    13 2 2 2 1
    14 4 1 2 2
    15 4 3 1 2
    16 2 2 2 2
    17 3 2 3 1
    18 3 2 2 2
    19 4 1 1 1
    20 1 3 1 1
    21 3 2 1 1
    22 2 3 2 1
    23 3 3 1 1
    24 3 2 1 2
    25 4 1 3 1
    26 2 1 2 2
    27 1 1 3 1
    28 4 1 1 2
    29 1 1 1 1
    30 1 2 2 2
    31 2 1 1 1
    32 1 3 3 2
    33 4 2 1 1
    34 3 3 2 1
    35 4 3 2 2
    36 1 3 2 2
    37 1 1 1 2
    38 2 2 1 2
    39 3 1 2 2
    40 3 2 2 1
    41 1 3 1 2
    42 2 2 3 1
    43 4 2 3 1
    44 3 1 3 2
    45 4 2 3 2
    46 1 2 3 1
    47 2 2 3 2
    48 2 3 1 2
    49 3 1 2 1
    50 1 1 2 1
    51 3 1 1 1
    52 3 3 2 2
    53 4 2 2 1
    54 4 3 3 2
    55 4 2 1 2
    56 4 3 2 1
    57 2 3 3 2
    58 4 1 3 2
    59 3 3 1 2
    60 2 1 2 1
    61 1 2 1 1
    62 3 3 3 2
    63 3 2 3 2
    64 1 2 3 2
    65 1 2 1 2
    66 2 3 3 1
    67 3 1 1 2
    68 4 3 1 1
    69 1 2 2 1
    70 1 1 3 2
    71 4 3 3 1
    72 2 3 2 2
    $determinant
    [1] 0.6738039

    $A
    [1] 6.96

    $diagonality
    [1] 0.31

    $gmean.variances
    [1] 1.638073
       (Intercept) A B C D
    1            1 1 3 2 1
    2            1 2 1 1 2
    3            1 2 3 1 1
    4            1 3 3 3 1
    5            1 2 1 3 1
    6            1 2 1 3 2
    7            1 1 1 2 2
    8            1 2 2 1 1
    9            1 3 1 3 1
    10           1 1 3 3 1
    11           1 4 1 2 1
    12           1 4 2 2 2
    13           1 2 2 2 1
    14           1 4 1 2 2
    15           1 4 3 1 2
    16           1 2 2 2 2
    17           1 3 2 3 1
    18           1 3 2 2 2
    19           1 4 1 1 1
    20           1 1 3 1 1
    21           1 3 2 1 1
    22           1 2 3 2 1
    23           1 3 3 1 1
    24           1 3 2 1 2
    25           1 4 1 3 1
    26           1 2 1 2 2
    27           1 1 1 3 1
    28           1 4 1 1 2
    29           1 1 1 1 1
    30           1 1 2 2 2
    31           1 2 1 1 1
    32           1 1 3 3 2
    33           1 4 2 1 1
    34           1 3 3 2 1
    35           1 4 3 2 2
    36           1 1 3 2 2
    37           1 1 1 1 2
    38           1 2 2 1 2
    39           1 3 1 2 2
    40           1 3 2 2 1
    41           1 1 3 1 2
    42           1 2 2 3 1
    43           1 4 2 3 1
    44           1 3 1 3 2
    45           1 4 2 3 2
    46           1 1 2 3 1
    47           1 2 2 3 2
    48           1 2 3 1 2
    49           1 3 1 2 1
    50           1 1 1 2 1
    51           1 3 1 1 1
    52           1 3 3 2 2
    53           1 4 2 2 1
    54           1 4 3 3 2
    55           1 4 2 1 2
    56           1 4 3 2 1
    57           1 2 3 3 2
    58           1 4 1 3 2
    59           1 3 3 1 2
    60           1 2 1 2 1
    61           1 1 2 1 1
    62           1 3 3 3 2
    63           1 3 2 3 2
    64           1 1 2 3 2
    65           1 1 2 1 2
    66           1 2 3 3 1
    67           1 3 1 1 2
    68           1 4 3 1 1
    69           1 1 2 2 1
    70           1 1 1 3 2
    71           1 4 3 3 1
    72           1 2 3 2 2
    attr(,"assign")
    [1] 0 1 2 3 4
                (Intercept)             A    B    C  D
    (Intercept)          27 -2.000000e+00 -3.0 -3.0 -6
    A                    -2  8.000000e-01  0.0  0.0  0
    B                    -3  0.000000e+00  1.5  0.0  0
    C                    -3  0.000000e+00  0.0  1.5  0
    D                    -6  4.440892e-16  0.0  0.0  4
    #+end_example
- A design is *balanced* when the elements of the *intercept* row and column are all zero, except
  for the element in the diagonal, that is, there can be non-zero elements in every other row and
  column
  - The following designs are all *balanced* but not *orthogonal*:
    #+HEADER: :results output :session *R*
    #+BEGIN_SRC R
    library(AlgDesign)
    library(FrF2)

    design <- data.frame(x1 = c(1, -1, 1, -1, 1, -1),
                         x2 = c(1, 1, 1, -1, -1, -1),
                         x3 = c(-1, 1, 1, 1, -1, -1))
    design[] <- lapply(design, as.character)
    design[] <- lapply(design, as.numeric)
    design

    eval.design(~., design)

    model_matrix <- model.matrix(~., design)
    model_matrix

    solve(t(model_matrix) %*% model_matrix) * nrow(design)

    eval.design(~., design)
    #+END_SRC

    #+RESULTS:
    #+begin_example
      x1 x2 x3
    1  1  1 -1
    2 -1  1  1
    3  1  1  1
    4 -1 -1  1
    5  1 -1 -1
    6 -1 -1 -1
    $determinant
    [1] 0.8773827

    $A
    [1] 1.375

    $diagonality
    [1] 0.84

    $gmean.variances
    [1] 1.5
      (Intercept) x1 x2 x3
    1           1  1  1 -1
    2           1 -1  1  1
    3           1  1  1  1
    4           1 -1 -1  1
    5           1  1 -1 -1
    6           1 -1 -1 -1
    attr(,"assign")
    [1] 0 1 2 3
                (Intercept)    x1    x2    x3
    (Intercept)           1  0.00  0.00  0.00
    x1                    0  1.50 -0.75  0.75
    x2                    0 -0.75  1.50 -0.75
    x3                    0  0.75 -0.75  1.50
    $determinant
    [1] 0.8773827

    $A
    [1] 1.375

    $diagonality
    [1] 0.84

    $gmean.variances
    [1] 1.5
    #+end_example
**** Application: stencil3d (Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/stencil3d/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)
data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])

str(data)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
[1] 10 34
[1] 10 34
[1] 300
[1] 235.3
'data.frame':	20 obs. of  34 variables:
 $ id       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ RT1_K    : int  1 1 32 1 1 32 1 8 1 1 ...
 $ RT1_J    : int  1 1 1 32 32 1 32 1 1 1 ...
 $ T1_Ja    : int  1 2048 256 256 64 64 128 2048 1024 1 ...
 $ RT1_I    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ T2_K     : int  512 16 512 1 128 512 1 512 16 256 ...
 $ T2_J     : int  256 128 16 1 16 512 16 32 16 16 ...
 $ T2_I     : int  512 1 1 32 64 16 32 16 256 64 ...
 $ T2_Ja    : int  256 512 1 1024 64 1 1024 1 1024 1 ...
 $ U2_I     : int  11 19 9 13 24 30 13 30 13 8 ...
 $ T1_I     : int  1 16 64 32 512 16 128 1 32 1 ...
 $ T1_J     : int  128 512 1 128 16 1 16 32 256 1 ...
 $ T1_K     : int  256 1 1 32 1 128 128 512 512 1 ...
 $ technique: chr  "RS" "RS" "DLMT" "RS" ...
 $ speedup  : num  0.871 1.002 0.668 1.371 0.907 ...
 $ RT2_J    : int  1 1 32 8 1 1 1 1 1 32 ...
 $ VEC2     : chr  "True" "True" "True" "True" ...
 $ VEC1     : chr  "True" "False" "False" "False" ...
 $ SCR      : chr  "True" "True" "True" "False" ...
 $ T1_Ia    : int  1 2048 64 1 1 1 1024 1 128 64 ...
 $ T1_Ka    : int  1 256 1 2048 256 256 1024 1 512 128 ...
 $ OMP1     : chr  "True" "False" "False" "True" ...
 $ U1_K     : int  7 1 1 29 2 4 18 1 5 1 ...
 $ U1_J     : int  1 30 6 1 12 1 13 5 1 1 ...
 $ U1_I     : int  14 28 17 6 1 4 1 16 26 17 ...
 $ T2_Ka    : int  1 256 1 1024 1024 2048 2048 2048 512 1 ...
 $ RT2_I    : int  8 1 1 1 8 1 1 1 8 1 ...
 $ T2_Ia    : int  1 512 1 1 1024 1 512 1 256 1 ...
 $ U2_J     : int  19 15 1 28 9 1 21 1 1 5 ...
 $ U2_K     : int  1 1 27 1 1 29 1 1 25 1 ...
 $ cost_mean: num  0.0596 0.0528 0.0772 0.0384 0.0585 ...
 $ RT2_K    : int  1 1 1 8 1 32 32 32 8 1 ...
 $ OMP2     : chr  "True" "False" "False" "True" ...
 $ points   : int  300 300 198 300 300 237 300 240 300 247 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figurecXzenc.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    geom_jitter() +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figurefptjRn.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(plyr)
library(ggplot2)
library(RColorBrewer)
library(colorRamps)

read.csv.id <- function(filename) {
    data <- read.csv(filename)
    data$file <- rep(filename, nrow(data))
    return(data)
}

target_dir <-"dlmt_spapt_experiments/data/stencil3d/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)
data <- lapply(csv_files, read.csv.id)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T1_Ia == 1 & data$T1_Ja == 1 & data$T1_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$T2_Ia == 1 & data$T2_Ja == 1 & data$T2_Ka == 1 &
                    data$U1_I == 1 & data$U1_J == 1 & data$U1_K == 1 &
                    data$RT1_I == 1 & data$RT1_J == 1 & data$RT1_K == 1 &
                    data$U2_I == 1 & data$U2_J == 1 & data$U2_K == 1 &
                    data$RT2_I == 1 & data$RT2_J == 1 & data$RT2_K == 1 &
                    data$SCR == "False" & data$VEC1 == "False" & data$VEC2 == "False" &
                    data$OMP1 == "False" & data$OMP2 == "False", ]

O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_Ia, U1_I, T2_I, T2_Ia, U1_I, RT1_I, U2_I, RT2_I,
                 SCR, OMP1, OMP2, .keep_all = TRUE)

#+END_SRC

#+RESULTS:
: There were 19 warnings (use warnings() to see them)

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
colourCount <- length(unique(data$file))
getPalette <- colorRampPalette(brewer.pal(9, "Spectral"))

ggplot(data, aes(fill = file)) +
    scale_fill_manual(values = colorRampPalette(brewer.pal(12, "Spectral"))(colourCount)) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 250)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18) +
    theme(legend.position="none")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-SrcUCi/figurel2BbOo.png]]
*** [2018-09-04 Tue]
**** Application: atax (8 Steps, Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/atax/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])
#+END_SRC

#+RESULTS:
: There were 31 warnings (use warnings() to see them)
: [1] 10 18
: [1] 10 18
: [1] 131
: [1] 115.7

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figure903s2q.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-YBOcEp/figureybvd4R.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)

target_dir <-"dlmt_spapt_experiments/data/atax/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" & data$VEC1 == "False" &
                    data$VEC2 == "False", ]


O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_J, T1_K, T2_I, T2_J, T2_K,
                 U1_I, U_I, U_J, U_K, RT_I, RT_J, RT_K, SCR,
                 VEC1, VEC2, .keep_all = TRUE)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
Warning message:
Trying to compute distinct() for variables not found in the data:
- `SCR`, `VEC1`, `VEC2`
This is an error, but only a warning is raised for compatibility reasons.
The following variables will be used:
- T1_I
- T1_J
- T1_K
- T2_I
- T2_J
- (8 more)
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 220)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figureTd8uqU.png]]
**** Application: dgemv3 (Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/dgemv3/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])
#+END_SRC

#+RESULTS:
: There were 50 or more warnings (use warnings() to see the first 50)
: [1] 10 54
: [1] 10 54
: [1] 360
: [1] 333.9

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figure87avL8.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figure8P1fGd.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)

target_dir <-"dlmt_spapt_experiments/data/dgemv3/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" & data$VEC1 == "False" &
                    data$VEC2 == "False", ]


O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_J, T1_K, T2_I, T2_J, T2_K,
                 U1_I, U_I, U_J, U_K, RT_I, RT_J, RT_K, SCR,
                 VEC1, VEC2, .keep_all = TRUE)
#+END_SRC

#+RESULTS:
#+begin_example
There were 21 warnings (use warnings() to see them)
Warning message:
Trying to compute distinct() for variables not found in the data:
- `T1_K`, `T2_I`, `T2_J`, `T2_K`, `U_I`, ...
This is an error, but only a warning is raised for compatibility reasons.
The following variables will be used:
- T1_I
- T1_J
- U1_I
- SCR
- VEC1
- VEC2
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 220)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-efDreH/figureVBQEIr.png]]
**** Experimental Settings on the Xeon E5 2630 v2
The tables in this section are for keeping track of what would be interesting to
measure and what was already measured.
***** Tracking Table Grid5000 Xeon E5 2630 v3
|----+--------------------+-------+-------+-------+-------+-------|
|    | Application        | RS    | RSL   | SIMA  | SIMP  | DLMT  |
|----+--------------------+-------+-------+-------+-------+-------|
|  1 | stencil3d          | DONE  |       |       |       | DONE  |
|  2 | dgemv3             | DONE  |       |       |       | DONE  |
|  3 | atax               | DONE  | DONE  | DONE  | DONE  | DONE  |
|  4 | gemver             |       |       |       |       |       |
|  5 | bicgkernel         |       |       |       |       |       |
|  6 | trmm               |       |       |       |       |       |
|  7 | adi                | DOING | DOING | DOING | DOING | DOING |
|  8 | gesummv            |       |       |       |       |       |
|  9 | mvt                |       |       |       |       |       |
| 10 | fdtd               |       |       |       |       |       |
| 11 | correlation        |       |       |       |       |       |
| 12 | lu                 |       |       |       |       |       |
| 13 | tensor-contraction |       |       |       |       |       |
| 14 | mm                 |       |       |       |       |       |
| 15 | jacobi             |       |       |       |       |       |
| 16 | covariance         |       |       |       |       |       |
| 17 | hessian            |       |       |       |       |       |
| 18 | seidel             |       |       |       |       |       |
|----+--------------------+-------+-------+-------+-------+-------|

****** DLMT:
- "Large" (8) number of ANOVA steps
- Model with linear terms
- Using complete model for prediction

******* Properties to measure:
Experiment size:
- "Large" (8) number of ANOVA steps
- "Small" (?) number of ANOVA steps

Model:
- Model with linear terms
- More complex models (?)

Prediction:
- Using complete model for prediction
- Using only relevant variables on model for prediction

******* Other concerns
- Size of initial sampled space (multipliers)
- Sampling again for prediction?
- Extra experiments due to runtime failures
*** [2018-09-05 Wed]
**** Application: atax (Latest, Linear Model, Xeon E5 2630 v3)
***** Clonning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

***** Histogram of Autotuning Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

target_dir <-"dlmt_spapt_experiments/data/atax/latest/"
csv_files <- list.files(path = target_dir, pattern = "results.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup))
data_median <- ddply(data, .(technique), summarize, median = median(speedup))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup),
                   err = 2 * sd(speedup) / sqrt(length(speedup)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup))

dim(data[data$technique == "RS", ])
dim(data[data$technique == "DLMT", ])
dim(data[data$technique == "SIMA", ])
mean(data[data$technique == "RS", "points"])
mean(data[data$technique == "DLMT", "points"])
mean(data[data$technique == "SIMA", "points"])
data_mean
#+END_SRC

#+RESULTS:
#+begin_example
[1] 0 0
Warning message:
Unknown or uninitialised column: 'technique'.
[1] 0 0
Warning message:
Unknown or uninitialised column: 'technique'.
[1] 0 0
Warning message:
Unknown or uninitialised column: 'technique'.
Error: Column `points` not found
Error: Column `points` not found
Error: Column `points` not found
# A tibble: 0 x 0
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup), #, y = 0.01 * ..density..),
    binwidth = 0.01) +
    labs(y = "Frequency", x = "Speedup vs O3") +
    coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0, 1)) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figuredKoeQd.png]]

***** Boxplot (better suited for the small number of repetitions)
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip() +
    #facet_grid(technique ~ .) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figure8pDX68.png]]

***** Histogram of Explored Search Spaces
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)

target_dir <-"dlmt_spapt_experiments/data/atax/latest/"
csv_files <- list.files(path = target_dir, pattern = "search_space.csv", recursive = TRUE)
csv_files <- paste0(target_dir, csv_files)

info <- file.info(csv_files)
non_empty <- rownames(info[info$size != 0, ])
csv_files <- csv_files[csv_files %in% non_empty]

data <- lapply(csv_files, read.csv)
data <- bind_rows(data)

data <- data[data$correct_result == "True", ]

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1, ]


O3_baseline <- O3_baseline[c(1,1), ]
O3_baseline[1, "technique"] <- "DLMT"
O3_baseline[2, "technique"] <- "RS"

data <- distinct(data, T1_I, T1_J, T1_K, T2_I, T2_J, T2_K,
                 U1_I, U_I, U_J, U_K, RT_I, RT_J, RT_K, .keep_all = TRUE)
#+END_SRC

#+RESULTS:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(cost_mean), binwidth = 0.01) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 500)) +
    geom_text(aes(x = cost_mean - 0.02, y = 123, label = "-O3"), data = O3_baseline) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-daFKax/figurev1dId1.png]]
*** [2018-09-06 Thu]
**** Plotting DOPT Sampling Strategies
***** Generate Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)

sample_size <- 50
pre_sample_size <- 5 * sample_size
Size <- 100

rs_data <- data.frame(x1 = sample(0:Size, sample_size, replace = T),
                      x2 = sample(0:Size, sample_size, replace = T))
rs_data$name <- rep("RS", nrow(rs_data))
data <- rs_data

lhs_data <- lhs.design(nruns = sample_size ,nfactors = 2, digits = 0, type = "maximin",
                       factor.names = list(x1 = c(0, Size), x2 = c(0, Size)))
lhs_data$name <- rep("LHS", nrow(lhs_data))
data <- bind_rows(data, lhs_data)

full_factorial <- gen.factorial(c(Size, Size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~., full_factorial, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("DOPT Linear", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(Size, Size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ I(1 / (10e-6 + x1)) + I(1 / (10e-6 + x2)), full_factorial, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("DOPT Inverse", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(Size, Size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ .^2, full_factorial, nTrials = sample_size)
dopti_data <- output$design
dopti_data$name <- rep("DOPT Interactions", nrow(dopti_data))
data <- bind_rows(data, dopti_data)

full_factorial <- gen.factorial(c(Size, Size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
#output <- optFederov(~ . + quad(.), full_factorial, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("DOPT Linear & Quadratic", nrow(doptq_data))
data <- bind_rows(data, doptq_data)

full_factorial <- gen.factorial(c(Size, Size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ . + I(x1 ^ 3) + I(x2 ^ 3), full_factorial, nTrials = sample_size)
#output <- optFederov(~ . + cubic(.), full_factorial, nTrials = sample_size)
doptc_data <- output$design
doptc_data$name <- rep("DOPT Linear & Cubic", nrow(doptc_data))
data <- bind_rows(data, doptc_data)

drs_data <- data.frame(x1 = sample(0:Size, pre_sample_size, replace = T),
                       x2 = sample(0:Size, pre_sample_size, replace = T))
output <- optFederov(~., drs_data, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("Large RS + DOPT Linear", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

drs_data <- data.frame(x1 = sample(0:Size, pre_sample_size, replace = T),
                       x2 = sample(0:Size, pre_sample_size, replace = T))
output <- optFederov(~ I(1 / (10e-6 + x1)) + I(1 / (10e-6 + x2)), drs_data, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("Large RS + DOPT Inverse", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

drs_data <- data.frame(x1 = sample(0:Size, pre_sample_size, replace = T),
                       x2 = sample(0:Size, pre_sample_size, replace = T))
output <- optFederov(~ . + .^2, drs_data, nTrials = sample_size)
dopti_data <- output$design
dopti_data$name <- rep("Large RS + DOPT Interactions", nrow(dopti_data))
data <- bind_rows(data, dopti_data)

drs_data <- data.frame(x1 = sample(0:Size, pre_sample_size, replace = T),
                       x2 = sample(0:Size, pre_sample_size, replace = T))
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), drs_data, nTrials = sample_size)
#output <- optFederov(~ . + quad(.), full_factorial, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("Large RS + DOPT Linear & Quadratic", nrow(doptq_data))
data <- bind_rows(data, doptq_data)

drs_data <- data.frame(x1 = sample(0:Size, pre_sample_size, replace = T),
                       x2 = sample(0:Size, pre_sample_size, replace = T))
output <- optFederov(~ . + I(x1 ^ 3) + I(x2 ^ 3), drs_data, nTrials = sample_size)
#output <- optFederov(~ . + cubic(.), full_factorial, nTrials = sample_size)
doptc_data <- output$design
doptc_data$name <- rep("Large RS + DOPT Linear & Cubic", nrow(doptc_data))
data <- bind_rows(data, doptc_data)

data$facet <- factor(data$name, levels = c("RS", "LHS", "DOPT Linear", "DOPT Interactions",
                                           "DOPT Inverse", "DOPT Linear & Quadratic",
                                           "DOPT Linear & Cubic", "Large RS + DOPT Linear",
                                           "Large RS + DOPT Interactions", "Large RS + DOPT Inverse",
                                           "Large RS + DOPT Linear & Quadratic",
                                           "Large RS + DOPT Linear & Cubic"))
#+END_SRC

#+RESULTS:

***** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = x1, y = x2)) +
    facet_wrap(facet ~ ., ncol = 4) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-FjF14L/figurecxUGKe.png]]
*** [2018-09-10 Mon]
**** Tests with Expanding Designs
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

full_factorial <- gen.factorial(c(2,2,3,2,3), center = FALSE)
full_factorial$Y <- (2.3 * full_factorial$X1 ^ 2) + (1.4 * full_factorial$X2) +
                    (3.4 * full_factorial$X3 * full_factorial$X4) +
                    (2.1 * full_factorial$X5)

str(full_factorial)

output <- optFederov(~ I(X1 ^ 2) + X2 + X3 + X4 + X5, full_factorial, nTrials = 6)
output$design

design <- output$design
design[sample(seq(1, nrow(design)), 1), "Y"] <- NA
design

regression <- lm(Y ~ I(X1 ^ 2) + X2 + X3 + X4 + X5, data = design)
summary.aov(regression)

design <- design[complete.cases(design), ]
design

regression <- lm(Y ~ I(X1 ^ 2) + X2 + X3 + X4 + X5, data = design)
summary.aov(regression)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	72 obs. of  6 variables:
 $ X1: num  1 2 1 2 1 2 1 2 1 2 ...
 $ X2: num  1 1 2 2 1 1 2 2 1 1 ...
 $ X3: num  1 1 1 1 2 2 2 2 3 3 ...
 $ X4: num  1 1 1 1 1 1 1 1 1 1 ...
 $ X5: num  1 1 1 1 1 1 1 1 1 1 ...
 $ Y : num  9.2 16.1 10.6 17.5 12.6 19.5 14 20.9 16 22.9 ...
   X1 X2 X3 X4 X5    Y
4   2  2  1  1  1 17.5
9   1  1  3  1  1 16.0
13  1  1  1  2  1 12.6
49  1  1  1  1  3 13.4
70  2  1  3  2  3 37.3
71  1  2  3  2  3 31.8
   X1 X2 X3 X4 X5    Y
4   2  2  1  1  1 17.5
9   1  1  3  1  1 16.0
13  1  1  1  2  1 12.6
49  1  1  1  1  3 13.4
70  2  1  3  2  3   NA
71  1  2  3  2  3 31.8
            Df Sum Sq Mean Sq
I(X1^2)      1   0.72    0.72
X2           1 237.63  237.63
X3           1   6.00    6.00
X4           1   0.32    0.32
1 observation deleted due to missingness
   X1 X2 X3 X4 X5    Y
4   2  2  1  1  1 17.5
9   1  1  3  1  1 16.0
13  1  1  1  2  1 12.6
49  1  1  1  1  3 13.4
71  1  2  3  2  3 31.8
            Df Sum Sq Mean Sq
I(X1^2)      1   0.72    0.72
X2           1 237.63  237.63
X3           1   6.00    6.00
X4           1   0.32    0.32
#+end_example
*** [2018-09-12 Wed]
**** Plotting Applications (Xeon E5 2630 v3, Grid5000)
***** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
***** Atax Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/atax"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 80)

#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 48 x 5
# Groups:   application [?]
   application                                     technique points_max
   <chr>                                           <chr>          <dbl>
 1 atax_1_step_4x_lin_bin_quad_others              DLMT              21
 2 atax_1_step_4x_lin_bin_quad_others              RS                22
 3 atax_1_step_4x_lin_cub_quad                     DLMT              59
 4 atax_1_step_4x_lin_quad                         DLMT              42
 5 atax_1_step_4x_lin_quad                         RS                43
 6 atax_1_step_4x_lin_quad_no_extra                DLMT              33
 7 atax_1_step_4x_lin_quad_no_extra                RS                33
 8 atax_1_step_4x_lin_quad_no_extra_no_constraints DLMT              33
 9 atax_1_step_4x_lin_quad_no_extra_no_constraints RS                33
10 atax_1_step_4x_lin_quad_small                   DLMT              36
11 atax_12_steps_small_model                       DLMT             539
12 atax_12_steps_small_model_large_prf             DLMT             254
13 atax_12_steps_small_model_min_prf               DLMT             565
14 atax_4x_lin                                     DLMT             101
15 atax_4x_lin                                     RS               101
16 atax_4x_lin_cub_quad                            DLMT             164
17 atax_4x_lin_cub_quad                            RS               201
18 atax_expanded_lin                               DLMT             480
19 atax_expanded_lin                               RS               151
20 atax_expanded_lin                               RSL              150
21 atax_expanded_lin                               SIMA             151
22 atax_expanded_lin                               SIMP             150
23 atax_expanded_lin_quad_cub                      DLMT             213
24 atax_expanded_lin_quad_cub                      RS               151
25 atax_expanded_lin_quad_cub                      RSL              150
26 atax_expanded_lin_quad_cub                      SIMA             151
27 atax_expanded_lin_quad_cub                      SIMP             150
28 atax_lin_quad                                   DLMT             231
29 atax_lin_quad                                   RS               301
30 atax_lin_quad_8_steps                           DLMT             233
31 atax_lin_quad_cub                               DLMT              96
32 atax_lin_quad_cub                               RS               101
33 atax_lin_quad_cub                               RSL              100
34 atax_lin_quad_cub                               SIMA             101
35 atax_lin_quad_cub                               SIMP             101
36 atax_no_binary_cubic_quad                       DLMT             297
37 atax_no_binary_cubic_quad                       RS               301
38 atax_no_binary_cubic_quad                       SIMA             301
39 atax_no_binary_lin                              DLMT             168
40 atax_no_binary_lin                              RS               171
41 atax_no_binary_lin                              RSL              170
42 atax_no_binary_lin                              SIMA             171
43 atax_no_binary_lin                              SIMP             171
44 atax_no_binary_no_dlmt                          RS               171
45 atax_no_binary_no_dlmt                          RSL              170
46 atax_no_binary_no_dlmt                          SIMA             171
47 atax_no_binary_no_dlmt                          SIMP             171
48 atax_small_model_quad                           DLMT             120
   max_speedup experiments
         <dbl>       <int>
 1        2.06          10
 2        1.97          10
 3        2.09           5
 4        2.18          10
 5        2.17          10
 6        2.24          20
 7        2.20          20
 8        2.06          12
 9        2.18          10
10        2.19          10
11        2.04           8
12        2.04           9
13        2.05          10
14        2.20          10
15        2.20          10
16        2.22          10
17        2.22          10
18        2.05           8
19        2.03          10
20        2.17          10
21        2.13          10
22        2.02          10
23        2.14           9
24        2.03          10
25        2.17          10
26        2.13          10
27        2.02          10
28        2.07           4
29        2.04           6
30        2.05          10
31        2.10          30
32        2.05          30
33        2.09          30
34        2.11          30
35        2.05          30
36        1.91          18
37        1.89          10
38        1.71          10
39        2.05          50
40        1.89          40
41        1.87          30
42        1.66          41
43        1.88          30
44        1.74          10
45        1.86          10
46        1.64          10
47        1.89          10
48        2.08          27
#+end_example

******* Plotting
******** Using all variables for prediction
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_no_binary_no_dlmt",
                                          "atax_no_binary_lin",
                                          "atax_no_binary_cubic_quad",
                                          "atax_lin_quad",
                                          "atax_lin_quad_8_steps",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 3) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-7lYTEt/figurezA610X.png]]

******** Using only relevant variables for prediction
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_small_model_quad",
                                          "atax_12_steps_small_model",
                                          "atax_12_steps_small_model_large_prf",
                                          "atax_12_steps_small_model_min_prf",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 3) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureKcGzbd.png]]

******** Expanding the search space and problem size
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_expanded_lin",
                                          "atax_expanded_lin_quad_cub",
                                          "atax_4x_lin",
                                          "atax_4x_lin_cub_quad",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 3) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureicD4sn.png]]

******** Making a single sampling step
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
plot_data <- data[data$application %in% c("atax_1_step_4x_lin_bin_quad_others",
                                          "atax_1_step_4x_lin_cub_quad",
                                          "atax_1_step_4x_lin_quad",
                                          "atax_1_step_4x_lin_quad_small",
                                          "atax_1_step_4x_lin_quad_no_extra",
                                          #"atax_1_step_4x_lin_quad_no_extra_no_constraints",
                                          "atax_lin_quad_cub"), ]

ggplot(plot_data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_x", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureNma9V0.png]]

******** All Experiments
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    #coord_flip() +
    #geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 5) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figurevloe9p.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/atax"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 48 x 6
# Groups:   application [?]
   application                                     technique baseline
   <chr>                                           <chr>        <dbl>
 1 atax_1_step_4x_lin_bin_quad_others              DLMT         3.08
 2 atax_1_step_4x_lin_bin_quad_others              RS           3.08
 3 atax_1_step_4x_lin_cub_quad                     DLMT         3.08
 4 atax_1_step_4x_lin_quad                         DLMT         3.07
 5 atax_1_step_4x_lin_quad                         RS           3.07
 6 atax_1_step_4x_lin_quad_no_extra                DLMT         3.09
 7 atax_1_step_4x_lin_quad_no_extra                RS           3.09
 8 atax_1_step_4x_lin_quad_no_extra_no_constraints DLMT         3.08
 9 atax_1_step_4x_lin_quad_no_extra_no_constraints RS           3.08
10 atax_1_step_4x_lin_quad_small                   DLMT         3.08
11 atax_12_steps_small_model                       DLMT         0.177
12 atax_12_steps_small_model_large_prf             DLMT         0.177
13 atax_12_steps_small_model_min_prf               DLMT         0.177
14 atax_4x_lin                                     DLMT         3.08
15 atax_4x_lin                                     RS           3.08
16 atax_4x_lin_cub_quad                            DLMT         3.08
17 atax_4x_lin_cub_quad                            RS           3.08
18 atax_expanded_lin                               DLMT         0.179
19 atax_expanded_lin                               RS           0.179
20 atax_expanded_lin                               RSL          0.179
21 atax_expanded_lin                               SIMA         0.179
22 atax_expanded_lin                               SIMP         0.179
23 atax_expanded_lin_quad_cub                      DLMT         0.178
24 atax_expanded_lin_quad_cub                      RS           0.178
25 atax_expanded_lin_quad_cub                      RSL          0.178
26 atax_expanded_lin_quad_cub                      SIMA         0.178
27 atax_expanded_lin_quad_cub                      SIMP         0.178
28 atax_lin_quad                                   DLMT         0.178
29 atax_lin_quad                                   RS           0.178
30 atax_lin_quad_8_steps                           DLMT         0.178
31 atax_lin_quad_cub                               DLMT         0.178
32 atax_lin_quad_cub                               RS           0.178
33 atax_lin_quad_cub                               RSL          0.178
34 atax_lin_quad_cub                               SIMA         0.178
35 atax_lin_quad_cub                               SIMP         0.178
36 atax_no_binary_cubic_quad                       DLMT         0.178
37 atax_no_binary_cubic_quad                       RS           0.178
38 atax_no_binary_cubic_quad                       SIMA         0.178
39 atax_no_binary_lin                              DLMT        NA
40 atax_no_binary_lin                              RS          NA
   max_cost_mean min_cost_mean experiments
           <dbl>         <dbl>       <int>
 1         8.73         1.50           176
 2        11.4          1.56           197
 3         8.59         1.47           259
 4        11.5          1.42           378
 5         9.12         1.42           366
 6        12.5          1.38           584
 7        11.0          1.40           567
 8        10.0          1.50           327
 9         9.88         1.41           285
10         9.04         1.41           322
11         0.505        0.0875        2560
12         0.510        0.0866        1513
13         0.483        0.0866        4314
14        13.2          1.40           865
15        13.5          1.41           868
16        12.9          1.39          1322
17        11.8          1.39          1767
18         0.710        0.0866        2867
19         0.727        0.0871        1245
20         0.783        0.0819        1269
21         0.682        0.0835        1433
22         0.889        0.0874        1371
23         0.675        0.0832        1519
24         0.727        0.0871        1245
25         0.783        0.0819        1269
26         0.682        0.0835        1433
27         0.889        0.0874        1371
28         0.434        0.0861         760
29         0.566        0.0865        1599
30         0.422        0.0874        1665
31         0.454        0.0848        2296
32         0.454        0.0863        2685
33         0.483        0.0852        2676
34         0.500        0.0846        2853
35         0.519        0.0868        2846
36         0.461        0.0930        3962
37         0.472        0.0942        2996
38         0.442        0.104         3009
39         0.556        0.0872        6212
40         0.491        0.0940        6412
# ... with 8 more rows
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 5) +
    geom_histogram(aes(cost_mean, fill = technique), binwidth = 0.05) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureJ8S6TN.png]]

***** Stencil3d Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/stencil3d"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)

#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 15 x 7
# Groups:   application [?]
   application                    technique points_mean points_min points_max
   <chr>                          <chr>           <dbl>      <dbl>      <dbl>
 1 stencil3d_3x_1_step_lin_quad   DLMT              55          55         55
 2 stencil3d_3x_1_step_lin_quad   RS                56          56         56
 3 stencil3d_3x_4_steps_lin       DLMT             133.        119        144
 4 stencil3d_3x_4_steps_lin       RS               201         201        201
 5 stencil3d_3x_lin_quad_cub      DLMT             276.        218        316
 6 stencil3d_3x_lin_quad_cub      RS               201         101        301
 7 stencil3d_8_steps_large_sample DLMT             332         304        383
 8 stencil3d_8_steps_small_sample DLMT             293.        176        344
 9 stencil3d_all_cubic_quad       DLMT             439.        339        517
10 stencil3d_all_cubic_quad       RS               300         300        300
11 stencil3d_lin_quad             DLMT             218.        194        266
12 stencil3d_lin_quad_8_steps     DLMT             349.        276        408
13 stencil3d_linear               DLMT             235.        198        265
14 stencil3d_linear               RS               300         300        300
15 stencil3d_small_model_min_prf  DLMT             546         544        548
   max_speedup experiments
         <dbl>       <int>
 1        2.31          10
 2        2.10          10
 3        2.45           7
 4        2.37          10
 5        2.43           9
 6        2.43          10
 7        1.18           6
 8        1.71          11
 9        2.62           9
10        1.37          10
11        1.35           5
12        3.25          11
13        2.85          10
14        1.37          10
15        1.12           2
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(0.5, 3.0)) +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figurel5jVrt.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/stencil3d"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 15 x 6
# Groups:   application [?]
   application                    technique baseline max_cost_mean min_cost_mean
   <chr>                          <chr>        <dbl>         <dbl>         <dbl>
 1 stencil3d_3x_1_step_lin_quad   DLMT        3.36          28.1          1.45
 2 stencil3d_3x_1_step_lin_quad   RS          3.36          33.1          1.60
 3 stencil3d_3x_4_steps_lin       DLMT        3.37          32.7          1.37
 4 stencil3d_3x_4_steps_lin       RS          3.37          32.7          1.42
 5 stencil3d_3x_lin_quad_cub      DLMT        3.35          33.5          1.38
 6 stencil3d_3x_lin_quad_cub      RS          3.35          33.2          1.38
 7 stencil3d_8_steps_large_sample DLMT        0.0525         0.381        0.0444
 8 stencil3d_8_steps_small_sample DLMT        0.0521         0.392        0.0301
 9 stencil3d_all_cubic_quad       DLMT       NA              0.412        0.0198
10 stencil3d_all_cubic_quad       RS         NA              0.437        0.0384
11 stencil3d_lin_quad             DLMT        0.0526         0.412        0.0385
12 stencil3d_lin_quad_8_steps     DLMT        0.0522         0.458        0.0161
13 stencil3d_linear               DLMT       NA              0.376        0.0183
14 stencil3d_linear               RS         NA              0.437        0.0384
15 stencil3d_small_model_min_prf  DLMT        0.0531         0.384        0.0473
   experiments
         <int>
 1         549
 2         555
 3         917
 4        2003
 5        2433
 6        1999
 7        1966
 8        3196
 9        3910
10        2965
11        1080
12        3792
13        2436
14        2965
15        1083
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 3) +
    geom_histogram(aes(cost_mean, fill = technique)) +#, binwidth = 0.05) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figure4vPMxv.png]]

***** Dgemv3 Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/dgemv3"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)

#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
5: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
6: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
7: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
8: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
9: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
# A tibble: 1 x 7
# Groups:   application [?]
  application                  technique points_mean points_min points_max
  <chr>                        <fct>           <dbl>      <dbl>      <dbl>
1 dgemv3_4_steps_lin_quad_true DLMT             228.        210        245
  max_speedup experiments
        <dbl>       <int>
1        1.72           2
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(0.5, 3.0)) +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figuretaoVFm.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/dgemv3"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
: # A tibble: 1 x 6
: # Groups:   application [?]
:   application                  technique baseline max_cost_mean min_cost_mean
:   <chr>                        <fct>        <dbl>         <dbl>         <dbl>
: 1 dgemv3_4_steps_lin_quad_true DLMT          2.59          6.55          1.51
:   experiments
:         <int>
: 1         455

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 3) +
    geom_histogram(aes(cost_mean, fill = technique)) +#, binwidth = 0.05) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-kRkPId/figure0Uy7o3.png]]

***** Adi Tests
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/adi"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)

#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 4 x 7
# Groups:   application [?]
  application                 technique points_mean points_min points_max
  <chr>                       <chr>           <dbl>      <dbl>      <dbl>
1 adi_3x_cubic_quad_big_model DLMT             420         420        420
2 adi1_cubic_quad_big_model   DLMT             320.        197        393
3 adi1_cubic_quad_big_model   RS               201         201        201
4 adi1_cubic_quad_big_model   SIMA             201         201        201
  max_speedup experiments
        <dbl>       <int>
1        1.01           1
2        1.01           5
3        1.00          10
4        1.01           5
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip(ylim = c(1.0, 3.0)) +
    coord_flip() +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureVrhR51.png]]

****** Histogram of Explored Search Spaces
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/tests/adi"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 4 x 6
# Groups:   application [?]
  application                 technique baseline max_cost_mean min_cost_mean
  <chr>                       <chr>        <dbl>         <dbl>         <dbl>
1 adi_3x_cubic_quad_big_model DLMT         4.41         15.8           4.36
2 adi1_cubic_quad_big_model   DLMT         0.273         0.825         0.271
3 adi1_cubic_quad_big_model   RS           0.273         1.31          0.271
4 adi1_cubic_quad_big_model   SIMA         0.273         1.23          0.271
  experiments
        <int>
1         281
2         660
3         879
4         464
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 3) +
    geom_histogram(aes(cost_mean, fill = technique)) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figurePa68p0.png]]

***** All Applications
****** Boxplot of Results
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
    target_data$application <- rep(target_dir, nrow(target_data))
    target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 8 x 7
# Groups:   application [?]
  application technique points_mean points_min points_max max_speedup
  <chr>       <chr>           <dbl>      <dbl>      <dbl>       <dbl>
1 adi4        DLMT             185.        104        295        1.01
2 atax        DLMT             176          89        213        2.14
3 atax        RS               151         151        151        2.03
4 atax        RSL              150         150        150        2.17
5 atax        SIMA             151         151        151        2.13
6 atax        SIMP             150         150        150        2.02
7 dgemv3      DLMT             334.        287        371        1.59
8 dgemv3      RS               360         360        360        1.64
  experiments
        <int>
1          10
2           9
3          10
4          10
5          10
6          10
7          10
8          10
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1500 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    coord_flip(ylim = c(1.0, 3.0)) +
    geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free_y", ncol = 2) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figureHrvTfV.png]]

****** Histogram of Explored Search Spaces
******* Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    csv_files <- paste0(target_path, csv_files)

    info <- file.info(csv_files)
    non_empty <- rownames(info[info$size != 0, ])
    csv_files <- csv_files[csv_files %in% non_empty]

    target_data <- lapply(csv_files, read.csv)
    target_data <- bind_rows(target_data)

    target_data <- target_data[target_data$correct_result == "True", ]
    target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

    target_data <- target_data[, c("cost_mean", "technique", "baseline")]

    target_data$application <- rep(target_dir, nrow(target_data))

    if (is.null(data)) {
        data <- target_data
    } else {
        data <- bind_rows(data, target_data)
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 8 x 6
# Groups:   application [?]
  application technique baseline max_cost_mean min_cost_mean experiments
  <chr>       <chr>        <dbl>         <dbl>         <dbl>       <int>
1 adi4        DLMT         1.10          1.90         1.09          1852
2 atax        DLMT         0.178         0.675        0.0832        1519
3 atax        RS           0.178         0.727        0.0871        1245
4 atax        RSL          0.178         0.783        0.0819        1269
5 atax        SIMA         0.178         0.682        0.0835        1433
6 atax        SIMP         0.178         0.889        0.0874        1371
7 dgemv3      DLMT        NA             0.340        0.0775        3428
8 dgemv3      RS          NA             0.344        0.0752        3610
#+end_example

******* Plotting
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_wrap(application ~ ., scales = "free", ncol = 2) +
    geom_histogram(aes(cost_mean, fill = technique), binwidth = 0.01) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-G1kGWE/figure5bja46.png]]
*** [2018-09-19 Wed]
**** Meeting with Arnaud and Brice
***** Topics on this Journal
-[[ DOE + Autotuning Summaries (Collected from previous entries)]]
- [[Plotting DOPT Sampling Strategies]]
- [[Interesting Papers on Autotuner Performance Comparison]]
- [[Plotting Applications (Xeon E5 2630 v3, Grid5000)]]
*** [2018-09-20 Thu]
**** Final Experimental Settings on the Xeon E5 2630 v2
After preliminary studies with some of the applications, I decided to fix some
of the parameters to explore and run all experiments at least once with fixed
settings. The settings for all experiments are the following:

***** Repetitions
I'll start with 10 repetitions for RS and DLMT, then complete for other search
algorithms.

***** DLMT Configuration
- 4 ANOVA steps
- ANOVA Prf threshold set to 0.05
- Use a new random sample for prediction
- Use only the variables identified in ANOVA in model for prediction
- Pick best point from failed regressions and skip model fitting
- Model for all applications: =Y ~ . + I(. ^ 2) + I(. ^ 3)=
- Quadratic and cubic terms exclude binary variables

***** SPAPT Configuration
- Add missing powers of two in all parameters
- Maximum available problem size
- Cost of a configuration is the mean of 10 runs
- Keep provided constraints

***** Budget
The DLMT budget is variable within the 4 ANOVA steps. This happens because
each regression is done on new data samples, and different samples could
be used to fix different numbers of variables at different times.

The RS budget was initially fixed to 300 points. Additional experiments
might have to be done depending on how many points DLMT uses for each
application.

***** Tracking Table Grid5000 Xeon E5 2630 v3
|----+--------------------+------+-----+------+------+------|
|    | Application        | RS   | RSL | SIMA | SIMP | DLMT |
|----+--------------------+------+-----+------+------+------|
|  1 | stencil3d          | DONE |     |      |      | DONE |
|  2 | dgemv3             | DONE |     |      |      | DONE |
|  3 | atax               | DONE |     |      |      | DONE |
|  4 | gemver             | DONE |     |      |      | DONE |
|  5 | bicgkernel         | DONE |     |      |      | DONE |
|  6 | trmm               | DONE |     |      |      | DONE |
|  7 | adi                | DONE |     |      |      | DONE |
|  8 | gesummv            | DONE |     |      |      | DONE |
|  9 | mvt                | DONE |     |      |      | DONE |
| 10 | fdtd               | DONE |     |      |      | DONE |
| 11 | correlation        | DONE |     |      |      | DONE |
| 12 | lu                 | DONE |     |      |      | DONE |
| 13 | tensor-contraction | DONE |     |      |      | DONE |
| 14 | mm                 | DONE |     |      |      | DONE |
| 15 | jacobi             | DONE |     |      |      | DONE |
| 16 | covariance         | FAIL |     |      |      | FAIL |
| 17 | hessian            | DONE |     |      |      | DONE |
| 18 | seidel             | DONE |     |      |      | DONE |
|----+--------------------+------+-----+------+------+------|

**** Plotting Applications (Xeon E5 2630 v3, Grid5000)
***** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
***** Speedup
This entry contains plots of the speedups achieved by DLMT
and other algorithms.

****** Loading Data
Run this before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)

data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "results.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]

        target_data <- lapply(csv_files, read.csv)
        target_data <- bind_rows(target_data)

        target_data <- target_data[, c("speedup", "cost_mean", "technique", "points")]
        target_data$application <- rep(target_dir, nrow(target_data))
        target_data$max_speedup <- rep(max(target_data$speedup), nrow(target_data))
        target_data$mean_points <- rep(mean(target_data$points), nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

options(dplyr.width = Inf)
print(summarise(points_mean = mean(points), points_min = min(points),
          points_max = max(points), max_speedup = max(speedup), experiments = n(),
          group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 34 x 7
# Groups:   application [?]
   application technique points_mean points_min points_max max_speedup
   <chr>       <chr>           <dbl>      <dbl>      <dbl>       <dbl>
 1 adi         DLMT            377.         340        417        1.42
 2 adi         RS              301          301        301        1.42
 3 atax        DLMT            186.         115        221        2.18
 4 atax        RS              301          301        301        2.22
 5 bicgkernel  DLMT            120.         102        139       13.5
 6 bicgkernel  RS              301          301        301       12.7
 7 correlation DLMT            205.         168        235        1.01
 8 correlation RS              301          301        301        1.02
 9 dgemv3      DLMT            301.         219        414        2.15
10 dgemv3      RS              391          301        401        2.21
11 fdtd        DLMT            329          329        329        1.06
12 fdtd        RS              301          301        301        1.05
13 gemver      DLMT            150.          76        225        3.83
14 gemver      RS              301          301        301        3.85
15 gesummv     DLMT            133          133        133        2.83
16 gesummv     RS              301          301        301        3.07
17 hessian     DLMT            105          105        105        1.07
18 hessian     RS              301          301        301        1.07
19 jacobi      DLMT            102           74        135        2.35
20 jacobi      RS              301          301        301        1.87
21 lu          DLMT             43           43         43       12.7
22 lu          RS              301          301        301       18.7
23 mm          DLMT            101.          73        126       20.4
24 mm          RS              294.         226        301       19.4
25 mvt         DLMT             83.1         51        105       11.2
26 mvt         RS              301          301        301       10.0
27 seidel      DLMT            146.          51        196       52.5
28 seidel      RS              301          301        301       54.3
29 stencil3d   DLMT            233.         153        292        2.40
30 stencil3d   RS              251          251        251        2.45
31 tensor      DLMT            136.         101        179       27.7
32 tensor      RS              301          301        301       26.0
33 trmm        DLMT            215.          81        285       15.1
34 trmm        RS              301          301        301       14.4
   experiments
         <int>
 1          10
 2          10
 3          10
 4          10
 5          10
 6          10
 7          11
 8          10
 9          11
10          10
11           7
12           7
13          10
14          10
15          10
16          10
17          10
18          10
19          18
20          18
21          10
22          10
23          10
24          10
25          10
26          10
27          15
28          11
29          10
30          10
31          19
32          20
33          10
34          10
#+end_example

****** Boxplots
I chose to represent the speedups of each of the 10 runs for DLMT and the other
algorithms using boxplots. The vertical dotted lines mark the best speedups
achieved overall for each application.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data, aes(x = technique, y = speedup)) +
    geom_boxplot() +
    #coord_flip(ylim = c(1.0, 16.0)) +
    coord_flip() +
    #geom_jitter() +
    geom_hline(aes(yintercept = max_speedup), color = "black", linetype = 8) +
    facet_wrap(. ~ application, scales = "free", ncol = 4) +
    ggtitle("") +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-S4IlLB/figuredWEg4k.png]]

****** Speedup & Budget
******* Setup
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(reshape2)

plot_data <- data[data$technique == "DLMT", c("points", "speedup", "application")]
plot_data <- plot_data %>%
             group_by(application) %>%
             summarize(mean_points = mean(points),
                       mean_speedup = mean(speedup))

random_data <- data[data$technique == "RS", c("points", "speedup", "application")]
random_data <- random_data %>%
               group_by(application) %>%
               summarize(mean_points = mean(points),
                         mean_speedup = mean(speedup))

plot_data <- plot_data[plot_data$application %in% random_data$application, ]
random_data <- random_data[random_data$application %in% plot_data$application, ]

plot_data$mean_speedup_ratio_RS <- plot_data$mean_speedup / random_data$mean_speedup
plot_data$mean_budget_ratio_RS <- plot_data$mean_points / random_data$mean_points

plot_data <- plot_data[, c("application", "mean_speedup_ratio_RS", "mean_budget_ratio_RS")]
plot_data <- melt(plot_data)
plot_data
#+END_SRC

#+RESULTS:
#+begin_example
Using application as id variables
   application              variable     value
1          adi mean_speedup_ratio_RS 0.9031920
2         atax mean_speedup_ratio_RS 0.9612893
3   bicgkernel mean_speedup_ratio_RS 1.0988750
4  correlation mean_speedup_ratio_RS 0.9997396
5       dgemv3 mean_speedup_ratio_RS 1.0106460
6       gemver mean_speedup_ratio_RS 1.0205835
7      gesummv mean_speedup_ratio_RS 0.9778234
8      hessian mean_speedup_ratio_RS 0.9981345
9       jacobi mean_speedup_ratio_RS 1.0215033
10          lu mean_speedup_ratio_RS 0.7079224
11          mm mean_speedup_ratio_RS 1.0030311
12         mvt mean_speedup_ratio_RS 0.9787649
13      seidel mean_speedup_ratio_RS 1.1608172
14   stencil3d mean_speedup_ratio_RS 1.0115290
15      tensor mean_speedup_ratio_RS 1.0289025
16        trmm mean_speedup_ratio_RS 1.1383463
17         adi  mean_budget_ratio_RS 1.2538206
18        atax  mean_budget_ratio_RS 0.6186047
19  bicgkernel  mean_budget_ratio_RS 0.3980066
20 correlation  mean_budget_ratio_RS 0.6822712
21      dgemv3  mean_budget_ratio_RS 0.7709835
22      gemver  mean_budget_ratio_RS 0.4990033
23     gesummv  mean_budget_ratio_RS 0.4418605
24     hessian  mean_budget_ratio_RS 0.3488372
25      jacobi  mean_budget_ratio_RS 0.3388704
26          lu  mean_budget_ratio_RS 0.1428571
27          mm  mean_budget_ratio_RS 0.3444634
28         mvt  mean_budget_ratio_RS 0.2760797
29      seidel  mean_budget_ratio_RS 0.4944629
30   stencil3d  mean_budget_ratio_RS 0.9294821
31      tensor  mean_budget_ratio_RS 0.4403839
32        trmm  mean_budget_ratio_RS 0.7139535
#+end_example
******* Plot
This plot shows the ratio between the maximum number of points
used by the DLMT and RS, and the ration between the maximum speedups
achieved by DLMT and RS, for all applications.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(plot_data, aes(application, value, fill = variable)) +
    #facet_grid(variable ~ .) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_hline(aes(yintercept = 1.0), color = "black", linetype = 8) +
    scale_y_continuous(breaks = seq(0, 2, by = 0.2)) +
    #coord_flip() +
    ggtitle("") +
    theme_bw(base_size = 18) +
    scale_fill_grey(start = 0.5, end = 0.8) +
    theme(legend.position = "top", legend.direction = "horizontal")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-7n0CWB/figure0GgYZz.png]]

***** Search Spaces
This entry contains histograms of the search spaces explored by each
search algorithm.

****** Load Data
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]

        target_data <- lapply(csv_files, read.csv)
        target_data <- bind_rows(target_data)

        target_data <- target_data[target_data$correct_result == "True", ]
        target_data$baseline <- rep(target_data[target_data$baseline == "True", "cost_mean"][1], nrow(target_data))

        target_data <- target_data[, c("cost_mean", "technique", "baseline")]

        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

print(summarise(baseline = max(baseline), max_cost_mean = max(cost_mean),
                min_cost_mean = min(cost_mean), experiments = n(),
                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
# A tibble: 30 x 6
# Groups:   application [?]
   application technique baseline max_cost_mean min_cost_mean experiments
   <chr>       <chr>        <dbl>         <dbl>         <dbl>       <int>
 1 adi         DLMT         4.41         22.7          3.11          3758
 2 adi         RS           4.41         19.8          3.12          3009
 3 atax        DLMT         3.08         11.5          1.41          1674
 4 atax        RS           3.08         12.5          1.39          2603
 5 bicgkernel  DLMT         5.91         11.0          0.437         1173
 6 bicgkernel  RS           5.91         12.3          0.465         2993
 7 correlation DLMT         7.02          7.18         6.94          2259
 8 correlation RS           7.02          7.20         6.94          3010
 9 dgemv3      DLMT         2.59          8.20         1.33          3286
10 dgemv3      RS           2.59         11.1          1.39          3875
11 gemver      DLMT         6.43         29.3          1.68          1426
12 gemver      RS           6.43         31.9          1.67          3000
13 gesummv     DLMT         2.52          8.90         0.892          524
14 gesummv     RS           2.52          9.00         0.818         1162
15 hessian     DLMT         0.204         1.38         0.190          532
16 hessian     RS           0.204         3.31         0.192         1479
17 jacobi      DLMT         0.204         1.51         0.0409        1355
18 jacobi      RS           0.204         3.31         0.0515        4035
19 lu          DLMT         1.93         76.1          0.152          237
20 lu          RS           1.93        109.           0.142         1001
21 mm          DLMT         1.47          6.94         0.0720        1011
22 mm          RS           1.47          7.20         0.0759        2935
23 mvt         DLMT         0.134         0.428        0.0138         831
24 mvt         RS           0.134         0.505        0.0134        3010
25 seidel      DLMT         2.90         20.4          0.0569         822
26 stencil3d   DLMT         3.37         31.8          1.40          2307
27 stencil3d   RS           3.37         32.8          1.37          2500
28 tensor      RS           4.76         19.9          0.184         3010
29 trmm        DLMT         1.42         13.9          0.0942        1932
30 trmm        RS           1.42         14.7          0.0985        2722
#+end_example

****** Histograms
The plots in this section show the mean over 10 executions of the cost of all
configurations evaluated during search. The vertical dotted line marks the mean
cost over 10 executions of the -O3 baseline. Search algorithms are coded by
color.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ application, scales = "free") +
    #geom_histogram(aes(y = ..density.., x = cost_mean, fill = technique)) +
    geom_histogram(aes(cost_mean, fill = technique)) +
    geom_vline(aes(xintercept = baseline), color = "black", linetype = 8) +
    theme_bw(base_size = 18) +
    scale_fill_grey(start = 0.3, end = 0.7) +
    theme(legend.position = "none")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-FAd6Wc/figurebFFnEB.png]]
***** Speedup for Half of the Random Samples
This entry contains plots of the speedups achieved by DLMT
and other algorithms.

****** Load Data
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.parse <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE)

    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <-  data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))

    if (data[1, "technique"] == "RS") {
        data <- data[seq(1, nrow(data) / 2), ]
    }

    data <- data[data$baseline == "False", ]

    data$points <- rep(nrow(data), nrow(data))

    data <- data[data$correct_result == "True", ]

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]

        target_data <- lapply(csv_files, read.csv.parse)
        target_data <- bind_rows(target_data)

        target_data <- target_data[, c("cost_mean", "technique", "cost_baseline", "speedup", "max_run_speedup", "points")]

        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

plot_data <- data[data$technique == "DLMT", c("points", "max_run_speedup", "application")]
plot_data <- plot_data %>%
             group_by(application) %>%
             summarize(mean_points = mean(points),
                       mean_speedup = mean(max_run_speedup))

random_data <- data[data$technique == "RS", c("points", "max_run_speedup", "application")]
random_data <- random_data %>%
               group_by(application) %>%
               summarize(mean_points = mean(points),
                         mean_speedup = mean(max_run_speedup))

plot_data <- plot_data[plot_data$application %in% random_data$application, ]
random_data <- random_data[random_data$application %in% plot_data$application, ]

plot_data$mean_speedup_ratio_RS <- plot_data$mean_speedup / random_data$mean_speedup
plot_data$mean_budget_ratio_RS <- plot_data$mean_points / random_data$mean_points

plot_data <- plot_data[, c("application", "mean_speedup_ratio_RS", "mean_budget_ratio_RS")]
plot_data <- melt(plot_data)
plot_data

#print(summarise(baseline = max(cost_baseline), mean_speedup = mean(max_run_speedup),
#                min_cost_mean = min(cost_mean), experiments = n(),
#                group_by(data, application, technique)), n = 40)
#+END_SRC

#+RESULTS:
#+begin_example
There were 50 or more warnings (use warnings() to see the first 50)
Using application as id variables
   application              variable     value
1          adi mean_speedup_ratio_RS 0.9065329
2         atax mean_speedup_ratio_RS 0.9636718
3   bicgkernel mean_speedup_ratio_RS 1.0994348
4  correlation mean_speedup_ratio_RS 0.9996541
5       dgemv3 mean_speedup_ratio_RS 1.0127743
6       gemver mean_speedup_ratio_RS 1.0279365
7      gesummv mean_speedup_ratio_RS 0.9752087
8      hessian mean_speedup_ratio_RS 0.9974783
9       jacobi mean_speedup_ratio_RS 1.0176566
10          mm mean_speedup_ratio_RS 1.0105284
11         mvt mean_speedup_ratio_RS 0.9750377
12   stencil3d mean_speedup_ratio_RS 1.0097359
13        trmm mean_speedup_ratio_RS 1.1476087
14         adi  mean_budget_ratio_RS 2.5195020
15        atax  mean_budget_ratio_RS 1.2830208
16  bicgkernel  mean_budget_ratio_RS 0.7983892
17 correlation  mean_budget_ratio_RS 1.3814472
18      dgemv3  mean_budget_ratio_RS 1.5980264
19      gemver  mean_budget_ratio_RS 1.0870669
20     gesummv  mean_budget_ratio_RS 0.8800000
21     hessian  mean_budget_ratio_RS 0.6933333
22      jacobi  mean_budget_ratio_RS 0.6835702
23          mm  mean_budget_ratio_RS 0.7010760
24         mvt  mean_budget_ratio_RS 0.5730978
25   stencil3d  mean_budget_ratio_RS 1.9280592
26        trmm  mean_budget_ratio_RS 1.5242074
#+end_example

****** Speedup & Budget
******* Plot
This plot shows the ratio between the maximum number of points
used by the DLMT and RS, and the ration between the maximum speedups
achieved by DLMT and RS, for all applications.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(plot_data, aes(application, value, fill = variable)) +
    #facet_grid(variable ~ .) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_hline(aes(yintercept = 1.0), color = "black", linetype = 8) +
    scale_y_continuous(breaks = seq(0, 2, by = 0.2)) +
    #coord_flip() +
    ggtitle("") +
    theme_bw(base_size = 18) +
    scale_fill_grey(start = 0.3, end = 0.7) +
    theme(legend.position = "top", legend.direction = "horizontal")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-YhD6Qv/figurexlSVJM.png]]
***** Iteration where Best Points were Found
This entry contains plots of the speedups achieved by DLMT
and other algorithms.

****** Load Data
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE)

    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <-  data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$speedup == max(data$speedup), ])), nrow(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations)
        target_data <- bind_rows(target_data)
        target_data <- target_data[, c("cost_mean", "technique", "cost_baseline", "speedup", "max_run_speedup", "points", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}
#+END_SRC

#+RESULTS:
: There were 50 or more warnings (use warnings() to see the first 50)

****** Process Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
subset_data <- data[, c("points", "max_run_speedup", "application", "best_iteration", "technique")]
plot_data <- subset_data %>%
             group_by(application, technique) %>%
             summarize(#mean_points = mean(points),
                       #sd_points = sd(points),
                       #max_points = max(points),
                       #min_points = min(points),
                       #min_iteration = min(best_iteration),
                       #max_iteration = max(best_iteration),
                       mean_best_iteration = mean(best_iteration),
                       sem_best_iteration = 2 * (sd(best_iteration) / sqrt(length(best_iteration))),
                       sd_best_iteration = sd(best_iteration))
#plot_data <- melt(plot_data)
as.data.frame(plot_data)
#+END_SRC

#+RESULTS:
#+begin_example
   application technique mean_best_iteration sem_best_iteration
1          adi      DLMT           141.12700           2.720830
2          adi        RS           131.04502           3.306886
3         atax      DLMT            80.52885           2.827222
4         atax        RS           133.45854           3.619822
5   bicgkernel      DLMT            85.32932           1.616365
6   bicgkernel        RS           155.33155           3.604279
7  correlation      DLMT           119.52980           2.494953
8  correlation        RS           150.80000           2.528018
9       dgemv3      DLMT           184.36427           2.701841
10      dgemv3        RS           160.43338           3.438742
11        fdtd      DLMT           134.90748           4.400738
12        fdtd        RS           148.18798           3.732289
13      gemver      DLMT           113.56215           2.788298
14      gemver        RS           151.81572           2.269083
15     gesummv      DLMT            73.29961           3.435621
16     gesummv        RS           151.04948           4.957122
17     hessian      DLMT            59.73946           2.798694
18     hessian        RS           144.43022           4.950010
19      jacobi      DLMT            51.36275           1.724136
20      jacobi        RS           157.90092           2.663673
21          lu      DLMT            22.11013           1.636640
22          lu        RS           202.57646           2.888862
23          mm      DLMT            69.31568           1.202784
24          mm        RS           182.66667           3.110298
25         mvt      DLMT            44.86602           1.574765
26         mvt        RS           146.70000           3.350108
27      seidel      DLMT           108.41667           2.013917
28      seidel        RS           202.33656           2.782804
29   stencil3d      DLMT           111.86678           2.866450
30   stencil3d        RS           134.72048           3.215613
31      tensor      DLMT            78.60919           1.589180
32      tensor        RS           102.57895           2.035097
33        trmm      DLMT           189.41363           2.728443
34        trmm        RS           127.89196           2.913587
   sd_best_iteration
1           83.28584
2           90.54772
3           57.66425
4           92.16341
5           27.56127
6           98.42719
7           59.14669
8           69.23263
9           77.30996
10         106.89179
11          96.23941
12          79.48112
13          52.46152
14          62.03774
15          38.94545
16          84.12515
17          31.97133
18          94.86084
19          31.52153
20          84.41153
21          12.32923
22          58.63750
23          19.02720
24          84.10753
25          22.56095
26          91.74648
27          45.88019
28          64.80109
29          68.69022
30          80.22940
31          39.40238
32          76.82321
33          59.80831
34          75.86523
#+end_example

****** Speedup & Budget
******* Plot
This plot shows the ratio between the maximum number of points
used by the DLMT and RS, and the ration between the maximum speedups
achieved by DLMT and RS, for all applications.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(plot_data, aes(application, mean_best_iteration, fill = technique)) +
    #facet_grid(technique ~ .) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_errorbar(aes(ymin = mean_best_iteration - sem_best_iteration,
                      ymax = mean_best_iteration + sem_best_iteration),
                  width = 0.0, position = position_dodge(0.9)) +
    #coord_flip() +
    ggtitle("") +
    ylab("Iteration where Best was Found") +
    theme_bw(base_size = 18) +
    scale_fill_grey(start = 0.5, end = 0.8) +
    theme(legend.position = "top", legend.direction = "horizontal", legend.title = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-S4IlLB/figuregmD5vf.png]]
***** Iteration where Best Points were Found vs. Execution Cost
This entry contains plots of the speedups achieved by DLMT
and other algorithms.

****** Load Data
Run this block before plotting:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(openssl)
library(RColorBrewer)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE)

    data$experiment_id <- rep(sha1(csv_file), nrow(data))
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$speedup == max(data$speedup), ])), nrow(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:plyrâ€™:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

****** Process Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
plot_data <- data %>%
             distinct(experiment_id, .keep_all = TRUE) %>%
             group_by(application) %>%
             mutate(mean_cost_baseline = mean(cost_baseline)) %>%
             ungroup()
#plot_data <- melt(plot_data)

rs_sample <- data[data$technique == "RS", c("application", "technique", "cost_mean")]
dlmt_sample <- data[data$technique == "DLMT", c("application", "technique", "cost_mean")]
names(plot_data)
#+END_SRC

#+RESULTS:
: [1] "cost_mean"          "experiment_id"      "technique"
: [4] "cost_baseline"      "min_run_cost"       "best_iteration"
: [7] "application"        "mean_cost_baseline"

****** Speedup & Budget
******* Plot
This plot shows the ratio between the maximum number of points
used by the DLMT and RS, and the ration between the maximum speedups
achieved by DLMT and RS, for all applications.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1080 :height 1520
#+BEGIN_SRC R
library(extrafont)

ggplot(plot_data, aes(min_run_cost, best_iteration, color = technique)) +
    facet_wrap(application ~ ., ncol = 3) +
    #geom_jitter(data = rs_sample, aes(x = cost_mean, y = 300), pch = 19, alpha = 0.1, height = 85, width = 0) +
    #geom_jitter(data = dlmt_sample, aes(x = cost_mean, y = 100), pch = 19, alpha = 0.1, height = 85, width = 0) +
    geom_point(size = 2, pch = 19) +
    #stat_ellipse(type = "t", linetype = 13) +
    stat_ellipse(type = "norm", linetype = 13) +
    geom_vline(aes(xintercept = mean_cost_baseline), linetype = 8, color = "black") +
    ylim(c(-10, 400)) +
    scale_x_continuous(trans = "log10") +
    #coord_flip() +
    ggtitle("") +
    ylab("Iteration where Best was Found") +
    xlab("Best Cost") +
    theme_bw(base_size = 23) +
    theme(legend.position = "bottom",
          legend.direction = "horizontal",
          legend.title = element_blank(),
          text = element_text(family="Noto Serif"),
          strip.background = element_rect(fill = "white"),
          plot.margin = unit(c(-1, 0.2, 0.2, 0.2), "cm"))  +
    scale_color_brewer(palette = "Set1")
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-S4IlLB/figureDO26Fe.png]]
** October
*** [2018-10-02 Tue]
**** Conceptual Example: Rosenbrock and D-Optimal Designs
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05), seq(-4, 4, 0.05))
rosenbrock_data$Y <- mapply(rosenbrock, rosenbrock_data$Var1, rosenbrock_data$Var2)

dim(rosenbrock_data)
rosenbrock(1, 1)
#+END_SRC

#+RESULTS:
: [1] 25921     3
: [1] 0

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
random_sample <- rosenbrock_data[sample(1:nrow(rosenbrock_data), 8, replace = TRUE), ]
dim(random_sample)
random_sample[random_sample$Y == min(random_sample$Y), ]
#+END_SRC

#+RESULTS:
: [1] 8 3
:        Var1 Var2        Y
: 19215 -1.25 1.95 20.07813

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
sampled_mins <- replicate(30, {
  random_sample <- rosenbrock_data[sample(1:nrow(rosenbrock_data), 10, replace = TRUE), ]
  sample_min <- random_sample[random_sample$Y == min(random_sample$Y), "Y"]
  sample_min
  })

sampled_mins <- as.numeric(unlist(sampled_mins))
random_summary <- summary(sampled_mins)
random_summary
#+END_SRC

#+RESULTS:
:     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
:   0.7031  32.3633  66.4350 121.1034 154.7594 459.4000

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
output <- optFederov(~ ., data = rosenbrock_data, nTrials = 10)
output
regression <- lm(Y ~ ., data = output$design)
summary.aov(regression)
prediction <- predict(regression, rosenbrock_data)
best_predicted <- rosenbrock_data[prediction == min(prediction), ]
best_predicted
#+END_SRC

#+RESULTS:
#+begin_example
$D
[1] 441.7721

$A
[1] 0.8475946

$Ge
[1] 0.797

$Dea
[1] 0.775

$design
       Var1  Var2         Y
1     -4.00 -4.00 40025.000
80    -0.05 -4.00  1603.103
89     0.40 -4.00  1730.920
161    4.00 -4.00 40009.000
162   -4.00 -3.95 39825.250
322    4.00 -3.95 39809.250
25760  4.00  3.95 14529.250
25761 -4.00  4.00 14425.000
25762 -3.95  4.00 13486.303
25921  4.00  4.00 14409.000

$rows
 [1]     1    80    89   161   162   322 25760 25761 25762 25921
            Df    Sum Sq   Mean Sq F value Pr(>F)
Var1         1 9.742e+04     97419   0.000  0.986
Var2         1 3.988e+08 398834888   1.428  0.271
Residuals    7 1.955e+09 279338170
      Var1 Var2     Y
25921    4    4 14409
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
i <- 0
linear_mins <- replicate(30, {
  output <- optFederov(~ ., data = rosenbrock_data, nTrials = 10)
  regression <- lm(Y ~ ., data = output$design)
  summary.aov(regression)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

linear_mins <- as.numeric(unlist(linear_mins))
linear_summary <- summary(linear_mins)
linear_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  14409   14409   14409   14414   14425   14425
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
output <- optFederov(~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = rosenbrock_data, nTrials = 8)
output
regression <- lm(Y ~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = output$design)
summary.aov(regression)
prediction <- predict(regression, rosenbrock_data)
best_predicted <- rosenbrock_data[prediction == min(prediction), ]
best_predicted
#+END_SRC

#+RESULTS:
#+begin_example
$D
[1] 36.02169

$A
[1] 1.629654

$Ge
[1] 0.66

$Dea
[1] 0.598

$design
       Var1  Var2         Y
1     -4.00 -4.00 40025.000
79    -0.10 -4.00  1609.220
161    4.00 -4.00 40009.000
11890  2.80 -0.35  6710.850
14330 -4.00  0.45 24205.250
25781 -3.00  4.00  2516.000
25842  0.05  4.00  1598.903
25921  4.00  4.00 14409.000

$rows
[1]     1    79   161 11890 14330 25781 25842 25921
               Df    Sum Sq   Mean Sq   F value   Pr(>F)
Var1            1 7.120e+06 7.120e+06 2.845e+29 1.19e-15 ***
Var2            1 6.236e+08 6.236e+08 2.492e+31  < 2e-16 ***
I(Var1^4)       1 1.076e+09 1.076e+09 4.300e+31  < 2e-16 ***
I(Var1^2)       1 1.219e+06 1.219e+06 4.873e+28 2.88e-15 ***
I(Var2^2)       1 1.364e+07 1.364e+07 5.450e+29 8.62e-16 ***
Var2:I(Var1^2)  1 1.835e+08 1.835e+08 7.332e+30 2.35e-16 ***
Residuals       1 0.000e+00 0.000e+00
---
codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
      Var1 Var2 Y
16201    1    1 0
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
i <- 0
modelled_mins <- replicate(30, {
  output <- optFederov(~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = rosenbrock_data, nTrials = 8)
  regression <- lm(Y ~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = output$design)
  summary.aov(regression)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

modelled_mins <- as.numeric(unlist(modelled_mins))
modelled_summary <- summary(modelled_mins)
modelled_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
      0       0       0       0       0       0
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)

summaries <- rbind(random_summary, linear_summary, modelled_summary)
summaries <- summaries[, c("Min.", "Mean", "Max.")]
summaries
#+END_SRC

#+RESULTS:
:                         Min.       Mean    Max.
: random_summary   7.03125e-01   121.1034   459.4
: linear_summary   1.44090e+04 14414.3333 14425.0
: modelled_summary 0.00000e+00     0.0000     0.0

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1000 :height 1000
#+BEGIN_SRC R
library(ggplot2)
library(RColorBrewer)

ggplot(rosenbrock_data, aes(Var1, Var2, z = Y)) +
      #geom_raster(aes(fill = Y), show.legend = FALSE) +
      scale_x_continuous(limits = c(-3, 3), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-3, 3), expand = c(0, 0)) +
      geom_contour(colour = "black", binwidth = 100, show.legend = FALSE) +
      geom_point(size = 4, colour = "black", pch = 21, data = rosenbrock_data[rosenbrock_data$Y == min(rosenbrock_data$Y), ]) +
      geom_label(size = 6, colour = "black", data = rosenbrock_data[rosenbrock_data$Y == min(rosenbrock_data$Y), ], aes(x = Var1, y = Var2 + 0.15, label = "rosenbrock(1,1) = 0")) +
      theme_bw(base_size = 20) +
      theme(panel.grid = element_blank(), panel.border = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-7n0CWB/figurebJWM0J.png]]
*** [2018-10-16 Tue]
**** Comparing Algorithms for D-Optimal Designs :ATTACH:
:PROPERTIES:
:Attachments: cook1980comparison.pdf
:ID:       a9ddd9cc-e0b3-4ee4-aca5-049902c417ba
:END:
**** Pan's Course on Online Learning & Optimization
- Multi-Armed Bandit (semi-bandit)
***** Online Convex Optimization
- Continuous space of possible actions
****** Oracles
- Gradient Feedback: 1st order (derivative of function at point)
- Bandit Feedback: 0th order (just value of function)
***** Online Linear Optimization
- Mixed strategies
***** Solution Concepts
- There is always a fixed target that works best
- Minimize aggregate loss -> perhaps best general solution
- Rigid structure
- Deterministic Offline Optimization Problem
  - f does not change over time
  - target minimum
- Stationary Stochastic Optimization Problem
  - f_t changes over time, fixed distribution
  - target some mean of minimuns over time
- Adversarial Framework
  - f(x,y) -> minimizing over x, maximizing over y
  - target nash equilibrium
  - Zero-sum game, saddle point problem
***** (External) Regret
- Online Opt. Problem: Minimize aggregate loss
- No-regret criterion: Reg(T) = o(T) as T -> Inf
- Local regret
- Dynamic regret (adversarial, hopeless to minimize in general)
  - Needs controls on the loss function
- Swapping/Shifiting/Adaptive regret: M_t function swaps over t stages
- Finite sets: internal regret, many others...
*** [2018-10-17 Wed]
**** Uniform Sampling with Constraints
- Bayesian Designs?
***** Constrained Sampling? (in SAT solving and IA)              :ATTACH:
:PROPERTIES:
:Attachments: meel2016constrained.pdf
:ID:       3860f414-d611-4b31-bff5-feae4720b3bf
:END:
- https://www.youtube.com/watch?v=KVgxx13eCjY
- slides: https://www.cs.rice.edu/~vardi/papers/cav16tk.pdf
***** Gibbs Sampling
- [[http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf]]
- https://www4.stat.ncsu.edu/~reich/st740/Computing2.pdf
- https://stats.stackexchange.com/questions/266665/gibbs-sampler-examples-in-r
***** Linear constraints and inequalities
- =xsample()= from the [[https://cran.r-project.org/web/packages/limSolve/index.html%0A][limSolve]] package
- [[https://cran.r-project.org/web/packages/ic.infer/index.html][ic.infer]] package
*** [2018-10-22 Mon]
**** Looking at Results for the Seidel Kernel
***** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
***** Looking at each Iteration
****** Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "seidel")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	16 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 12 12 12 12 12 12 12 12 12 12 ...
 $ run_id               : int  1538763579 1538763575 1538763576 1537910970 1537910954 1538763571 1537910963 1537910958 1537910950 1538763577 ...
 $ de_step_1            : num  0.112 0.111 0.124 0.12 0.108 0.102 0.096 0.165 0.127 0.126 ...
 $ de_step_2            : num  0.268 0.112 0.217 0.379 0.129 0.092 0.133 0.155 0.283 0.107 ...
 $ de_step_3            : num  0.25 0.198 0.237 NA 0.138 0.128 0.136 0.136 0.186 1 ...
 $ de_step_4            : num  0.177 0.14 0.201 NA 0.145 0.145 0.152 0.215 0.235 NA ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 7 101 93 106 92 1 1 103 8 1 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 74 63 92 72 1 1 1 79 78 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 63 51 21 1 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 14 1 1 1 1 1 1 1 ...
 $ design_best_step_1   : num  0.0399 0.0433 0.0373 0.0394 0.0321 ...
 $ design_best_step_2   : num  0.0244 0.0319 0.0249 0.0265 0.0329 ...
 $ design_best_step_3   : num  0.0206 0.031 0.0312 NA 0.0196 ...
 $ design_best_step_4   : num  0.019 0.0273 0.0203 NA 0.0495 ...
 $ predicted_best_step_1: num  0.0281 0.1035 0.0501 0.2992 0.3662 ...
 $ predicted_best_step_2: num  0.0545 0.0764 0.174 0.0243 0.4022 ...
 $ predicted_best_step_3: num  0.0289 0.0622 0.1783 NA 0.3347 ...
 $ predicted_best_step_4: num  0.0309 NA 0.0433 NA 0.0391 ...
#+end_example
****** Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) str_trim(str_replace_all(x, "'", ""))))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	64 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 12 12 12 12 12 12 12 12 12 12 ...
 $ run_id               : Factor w/ 16 levels "328","1537910950",..: 16 13 14 7 4 10 6 5 2 15 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 7 101 93 106 92 1 1 103 8 1 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 74 63 92 72 1 1 1 79 78 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 63 51 21 1 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 14 1 1 1 1 1 1 1 ...
 $ design_best_step_1   : num  0.0399 0.0433 0.0373 0.0394 0.0321 ...
 $ design_best_step_2   : num  0.0244 0.0319 0.0249 0.0265 0.0329 ...
 $ design_best_step_3   : num  0.0206 0.031 0.0312 NA 0.0196 ...
 $ design_best_step_4   : num  0.019 0.0273 0.0203 NA 0.0495 ...
 $ predicted_best_step_1: num  0.0281 0.1035 0.0501 0.2992 0.3662 ...
 $ predicted_best_step_2: num  0.0545 0.0764 0.174 0.0243 0.4022 ...
 $ predicted_best_step_3: num  0.0289 0.0622 0.1783 NA 0.3347 ...
 $ predicted_best_step_4: num  0.0309 NA 0.0433 NA 0.0391 ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.112 0.111 0.124 0.12 0.108 0.102 0.096 0.165 0.127 0.126 ...
#+end_example
****** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              facet_wrap(step ~ ., ncol = 4) +
                              geom_bar(stat = "count", show.legend = FALSE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot, eliminated_histogram_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5),
                                    c(6, 6),
                                    c(6, 6)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-nELaGr/figureEN6G9Y.png]]

*** [2018-10-23 Tue]
**** Pan's Course on Online Learning & Optimization II
***** Policies
- Follow The Leader (FTL)
- Follow The Regularized Leader
  - Requires a new zero-epoch, where an "easy function" is given,
    for example, a quadratic function (Euclidean Regularization)
  - The regularizing function is a choice of the optimizer
  - FTRL is Online Gradient Descent when:
    - Linear function
    - Euclidean Regularization
    - No constraints
  - [[https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37013.pdf][Google paper on FTRL]]
  - \eta must be proportional \radic{}T
*** [2018-10-26 Fri]
**** Talking about orgmode with Andrei
- I showed Andrei my journal and the paper org file
- https://orgmode.org/worg/org-tutorials/orgtutorial_dto.html
- https://orgmode.org/worg/org-tutorials/
- http://pragmaticemacs.com/org-mode-tutorials/
- https://www.gnu.org/software/emacs/tour/
- http://ergoemacs.org/emacs/emacs_basics.html
** November
*** [2018-11-09 Fri]
**** Looking at other SPAPT kernels
***** Looking at Results for the MVT Kernel
****** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
****** Looking at each Iteration
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "mvt")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	10 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 11 11 11 11 11 11 11 11 11 11
 $ run_id               : int  1537549980 1537549966 1537549971 1537549973 1537549977 1537549982 389 1537549974 1537549979 98
 $ de_step_1            : num  0.206 0.221 0.229 0.183 0.202 0.316 0.304 0.23 0.137 0.251
 $ de_step_2            : num  0.173 0.287 0.298 0.256 0.229 0.366 0.491 0.185 0.224 0.515
 $ de_step_3            : num  0.295 0.552 1 0.479 0.443 0.403 0.87 0.24 0.283 NA
 $ de_step_4            : num  0.347 0.422 1 0.508 0.405 0.293 0.87 0.309 0.882 NA
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 55 40 78 54 60 79 61 40 63 62
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 39 34 46 87 15 1 13 2 1 84
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 69 1 1 1 1 1 1 10 22 1
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 42 41 31 1 1 16 1 40 1 1
 $ design_best_step_1   : num  0.1947 0.1807 0.13 0.0997 0.4671 ...
 $ design_best_step_2   : num  0.105 0.146 0.164 0.089 0.41 ...
 $ design_best_step_3   : num  0.147 0.129 0.386 0.106 0.448 ...
 $ design_best_step_4   : num  0.129 NA 0.387 NA 0.451 ...
 $ predicted_best_step_1: num  0.532 0.507 0.426 0.279 0.495 ...
 $ predicted_best_step_2: num  0.219 0.495 0.418 0.114 0.518 ...
 $ predicted_best_step_3: num  0.724 0.157 0.387 0.119 0.484 ...
 $ predicted_best_step_4: num  0.13 0.183 0.389 NA 0.501 ...
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) str_trim(str_replace_all(x, "'", ""))))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	40 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 11 11 11 11 11 11 11 11 11 11 ...
 $ run_id               : Factor w/ 10 levels "98","389","1537549966",..: 9 3 4 5 7 10 2 6 8 1 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 55 40 78 54 60 79 61 40 63 62 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 39 34 46 87 15 1 13 2 1 84 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 69 1 1 1 1 1 1 10 22 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 42 41 31 1 1 16 1 40 1 1 ...
 $ design_best_step_1   : num  0.1947 0.1807 0.13 0.0997 0.4671 ...
 $ design_best_step_2   : num  0.105 0.146 0.164 0.089 0.41 ...
 $ design_best_step_3   : num  0.147 0.129 0.386 0.106 0.448 ...
 $ design_best_step_4   : num  0.129 NA 0.387 NA 0.451 ...
 $ predicted_best_step_1: num  0.532 0.507 0.426 0.279 0.495 ...
 $ predicted_best_step_2: num  0.219 0.495 0.418 0.114 0.518 ...
 $ predicted_best_step_3: num  0.724 0.157 0.387 0.119 0.484 ...
 $ predicted_best_step_4: num  0.13 0.183 0.389 NA 0.501 ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.206 0.221 0.229 0.183 0.202 0.316 0.304 0.23 0.137 0.251 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              facet_wrap(step ~ ., ncol = 4) +
                              geom_bar(stat = "count", show.legend = FALSE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot, eliminated_histogram_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5),
                                    c(6, 6),
                                    c(6, 6)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-0SJBEn/figurei7NzFT.png]]
***** Looking at Results for the LU Kernel
****** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
****** Looking at each Iteration
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "lu")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

'data.frame':	10 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 9 9 9 9 9 9 9 9 9 9
 $ run_id               : int  1537554184 1537554180 1537554181 126 1537554176 1537554182 273 59 123 114
 $ de_step_1            : num  0.143 0.131 0.135 0.115 0.151 0.107 0.159 0.131 0.15 0.16
 $ de_step_2            : num  NA NA NA NA NA NA NA NA NA NA
 $ de_step_3            : num  NA NA NA NA NA NA NA NA NA NA
 $ de_step_4            : num  NA NA NA NA NA NA NA NA NA NA
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 1 1 1 1 1 1 1 1 1 1
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 1 1 1 1 1 1 1 1 1
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 1 1 1 1 1
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 1 1 1 1 1 1 1 1
 $ design_best_step_1   : num  0.138 0.189 0.137 0.152 0.12 ...
 $ design_best_step_2   : num  NA NA NA NA NA NA NA NA NA NA
 $ design_best_step_3   : num  NA NA NA NA NA NA NA NA NA NA
 $ design_best_step_4   : num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_1: num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_2: num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_3: num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_4: num  NA NA NA NA NA NA NA NA NA NA
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) str_trim(str_replace_all(x, "'", ""))))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	40 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 9 9 9 9 9 9 9 9 9 9 ...
 $ run_id               : Factor w/ 10 levels "59","114","123",..: 10 7 8 4 6 9 5 1 3 2 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 1 1 1 1 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ design_best_step_1   : num  0.138 0.189 0.137 0.152 0.12 ...
 $ design_best_step_2   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ design_best_step_3   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ design_best_step_4   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_1: num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_2: num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_3: num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_4: num  NA NA NA NA NA NA NA NA NA NA ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.143 0.131 0.135 0.115 0.151 0.107 0.159 0.131 0.15 0.16 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

#eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
#                              facet_wrap(step ~ ., ncol = 4) +
#                              geom_bar(stat = "count", show.legend = FALSE) +
#                              ylab("count of eliminated factors") +
#                              xlab("") +
#                              scale_y_continuous(breaks = 0:12) +
#                              theme_bw(base_size = 16) +
#                              theme(text = element_text(family = "sans"),
#                                    legend.position = "bottom",
#                                    legend.direction = "horizontal",
#                                    legend.title = element_blank(),
#                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
#                              guides(fill = guide_legend(nrow = 1)) +
#                              #scale_color_manual(values = my_palette)
#                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-0SJBEn/figure1nXZ2q.png]]
**** Facebook Research's Post on Tuning Online Systems with Bayesian Optimization :ATTACH:
:PROPERTIES:
:ID:       8af31dbf-2c18-4b97-9c64-24d4b8e5ec91
:Attachments: letham2018constrained.pdf
:END:
- The [[https://research.fb.com/efficient-tuning-of-online-systems-using-bayesian-optimization/][blog post]]
- The paper is attached to this entry
*** [2018-11-12 Mon]
**** Overall Evaluation of the Thesis Progress
I arrived at LIG and UGA in a CAPES scientific exchange program, but I already
had the objective of starting a thesis in cotutelle with Prof. Arnaud Legrand,
between USP and UGA. The initial objective of my stay here was to learn and
study the Design of Experiments approach, in the context of program autotuning,
and its available implementations. We also wanted to evaluate the applicability
and effectiveness of this approach to program autotuning.

The initial autotuning problem we intended to apply the Design of Experiments
approach to was the autotuning of High-Level Synthesis for FPGAs, a domain in
which we had recently published a paper. Our analyses of existing experimental
data made it clear that we needed more experiments to be able to better
understand the properties of the search spaces in question. We had problems
gaining access to the proprietary software needed to repeat our initial
experiments.

During this period we also reviewed the bibliography for different strategies
for constructing designs for mixed level factors, and determined that D-Optimal
designs would the most general and applicable approach. Since we had issues
accessing proprietary software to repeat our experiments with FPGAs, we decided
to move towards an implementation of a Design of Experiments approach based on
D-Optimal designs, and evaluated this approach on a Laplacian Kernel for GPUs
and on HPC kernels from the SPAPT benchmark suite. We submitted a paper presenting
our approach and using these experimental evaluations to IPDPS 2019.

We will continue the experimental evaluation of our approach by analysing the
data we collected to construct less general performance models for each
application. We expect to see differences between our previous results and
optimizations done with tailored models. We are also studying sampling
strategies that would allow uniform sampling for the constrained spaces from
SPAPT. Finally, it would be ideal to apply our approach to the initially
intended domain of High-Level Synthesis for FPGAs, but we are still not able
to access the proprietary software.
**** LHS sampling with constraints
- [[https://hal.archives-ouvertes.fr/hal-00412235v2/document][Latin hypercube sampling with inequality constraints]]

Their ideia is to first generate an LHS for a multivariate problem, then check
if inequality constraints are not respected, then swap sampled values of
variables so that constraints are respected, if possible. Its based on the
property that we can swap variable values in a LHS sample and still get a LHS.
*** [2018-11-13 Tue]
**** Les Bases du Systeme Linux pour le Calcul Scientifique (I)
- [[https://pole-calcul-formation.gricad-pages.univ-grenoble-alpes.fr/ced/unix/][Site]]
- [[https://pole-calcul-formation.gricad-pages.univ-grenoble-alpes.fr/ced/pdf/Introduction_Linux_part1.pdf][Slides]]
*** [2018-11-15 Thu]
**** Meeting with Arnaud and Jean-Marc
***** Facebook Research's Post on Tuning Online Systems with Bayesian Optimization :ATTACH:
:PROPERTIES:
:ID:       8af31dbf-2c18-4b97-9c64-24d4b8e5ec91
:Attachments: letham2018constrained.pdf
:END:
- The [[https://research.fb.com/efficient-tuning-of-online-systems-using-bayesian-optimization/][blog post]]
- The paper is attached to this entry
- A sort of [[https://en.wikipedia.org/wiki/Kriging][Kriging]], but keeping intervals of uncertainty for measurements
- Could be applied to improve existing designs
- Useful because it provides comparisons and different strategies for the similar problems
***** Uniform Sampling with Constraints
- Bayesian Designs?
****** LHS sampling with constraints
- [[https://hal.archives-ouvertes.fr/hal-00412235v2/document][Latin hypercube sampling with inequality constraints]]

Their ideia is to first generate an LHS for a multivariate problem, then check
if inequality constraints are not respected, then swap sampled values of
variables so that constraints are respected, if possible. Its based on the
property that we can swap variable values in a LHS sample and still get a LHS.

******* Conditioned LHS with ancilliary information :ATTACH:
:PROPERTIES:
:Attachments: minasny2006conditioned.pdf
:ID:       c70bc482-6d26-4d27-8065-f889a806ad7a
:END:
- Similar to above, but seems to be useful for respecting known variable
  distributions when sampling, the "ancillary data"

****** Constrained Sampling? (in SAT solving and IA)            :ATTACH:
:PROPERTIES:
:Attachments: meel2016constrained.pdf
:ID:       3860f414-d611-4b31-bff5-feae4720b3bf
:END:
- https://www.youtube.com/watch?v=KVgxx13eCjY
- slides: https://www.cs.rice.edu/~vardi/papers/cav16tk.pdf
****** Gibbs & Monte-Carlo Sampling
- [[http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf]]
- https://www4.stat.ncsu.edu/~reich/st740/Computing2.pdf
- https://stats.stackexchange.com/questions/266665/gibbs-sampler-examples-in-r
****** Linear constraints and inequalities
- =xsample()= from the [[https://cran.r-project.org/web/packages/limSolve/index.html%0A][limSolve]] package
- [[https://cran.r-project.org/web/packages/ic.infer/index.html][ic.infer]] package
***** Looking deeper at SPAPT kernels
- What was the =Pr(<F)= for each model term at each step, at each run?
- How many actual factors we have at each step, at each run?
- How did the fixed factor values compare at each step, at each run?
****** Cloning/Pulling the Repository
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.
****** Seidel
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "seidel")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	16 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 12 12 12 12 12 12 12 12 12 12 ...
 $ run_id               : int  1538763579 1538763575 1538763576 1537910970 1537910954 1538763571 1537910963 1537910958 1537910950 1538763577 ...
 $ de_step_1            : num  0.112 0.111 0.124 0.12 0.108 0.102 0.096 0.165 0.127 0.126 ...
 $ de_step_2            : num  0.268 0.112 0.217 0.379 0.129 0.092 0.133 0.155 0.283 0.107 ...
 $ de_step_3            : num  0.25 0.198 0.237 NA 0.138 0.128 0.136 0.136 0.186 1 ...
 $ de_step_4            : num  0.177 0.14 0.201 NA 0.145 0.145 0.152 0.215 0.235 NA ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 7 101 93 106 92 1 1 103 8 1 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 74 63 92 72 1 1 1 79 78 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 63 51 21 1 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 14 1 1 1 1 1 1 1 ...
 $ design_best_step_1   : num  0.0399 0.0433 0.0373 0.0394 0.0321 ...
 $ design_best_step_2   : num  0.0244 0.0319 0.0249 0.0265 0.0329 ...
 $ design_best_step_3   : num  0.0206 0.031 0.0312 NA 0.0196 ...
 $ design_best_step_4   : num  0.019 0.0273 0.0203 NA 0.0495 ...
 $ predicted_best_step_1: num  0.0281 0.1035 0.0501 0.2992 0.3662 ...
 $ predicted_best_step_2: num  0.0545 0.0764 0.174 0.0243 0.4022 ...
 $ predicted_best_step_3: num  0.0289 0.0622 0.1783 NA 0.3347 ...
 $ predicted_best_step_4: num  0.0309 NA 0.0433 NA 0.0391 ...
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) if (x == "") {
                                                                                           return(x)
                                                                                       } else {
                                                                                           str_trim(str_replace_all(strsplit(x, ":"), c("I\\(" = "", "\\)" = "",
                                                                                                                                        "\\^2" = "", "\\^3" = "")))
                                                                                       } ))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	64 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 12 12 12 12 12 12 12 12 12 12 ...
 $ run_id               : Factor w/ 16 levels "83","1537910950",..: 16 13 14 7 4 10 6 5 2 15 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 7 101 93 106 92 1 1 103 8 1 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 74 63 92 72 1 1 1 79 78 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 63 51 21 1 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 14 1 1 1 1 1 1 1 ...
 $ design_best_step_1   : num  0.0399 0.0433 0.0373 0.0394 0.0321 ...
 $ design_best_step_2   : num  0.0244 0.0319 0.0249 0.0265 0.0329 ...
 $ design_best_step_3   : num  0.0206 0.031 0.0312 NA 0.0196 ...
 $ design_best_step_4   : num  0.019 0.0273 0.0203 NA 0.0495 ...
 $ predicted_best_step_1: num  0.0281 0.1035 0.0501 0.2992 0.3662 ...
 $ predicted_best_step_2: num  0.0545 0.0764 0.174 0.0243 0.4022 ...
 $ predicted_best_step_3: num  0.0289 0.0622 0.1783 NA 0.3347 ...
 $ predicted_best_step_4: num  0.0309 NA 0.0433 NA 0.0391 ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.112 0.111 0.124 0.12 0.108 0.102 0.096 0.165 0.127 0.126 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              facet_wrap(step ~ ., ncol = 4) +
                              geom_bar(stat = "count", show.legend = FALSE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot, eliminated_histogram_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5),
                                    c(6, 6),
                                    c(6, 6)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureR0h6td.png]]
****** MVT
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "mvt")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	10 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 11 11 11 11 11 11 11 11 11 11
 $ run_id               : int  1537549980 1537549966 1537549971 1537549973 1537549977 1537549982 115 1537549974 1537549979 352
 $ de_step_1            : num  0.206 0.221 0.229 0.183 0.202 0.316 0.304 0.23 0.137 0.251
 $ de_step_2            : num  0.173 0.287 0.298 0.256 0.229 0.366 0.491 0.185 0.224 0.515
 $ de_step_3            : num  0.295 0.552 1 0.479 0.443 0.403 0.87 0.24 0.283 NA
 $ de_step_4            : num  0.347 0.422 1 0.508 0.405 0.293 0.87 0.309 0.882 NA
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 55 40 78 54 60 79 61 40 63 62
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 39 34 46 87 15 1 13 2 1 84
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 69 1 1 1 1 1 1 10 22 1
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 42 41 31 1 1 16 1 40 1 1
 $ design_best_step_1   : num  0.1947 0.1807 0.13 0.0997 0.4671 ...
 $ design_best_step_2   : num  0.105 0.146 0.164 0.089 0.41 ...
 $ design_best_step_3   : num  0.147 0.129 0.386 0.106 0.448 ...
 $ design_best_step_4   : num  0.129 NA 0.387 NA 0.451 ...
 $ predicted_best_step_1: num  0.532 0.507 0.426 0.279 0.495 ...
 $ predicted_best_step_2: num  0.219 0.495 0.418 0.114 0.518 ...
 $ predicted_best_step_3: num  0.724 0.157 0.387 0.119 0.484 ...
 $ predicted_best_step_4: num  0.13 0.183 0.389 NA 0.501 ...
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) if (x == "") {
                                                                                           return(x)
                                                                                       } else {
                                                                                           str_trim(str_replace_all(strsplit(x, ":"), c("I\\(" = "", "\\)" = "",
                                                                                                                                        "\\^2" = "", "\\^3" = "")))
                                                                                       } ))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	40 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 11 11 11 11 11 11 11 11 11 11 ...
 $ run_id               : Factor w/ 10 levels "115","352","1537549966",..: 9 3 4 5 7 10 1 6 8 2 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 55 40 78 54 60 79 61 40 63 62 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 39 34 46 87 15 1 13 2 1 84 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 69 1 1 1 1 1 1 10 22 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 42 41 31 1 1 16 1 40 1 1 ...
 $ design_best_step_1   : num  0.1947 0.1807 0.13 0.0997 0.4671 ...
 $ design_best_step_2   : num  0.105 0.146 0.164 0.089 0.41 ...
 $ design_best_step_3   : num  0.147 0.129 0.386 0.106 0.448 ...
 $ design_best_step_4   : num  0.129 NA 0.387 NA 0.451 ...
 $ predicted_best_step_1: num  0.532 0.507 0.426 0.279 0.495 ...
 $ predicted_best_step_2: num  0.219 0.495 0.418 0.114 0.518 ...
 $ predicted_best_step_3: num  0.724 0.157 0.387 0.119 0.484 ...
 $ predicted_best_step_4: num  0.13 0.183 0.389 NA 0.501 ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.206 0.221 0.229 0.183 0.202 0.316 0.304 0.23 0.137 0.251 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              facet_wrap(step ~ ., ncol = 4) +
                              geom_bar(stat = "count", show.legend = FALSE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:12) +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot, eliminated_histogram_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5),
                                    c(6, 6),
                                    c(6, 6)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurefILJKd.png]]
****** LU
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "lu")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	10 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 9 9 9 9 9 9 9 9 9 9
 $ run_id               : int  1537554184 1537554180 1537554181 141 1537554176 1537554182 33 111 70 155
 $ de_step_1            : num  0.143 0.131 0.135 0.115 0.151 0.107 0.159 0.131 0.15 0.16
 $ de_step_2            : num  NA NA NA NA NA NA NA NA NA NA
 $ de_step_3            : num  NA NA NA NA NA NA NA NA NA NA
 $ de_step_4            : num  NA NA NA NA NA NA NA NA NA NA
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 1 1 1 1 1 1 1 1 1 1
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 1 1 1 1 1 1 1 1 1
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 1 1 1 1 1
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 1 1 1 1 1 1 1 1
 $ design_best_step_1   : num  0.138 0.189 0.137 0.152 0.12 ...
 $ design_best_step_2   : num  NA NA NA NA NA NA NA NA NA NA
 $ design_best_step_3   : num  NA NA NA NA NA NA NA NA NA NA
 $ design_best_step_4   : num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_1: num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_2: num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_3: num  NA NA NA NA NA NA NA NA NA NA
 $ predicted_best_step_4: num  NA NA NA NA NA NA NA NA NA NA
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) if (x == "") {
                                                                                           return(x)
                                                                                       } else {
                                                                                           str_trim(str_replace_all(strsplit(x, ":"), c("I\\(" = "", "\\)" = "",
                                                                                                                                        "\\^2" = "", "\\^3" = "")))
                                                                                       } ))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	40 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 9 9 9 9 9 9 9 9 9 9 ...
 $ run_id               : Factor w/ 10 levels "33","70","111",..: 10 7 8 4 6 9 1 3 2 5 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 1 1 1 1 1 1 1 1 1 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ design_best_step_1   : num  0.138 0.189 0.137 0.152 0.12 ...
 $ design_best_step_2   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ design_best_step_3   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ design_best_step_4   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_1: num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_2: num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_3: num  NA NA NA NA NA NA NA NA NA NA ...
 $ predicted_best_step_4: num  NA NA NA NA NA NA NA NA NA NA ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.143 0.131 0.135 0.115 0.151 0.107 0.159 0.131 0.15 0.16 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

#eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
#                              facet_wrap(step ~ ., ncol = 4) +
#                              geom_bar(stat = "count", show.legend = FALSE) +
#                              ylab("count of eliminated factors") +
#                              xlab("") +
#                              scale_y_continuous(breaks = 0:12) +
#                              theme_bw(base_size = 16) +
#                              theme(text = element_text(family = "sans"),
#                                    legend.position = "bottom",
#                                    legend.direction = "horizontal",
#                                    legend.title = element_blank(),
#                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
#                              guides(fill = guide_legend(nrow = 1)) +
#                              #scale_color_manual(values = my_palette)
#                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              theme_bw(base_size = 16) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figureqRfYo2.png]]
****** Tensor
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "tensor")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	19 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 14 14 14 14 14 14 14 14 14 14 ...
 $ run_id               : int  1538471197 1538763432 1538763445 1538471193 1538471192 1538763447 1538471199 1538763434 1538763433 179 ...
 $ de_step_1            : num  0.117 0.099 0.102 0.106 0.095 0.099 0.12 0.094 0.092 0.07 ...
 $ de_step_2            : num  0.118 0.158 0.103 0.072 0.429 0.262 0.211 0.39 0.12 0.632 ...
 $ de_step_3            : num  0.493 0.353 0.415 0.392 0.463 0.873 0.218 0.58 0.225 0.612 ...
 $ de_step_4            : num  0.351 0.439 0.468 0.477 0.451 NA 0.246 0.59 0.199 0.547 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 22 67 35 18 80 13 112 76 48 52 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 36 23 64 65 33 94 31 83 37 38 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 38 56 64 1 65 19 16 55 66 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 10 26 1 1 1 45 1 8 11 ...
 $ design_best_step_1   : num  0.0502 0.0508 0.04 0.0422 0.0499 ...
 $ design_best_step_2   : num  0.0361 0.0411 0.0436 0.0381 0.0465 ...
 $ design_best_step_3   : num  0.0373 0.0458 0.0401 0.0389 0.0461 ...
 $ design_best_step_4   : num  0.0388 0.0487 0.045 0.0382 0.0463 ...
 $ predicted_best_step_1: num  0.0466 2.1001 0.6152 1.1884 0.0488 ...
 $ predicted_best_step_2: num  0.0396 0.0828 0.0675 0.047 0.0537 ...
 $ predicted_best_step_3: num  0.0395 0.0488 0.0557 0.0395 0.0466 ...
 $ predicted_best_step_4: num  0.0399 0.0484 0.0454 0.0401 0.0482 ...
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) if (x == "") {
                                                                                           return(x)
                                                                                       } else {
                                                                                           str_trim(str_replace_all(strsplit(x, ":"), c("I\\(" = "", "\\)" = "",
                                                                                                                                        "\\^2" = "", "\\^3" = "")))
                                                                                       } ))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	76 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 14 14 14 14 14 14 14 14 14 14 ...
 $ run_id               : Factor w/ 19 levels "179","274","313",..: 9 12 16 6 5 18 11 14 13 1 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 22 67 35 18 80 13 112 76 48 52 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 36 23 64 65 33 94 31 83 37 38 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 1 38 56 64 1 65 19 16 55 66 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 10 26 1 1 1 45 1 8 11 ...
 $ design_best_step_1   : num  0.0502 0.0508 0.04 0.0422 0.0499 ...
 $ design_best_step_2   : num  0.0361 0.0411 0.0436 0.0381 0.0465 ...
 $ design_best_step_3   : num  0.0373 0.0458 0.0401 0.0389 0.0461 ...
 $ design_best_step_4   : num  0.0388 0.0487 0.045 0.0382 0.0463 ...
 $ predicted_best_step_1: num  0.0466 2.1001 0.6152 1.1884 0.0488 ...
 $ predicted_best_step_2: num  0.0396 0.0828 0.0675 0.047 0.0537 ...
 $ predicted_best_step_3: num  0.0395 0.0488 0.0557 0.0395 0.0466 ...
 $ predicted_best_step_4: num  0.0399 0.0484 0.0454 0.0401 0.0482 ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.117 0.099 0.102 0.106 0.095 0.099 0.12 0.094 0.092 0.07 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              facet_wrap(step ~ ., ncol = 4) +
                              geom_bar(stat = "count", show.legend = FALSE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              theme_bw(base_size = 14) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              theme_bw(base_size = 14) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot, eliminated_histogram_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5),
                                    c(6, 6),
                                    c(6, 6)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figurevLElUn.png]]
****** Bicgkernel
******* Loading Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(stringr)
data <- read.csv("./dlmt_spapt_experiments/data/results/dlmt_run_summary.csv", header = TRUE)
data <- subset(data, kernel == "bicgkernel")
data[duplicated(data$run_id), "run_id"] <- sample(1:400, length(which(duplicated(data$run_id))))

str(data)
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	10 obs. of  18 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 2 2 2 2 2 2 2 2 2 2
 $ run_id               : int  1537463771 1537463780 1537463775 1537463776 81 260 1537463762 1537463781 399 1537463765
 $ de_step_1            : num  0.126 0.177 0.09 0.118 0.185 0.185 0.142 0.103 0.15 0.153
 $ de_step_2            : num  0.178 0.158 0.129 0.19 0.239 0.149 0.182 0.16 0.4 0.27
 $ de_step_3            : num  0.235 0.295 0.528 0.233 0.308 0.219 0.358 0.251 0.545 0.244
 $ de_step_4            : num  0.299 0.271 0.324 0.168 0.475 0.434 0.33 0.453 0.391 0.231
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 41 71 71 73 72 71 66 60 74 75
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 15 40 17 1 70 12 12 12 16 42
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 46 1 1 1 12 48 45 53 1 47
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 1 1 1 2 1 1 1 1
 $ design_best_step_1   : num  0.0976 0.1142 0.1119 0.1029 0.1063 ...
 $ design_best_step_2   : num  0.1 0.0958 0.0928 0.0988 0.0803 ...
 $ design_best_step_3   : num  0.0938 0.0937 0.0907 0.098 0.1095 ...
 $ design_best_step_4   : num  0.077 0.0955 0.0792 0.0923 0.1044 ...
 $ predicted_best_step_1: num  0.149 0.119 0.101 0.161 0.105 ...
 $ predicted_best_step_2: num  0.1215 0.1116 0.0885 0.108 0.1182 ...
 $ predicted_best_step_3: num  0.101 0.091 0.086 0.0932 0.1353 ...
 $ predicted_best_step_4: num  0.0937 0.0944 0.1001 0.1029 0.1189 ...
#+end_example
******* Processing Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
d_eff_data <- data %>%
              gather("step", "d_efficiency", paste0("de_step_", 1:4)) %>%
              mutate(step = as.numeric(as.factor(step)),
                     run_id = as.factor(run_id))

predicted_best_data <- data %>%
                       gather("step", "runtime", paste0("predicted_best_step_", 1:4)) %>%
                       mutate(step = as.numeric(as.factor(step)),
                              run_id = as.factor(run_id))

predicted_best_data$inf_points <- ""
predicted_best_data[is.infinite(predicted_best_data$runtime), "inf_points"] <- "Inf"

design_best_data <- data %>%
                    gather("step", "runtime", paste0("design_best_step_", 1:4)) %>%
                    mutate(step = as.numeric(as.factor(step)),
                           run_id = as.factor(run_id))

design_best_data$inf_points <- ""
design_best_data[is.infinite(design_best_data$runtime), "inf_points"] <- "Inf"

eliminated_data <- data %>%
                   gather("step", "factors", paste0("removed_step_", 1:4)) %>%
                   mutate(step = as.numeric(as.factor(step)),
                          run_id = as.factor(run_id),
                          factors = sapply(strsplit(factors, ":"),
                                           length))

complete_eliminated_data <- data %>%
                            gather("step", "factors",
                                   paste0("removed_step_", 1:4)) %>%
                            mutate(step = as.factor(step),
                                   run_id = as.factor(run_id))

complete_eliminated_data <- complete_eliminated_data %>% separate_rows(factors, sep = ":") %>%
                            mutate(factors = sapply(factors, function (x) if (x == "") {
                                                                                           return(x)
                                                                                       } else {
                                                                                           str_trim(str_replace_all(strsplit(x, ":"), c("I\\(" = "", "\\)" = "",
                                                                                                                                        "\\^2" = "", "\\^3" = "")))
                                                                                       } ))

complete_eliminated_data$factors <- factor(complete_eliminated_data$factors, levels = names(sort(table(complete_eliminated_data$factors), decreasing = TRUE)))

str(d_eff_data)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
attributes are not identical across measure variables;
they will be dropped

Warning message:
attributes are not identical across measure variables;
they will be dropped

'data.frame':	40 obs. of  16 variables:
 $ kernel               : Factor w/ 15 levels "atax2","bicgkernel",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ run_id               : Factor w/ 10 levels "81","260","399",..: 6 9 7 8 1 2 4 10 3 5 ...
 $ removed_step_1       : Factor w/ 113 levels "","'I(RT_I^2)': 'RT_I'",..: 41 71 71 73 72 71 66 60 74 75 ...
 $ removed_step_2       : Factor w/ 100 levels "","'ACOPY_A'",..: 15 40 17 1 70 12 12 12 16 42 ...
 $ removed_step_3       : Factor w/ 72 levels "","'ACOPY_x': 'I(T1_K^2)': 'T1_K': 'RT_J': 'I(T1_J^2)': 'I(RT_K^2)': 'I(U_I^2)': 'I(RT_J^3)': 'I(U1_I^2)': 'U_K': "| __truncated__,..: 46 1 1 1 12 48 45 53 1 47 ...
 $ removed_step_4       : Factor w/ 47 levels "","'I(RT_J^2)'",..: 1 1 1 1 1 2 1 1 1 1 ...
 $ design_best_step_1   : num  0.0976 0.1142 0.1119 0.1029 0.1063 ...
 $ design_best_step_2   : num  0.1 0.0958 0.0928 0.0988 0.0803 ...
 $ design_best_step_3   : num  0.0938 0.0937 0.0907 0.098 0.1095 ...
 $ design_best_step_4   : num  0.077 0.0955 0.0792 0.0923 0.1044 ...
 $ predicted_best_step_1: num  0.149 0.119 0.101 0.161 0.105 ...
 $ predicted_best_step_2: num  0.1215 0.1116 0.0885 0.108 0.1182 ...
 $ predicted_best_step_3: num  0.101 0.091 0.086 0.0932 0.1353 ...
 $ predicted_best_step_4: num  0.0937 0.0944 0.1001 0.1029 0.1189 ...
 $ step                 : num  1 1 1 1 1 1 1 1 1 1 ...
 $ d_efficiency         : num  0.126 0.177 0.09 0.118 0.185 0.185 0.142 0.103 0.15 0.153 ...
#+end_example
******* Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1750 :height 1000
#+BEGIN_SRC R
library(gridExtra)
library(ggplot2)
library(RColorBrewer)
library(ggrepel)

my_palette = c(brewer.pal(name = "Set1", n = 9),
                brewer.pal(name = "Set2", n = 7),
                brewer.pal(name = "Set3", n = 8),
                brewer.pal(name = "Dark2", n = 7),
                brewer.pal(name = "Accent", n = 8))

d_eff_plot <- ggplot(d_eff_data, aes(y = d_efficiency, x = step)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.), size = 2, show.legend = FALSE) +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

predicted_plot <- ggplot(predicted_best_data, aes(y = runtime, x = step, label = inf_points)) +
                          geom_point(alpha = 0.6, pch = 21, size = 2) +
                          geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                          geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                          ylab("runtime of best prediction") +
                          theme_bw(base_size = 16) +
                          scale_color_manual(values = my_palette)

design_plot <- ggplot(design_best_data, aes(y = runtime, x = step, label = inf_points)) +
                      geom_point(alpha = 0.6, pch = 21, size = 2) +
                      geom_line(aes(group = run_id, color = run_id, alpha = 0.7), size = 2, show.legend = FALSE) +
                      geom_label_repel(size = 7, nudge_x = 0.4, force = 8) +
                      ylab("runtime of design best") +
                      theme_bw(base_size = 16) +
                      scale_color_manual(values = my_palette)

eliminated_number_plot <- ggplot(eliminated_data, aes(y = factors, x = step)) +
                                  geom_point(alpha = 0.6, pch = 21, size = 2) +
                                  geom_line(aes(group = run_id, color = run_id, alpha = 0.7),
                                                size = 2, show.legend = FALSE) +
                                  ylab("eliminated factors") +
                                  theme_bw(base_size = 16) +
                                  scale_color_manual(values = my_palette)

eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              facet_wrap(step ~ ., ncol = 4) +
                              geom_bar(stat = "count", show.legend = FALSE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              scale_y_continuous(breaks = 0:10) +
                              theme_bw(base_size = 14) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

full_eliminated_histogram_plot <- ggplot(subset(complete_eliminated_data, factors != ""), aes(x = factors, fill = step)) +
                              geom_bar(stat = "count", show.legend = TRUE) +
                              ylab("count of eliminated factors") +
                              xlab("") +
                              theme_bw(base_size = 14) +
                              theme(text = element_text(family = "sans"),
                                    legend.position = "bottom",
                                    legend.direction = "horizontal",
                                    legend.title = element_blank(),
                                    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
                              guides(fill = guide_legend(nrow = 1)) +
                              #scale_color_manual(values = my_palette)
                              scale_fill_brewer(palette = "Set2")

grid.arrange(d_eff_plot, predicted_plot, design_plot,
              eliminated_number_plot, eliminated_histogram_plot,
              full_eliminated_histogram_plot,
              layout_matrix = rbind(c(1, 2),
                                    c(3, 4),
                                    c(5, 5),
                                    c(5, 5),
                                    c(6, 6),
                                    c(6, 6)))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figurehe8tZz.png]]
*** [2018-11-20 Tue]
**** More on the Gibbs Sampler :ATTACH:
:PROPERTIES:
:Attachments: casella1992explaining.pdf
:ID:       6bac6733-5d58-47b3-90f0-b01501ed28ad
:END:
This [[https://stats.stackexchange.com/questions/266665/gibbs-sampler-examples-in-r][stack exchange thread]] suggested a paper that explains the Gibbs sampler,
attached to this section.
*** [2018-11-22 Thu]
**** Looking at Federov's Selected Values
Being able to generate good samples for Federov's algorithm would be
interesting. This could be possible if, given a model, we know how to produce
the distribution of factor levels that would give the best D-Efficiency. With
this distribution, we could use something akin to Gibbs sampling to generate
only interesting levels.

I want to look at the distribution of factor levels selected in a D-Optimal
design produced by Federov's algorithm, to see if it gives any insight into
what factor level distributions could look like.

***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_uniform.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

***** Generate Data
****** Uniform Sample
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

samples <- 150

uniform_sample <- data.frame(replicate(30, runif(samples)))

runs <- 100

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

output <- optFederov(model_formula, uniform_sample, nTrials = 40)
design <- output$design
design$model <- "linear"
design$id <- 1
design$deff <- output$Dea
if (is.null(data)) {
    data <- design
} else {
    data <- bind_rows(data, design)
}

for (i in 2:runs) {
    uniform_sample <- data.frame(replicate(30, runif(samples)))
    output <- optFederov(model_formula, uniform_sample, nTrials = 40)
    design <- output$design
    design$model <- "linear"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(30, runif(samples)))
    output <- optFederov(model_formula, uniform_sample, nTrials = 70)
    design <- output$design
    design$model <- "quadratic"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(30, runif(samples)))
    output <- optFederov(model_formula, uniform_sample, nTrials = 100)
    design <- output$design
    design$model <- "cubic"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)
****** Biased Samples (Different Regions for Each Model Term)
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

samples <- 150

biased_sample <- data.frame(replicate(30, runif(samples / 2, min = 0.0, max = 0.05)))
biased_sample <- bind_rows(biased_sample,
                           data.frame(replicate(30,
                                                runif(samples / 2,
                                                      min = 0.95,
                                                      max = 1.0))))
biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

runs <- 100

model_formula <- formula(paste("~ ",
                               paste(names(uniform_sample),
                                     collapse = "+")))

output <- optFederov(model_formula, biased_sample, nTrials = 40)
design <- output$design
design$model <- "linear_biased"
design$id <- 1
design$deff <- output$Dea

if (is.null(data)) {
    data <- design
} else {
    data <- bind_rows(data, design)
}

for (i in 2:runs) {
    biased_sample <- data.frame(replicate(30, runif(samples / 2, min = 0.0, max = 0.05)))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 2,
                                                          min = 0.95,
                                                          max = 1.0))))
    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = 40)
    design <- output$design
    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(30, runif(samples / 3, min = 0.0, max = 0.05)))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 3,
                                                          min = 0.95,
                                                          max = 1.0))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 3,
                                                          min = 0.475,
                                                          max = 0.525))))
    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = 70)
    design <- output$design
    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(30, runif(samples / 5, min = 0.0, max = 0.05)))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.95,
                                                          max = 1.0))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.245,
                                                          max = 0.295))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.475,
                                                          max = 0.525))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.745,
                                                          max = 0.795))))

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]
    output <- optFederov(model_formula, biased_sample, nTrials = 100)
    design <- output$design
    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}
#+END_SRC

#+RESULTS:
:
: Warning messages:
: 1: In bind_rows_(x, .id) :
:   binding factor and character vector, coercing into character vector
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
:
: There were 50 or more warnings (use warnings() to see the first 50)

****** Biased Sample (Same Region for Each Model Term)
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

samples <- 150

biased_sample <- data.frame(replicate(30, runif(samples / 5, min = 0.0, max = 0.05)))
biased_sample <- bind_rows(biased_sample,
                          data.frame(replicate(30,
                                                runif(samples / 5,
                                                      min = 0.95,
                                                      max = 1.0))))
biased_sample <- bind_rows(biased_sample,
                          data.frame(replicate(30,
                                                runif(samples / 5,
                                                      min = 0.245,
                                                      max = 0.295))))
biased_sample <- bind_rows(biased_sample,
                          data.frame(replicate(30,
                                                runif(samples / 5,
                                                      min = 0.475,
                                                      max = 0.525))))
biased_sample <- bind_rows(biased_sample,
                          data.frame(replicate(30,
                                                runif(samples / 5,
                                                      min = 0.745,
                                                      max = 0.795))))

biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

runs <- 100

model_formula <- formula(paste("~ ",
                               paste(names(uniform_sample),
                                     collapse = "+")))

output <- optFederov(model_formula, biased_sample, nTrials = 40)
design <- output$design
design$model <- "linear_biased"
design$id <- 1
design$deff <- output$Dea

if (is.null(data)) {
    data <- design
} else {
    data <- bind_rows(data, design)
}

for (i in 2:runs) {
    biased_sample <- data.frame(replicate(30, runif(samples / 5, min = 0.0, max = 0.05)))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.95,
                                                          max = 1.0))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.245,
                                                          max = 0.295))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.475,
                                                          max = 0.525))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.745,
                                                          max = 0.795))))

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = 40)
    design <- output$design
    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(30, runif(samples / 5, min = 0.0, max = 0.05)))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.95,
                                                          max = 1.0))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.245,
                                                          max = 0.295))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.475,
                                                          max = 0.525))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.745,
                                                          max = 0.795))))

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = 70)
    design <- output$design
    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(30, runif(samples / 5, min = 0.0, max = 0.05)))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.95,
                                                          max = 1.0))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.245,
                                                          max = 0.295))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.475,
                                                          max = 0.525))))
    biased_sample <- bind_rows(biased_sample,
                              data.frame(replicate(30,
                                                    runif(samples / 5,
                                                          min = 0.745,
                                                          max = 0.795))))

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]
    output <- optFederov(model_formula, biased_sample, nTrials = 100)
    design <- output$design
    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    data <- bind_rows(data, design)
}
#+END_SRC

#+RESULTS:
:
: Warning messages:
: 1: In bind_rows_(x, .id) :
:   binding factor and character vector, coercing into character vector
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
:
: There were 50 or more warnings (use warnings() to see the first 50)

***** Plots
****** Biased Samples with Different Regions
The next figure shows the frequencies of values for one of the numerical factors
in the experiments selected by Federov's algorithm. The experiments were
selected first from a random sample of size 150 across the factor values, then
from a biased sample in different selected regions for each model (the =_biased=
plots). Each experiment was repeated 100 times, and in the first figure we see
the frequency, as a count, for the 100 repetitions.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_150_samples.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(color = id, x = X5, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figurezHuDi2.png]]

Now, we'll look at the D-Efficiency of the design obtained at each of the 100
repetitions, for the biased and uniform cases, for each of the models (linear,
quadratic, cubic). The next plot shows the 100 D-Efficiency values, jittered
in the x-axis for better visualization.

We see a difference in the spread of samples in the cubic and quadratic model
cases, but no perceptible difference in the linear model.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = model, y = deff)) +
       geom_jitter(alpha = 0.5, height = 0.0) +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figure52dUWc.png]]

****** Biased Samples with Same Regions
The next figure shows the frequencies of values for one of the numerical factors
in the experiments selected by Federov's algorithm. The experiments were
selected first from a random sample of size 150 across the factor values, then
from a biased sample in the same selected regions for each model (the =_biased=
plots). Each experiment was repeated 100 times, and in the first figure we see
the frequency, as a count, for the 100 repetitions.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_150_samples_same_regions.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X15, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figureXPuRvC.png]]

The next plot shows the 100 D-Efficiency values, jittered in the x-axis for
better visualization. As in the previous case, we see a difference in the spread
of samples in the cubic and quadratic model cases, but no perceptible difference
in the linear model. The results seem to be the same as in the previous case.

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = model, y = deff)) +
       geom_jitter(alpha = 0.5) +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-b6VOaH/figureBejIh7.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_biased_150_samples_same_regions.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
*** [2018-11-26 Mon]
**** Looking at Federov's Selected Values (Using rnorm)
Being able to generate good samples for Federov's algorithm would be
interesting. This could be possible if, given a model, we know how to produce
the distribution of factor levels that would give the best D-Efficiency. With
this distribution, we could use something akin to Gibbs sampling to generate
only interesting levels.

I want to look at the distribution of factor levels selected in a D-Optimal
design produced by Federov's algorithm, to see if it gives any insight into
what factor level distributions could look like.

***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

***** Generate Data
****** Uniform Sample
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

abs_rnorm <- function(n, mean, sd) {
    sample <- c()
    while(length(sample) < n) {
        full_sample <- rnorm(n, mean = mean, sd = sd)
        sample <- c(sample, full_sample[full_sample >= 0.0 & full_sample <= 1.0])
    }
    return(sample[1:n])
}

data <- NULL
samples <- 1200
factors <- 8
extra_experiments <- 1

uniform_sample <- data.frame(replicate(factors, runif(samples)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 100

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
design <- output$design
design$model <- "linear_unif"
design$id <- 1
design$deff <- output$Dea
design$d<- output$D
if (is.null(data)) {
    data <- design
} else {
    data <- bind_rows(data, design)
}

for (i in 2:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, abs_rnorm(samples / 2, mean = 0.0, sd = 0.1)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")
    biased_sample_1 <- data.frame(replicate(factors, abs_rnorm(samples / 2, mean = 1.0 , sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_biased_region"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, replicate(samples / 2, 0.0)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")
    biased_sample_1 <- data.frame(replicate(factors, replicate(samples / 2, 1.0)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_extremes"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, abs_rnorm(samples / 3,
                                                           mean = 0.0,
                                                           sd = 0.1)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 3,
                                                  mean = 1.0,
                                                  sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 3,
                                                  mean = 0.5,
                                                  sd = 0.05)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, abs_rnorm(samples / 4,
                                                           mean = 0.0, sd = 0.1)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 4,
                                                  mean = 1.0,
                                                  sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 4,
                                                  mean = 0.3,
                                                  sd = 0.05)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 4,
                                                  mean = 0.7,
                                                  sd = 0.05)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}
#+END_SRC

#+RESULTS:
:
: Error in optFederov(model_formula, biased_sample, nTrials = factors +  :
:   Singular design.
***** Plots
****** "Sanity Check" for the Linear Model with 1 Factor
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-jIntM5/figurenKprqc.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = model, y = deff)) +
       geom_jitter(alpha = 0.5, height = 0.0) +
       coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-jIntM5/figure54eSho.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1200_samples_8_factors_rnorm.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
*** [2018-11-28 Wed]
**** Looking at Federov's Selected Values (Categorical)
Being able to generate good samples for Federov's algorithm would be
interesting. This could be possible if, given a model, we know how to produce
the distribution of factor levels that would give the best D-Efficiency. With
this distribution, we could use something akin to Gibbs sampling to generate
only interesting levels.

I want to look at the distribution of factor levels selected in a D-Optimal
design produced by Federov's algorithm, to see if it gives any insight into
what factor level distributions could look like.

***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

***** Generate Data
****** Uniform Sample
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

abs_rnorm <- function(n, mean, sd) {
    sample <- c()
    while(length(sample) < n) {
        full_sample <- rnorm(n, mean = mean, sd = sd)
        sample <- c(sample, full_sample[full_sample >= 0.0 & full_sample <= 1.0])
    }
    return(sample[1:n])
}

data <- NULL
samples <- 1200
factors <- 30
levels <- 24
extra_experiments <- 1

uniform_sample <- data.frame(replicate(factors, sample(1:levels, samples, replace = TRUE)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 100

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
design <- output$design
design$model <- "linear_unif"
design$id <- 1
design$deff <- output$Dea
design$d<- output$D
if (is.null(data)) {
    data <- design
} else {
    data <- bind_rows(data, design)
}

for (i in 2:runs) {
    uniform_sample <- data.frame(replicate(factors, sample(1:levels, samples, replace = TRUE)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, sample(1:6, samples / 2, replace = TRUE)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")
    biased_sample_1 <- data.frame(replicate(factors, sample((levels - 6):levels, samples / 2, replace = TRUE)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_biased_region"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, sample(1:levels, samples, replace = TRUE)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, sample(1:levels, samples, replace = TRUE)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, sample(1:6,
                                                            samples / 3,
                                                            replace = TRUE)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors,
                                            sample(((levels / 2) - 3):((levels / 2) + 3),
                                                   samples / 3,
                                                   replace = TRUE)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            sample((levels - 6):levels,
                                                   samples / 3,
                                                   replace = TRUE)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, sample(1:6,
                                                            samples / 4,
                                                            replace = TRUE)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors,
                                            sample(((levels / 3) - 3):((levels / 3) + 3),
                                                   samples / 4,
                                                   replace = TRUE)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            sample(((2 * (levels / 3)) - 3):((2 * (levels / 3)) + 3),
                                                   samples / 4,
                                                   replace = TRUE)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            sample((levels - 6):levels,
                                                   samples / 4,
                                                   replace = TRUE)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)
:
: There were 50 or more warnings (use warnings() to see the first 50)
***** Plots
****** "Sanity Check" for the Linear Model with 1 Factor
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xA3dyr/figureqEtE4n.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = model, y = deff)) +
       geom_jitter(alpha = 0.5, height = 0.0) +
       coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xA3dyr/figurehlBhAU.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1200_samples_30_factors_24_levels.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
*** [2018-11-29 Thu]
**** Looking at Federov's Selected Values (runif + rnorm)
Being able to generate good samples for Federov's algorithm would be
interesting. This could be possible if, given a model, we know how to produce
the distribution of factor levels that would give the best D-Efficiency. With
this distribution, we could use something akin to Gibbs sampling to generate
only interesting levels.

I want to look at the distribution of factor levels selected in a D-Optimal
design produced by Federov's algorithm, to see if it gives any insight into
what factor level distributions could look like.

***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

***** Generate Data
****** Uniform Sample
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

abs_rnorm <- function(n, mean, sd) {
    sample <- c()
    while(length(sample) < n) {
        full_sample <- rnorm(n, mean = mean, sd = sd)
        sample <- c(sample, full_sample[full_sample >= 0.0 & full_sample <= 1.0])
    }
    return(sample[1:n])
}

data <- NULL
samples <- 1200
factors <- 8
extra_experiments <- 1

uniform_sample <- data.frame(replicate(factors, runif(samples)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 100

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
design <- output$design
design$model <- "linear_unif"
design$id <- 1
design$deff <- output$Dea
design$d<- output$D
if (is.null(data)) {
    data <- design
} else {
    data <- bind_rows(data, design)
}

for (i in 2:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, runif(samples / 3)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors, abs_rnorm(samples / 3, mean = 0.0, sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors, abs_rnorm(samples / 3, mean = 1.0 , sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_biased_region"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, runif(samples / 3)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors, replicate(samples / 3, 0.0)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors, replicate(samples / 3, 1.0)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design
    design$model <- "linear_extremes"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, runif(samples / 4)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors, abs_rnorm(samples / 4,
                                                           mean = 0.0,
                                                           sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 4,
                                                  mean = 1.0,
                                                  sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 4,
                                                  mean = 0.5,
                                                  sd = 0.05)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    biased_sample_0 <- data.frame(replicate(factors, runif(samples / 5)))
    names(biased_sample_0) <- paste("X", 1:factors, sep = "")

    biased_sample_1 <- data.frame(replicate(factors, abs_rnorm(samples / 5,
                                                           mean = 0.0, sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample_0, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 5,
                                                  mean = 1.0,
                                                  sd = 0.1)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 5,
                                                  mean = 0.3,
                                                  sd = 0.05)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample_1 <- data.frame(replicate(factors,
                                            abs_rnorm(samples / 5,
                                                  mean = 0.7,
                                                  sd = 0.05)))
    names(biased_sample_1) <- paste("X", 1:factors, sep = "")
    biased_sample <- bind_rows(biased_sample, biased_sample_1)

    biased_sample <- biased_sample[sample(nrow(biased_sample)), ]

    output <- optFederov(model_formula, biased_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}
#+END_SRC

#+RESULTS:
:
: Error in optFederov(model_formula, biased_sample, nTrials = factors +  :
:   Singular design.
***** Plots
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv': No such file or directory

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-jIntM5/figureUcBvGE.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = model, y = deff)) +
       geom_jitter(alpha = 0.5, height = 0.0) +
       coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-jIntM5/figurefD2tHa.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1200_samples_8_factors_rnorm_runif.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
*** [2018-11-30 Fri]
**** Looking at Federov's Selected Values (runif, FIXED)
Being able to generate good samples for Federov's algorithm would be
interesting. This could be possible if, given a model, we know how to produce
the distribution of factor levels that would give the best D-Efficiency. With
this distribution, we could use something akin to Gibbs sampling to generate
only interesting levels.

I want to look at the distribution of factor levels selected in a D-Optimal
design produced by Federov's algorithm, to see if it gives any insight into
what factor level distributions could look like.

***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

***** Generate Data
****** Uniform Sample
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

abs_rnorm <- function(n, mean, sd) {
    sample <- c()
    while(length(sample) < n) {
        full_sample <- rnorm(n, mean = mean, sd = sd)
        sample <- c(sample, full_sample[full_sample >= 0.0 & full_sample <= 1.0])
    }
    return(sample[1:n])
}

data <- NULL
samples <- 200
factors <- 8
extra_experiments <- 1

uniform_sample <- data.frame(replicate(factors, runif(samples)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 30

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

for (i in 1:runs) {
    sample <- data.frame(replicate(factors, runif(samples)))
    names(sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    sample <- NULL
    current_size <- 0

    while (current_size < samples) {
        new_sample <- data.frame(replicate(factors, runif(samples)))
        names(new_sample) <- paste("X", 1:factors, sep = "")

        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9))

        if (is.null(sample)) {
            sample <- new_sample
        } else {
            sample <- bind_rows(sample, new_sample)
        }

        current_size <- nrow(sample)
    }

    sample <- sample[1:samples, ]

    output <- optFederov(model_formula, sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    sample <- NULL
    current_size <- 0

    while (current_size < samples) {
        new_sample <- data.frame(replicate(factors, runif(samples)))
        names(new_sample) <- paste("X", 1:factors, sep = "")

        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.45 & . <= 0.55)))

        if (is.null(sample)) {
            sample <- new_sample
        } else {
            sample <- bind_rows(sample, new_sample)
        }

        current_size <- nrow(sample)
    }

    sample <- sample[1:samples, ]

    output <- optFederov(model_formula, sample, nTrials = (2 * factors) + extra_experiments)
    design <- output$design

    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    sample <- NULL
    current_size <- 0

    while (current_size < samples) {
        new_sample <- data.frame(replicate(factors, runif(samples)))
        names(new_sample) <- paste("X", 1:factors, sep = "")

        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.25 & . <= 0.35) | (. >= 0.65 & . <= 0.75) | (. >= 0.45 & . <= 0.55)))

        if (is.null(sample)) {
            sample <- new_sample
        } else {
            sample <- bind_rows(sample, new_sample)
        }

        current_size <- nrow(sample)
    }

    sample <- sample[1:samples, ]

    output <- optFederov(model_formula, sample, nTrials = (3 * factors) + extra_experiments)
    design <- output$design

    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}
#+END_SRC

#+RESULTS:

******* Sample Sanity Test
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

#for (i in 1:1) {
#    sample <- NULL
#    current_size <- 0
#
#    while (current_size < samples) {
#        new_sample <- data.frame(replicate(factors, runif(samples)))
#        names(new_sample) <- paste("X", 1:factors, sep = "")
#
#        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.25 & . <= 0.35) | (. >= 0.65 & . <= 0.75)))
#
#        if (is.null(sample)) {
#            sample <- new_sample
#        } else {
#            sample <- bind_rows(sample, new_sample)
#        }
#
#        current_size <- nrow(sample)
#    }
#
#    sample <- sample[1:samples, ]
#}

ggplot(sample) + geom_point(aes(x = X1, y = X4))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-UVb49j/figurePjiodU.png]]
***** Plots
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv': No such file or directory

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.



#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-UVb49j/figurercJSu4.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = model, y = deff)) +
       geom_jitter(alpha = 0.5, height = 0.0) +
       coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-UVb49j/figure4W9wpu.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1200_samples_4_factors_runif_fixed.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
** December
*** [2018-12-03 Mon]
**** Sampling from an Arbitrary Distribution in R
We are constructing D-Optimal designs for mixed types of factors using Federov's
algorithm, which needs a starting experiment sample to select the best experiment
combination. Sampling a valid initial candidate set becomes a larger problem as
the number of factors increases, and we are most interested in cases with large
numbers of factors.

Since the volume of the shell of a hypercube becomes proportionally larger than
its core as dimension increases, most samples in higher dimensions will end up
in the shell. This makes it harder to provide good initial sets to Federov's
algorithm. It is also not trivial to determine the best solution for sampling in
high dimensions.

I run experiments with rejection sampling, where a uniform sample for each
parameter was restricted to a set of interesting intervals. This works in
certain cases, but as dimension increases it becomes harder and harder to obtain
samples in valid regions, which is compounded by the lack of samples in the core
of the hypercube.

Another simple solution for this problem is sampling from arbitrary
distributions, adapted to each model term. These distributions could also
encapsulate other unfeasible regions of parameters, but it is not clear how we
would construct them.

I wrote =R= snippets that enable sampling from arbitrary distributions defined
by a generic function. The sampling is exact for discrete distributions and
approximate for continuous distributions. First, I /arbitrarily/ defined two
functions that represent the regions we want to sample for /quadratic/ and
/cubic/ factors:

#+HEADER: :results output :session *R* :exports code :eval no-export
#+BEGIN_SRC R
library(dplyr)

x <- seq(0.0, 1.0, 0.0001)
y_quad <- mapply(function(x) (64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10), x)
y_cube <- mapply(function(x) (64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) +
                             (dnorm(x, 0.65, 0.04) / 10), x)

quad_data <- data.frame(x = x, y = y_quad, name = "Quadratic Factors")
cube_data <- data.frame(x = x, y = y_cube, name = "Cubic Factors")

data <- bind_rows(quad_data, cube_data)
#+END_SRC

#+RESULTS:
:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector

The resulting data looks like this:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(subset(data, name == "Quadratic Factors" | name == "Cubic Factors"), aes(x = x, y = y)) +
       facet_wrap(. ~ name, ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureDSelKs.png]]


Then I constructed the probability density functions based on the
initial functions in a very naive way:

#+HEADER: :results output :session *R* :exports code :eval no-export
#+BEGIN_SRC R
cube_pdf <- data.frame(x = x, y = subset(data, name == "Cubic Factors")$y, name = "Cubic Factors PDF")
quad_pdf <- data.frame(x = x, y = subset(data, name == "Quadratic Factors")$y, name = "Quadratic Factors PDF")

cube_pdf$y <- cube_pdf$y / sum(cube_pdf$y)
quad_pdf$y <- quad_pdf$y / sum(quad_pdf$y)

data <- bind_rows(data, cube_pdf, quad_pdf)
#+END_SRC

#+RESULTS:
:
: Warning messages:
: 1: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector

And the resulting probability density functions look like this:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(subset(data, name == "Cubic Factors PDF" | name == "Quadratic Factors PDF"),
       aes(x = x, y = y)) +
       facet_wrap(. ~ name, ncol = 2, scale = "free_y") +
       geom_line(size = 1.3) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureodAhYJ.png]]

With the PDFs we can obtain the cumulative distribution functions with something like this:

#+HEADER: :results output :session *R* :exports code :eval no-export
#+BEGIN_SRC R
y_cube_cdf <- c()

pdf_data <- subset(data, name == "Cubic Factors PDF")$y

for (i in 1:length(pdf_data)) {
    y_cube_cdf <- c(y_cube_cdf, sum(pdf_data[1:i]))
}

y_quad_cdf <- c()

pdf_data <- subset(data, name == "Quadratic Factors PDF")$y

for (i in 1:length(pdf_data)) {
    y_quad_cdf <- c(y_quad_cdf, sum(pdf_data[1:i]))
}

data <- bind_rows(data,
                  data.frame(x = x, y = y_cube_cdf, name = "Cubic Factors CDF"),
                  data.frame(x = x, y = y_quad_cdf, name = "Quadratic Factors CDF"))
#+END_SRC

#+RESULTS:
:
: Warning messages:
: 1: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(subset(data, name == "Cubic Factors CDF" | name == "Quadratic Factors CDF"),
       aes(x = x, y = y)) +
       facet_wrap(. ~ name, ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurebTslrl.png]]

Using the CDFs we can now sample from the arbitrarily defined functions using
=runif= and the following snippet. It works by drawing a sample from =runif= and
finding the correspondent CDF value. The =x= value with such a value is the
sample from the arbitrary distribution.

#+HEADER: :results output :session *R* :exports code :eval no-export
#+BEGIN_SRC R
samples <- 2000
x_sampled <- c()

cdf_data <- subset(data, name == "Cubic Factors CDF")

for (i in 1:samples) {
    new_sample <- cdf_data$x[length(cdf_data$y[cdf_data$y <= runif(1)])]
    x_sampled <- c(x_sampled, new_sample)
}

sampled_data <- data.frame(x = x_sampled, name = "Cubic Factors Sample")

samples <- 2000
x_sampled <- c()

cdf_data <- subset(data, name == "Quadratic Factors CDF")

for (i in 1:samples) {
    new_sample <- cdf_data$x[length(cdf_data$y[cdf_data$y <= runif(1)])]
    x_sampled <- c(x_sampled, new_sample)
}

sampled_data <- bind_rows(sampled_data,
                          data.frame(x = x_sampled, name = "Quadratic Factors Sample"))
#+END_SRC

#+RESULTS:
:
: Warning messages:
: 1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
: 2: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector
: 3: In bind_rows_(x, .id) :
:   binding character and factor vector, coercing into character vector

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(sampled_data,
       aes(x = x, y = ..count..)) +
       facet_wrap(. ~ name, ncol = 2, scale = "free_y") +
       geom_histogram(bins = 200) +
       theme_bw(base_size = 33)
#+END_SRC

Looking at the samples we can see they match the initial arbitrary functions.

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureuomEvv.png]]

At this moment, we are simply picking the value of =x= that matches a given sum
of CDF. Since we are using a discrete =x=, not all values of =x= are possible to
sample, only those in the discrete scale we chose. To get back the entire
interval, we could sample uniformly in a given CDF interval, because we are not
interested in the exact shape of the PDF, but we want to guarantee samples in
certain regions.

Now we can group the cubic and quadratic CDF construction processes in functions:

#+HEADER: :results output :session *R* :exports code :eval no-export
#+BEGIN_SRC R
library(dplyr)

cubic_factor_cdf <- function(interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y_cube <- mapply(function(x) (64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) +
                                 (dnorm(x, 0.65, 0.04) / 10), x)

    data <- data.frame(x = x, y = y_cube, name = "Cubic Factors")
    cube_pdf <- data.frame(x = x, y = data$y, name = "Cubic Factors PDF")

    cube_pdf$y <- cube_pdf$y / sum(cube_pdf$y)
    data <- bind_rows(data, cube_pdf)

    y_cube_cdf <- c()

    pdf_data <- subset(data, name == "Cubic Factors PDF")$y

    for (i in 1:length(pdf_data)) {
        y_cube_cdf <- c(y_cube_cdf, sum(pdf_data[1:i]))
    }

    data <- bind_rows(data,
                      data.frame(x = x, y = y_cube_cdf, name = "Cubic Factors CDF"))
    return(data)
}

quadratic_factor_cdf <- function(interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y_quad <- mapply(function(x) (64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10), x)

    data <- data.frame(x = x, y = y_quad, name = "Quadratic Factors")
    quad_pdf <- data.frame(x = x, y = data$y, name = "Quadratic Factors PDF")

    quad_pdf$y <- quad_pdf$y / sum(quad_pdf$y)
    data <- bind_rows(data, quad_pdf)

    y_quad_cdf <- c()
    pdf_data <- subset(data, name == "Quadratic Factors PDF")$y

    for (i in 1:length(pdf_data)) {
        y_quad_cdf <- c(y_quad_cdf, sum(pdf_data[1:i]))
    }

    data <- bind_rows(data,
                      data.frame(x = x, y = y_quad_cdf, name = "Quadratic Factors CDF"))
    return(data)
}
#+END_SRC

#+RESULTS:

We can also write a sampling function based on a pre-computed CDF:

#+HEADER: :results output :session *R* :export code :eval no-export
#+BEGIN_SRC R
sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}
#+END_SRC

#+RESULTS:

Now, let's use these functions to generate a sample for 2 numerical factors
using regions corresponding to quadratic and cubic factors. To be certain our
functions produce the desired result, let's look at the CDFs and PDFs we get.

#+HEADER: :results output :session *R* :export code :eval no-export
#+BEGIN_SRC R
cube_data <- cubic_factor_cdf()
quad_data <- quadratic_factor_cdf()

cube_cdf <- subset(cube_cdf, name == "Cubic Factors CDF")
quad_cdf <- subset(quad_cdf, name == "Quadratic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example
Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
data <- bind_rows(cube_data, quad_data)
ggplot(subset(data, name == "Cubic Factors CDF" | name == "Quadratic Factors CDF"),
       aes(x = x, y = y)) +
       facet_wrap(. ~ name, ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureFANVyj.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
data <- bind_rows(cube_data, quad_data)
ggplot(subset(data, name == "Cubic Factors PDF" | name == "Quadratic Factors PDF"),
       aes(x = x, y = y)) +
       facet_wrap(. ~ name, ncol = 2, scale = "free_y") +
       geom_line(size = 1.3) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureLpwbtl.png]]

Now, let's obtain $1000$ samples for quadratic, cubic and mixed
combinations of 2 numerical factors:

#+HEADER: :results output :session *R* :export code :eval no-export
#+BEGIN_SRC R
biased_sample <- data.frame(x1 = sample_factor(quad_cdf, 1000),
                            x2 = sample_factor(quad_cdf, 1000),
                            name = "Quadratic Factors Sample")

biased_sample <- bind_rows(biased_sample,
                           data.frame(x1 = sample_factor(cube_cdf, 1000),
                                      x2 = sample_factor(cube_cdf, 1000),
                                      name = "Cubic Factors Sample"))

biased_sample <- bind_rows(biased_sample,
                           data.frame(x1 = sample_factor(cube_cdf, 1000),
                                      x2 = sample_factor(quad_cdf, 1000),
                                      name = "Mixed Factors Sample"))
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning message:
In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

We can also check the sampled distributions, verifying our results are still the
same:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 720
#+BEGIN_SRC R
ggplot(subset(biased_sample, name != "Mixed Factors Sample"),
       aes(x = x1, y = ..count..)) +
       facet_wrap(. ~ name, ncol = 2, scale = "free_y") +
       geom_histogram(bins = 200) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureTJlJPE.png]]

Now, we check the experiment samples we've got, that is, the combined values of
=x1= and =x2= in our 3 scenarios:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 1280
#+BEGIN_SRC R
ggplot(biased_sample,
       aes(x = x1, y = x2)) +
       facet_wrap(. ~ name, ncol = 2) +
       geom_point(alpha = 0.5) +
       theme_bw(base_size = 33)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurenlfKvq.png]]

We see that sampling in this way seems to produce interesting results in an
acceptable time. Since we are sampling from pre-defined distributions, we do not
spend too much time rejecting invalid samples, and we can expect samples to be
in the regions we desire. Provided we can generate PDFs and CDFs, this method
enables sampling around invalid and undefined regions for single parameters. At
the moment I do not know whether this would work with constraints. Perhaps the
CDFs would have to be computed dinamically as we restrict the search space.

The next step I intend to take is to remake the tests with Federov's algorithm
I've done before.
**** Experiments with Sampling and =optFederov=
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
***** Generate Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
lin_data <- factor_cdf(linear_function, name = "Linear Factors")

cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL

samples <- 1000
factors <- 16

extra_experiments <- 1

uniform_sample <- data.frame(replicate(factors, runif(samples)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 100

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples / 10)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    normal_sample <- data.frame(replicate(factors, rnorm((9 * samples) / 10, mean = 0.5, sd = 0.02)))
    names(normal_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- bind_rows(uniform_sample, normal_sample)

    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_normal"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(lin_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^2)+ ",
#                                     sep = ""),
#                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^3)+",
#                                     sep = ""),
#                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")
    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^2)+ ",
#                                     sep = ""),
#                               " ^2)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(quad_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, biased_sample, nTrials = (2 * factors) + extra_experiments)
    design <- output$design

    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples / 10)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    normal_sample <- data.frame(replicate(factors, rnorm((9 * samples) / 10, mean = 0.5, sd = 0.02)))
    names(normal_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- bind_rows(uniform_sample, normal_sample)

    output <- optFederov(model_formula, biased_sample, nTrials = (2 * factors) + extra_experiments)
    design <- output$design

    design$model <- "quadratic_normal"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^3)+",
#                                     sep = ""),
#                               "^3)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(cube_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, biased_sample, nTrials = (3 * factors) + extra_experiments)
    design <- output$design

    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples / 10)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    normal_sample <- data.frame(replicate(factors, rnorm((9 * samples) / 10, mean = 0.5, sd = 0.02)))
    names(normal_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- bind_rows(uniform_sample, normal_sample)

    output <- optFederov(model_formula, biased_sample, nTrials = (3 * factors) + extra_experiments)
    design <- output$design

    design$model <- "cubic_normal"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}
#+END_SRC

#+RESULTS:
****** Sample Sanity Test
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

#for (i in 1:1) {
#    sample <- NULL
#    current_size <- 0
#
#    while (current_size < samples) {
#        new_sample <- data.frame(replicate(factors, runif(samples)))
#        names(new_sample) <- paste("X", 1:factors, sep = "")
#
#        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.25 & . <= 0.35) | (. >= 0.65 & . <= 0.75)))
#
#        if (is.null(sample)) {
#            sample <- new_sample
#        } else {
#            sample <- bind_rows(sample, new_sample)
#        }
#
#        current_size <- nrow(sample)
#    }
#
#    sample <- sample[1:samples, ]
#}

ggplot(uniform_sample) + geom_point(aes(x = X1, y = X2))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureaqu2qo.png]]
***** Plots
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv': No such file or directory

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count.., fill = id)) +
    facet_wrap(model ~ ., ncol = 3, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurerxkrOE.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 3) +
       ylim(c(0,1)) +
       scale_x_discrete(name = "Jittered Experiments") +
       geom_jitter(alpha = 0.5, height = 0.0) +
       #coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureqVbdwA.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1000_samples_16_factors_normal.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
*** [2018-12-05 Wed]
**** Comparing Samples
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
***** Generate Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
lin_data <- factor_cdf(linear_function, name = "Linear Factors")

cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL

samples <- 1000
factors <- 4

extra_experiments <- 1

uniform_sample <- data.frame(replicate(factors, runif(samples)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 100

model_formula <- formula(paste("~ ", paste(names(uniform_sample), collapse = "+")))

for (i in 1:runs) {
    design <- data.frame(replicate(factors, runif(factors + extra_experiments)))
    names(design) <- paste("X", 1:factors, sep = "")

    output <- eval.design(model_formula, design)

    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$determinant

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    design <- data.frame(replicate(factors, sample_factor(lin_cdf, factors + extra_experiments)))
    names(design) <- paste("X", 1:factors, sep = "")

    output <- eval.design(model_formula, design)

    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$determinant

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^2)+ ",
#                                     sep = ""),
#                               " ^2)"))

for (i in 1:runs) {
    design <- data.frame(replicate(factors, runif((2 * factors) + extra_experiments)))
    names(design) <- paste("X", 1:factors, sep = "")

    output <- eval.design(model_formula, design)

    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$determinant

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^3)+",
#                                     sep = ""),
#                               "^3)"))

for (i in 1:runs) {
    design <- data.frame(replicate(factors, runif((3 * factors) + extra_experiments)))
    names(design) <- paste("X", 1:factors, sep = "")

    output <- eval.design(model_formula, design)

    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$determinant

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^2)+ ",
#                                     sep = ""),
#                               " ^2)"))

for (i in 1:runs) {
    design <- data.frame(replicate(factors, sample_factor(quad_cdf, (2 * factors) + extra_experiments)))
    names(design) <- paste("X", 1:factors, sep = "")

    output <- eval.design(model_formula, design)

    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$determinant

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^3)+",
#                                     sep = ""),
#                               "^3)"))

for (i in 1:runs) {
    design <- data.frame(replicate(factors, sample_factor(cube_cdf, (3 * factors) + extra_experiments)))
    names(design) <- paste("X", 1:factors, sep = "")

    output <- eval.design(model_formula, design)

    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$determinant

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}
#+END_SRC

#+RESULTS:
****** Sample Sanity Test
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

#for (i in 1:1) {
#    sample <- NULL
#    current_size <- 0
#
#    while (current_size < samples) {
#        new_sample <- data.frame(replicate(factors, runif(samples)))
#        names(new_sample) <- paste("X", 1:factors, sep = "")
#
#        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.25 & . <= 0.35) | (. >= 0.65 & . <= 0.75)))
#
#        if (is.null(sample)) {
#            sample <- new_sample
#        } else {
#            sample <- bind_rows(sample, new_sample)
#        }
#
#        current_size <- nrow(sample)
#    }
#
#    sample <- sample[1:samples, ]
#}

ggplot(biased_sample) + geom_point(aes(x = X1, y = X4))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureUZt5J9.png]]
***** Plots
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv': No such file or directory

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count.., fill = id)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurekyaJ07.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = deff)) +
       facet_wrap(model ~ ., ncol = 2) +
       ylim(c(0,1)) +
       scale_x_discrete(name = "Jittered Experiments") +
       geom_jitter(alpha = 0.5, height = 0.0) +
       #coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureL1i0be.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1000_samples_16_factors.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
**** Experiments with Sampling and =AlgDesign='s =optFederov=
***** Clone Repository                                         :noexport:
Make sure you have the latest data:
#+BEGIN_SRC sh :results output :eval no-export :exports none
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Sampling Levels of Numerical Factors
****** Baseline: Sampling for 1 Numerical Factor
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_1000_samples_1_factors.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 30)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figure0VOJYK.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D criterion") +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureXzZLKE.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = deff)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D-Eff Lowerbound") +
       ylim(c(0, 1)) +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurewIvAnQ.png]]

****** 2 Numerical Factors
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_1000_samples_2_factors.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 30)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figure0zjJ6b.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D criterion") +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurefoI6fk.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = deff)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D-Eff Lowerbound") +
       ylim(c(0, 1)) +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureeCykUP.png]]

****** 4 Numerical Factors
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_1000_samples_4_factors.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 30)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureZDJFfo.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D criterion") +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureant9bU.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = deff)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D-Eff Lowerbound") +
       ylim(c(0, 1)) +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureZYa2xW.png]]

****** 8 Numerical Factors
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_1000_samples_8_factors.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 30)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurebZoRRg.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D criterion") +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurevc9TYM.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = deff)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D-Eff Lowerbound") +
       ylim(c(0, 1)) +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureuos9r6.png]]

****** 16 Numerical Factors
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_1000_samples_16_factors.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count..)) +
    facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 30)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figure5LPx4c.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D criterion") +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurevvg4gS.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 1280 :height 900
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = deff)) +
       facet_wrap(model ~ ., ncol = 2) +
       scale_x_discrete(name = "Jittered Experiments") +
       ylab("D-Eff Lowerbound") +
       ylim(c(0, 1)) +
       geom_jitter(size = 2, alpha = 0.5, height = 0.0) +
       theme_bw(base_size = 30) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureqjyfJC.png]]
**** Experiments with Sampling and =optFederov=
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
***** Generate Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
lin_data <- factor_cdf(linear_function, name = "Linear Factors")

cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
#biased_sample <- data.frame(replicate(4, sample_factor(quad_cdf, 1000)))
biased_sample <- data.frame(replicate(30, rnorm(1000, mean = 0.5, sd = 0.05)))
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
dim(biased_sample)
head(biased_sample)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1000   30

         X1        X2        X3        X4        X5        X6        X7
1 0.4958149 0.4405408 0.5661886 0.5909414 0.5779523 0.5672937 0.5069255
2 0.5216132 0.5008759 0.5261949 0.4982797 0.4638772 0.5647152 0.5013035
3 0.5377022 0.4502152 0.5351357 0.4971554 0.4425560 0.4484254 0.5356334
4 0.5111719 0.4920589 0.4582215 0.5400119 0.5715758 0.5091692 0.4513557
5 0.5206128 0.5496239 0.4484420 0.4209305 0.4876178 0.5266049 0.4843617
6 0.4680690 0.4806462 0.5662747 0.5541188 0.5169957 0.4087654 0.5726748
         X8        X9       X10       X11       X12       X13       X14
1 0.5429327 0.4896855 0.5426568 0.4866765 0.5300847 0.5303194 0.3914867
2 0.4994371 0.5037452 0.4841519 0.5487832 0.5398791 0.4601327 0.4790544
3 0.4354802 0.4985418 0.5553337 0.4917739 0.5431963 0.4648238 0.4922096
4 0.4479097 0.4631436 0.5411018 0.5832117 0.5248540 0.3628683 0.4990992
5 0.5413598 0.4755553 0.4994663 0.4925666 0.5780525 0.4918754 0.5431612
6 0.5503436 0.4663634 0.4804488 0.4613449 0.3884554 0.4790470 0.5046359
        X15       X16       X17       X18       X19       X20       X21
1 0.4772096 0.6116114 0.4215721 0.5639071 0.5342295 0.4958506 0.4334112
2 0.5337742 0.4114659 0.6254815 0.5143135 0.4709045 0.4759814 0.4909569
3 0.4801920 0.4934720 0.5256519 0.4958561 0.4922307 0.4512127 0.5377148
4 0.4652888 0.5829699 0.5930090 0.4752843 0.5033543 0.6091310 0.3973145
5 0.4571554 0.5069214 0.4963059 0.5337339 0.5605743 0.5046651 0.4910615
6 0.5086235 0.5002959 0.5234501 0.5555824 0.4991006 0.5124023 0.5387884
        X22       X23       X24       X25       X26       X27       X28
1 0.5461074 0.5017868 0.4689750 0.5574767 0.4585234 0.4414650 0.4690905
2 0.5350600 0.4916054 0.5998847 0.4696461 0.5597062 0.5289598 0.5161302
3 0.4815272 0.5586676 0.4500824 0.5832715 0.5318447 0.5117314 0.5472547
4 0.4858816 0.5183884 0.5308895 0.5293312 0.4891215 0.5249233 0.4531990
5 0.4997377 0.4904087 0.4777940 0.5104136 0.4647391 0.4879821 0.4997653
6 0.5410121 0.4657576 0.5821968 0.5316135 0.4656868 0.5832513 0.5567264
        X29       X30
1 0.5146482 0.4634564
2 0.4469611 0.5608186
3 0.4862677 0.5716898
4 0.3810180 0.4767117
5 0.3961776 0.4845027
6 0.5420624 0.4822903
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
filter_all(biased_sample, all_vars(. >= 0.4 & . <= 0.6))
#+END_SRC

#+RESULTS:
#+begin_example
           X1        X2        X3        X4        X5        X6        X7
1   0.5377022 0.4502152 0.5351357 0.4971554 0.4425560 0.4484254 0.5356334
2   0.4795040 0.4733188 0.5102849 0.5919331 0.5935934 0.4788737 0.4277723
3   0.5227719 0.4770893 0.4743523 0.4725085 0.4854872 0.5984449 0.4703951
4   0.4864213 0.5541475 0.5002060 0.4439940 0.4502439 0.4305865 0.5233868
5   0.5115086 0.5064968 0.4334457 0.4289476 0.4881520 0.4479724 0.5193406
6   0.4765394 0.5503336 0.5753263 0.5391610 0.5504680 0.5226666 0.5361923
7   0.4591012 0.4759535 0.4745617 0.4836665 0.5851028 0.5120093 0.4904555
8   0.4219680 0.5288945 0.5156427 0.4799420 0.5146232 0.5719634 0.5070762
9   0.4567324 0.5478132 0.4175534 0.4893792 0.5444091 0.4707614 0.4199754
10  0.5859494 0.4659057 0.4272299 0.4747227 0.4820711 0.4042181 0.5298558
11  0.5233648 0.5031303 0.4461998 0.4764337 0.4841400 0.5306953 0.5698646
12  0.4780329 0.5087381 0.5326135 0.4118763 0.4851762 0.5202825 0.4514877
13  0.4741085 0.5201534 0.5031449 0.5070749 0.4888527 0.4876703 0.4769572
14  0.4893633 0.5660848 0.4918471 0.5391319 0.5606140 0.5162479 0.4478235
15  0.4921164 0.4803758 0.5697877 0.4585842 0.5909752 0.4978328 0.5371548
16  0.5005954 0.5106445 0.4975358 0.5225358 0.5058808 0.4764992 0.5373864
17  0.4629869 0.5156649 0.4339348 0.5138731 0.4305699 0.5782727 0.5621943
18  0.5310940 0.5364248 0.4386591 0.4382067 0.5113705 0.4619343 0.4629576
19  0.5410904 0.5273162 0.5305883 0.5073518 0.4622600 0.4846877 0.4477863
20  0.5198471 0.4913026 0.5525477 0.5961915 0.4355529 0.4995038 0.4725668
21  0.4433638 0.4616947 0.4877565 0.5442009 0.5148793 0.4795336 0.5637364
22  0.4535326 0.5415471 0.4876401 0.5480857 0.4918517 0.5330799 0.5870601
23  0.4549908 0.4623264 0.5256104 0.4718351 0.4454011 0.4602492 0.5588916
24  0.5153525 0.5306962 0.5067993 0.4372379 0.4277922 0.4783714 0.5904515
25  0.4253925 0.4367621 0.4626340 0.5156564 0.4984093 0.4064310 0.4918732
26  0.4253520 0.4940571 0.4824019 0.5163319 0.5143955 0.4412873 0.5069517
27  0.5219627 0.4597379 0.4977840 0.4959013 0.5123723 0.5062067 0.4578091
28  0.4570409 0.5436294 0.5992878 0.5019856 0.4790701 0.4977252 0.5015493
29  0.4748428 0.4062414 0.4621883 0.4459320 0.4719160 0.5008506 0.4413901
30  0.4656215 0.5877608 0.4535690 0.4834733 0.5165872 0.5198451 0.4141283
31  0.4840655 0.4934295 0.4078713 0.5215702 0.4897904 0.5790956 0.4172541
32  0.5657428 0.4022730 0.5454366 0.4463674 0.4263920 0.4165487 0.5331586
33  0.5337375 0.5369300 0.5361952 0.5420369 0.5023632 0.5209535 0.5207090
34  0.4585294 0.5413865 0.4655728 0.4604164 0.5680835 0.5356239 0.4847080
35  0.5546273 0.4569549 0.4851988 0.5364848 0.4081973 0.5633595 0.5648154
36  0.5046359 0.4656594 0.4598296 0.5688788 0.4844487 0.5435521 0.5098484
37  0.4893888 0.4558277 0.5479319 0.5122483 0.4977045 0.4922064 0.4586349
38  0.4216081 0.4953280 0.4865001 0.5340640 0.4618460 0.5116459 0.4771898
39  0.5257814 0.5015917 0.5379315 0.4769575 0.4830527 0.5096517 0.4435549
40  0.4889753 0.4764131 0.4582844 0.5639968 0.4868301 0.5448151 0.4586782
41  0.4744184 0.5199052 0.5769853 0.5217612 0.5308051 0.4665153 0.5146557
42  0.4638895 0.4642822 0.4632170 0.4488328 0.4989891 0.4595600 0.4337446
43  0.4797936 0.4783639 0.5543461 0.4994020 0.4964430 0.5458007 0.4658979
44  0.5156861 0.5277329 0.5772416 0.5240076 0.4687923 0.5528494 0.5680382
45  0.4786328 0.4695232 0.5498200 0.5472903 0.4976763 0.5032347 0.4910482
46  0.4659976 0.4831245 0.5625090 0.4970852 0.4417416 0.4498056 0.5214621
47  0.4899069 0.5428975 0.5123270 0.5471912 0.5301135 0.5186560 0.4920948
48  0.4324472 0.4114340 0.4822786 0.5287692 0.5352264 0.4499638 0.5460514
49  0.5571702 0.5226735 0.4925406 0.4610861 0.5056475 0.4716971 0.4664817
50  0.5044452 0.5013994 0.4567522 0.4792800 0.4658749 0.4429922 0.5728310
51  0.4165572 0.5173008 0.4442415 0.4599011 0.5297522 0.5132246 0.5033909
52  0.4393847 0.5220171 0.4797603 0.4089873 0.5647764 0.4623947 0.4404795
53  0.5404922 0.4512309 0.4636844 0.5472478 0.4917328 0.5038963 0.5179365
54  0.5526392 0.5152209 0.4836421 0.4590776 0.4664478 0.5610526 0.5166004
55  0.5082057 0.4660774 0.5238833 0.4210773 0.4744187 0.4047891 0.5354859
56  0.4768698 0.5199745 0.5169551 0.5340511 0.4903641 0.5552031 0.4011177
57  0.4480166 0.4419478 0.4035482 0.5702671 0.4616514 0.5380155 0.4177915
58  0.4979806 0.5715012 0.5310184 0.4398198 0.4822018 0.5081421 0.5556712
59  0.5745821 0.5149776 0.4394697 0.4535692 0.5031065 0.5440214 0.5230352
60  0.4698546 0.5253734 0.5284501 0.5012246 0.5025938 0.5519259 0.5078164
61  0.5932418 0.5196498 0.4457038 0.4485730 0.4785707 0.5074691 0.4573054
62  0.4892951 0.5452966 0.4529396 0.5048331 0.5056682 0.4345674 0.4749237
63  0.5241179 0.4418869 0.5370626 0.5296310 0.4654977 0.5633556 0.5176235
64  0.5125949 0.5616240 0.4898864 0.5166343 0.5121884 0.5160574 0.4987718
65  0.4655123 0.5536313 0.4749284 0.5655519 0.5305639 0.5057881 0.4749732
66  0.4815770 0.4976688 0.5253078 0.5424048 0.5325736 0.5772158 0.5030652
67  0.5465087 0.5360486 0.5373106 0.5397645 0.4356821 0.5571331 0.4729126
68  0.5095674 0.5124216 0.4954175 0.4873418 0.5227605 0.4954976 0.5629006
69  0.5495819 0.4785188 0.4683111 0.4976652 0.4918069 0.5044342 0.5127312
70  0.5032686 0.4763746 0.5819515 0.5214485 0.4191757 0.4762404 0.4879651
71  0.4871210 0.4877338 0.4788868 0.4904013 0.5294465 0.5320394 0.5078677
72  0.4949002 0.5313960 0.4388121 0.5190313 0.4925313 0.5414358 0.4737614
73  0.4598592 0.5237977 0.4951532 0.4514585 0.5159945 0.5086076 0.5710891
74  0.5041223 0.5219820 0.4817069 0.5221960 0.4367942 0.5485721 0.5628807
75  0.4204653 0.5243666 0.5294811 0.5100797 0.5819677 0.4698416 0.5105026
76  0.5650769 0.4104734 0.5849444 0.5426775 0.4859553 0.4673638 0.5099325
77  0.5262750 0.4998758 0.5551006 0.5063833 0.4728247 0.5091890 0.4794670
78  0.4908793 0.5105116 0.5397038 0.5544869 0.5909736 0.5465571 0.5391128
79  0.5644158 0.5227162 0.5079634 0.5366485 0.4811890 0.5454899 0.5563728
80  0.4462237 0.4913256 0.4956167 0.4996800 0.4348054 0.4619383 0.4441443
81  0.4969036 0.5191643 0.5095210 0.5048454 0.5518652 0.4336549 0.5045820
82  0.5941351 0.4319863 0.4410782 0.4621688 0.5320576 0.5074322 0.4007351
83  0.4735988 0.4798193 0.5284656 0.5018173 0.5076567 0.5407576 0.5276270
84  0.5524266 0.4809847 0.5855570 0.4631717 0.5321128 0.4742722 0.5295176
85  0.4658725 0.5166949 0.4781892 0.5643256 0.4969368 0.5055486 0.5092130
86  0.4471279 0.5264893 0.5388337 0.5676415 0.5107151 0.4764375 0.5009679
87  0.5702256 0.5591088 0.5040960 0.4927100 0.4769648 0.5234787 0.4463103
88  0.5803574 0.5289529 0.5246086 0.5694138 0.4663371 0.4004232 0.5815162
89  0.4792309 0.4526382 0.4610772 0.4874046 0.4742555 0.5047530 0.4833332
90  0.5183596 0.5112864 0.4708519 0.4761046 0.5971767 0.4603420 0.4674946
91  0.4701264 0.5666915 0.5018584 0.4497011 0.5319267 0.4702672 0.5219248
92  0.4525564 0.4878083 0.5049464 0.5269207 0.4595024 0.5140102 0.5275282
93  0.4622124 0.4675234 0.5909946 0.5050143 0.4670611 0.5317350 0.4387251
94  0.4488102 0.4118335 0.5533623 0.5106786 0.4806234 0.4502293 0.5197392
95  0.5065925 0.5731317 0.4832970 0.5091099 0.5009317 0.5542198 0.4579912
96  0.5111303 0.5763203 0.5286931 0.5546201 0.4381856 0.5134240 0.5426869
97  0.4838296 0.5769568 0.5158501 0.4663027 0.4227957 0.5279940 0.5557246
98  0.5211879 0.4294617 0.4436185 0.4794771 0.4405331 0.5089763 0.5362308
99  0.4965653 0.5404354 0.4976106 0.4981303 0.4468616 0.5492946 0.5442664
100 0.5653699 0.5307790 0.5211228 0.5365159 0.5479685 0.4492531 0.5272595
101 0.5569442 0.4756566 0.4438434 0.4742289 0.4899511 0.5184699 0.5217751
102 0.5957876 0.4914663 0.5592034 0.4965884 0.4466435 0.4557963 0.4471694
103 0.4870920 0.5118209 0.5209195 0.5595979 0.5082043 0.4608121 0.4921311
104 0.4961319 0.5326650 0.4894443 0.4321792 0.4094409 0.4568281 0.4837070
105 0.5573745 0.4531358 0.4883319 0.4707881 0.4659578 0.5193129 0.4113705
106 0.4842911 0.5166390 0.4356058 0.5609919 0.4375219 0.4661354 0.4219432
107 0.5688765 0.5819624 0.4376520 0.4879515 0.5307069 0.4489530 0.4401596
108 0.4882555 0.4569877 0.4840424 0.5075572 0.4981707 0.5634017 0.5437030
109 0.4679817 0.4682994 0.4396140 0.4479701 0.4331778 0.4458762 0.4898854
110 0.4815238 0.4268012 0.4854244 0.5281235 0.4587311 0.4099521 0.4551645
111 0.4866875 0.4995210 0.4364477 0.5084992 0.5321697 0.5530228 0.4724251
112 0.4666782 0.4629061 0.4792804 0.4877869 0.5618639 0.4882340 0.4567755
113 0.5389054 0.5351340 0.4789469 0.5003885 0.5534629 0.4832180 0.5257311
114 0.5098751 0.4982480 0.5359894 0.4597595 0.5274926 0.5064546 0.4700757
115 0.5361207 0.5616381 0.5410865 0.4501171 0.5703070 0.4864513 0.4451584
116 0.5679844 0.4970360 0.4302579 0.5053342 0.5069719 0.4715517 0.5147434
117 0.5027465 0.4965124 0.5448422 0.5131757 0.4031349 0.5687641 0.5479640
118 0.5737729 0.5374003 0.5913759 0.5105096 0.4359963 0.4813083 0.4360666
119 0.5435527 0.5061278 0.4911430 0.4991724 0.4437730 0.4614923 0.4545558
120 0.5204505 0.4851990 0.5140687 0.4932189 0.5590667 0.4911959 0.5188516
121 0.4228519 0.4140150 0.5107626 0.5236480 0.4306875 0.4922588 0.5205368
122 0.5108582 0.4781944 0.4504864 0.5016853 0.4835406 0.5466723 0.5477683
123 0.4758076 0.5093034 0.5333752 0.4901648 0.5455624 0.4316457 0.5014735
124 0.4671235 0.5275884 0.5422116 0.4492953 0.5192932 0.4517601 0.5781777
125 0.4810554 0.5786612 0.5324187 0.5596962 0.4625345 0.5387572 0.4530717
126 0.4701037 0.5282415 0.4516483 0.5133826 0.4947367 0.4776212 0.5521088
127 0.4857595 0.4283247 0.5617843 0.4578539 0.4950720 0.5369896 0.5017766
128 0.5007945 0.4388053 0.4804248 0.5589561 0.4319344 0.5087346 0.4071006
129 0.4652597 0.5709395 0.4307364 0.5005360 0.4723700 0.4615403 0.4464836
130 0.5024432 0.4239039 0.5038663 0.5649784 0.5405129 0.4938823 0.4719498
131 0.4545630 0.5045592 0.4420954 0.5102772 0.4547811 0.4989727 0.4124853
132 0.4940621 0.5096057 0.5517079 0.4506450 0.4652830 0.5478227 0.5045161
133 0.4947432 0.5071280 0.4515363 0.4500408 0.5250751 0.5365349 0.4869622
134 0.5169342 0.5298740 0.4470040 0.4723902 0.4747205 0.4590746 0.4503716
135 0.5120732 0.5052148 0.4314775 0.4202898 0.4872802 0.5320818 0.4573488
136 0.4198413 0.4799311 0.5843997 0.5224080 0.4934843 0.4923314 0.4799847
137 0.5324035 0.5123419 0.4442750 0.4752918 0.4847170 0.4768298 0.5975235
138 0.5375766 0.4836741 0.5512590 0.4529802 0.4824856 0.4627221 0.4756607
139 0.4108032 0.5510393 0.5099445 0.5285355 0.5136975 0.4097902 0.4562217
140 0.5220066 0.5208348 0.4948128 0.4234781 0.5189847 0.5335549 0.4710467
141 0.5640550 0.5300959 0.5086559 0.4686066 0.4520621 0.4941067 0.4968740
142 0.4361862 0.5544368 0.5679251 0.5249988 0.5014095 0.5981067 0.4498071
143 0.5218085 0.5082092 0.4192295 0.5063115 0.5186645 0.4201858 0.5250287
144 0.5586268 0.5329423 0.4395917 0.4070414 0.5548756 0.4908617 0.4769654
145 0.4632622 0.4554630 0.4725627 0.5718871 0.5373031 0.5408389 0.4337206
146 0.5169081 0.5695385 0.4754591 0.5402599 0.4934181 0.5117804 0.4991333
147 0.5558687 0.5507565 0.5218920 0.5308123 0.4313117 0.5478052 0.5065883
148 0.5294063 0.5489685 0.5452006 0.4760532 0.5408137 0.4084121 0.5486724
149 0.5748535 0.4304533 0.4664269 0.4742904 0.4719940 0.5128491 0.5140670
150 0.4675120 0.5584709 0.5032750 0.5378969 0.5225701 0.4427382 0.5386915
151 0.4801551 0.5416913 0.5573188 0.5397756 0.4804331 0.4370118 0.5022980
152 0.4469968 0.5262172 0.5490160 0.4819820 0.4399489 0.5041056 0.5683821
153 0.4044230 0.5181042 0.5184321 0.4213108 0.4588879 0.4844083 0.5198713
154 0.5410611 0.5968970 0.4937808 0.4314213 0.5043330 0.4670483 0.5591841
155 0.4666903 0.5494660 0.4560379 0.4303208 0.5824411 0.4266971 0.4286755
156 0.4471344 0.4723522 0.4688614 0.4282735 0.4173288 0.4942587 0.4703270
157 0.4975187 0.5029425 0.5067291 0.5616492 0.4244527 0.4374895 0.4990576
158 0.5318963 0.4773943 0.4888548 0.4859344 0.4622591 0.5053593 0.4354711
159 0.5484902 0.5479270 0.5152497 0.5017476 0.5364858 0.5097904 0.5085483
160 0.4946192 0.4422869 0.5705171 0.5782372 0.4871339 0.4975170 0.5093726
161 0.5505690 0.4641934 0.4986797 0.5332193 0.5593606 0.5263215 0.4840988
162 0.4694263 0.5054147 0.4661561 0.5403978 0.4541869 0.5171937 0.4939496
163 0.5301121 0.5328878 0.5043210 0.5009303 0.4859446 0.5678680 0.4871439
164 0.5133940 0.4776571 0.4891060 0.5901742 0.4474492 0.5596329 0.5612799
165 0.4657614 0.4885540 0.4858116 0.4130659 0.5832627 0.5184496 0.4990549
166 0.5140031 0.5442588 0.5923130 0.5293657 0.4945228 0.5206193 0.4183489
167 0.4571584 0.4955019 0.5391719 0.4207792 0.5760715 0.5125009 0.4497306
168 0.4901075 0.5188265 0.4558807 0.5283136 0.5356224 0.5133607 0.5277277
169 0.5219608 0.4969515 0.4636743 0.5186737 0.5096579 0.4565628 0.5828402
170 0.4747735 0.4362141 0.4317420 0.5570456 0.5118640 0.5928741 0.4638456
171 0.4476320 0.5005392 0.5169310 0.4739445 0.5935585 0.5535772 0.5861601
172 0.4261765 0.4966383 0.4705724 0.5282500 0.5351558 0.5316809 0.5543731
173 0.5190056 0.4510451 0.4455999 0.5779056 0.4485290 0.4555023 0.4978957
174 0.5360932 0.4538276 0.5369857 0.5281010 0.5028330 0.5128761 0.5567690
175 0.4398078 0.5525689 0.5960468 0.5650562 0.5034344 0.5429355 0.5676516
176 0.5895620 0.4682749 0.5363318 0.4480180 0.5078304 0.4633165 0.5640118
177 0.4385458 0.4958452 0.4317455 0.5608725 0.5418966 0.5982201 0.4243043
178 0.5869498 0.4538698 0.4990248 0.4018102 0.4875277 0.5173300 0.5507268
179 0.5406052 0.4678140 0.4427397 0.5122897 0.5105671 0.4955624 0.5312108
180 0.4984816 0.4897714 0.5024067 0.5044289 0.4672915 0.5261403 0.4764867
181 0.4700175 0.5179719 0.5225680 0.4581047 0.5439737 0.4092370 0.4791977
182 0.4142643 0.5008720 0.4713309 0.5370667 0.4652179 0.4920791 0.4976527
183 0.4484755 0.5976262 0.4314085 0.4710997 0.5084539 0.5169475 0.5481880
184 0.5448623 0.4560335 0.5320728 0.4212315 0.5029084 0.5056717 0.4992369
185 0.5356699 0.4605494 0.5618506 0.4887464 0.4747259 0.5151633 0.4613943
186 0.5265201 0.4794180 0.4522163 0.4418947 0.5019398 0.4999501 0.5696824
187 0.5285913 0.4573914 0.5408758 0.4713735 0.5137543 0.5790615 0.4430004
188 0.5201404 0.5129304 0.4738049 0.4718142 0.5097604 0.4618268 0.4725305
189 0.4323088 0.5440542 0.5490879 0.5255476 0.4766136 0.4650245 0.4303482
190 0.5011939 0.4520061 0.5491349 0.4611569 0.5041112 0.5099405 0.4373389
191 0.5029712 0.5470269 0.4934959 0.4705024 0.4603268 0.4885398 0.4278569
192 0.5028514 0.4807989 0.4859761 0.4406979 0.4832516 0.4903458 0.5362987
193 0.4879467 0.4942843 0.5995288 0.4822531 0.5018983 0.5162516 0.5574567
194 0.4917290 0.4504449 0.4767848 0.4120152 0.5087258 0.5324959 0.4337433
195 0.4788745 0.4229260 0.4976139 0.5312271 0.4679464 0.4767224 0.4956119
196 0.5098094 0.5323586 0.4842230 0.4319075 0.5110584 0.4357749 0.4494090
197 0.4775547 0.5184946 0.5535374 0.4577851 0.5212050 0.4500676 0.4894397
198 0.4348249 0.5241463 0.5186387 0.5024292 0.4634477 0.5519019 0.4284905
199 0.4072601 0.4434532 0.5992392 0.4903926 0.4634312 0.4267757 0.4631990
200 0.5279638 0.5019730 0.4560421 0.5621180 0.5298521 0.4570757 0.4382917
201 0.5569281 0.5723482 0.4454052 0.5152620 0.4854250 0.5310670 0.5603941
202 0.5103321 0.5066003 0.4830577 0.4264337 0.4562187 0.5153645 0.4590328
203 0.5666958 0.4547388 0.5000850 0.4147983 0.4628257 0.5659322 0.4731594
204 0.5414765 0.5062244 0.4379452 0.5000658 0.4673334 0.4862045 0.4633904
205 0.4648364 0.5616351 0.4982560 0.5016882 0.5274120 0.5248012 0.4361110
206 0.4612225 0.4513789 0.5799774 0.5185833 0.4980323 0.5919762 0.5274167
207 0.4382841 0.4130589 0.4160239 0.4234036 0.5237840 0.4824458 0.4738317
208 0.5211028 0.4213941 0.4647732 0.5014986 0.4016271 0.4884118 0.5481381
209 0.4836395 0.4922231 0.5118786 0.4940942 0.5058344 0.5221044 0.4807400
210 0.4891451 0.4648451 0.4339345 0.4200489 0.5256848 0.4881987 0.5323027
211 0.5116682 0.5703267 0.5071432 0.4034461 0.4398991 0.4920087 0.5248230
212 0.4320117 0.4142052 0.5535639 0.4758533 0.5705538 0.4132926 0.5228039
213 0.4830755 0.5320487 0.4400787 0.4989567 0.4970107 0.5702267 0.4966958
214 0.4834959 0.4548722 0.4931490 0.4871653 0.5193515 0.4525054 0.5100950
215 0.4554201 0.4432255 0.4975923 0.5052232 0.4142688 0.5101424 0.4779929
216 0.5618565 0.5403569 0.4406880 0.4944866 0.4595334 0.5288800 0.4894315
217 0.5035026 0.4527176 0.5025611 0.5980318 0.5531886 0.5317851 0.4911017
218 0.4997138 0.5924931 0.4900721 0.5566073 0.5482732 0.4860190 0.4532988
219 0.4318264 0.4575373 0.5220740 0.5658911 0.4989848 0.4527792 0.4689319
220 0.5438051 0.5798097 0.4648087 0.5806309 0.5580513 0.5055648 0.4997874
221 0.4641784 0.5347977 0.4874315 0.4345850 0.4642788 0.4881070 0.5083730
222 0.5544237 0.5071747 0.5248026 0.4524251 0.4938967 0.5048858 0.5996954
223 0.4347564 0.4215929 0.5496401 0.4005014 0.5167653 0.5669219 0.5356371
224 0.4631519 0.5358643 0.5260165 0.5390878 0.5296189 0.5750039 0.5161755
225 0.5018319 0.4790923 0.5694331 0.4893272 0.4414929 0.4982010 0.5729873
226 0.4748666 0.4364925 0.5022414 0.4956812 0.4669675 0.5524167 0.4841494
227 0.5040641 0.5033062 0.4703289 0.4384777 0.5803070 0.4765191 0.5509059
228 0.4598775 0.5224598 0.4764516 0.4201450 0.4564849 0.4554479 0.5639496
229 0.4639356 0.5216122 0.4432264 0.4761903 0.4292618 0.5126209 0.4451643
230 0.4661747 0.4400605 0.4419644 0.4061248 0.4568019 0.4922712 0.5340144
231 0.5034786 0.5427470 0.4966681 0.5659422 0.4175475 0.4663224 0.4971585
232 0.5660777 0.5487905 0.5398189 0.4570106 0.5458152 0.4452611 0.4785882
233 0.5037377 0.4643881 0.4708479 0.4795440 0.4501919 0.5716570 0.5404069
234 0.5494269 0.4456369 0.4797819 0.5303099 0.5162499 0.5200320 0.5894051
235 0.5507831 0.4127784 0.4957399 0.4404825 0.5576732 0.5002537 0.4884288
236 0.4367969 0.5400444 0.4653522 0.5432179 0.4207728 0.5395651 0.4951770
237 0.4839123 0.4847442 0.5065274 0.5360139 0.5342613 0.4807147 0.4765725
238 0.4816784 0.5345850 0.4705666 0.5307531 0.4900205 0.4889657 0.5894176
239 0.5743554 0.5740253 0.5993438 0.4887377 0.5111386 0.5878586 0.4827096
240 0.5437230 0.5456758 0.4904174 0.4697150 0.5802442 0.5329070 0.5150857
241 0.5958767 0.4535728 0.5107990 0.4243193 0.4846565 0.5650793 0.4824872
242 0.4767062 0.5809349 0.4945105 0.5229592 0.5129255 0.4517796 0.5897725
243 0.4692706 0.5870321 0.5220653 0.5259880 0.4921379 0.4471891 0.5439573
244 0.4819033 0.5201176 0.5148251 0.4374062 0.5195482 0.5593674 0.4410765
245 0.5086477 0.4760724 0.5346995 0.4774400 0.4177832 0.4391288 0.5586362
246 0.5306381 0.4171351 0.4322895 0.5186549 0.5068586 0.4460562 0.4945840
247 0.5678426 0.4952679 0.5004464 0.5672383 0.5185190 0.4847338 0.4877571
           X8        X9       X10       X11       X12       X13       X14
1   0.4354802 0.4985418 0.5553337 0.4917739 0.5431963 0.4648238 0.4922096
2   0.5701383 0.5045588 0.4731899 0.5459733 0.4882876 0.4669996 0.4168720
3   0.5198219 0.5028823 0.4482352 0.5007387 0.5074420 0.5796006 0.5060424
4   0.5323503 0.5590343 0.4398302 0.4781861 0.5112539 0.4684251 0.5492982
5   0.5537176 0.5243875 0.4786862 0.5114456 0.5459315 0.5454157 0.5009966
6   0.4884288 0.5286451 0.5335247 0.4160520 0.5382440 0.5196380 0.5000193
7   0.5325143 0.4917459 0.5381560 0.4634120 0.5862381 0.5281230 0.4420218
8   0.4621234 0.5397696 0.4930776 0.4648066 0.4364848 0.4403793 0.5454745
9   0.5092543 0.5659620 0.5674828 0.5734511 0.4190016 0.5528640 0.4721885
10  0.5195416 0.4791514 0.5595808 0.4915502 0.4707332 0.4517730 0.4444416
11  0.4917258 0.5259650 0.4401318 0.5261097 0.5002673 0.5000555 0.4129857
12  0.5049316 0.4053075 0.5411342 0.5134140 0.4682962 0.4029015 0.5078174
13  0.5403165 0.4720710 0.5130915 0.5235622 0.5726351 0.4892974 0.4352306
14  0.5585358 0.4480524 0.4905923 0.4607270 0.4964608 0.5055565 0.5321308
15  0.5084760 0.5858393 0.5221078 0.5474743 0.4863775 0.5644202 0.5267096
16  0.4373192 0.4643988 0.5142562 0.4477218 0.4994964 0.5403711 0.5288750
17  0.5505287 0.4675218 0.5497770 0.5613670 0.4789359 0.5900460 0.5872032
18  0.4614032 0.4324004 0.4973822 0.4204019 0.5008471 0.4950744 0.5289710
19  0.5335373 0.5081340 0.5238083 0.5449495 0.5586716 0.4625193 0.5109664
20  0.4923243 0.4297690 0.5097073 0.4921650 0.5788951 0.4826251 0.5690955
21  0.4608364 0.5305387 0.4987757 0.5249295 0.5462590 0.5441846 0.4562645
22  0.4536135 0.5245813 0.4893868 0.5287689 0.5465823 0.4935830 0.5036500
23  0.4616524 0.4234401 0.4246411 0.4854144 0.4571596 0.5153788 0.4656194
24  0.4593097 0.5675670 0.5731984 0.4418992 0.4422679 0.4494160 0.4377076
25  0.4337948 0.5690928 0.5657876 0.5037421 0.5986365 0.5916312 0.5302857
26  0.4755828 0.4853553 0.5294211 0.5880085 0.4860009 0.5075209 0.4717304
27  0.5907278 0.5430503 0.4965123 0.4760456 0.5821786 0.5496405 0.5050331
28  0.5573555 0.4740673 0.5165849 0.4947343 0.5411561 0.5198595 0.4693804
29  0.4960690 0.5327272 0.5134210 0.4726275 0.4876079 0.5124882 0.4527140
30  0.5732116 0.4943412 0.4428433 0.4892235 0.4920512 0.5840406 0.5609309
31  0.5113798 0.4310584 0.5140150 0.5213756 0.4163630 0.4663936 0.5484818
32  0.5224703 0.5176586 0.5261998 0.4939538 0.4936395 0.5463654 0.5147815
33  0.4947524 0.5268133 0.4707131 0.4761487 0.5475912 0.5817071 0.4948658
34  0.5333010 0.4462070 0.5535949 0.5493352 0.4966551 0.4538851 0.4622865
35  0.5378214 0.5242016 0.4060058 0.4727934 0.5840511 0.4551191 0.4957293
36  0.5343199 0.5844611 0.4989495 0.5273302 0.4563061 0.5207821 0.4616771
37  0.5051019 0.4503890 0.4376870 0.4944170 0.5452614 0.4493634 0.4223616
38  0.4125512 0.4966351 0.4810963 0.5309265 0.4392272 0.5969735 0.5578854
39  0.4414412 0.5648345 0.4772121 0.4970402 0.4569790 0.5101666 0.4939485
40  0.4996299 0.4436454 0.5108263 0.5099849 0.4696699 0.4934146 0.4977659
41  0.4922252 0.5167377 0.4743020 0.4491767 0.4504102 0.5322499 0.5281979
42  0.4302490 0.5667204 0.5411127 0.5082700 0.5774859 0.5387037 0.5396078
43  0.5513289 0.5047266 0.5061750 0.4755281 0.5269934 0.5152725 0.4228556
44  0.4801269 0.5018305 0.4996138 0.4681123 0.5975008 0.5030164 0.5604072
45  0.4860860 0.5707285 0.5342171 0.4752727 0.5061434 0.5927816 0.5360321
46  0.4622261 0.5388813 0.4887533 0.4862500 0.4847315 0.4581771 0.5464170
47  0.4356767 0.4459665 0.4613187 0.5402615 0.4031812 0.5412410 0.5593307
48  0.4338587 0.5626324 0.5706907 0.5987777 0.5307548 0.5399306 0.5783448
49  0.5693840 0.5079958 0.5075098 0.5178600 0.5217342 0.4553636 0.5157497
50  0.4856745 0.5357446 0.5569232 0.5042423 0.5159342 0.5440196 0.5085753
51  0.5159209 0.5596520 0.4604016 0.5268241 0.4383247 0.4751251 0.5308592
52  0.4604173 0.5990547 0.4831313 0.5078229 0.5232099 0.4638379 0.4726631
53  0.5028120 0.5169159 0.5116245 0.5504143 0.5258971 0.5489408 0.4883793
54  0.4444850 0.5031777 0.5181370 0.5905719 0.5561650 0.4889843 0.4238779
55  0.4955558 0.4624524 0.4455531 0.4934893 0.4697722 0.5299746 0.5188994
56  0.5499786 0.4514002 0.4740160 0.5632111 0.5224570 0.5622418 0.5228486
57  0.5747173 0.4272278 0.4993398 0.5403635 0.5129695 0.5578332 0.4706054
58  0.5617618 0.5620564 0.5531900 0.4655884 0.5666128 0.5019039 0.4014594
59  0.4559035 0.4606042 0.4597649 0.5347132 0.5556381 0.5670605 0.5004814
60  0.4345401 0.5482291 0.4980153 0.5860033 0.4971615 0.5711636 0.5576296
61  0.4956527 0.5231946 0.5368247 0.4496752 0.5738131 0.4379429 0.4420243
62  0.4615527 0.4708800 0.4251165 0.5725987 0.5417963 0.4582022 0.5883711
63  0.4847959 0.4738647 0.4677793 0.5762745 0.5570127 0.5166135 0.4904833
64  0.5717894 0.4866734 0.4996291 0.5772064 0.4816133 0.4911664 0.4844452
65  0.4234659 0.5635091 0.4743373 0.5033861 0.4298237 0.5214948 0.5402392
66  0.5463576 0.5402085 0.4773655 0.5382145 0.4194143 0.5610073 0.4640302
67  0.4910361 0.5514458 0.5031585 0.4617942 0.5746022 0.4442730 0.5926254
68  0.4590492 0.4400579 0.4974241 0.4502210 0.4963493 0.4620024 0.4955019
69  0.4712418 0.4600962 0.4255785 0.4804204 0.5192655 0.4662113 0.5207878
70  0.5642723 0.5347145 0.5134897 0.5171949 0.4673531 0.4929503 0.5015018
71  0.5202215 0.4547182 0.4282008 0.4197487 0.4525439 0.4982778 0.5151333
72  0.4633392 0.4865331 0.4991439 0.5747806 0.5920561 0.4898010 0.5340873
73  0.4336818 0.5052957 0.4887019 0.5106032 0.5880972 0.4927614 0.4251334
74  0.5249910 0.4797424 0.5760548 0.5934323 0.5867536 0.5171663 0.5639531
75  0.4594663 0.5434125 0.4739263 0.5374298 0.4467482 0.5026480 0.4890772
76  0.4842376 0.4327888 0.4471590 0.4880080 0.5453801 0.4561198 0.5378746
77  0.5217311 0.4347064 0.5319764 0.5629332 0.4912386 0.4143335 0.4903530
78  0.5535065 0.5715564 0.4464210 0.5304859 0.5484694 0.5140475 0.4997514
79  0.4095862 0.4746990 0.5408980 0.4467088 0.4899645 0.4093463 0.4687352
80  0.5027533 0.5535831 0.5120002 0.4551258 0.4342773 0.5543606 0.4857123
81  0.5687478 0.4122665 0.5047596 0.4363476 0.5582510 0.4496686 0.4302441
82  0.4707864 0.4494835 0.5089558 0.5388520 0.5229891 0.5387186 0.5040025
83  0.4430925 0.4226208 0.4877776 0.5080708 0.4491581 0.5259218 0.4535599
84  0.4458883 0.4779795 0.4394552 0.4047025 0.5319354 0.4480476 0.4731191
85  0.4985211 0.4497958 0.5287881 0.4761492 0.4836228 0.5155547 0.5749259
86  0.5703015 0.5701050 0.5382445 0.5050069 0.5053254 0.4648780 0.5194382
87  0.5484969 0.4101701 0.5115699 0.5591460 0.4906428 0.4876416 0.4934097
88  0.5067240 0.4344157 0.5696400 0.5698179 0.5092120 0.4867255 0.4918898
89  0.4536823 0.4618685 0.5492052 0.4845699 0.4878357 0.5596788 0.5321054
90  0.5199377 0.5302870 0.4844929 0.4550512 0.5273851 0.4768463 0.5477116
91  0.5757213 0.5578939 0.5082453 0.5291938 0.5100225 0.5095105 0.5555177
92  0.5566244 0.5149981 0.5598926 0.4949973 0.4736957 0.4668572 0.5466874
93  0.4657439 0.5174063 0.5348947 0.4694801 0.5339846 0.4729303 0.4260947
94  0.5710310 0.5538478 0.5900333 0.5191426 0.4608554 0.5893954 0.4926934
95  0.4617803 0.4423702 0.4474336 0.5305458 0.5065775 0.5288695 0.5055467
96  0.4541275 0.5366748 0.5929083 0.4684459 0.5403105 0.4676022 0.4742089
97  0.5397228 0.5107802 0.5757636 0.5658636 0.4636237 0.5540542 0.5126896
98  0.4799812 0.5121275 0.5756579 0.5070629 0.5287550 0.4801919 0.4133436
99  0.4605233 0.5255478 0.4143461 0.5023026 0.4508205 0.5641374 0.4755592
100 0.4455789 0.4841483 0.4583918 0.4384740 0.5607503 0.4139087 0.4963549
101 0.5642608 0.5183813 0.4696055 0.5124153 0.4703634 0.5413034 0.5073558
102 0.5288226 0.4547284 0.4131724 0.4910533 0.5366611 0.5463180 0.4869590
103 0.5806873 0.5226118 0.5389483 0.4382554 0.4054953 0.5812151 0.4369718
104 0.5477531 0.4813400 0.4526257 0.4831709 0.5812028 0.5545658 0.4655585
105 0.4131915 0.5445652 0.4351189 0.4910662 0.5893205 0.5680062 0.5570297
106 0.5402982 0.4535622 0.4473816 0.5612248 0.4908086 0.5150457 0.4340430
107 0.5463109 0.4888314 0.4651436 0.4952221 0.5534847 0.5190928 0.4670606
108 0.4724094 0.5015071 0.4590893 0.4898301 0.4933503 0.4949118 0.5406734
109 0.4948360 0.5356410 0.5500980 0.4750807 0.5568532 0.5523273 0.4907164
110 0.5264214 0.4685220 0.5392355 0.4975993 0.4870317 0.4879689 0.5364676
111 0.4531262 0.4844734 0.5816458 0.5144526 0.5862012 0.5211148 0.4594100
112 0.5696550 0.5653158 0.4942674 0.4455774 0.4148483 0.5060863 0.4623588
113 0.4698759 0.5032905 0.5291063 0.5327929 0.5452395 0.5510318 0.5343034
114 0.5212545 0.4952951 0.4453356 0.5541590 0.4467412 0.5369759 0.5924763
115 0.4259177 0.5219966 0.4361039 0.5326138 0.5494599 0.4827478 0.4557751
116 0.5613407 0.5249278 0.4830729 0.5239294 0.4582560 0.4289241 0.4677818
117 0.5045055 0.4738819 0.5572349 0.5341750 0.4813552 0.4365751 0.4902234
118 0.5273574 0.4356760 0.4200960 0.5395737 0.4738886 0.5217794 0.4751827
119 0.4048354 0.4505263 0.5029677 0.4507269 0.5424228 0.4951746 0.5263641
120 0.5179521 0.4220961 0.4232117 0.4046178 0.4755706 0.5139421 0.4710370
121 0.4783324 0.4904754 0.5110727 0.5247306 0.5302722 0.5502053 0.5187798
122 0.5772350 0.5479232 0.4577864 0.4191824 0.4613525 0.5569399 0.5296169
123 0.4352379 0.5662127 0.5767538 0.5700446 0.4779093 0.4748456 0.4857335
124 0.5588148 0.5287714 0.4811625 0.5047101 0.4883814 0.4633152 0.4896112
125 0.5041990 0.5141265 0.5219843 0.4288336 0.5531756 0.5459315 0.5016472
126 0.4967311 0.5669544 0.4652179 0.5553805 0.5269719 0.4854318 0.5162184
127 0.5799994 0.4506587 0.5253539 0.5324245 0.4983022 0.5182692 0.4478681
128 0.4359463 0.5529346 0.5020553 0.4244327 0.5817844 0.4188304 0.4989792
129 0.4575053 0.5417279 0.5120206 0.4152749 0.5284385 0.4130756 0.5892049
130 0.4738010 0.4842746 0.4971253 0.4611707 0.5578681 0.4611421 0.5897347
131 0.4206816 0.4030757 0.4124638 0.4144132 0.5400017 0.5354451 0.5125806
132 0.4909646 0.4516872 0.4577741 0.5080088 0.5480622 0.4243560 0.5268992
133 0.5225820 0.4725416 0.5158533 0.5648810 0.5709459 0.5300204 0.4492916
134 0.5460493 0.5144421 0.4734650 0.5170709 0.5107248 0.4853535 0.4623829
135 0.5100860 0.4785848 0.4603178 0.4202366 0.5266535 0.5048180 0.5273579
136 0.4698314 0.5017286 0.5098245 0.4430520 0.5062063 0.5251257 0.4900592
137 0.4232277 0.4169407 0.5399600 0.5878051 0.4393374 0.5668685 0.4894941
138 0.5282712 0.5002932 0.5134945 0.4578649 0.5114560 0.4966165 0.5586919
139 0.5252146 0.5713340 0.5606303 0.5070323 0.4916371 0.4901287 0.4302703
140 0.5506914 0.4695496 0.4940621 0.4429986 0.5180270 0.5453967 0.4772486
141 0.5453840 0.5147168 0.5161546 0.4430838 0.5429532 0.5210441 0.5697570
142 0.4105643 0.5045862 0.5054622 0.5494130 0.5063354 0.5056287 0.4506300
143 0.5094505 0.5457480 0.4861412 0.4239978 0.4962610 0.4575183 0.5365347
144 0.4430107 0.5057866 0.4439685 0.5100049 0.4489586 0.5677761 0.4889403
145 0.4954417 0.5558849 0.4447977 0.4952149 0.5894392 0.4868315 0.5692242
146 0.5145205 0.5973641 0.5099809 0.4794781 0.5763879 0.4805882 0.4857139
147 0.5342884 0.4668147 0.5438783 0.5055139 0.5138549 0.5241152 0.4959792
148 0.4694010 0.5257836 0.4712765 0.5070909 0.4739755 0.4930865 0.4762840
149 0.4846906 0.5054391 0.5228146 0.4291679 0.5686742 0.4303395 0.4429433
150 0.5153410 0.5192503 0.5093225 0.4719665 0.5904883 0.5058586 0.4940955
151 0.5324685 0.4777571 0.5858697 0.4915365 0.4483074 0.5769989 0.4397693
152 0.4518682 0.4499497 0.4804291 0.4924142 0.5379447 0.5936896 0.5576171
153 0.5224922 0.4958923 0.4791335 0.4454691 0.4716436 0.4565828 0.5496806
154 0.4436122 0.5110086 0.5773885 0.5104368 0.4828967 0.4332770 0.4899163
155 0.5080001 0.4798774 0.4637610 0.5128735 0.5362727 0.4950842 0.5019213
156 0.4314034 0.4427120 0.5292670 0.4779243 0.5493441 0.5248300 0.4565845
157 0.4958168 0.5496003 0.4545514 0.4814259 0.5175412 0.5398728 0.5362713
158 0.5026949 0.5585500 0.4651935 0.4804350 0.4437732 0.5426043 0.5691356
159 0.5015761 0.5188654 0.4938038 0.5285163 0.5089756 0.5406740 0.4649640
160 0.4504367 0.4668075 0.4067376 0.4710982 0.5495396 0.4696384 0.4413452
161 0.5217879 0.5353874 0.4281581 0.5044582 0.4484146 0.5143715 0.4989590
162 0.5562160 0.5593245 0.5180634 0.4759367 0.5617932 0.5370748 0.5737176
163 0.4762297 0.4734362 0.4732571 0.4969390 0.5537790 0.4967508 0.5713187
164 0.5385631 0.5334673 0.5481491 0.5358091 0.5254661 0.5328894 0.5033616
165 0.5838107 0.4805494 0.4792955 0.5832201 0.5037003 0.4864774 0.4989099
166 0.5459017 0.5064421 0.5023848 0.5173046 0.4949623 0.4876592 0.4021668
167 0.4927956 0.4029059 0.4987012 0.5328711 0.5429172 0.4710429 0.4768059
168 0.5019005 0.5518648 0.5335458 0.4954409 0.5641985 0.4567409 0.4992259
169 0.5504721 0.4360897 0.5267132 0.5150020 0.4904942 0.4941744 0.4420507
170 0.5059002 0.4470049 0.4189927 0.4169373 0.5487043 0.5425398 0.4975998
171 0.4860628 0.5157621 0.5560436 0.5180392 0.5098647 0.4607420 0.5713355
172 0.4776482 0.5777500 0.5026594 0.4881136 0.4981071 0.5801780 0.5370830
173 0.5220229 0.4756907 0.5290013 0.5691856 0.4443596 0.5228104 0.4911174
174 0.4495754 0.5608408 0.5002393 0.4800004 0.4914265 0.5481989 0.5468701
175 0.4896588 0.4973080 0.5088008 0.4859074 0.4063468 0.5031014 0.5626890
176 0.5838099 0.5481383 0.5474212 0.5684909 0.5071370 0.4798813 0.5359743
177 0.4965749 0.4815905 0.4515418 0.4987838 0.4242019 0.5211712 0.5613847
178 0.5144758 0.5911427 0.5446926 0.4362566 0.4031092 0.4955689 0.4878705
179 0.4335963 0.5138006 0.4649644 0.4582564 0.5138180 0.4330868 0.5914539
180 0.4877922 0.4506057 0.5807542 0.5476161 0.5772106 0.4652190 0.4243575
181 0.4632416 0.4650300 0.4469217 0.5110644 0.5330041 0.4430827 0.5052285
182 0.5725808 0.5001404 0.5030435 0.5178947 0.4783627 0.5548755 0.4787332
183 0.4663559 0.4977998 0.5417614 0.4585602 0.5280136 0.4596056 0.4288523
184 0.5510445 0.4662864 0.4990150 0.4585435 0.5595173 0.5471179 0.4823524
185 0.5176413 0.5178425 0.5556281 0.5115134 0.5461876 0.4660754 0.5948943
186 0.5160232 0.5539997 0.4887216 0.5545848 0.5921322 0.4314942 0.4453916
187 0.4990773 0.5219623 0.5362587 0.4865822 0.5240978 0.5029882 0.5513821
188 0.5168185 0.5720920 0.5054418 0.4694153 0.4709114 0.5553688 0.4376954
189 0.5358695 0.5710062 0.4564337 0.4485713 0.5047801 0.5839096 0.4618817
190 0.4713771 0.4536496 0.5241375 0.4910004 0.5444865 0.4727136 0.4770938
191 0.4947865 0.5113369 0.5475278 0.5384771 0.4818709 0.5138200 0.5053567
192 0.5164771 0.5319991 0.5149817 0.4951651 0.4846225 0.4364123 0.5203875
193 0.4895055 0.5267890 0.5318079 0.5602826 0.4318983 0.4295197 0.4679542
194 0.5640383 0.5275958 0.5125341 0.5441202 0.4983658 0.5474451 0.5341823
195 0.5297172 0.5516676 0.4782839 0.4347634 0.5053573 0.5099218 0.5927571
196 0.4759454 0.4905538 0.5513861 0.4446132 0.5097919 0.4856692 0.4830521
197 0.5205973 0.4621599 0.4346920 0.4906371 0.4927911 0.4754408 0.4297178
198 0.5383815 0.5678815 0.4741763 0.5185920 0.4944635 0.4382107 0.4460520
199 0.4919487 0.4806269 0.5175383 0.4750589 0.5317079 0.5135545 0.5402421
200 0.5831516 0.4191331 0.4770073 0.4183986 0.5497411 0.5083156 0.5850033
201 0.4909192 0.5384319 0.5169414 0.5085058 0.5120439 0.5756836 0.5209677
202 0.4826327 0.4991715 0.5064551 0.5541408 0.4874357 0.4943875 0.4464535
203 0.5015136 0.4541600 0.4933447 0.4526462 0.5401809 0.5236278 0.5266492
204 0.5350522 0.5853868 0.4525552 0.4675254 0.5102111 0.4880053 0.5329308
205 0.5746775 0.4916988 0.4212257 0.4312749 0.4515149 0.4239901 0.5515660
206 0.5014492 0.4198303 0.4455767 0.4910078 0.5356203 0.4552316 0.4719759
207 0.4867949 0.5247555 0.4721061 0.4531617 0.4288859 0.5861909 0.4557658
208 0.4806720 0.4446520 0.4120217 0.5886926 0.4391659 0.5101484 0.4294163
209 0.5584622 0.4724252 0.4401020 0.5381982 0.5112662 0.4998040 0.5033931
210 0.5049806 0.5219138 0.4954976 0.5453214 0.4385373 0.5439482 0.5612477
211 0.4973698 0.5079508 0.4666301 0.5252815 0.5466246 0.4916573 0.4832471
212 0.4873265 0.4868156 0.4841097 0.4243593 0.5031741 0.5183953 0.4152863
213 0.4579657 0.4939738 0.5552316 0.4838534 0.4962597 0.5448236 0.5064932
214 0.5708593 0.5006632 0.4971984 0.4892367 0.4539181 0.5257509 0.5524672
215 0.4775954 0.5843687 0.4828697 0.4988505 0.5494754 0.4818903 0.4419048
216 0.4548933 0.4928984 0.4869696 0.5020221 0.5506021 0.5412058 0.4685915
217 0.4795513 0.4596446 0.5528438 0.4654521 0.4420729 0.4707330 0.5035316
218 0.4935391 0.5248925 0.5312804 0.5056328 0.5207907 0.5312080 0.5602540
219 0.4896333 0.4586906 0.5183989 0.4977467 0.5093213 0.5193659 0.5323513
220 0.4717432 0.5346725 0.5180639 0.5437294 0.5321504 0.5907321 0.5954290
221 0.4652172 0.5205845 0.4798397 0.5330716 0.5256797 0.4853286 0.4251615
222 0.5809357 0.4739986 0.4366207 0.4808237 0.4437861 0.4720646 0.5636538
223 0.4397517 0.4973336 0.5245764 0.4488218 0.4980869 0.5539676 0.5378727
224 0.5296042 0.4648488 0.4391107 0.4759973 0.4796053 0.5567152 0.4309223
225 0.5474596 0.5206947 0.5138932 0.5373704 0.5103643 0.5493364 0.4240793
226 0.4798954 0.5200041 0.5251925 0.5237905 0.5375273 0.4526538 0.4646401
227 0.4383106 0.4699097 0.5313007 0.4709482 0.4425176 0.5531560 0.5031526
228 0.5311220 0.5436593 0.4458506 0.4764941 0.4762984 0.5459831 0.4777586
229 0.5872870 0.5054805 0.5066845 0.5390097 0.5726526 0.4856111 0.4831019
230 0.5321341 0.5277120 0.5128516 0.5187794 0.4973551 0.4482467 0.4645449
231 0.4940477 0.5197777 0.4737758 0.5247276 0.4488759 0.4926179 0.4966589
232 0.5442478 0.4391436 0.5161898 0.4401298 0.4119059 0.4915823 0.5248810
233 0.4106681 0.5231386 0.4710165 0.5001302 0.4324289 0.5259465 0.5915232
234 0.5059677 0.4957882 0.4558194 0.4666014 0.4489340 0.5468990 0.5162621
235 0.4725122 0.4531884 0.4806567 0.4556305 0.4983987 0.4819376 0.4631897
236 0.5414160 0.4319818 0.4157720 0.5339374 0.4909978 0.4436377 0.4338021
237 0.4540691 0.5872264 0.4845061 0.5317549 0.4971154 0.4745819 0.4944518
238 0.5292425 0.5191682 0.4389128 0.5488927 0.5277585 0.4827876 0.5257069
239 0.5731117 0.5119281 0.5381580 0.5236062 0.4377435 0.4654574 0.4832438
240 0.5255235 0.5483504 0.4310615 0.5013216 0.4867283 0.5313432 0.5325864
241 0.4729289 0.4774569 0.5055636 0.4391455 0.4500548 0.4499225 0.4389944
242 0.5088548 0.5197568 0.5426186 0.5592545 0.4706449 0.4880076 0.4908928
243 0.4708918 0.5605564 0.5934530 0.5228935 0.5052931 0.5276977 0.5488525
244 0.4925930 0.4523343 0.4728099 0.4769255 0.4650781 0.5473203 0.4371717
245 0.4153622 0.5996721 0.4842680 0.5575029 0.4709868 0.4567664 0.5439082
246 0.4543613 0.4972041 0.5247323 0.5155671 0.5107952 0.4884406 0.5269463
247 0.5292872 0.4450972 0.4213920 0.4971848 0.5390157 0.5620074 0.5018709
          X15       X16       X17       X18       X19       X20       X21
1   0.4801920 0.4934720 0.5256519 0.4958561 0.4922307 0.4512127 0.5377148
2   0.4941997 0.4813484 0.4721556 0.4923803 0.4998179 0.4572160 0.4533792
3   0.5102197 0.4912865 0.4884330 0.5459288 0.5090255 0.5376909 0.5758561
4   0.5564036 0.5180141 0.5459587 0.5586859 0.4708687 0.5228243 0.4950229
5   0.5491462 0.5946307 0.5094113 0.4758880 0.4556104 0.4784995 0.4864191
6   0.5303800 0.4480313 0.4730232 0.5212368 0.4945852 0.5408880 0.4513460
7   0.5561998 0.4413144 0.4506186 0.4678103 0.5191661 0.4362416 0.5181963
8   0.4383077 0.4525554 0.5316955 0.5011260 0.4381214 0.5169685 0.4722646
9   0.5325905 0.5010647 0.5362108 0.5244328 0.5404624 0.5245084 0.5856123
10  0.4981679 0.4946700 0.4806465 0.4115990 0.4857424 0.5426391 0.4954357
11  0.4706092 0.4342992 0.5174390 0.5296585 0.5489875 0.5844135 0.5213837
12  0.4585358 0.5015037 0.5322580 0.5163820 0.4789206 0.4144280 0.5469416
13  0.4628350 0.4363507 0.4968346 0.5199214 0.4914170 0.5015134 0.4895432
14  0.4343617 0.4879664 0.5061552 0.5024503 0.5050345 0.5982954 0.5443854
15  0.5062508 0.4639089 0.4802741 0.4902187 0.5272200 0.5569691 0.5519872
16  0.4598423 0.5099927 0.5291551 0.4689157 0.5368939 0.4672758 0.4585547
17  0.5156560 0.5542338 0.4871808 0.5591576 0.4482821 0.4824683 0.5540729
18  0.5457844 0.4916913 0.4771644 0.4972287 0.5113070 0.4475976 0.4864884
19  0.5022657 0.5055080 0.5262276 0.5636651 0.4206636 0.4758192 0.5464316
20  0.4571410 0.4440148 0.4743698 0.4972217 0.5123807 0.5902967 0.4765276
21  0.5269644 0.5479686 0.5305105 0.5465009 0.5138643 0.4862369 0.4342548
22  0.4544883 0.4800599 0.5313121 0.4723624 0.4745080 0.5210587 0.4458203
23  0.4682540 0.4681078 0.4983106 0.4940436 0.5116409 0.4355421 0.4642433
24  0.5371571 0.4956006 0.4557116 0.5444445 0.5220825 0.5049168 0.5210914
25  0.4796935 0.4510019 0.4542838 0.5690777 0.4931359 0.4857124 0.4866139
26  0.5796531 0.4871494 0.5075865 0.5485943 0.5256774 0.4438266 0.5538349
27  0.4913195 0.4615833 0.4644162 0.5353452 0.5479839 0.5793949 0.4648390
28  0.5032622 0.4788575 0.4399126 0.5084317 0.5322625 0.4990922 0.5070831
29  0.4806214 0.5509178 0.4440776 0.4382055 0.5038383 0.5215108 0.4936061
30  0.5118747 0.4699234 0.5220910 0.5750899 0.4805859 0.5253752 0.5113930
31  0.4837184 0.5112521 0.4737505 0.4456331 0.5142445 0.4012683 0.4461088
32  0.5188462 0.4892402 0.5352266 0.5497509 0.5399643 0.5521807 0.4714237
33  0.5794265 0.4837562 0.4674536 0.5362002 0.5177823 0.4480060 0.4473168
34  0.4262112 0.4876788 0.4891515 0.4856039 0.5256108 0.5385127 0.4859706
35  0.5413753 0.5234167 0.4954936 0.5585717 0.5201394 0.4950824 0.5434242
36  0.5054169 0.4998204 0.4156577 0.4983918 0.5638495 0.4461820 0.5595790
37  0.4824941 0.5630215 0.4889798 0.5005397 0.4589297 0.5148171 0.5463663
38  0.5207138 0.5836306 0.4866152 0.5639299 0.4644027 0.4762986 0.4805852
39  0.4865760 0.5409714 0.5734345 0.4753116 0.5383051 0.4585272 0.4397085
40  0.5104861 0.5242709 0.4859094 0.4132128 0.5043182 0.4248273 0.4655599
41  0.5675847 0.4623253 0.5362731 0.5316010 0.4587862 0.4793703 0.5158108
42  0.5080180 0.4206116 0.5170338 0.4751499 0.4756122 0.4976551 0.4921074
43  0.5757782 0.4625944 0.5547377 0.4600070 0.5022019 0.4214162 0.4906134
44  0.5154334 0.5173375 0.5303753 0.5190137 0.4316398 0.5252952 0.4315669
45  0.5722034 0.4807902 0.5495699 0.4465490 0.5284851 0.5439614 0.5205440
46  0.4775569 0.4681079 0.5934091 0.5085622 0.5048075 0.5071794 0.5316205
47  0.5575948 0.5254345 0.5360322 0.5359447 0.5170668 0.4550591 0.5356961
48  0.5420533 0.4970629 0.5093068 0.5318227 0.4985626 0.5650536 0.4520961
49  0.5622316 0.4575549 0.4250547 0.4299112 0.5254138 0.5700924 0.5020145
50  0.4825697 0.4577615 0.4291081 0.5381239 0.5308716 0.4531380 0.4165761
51  0.5266719 0.4356267 0.5041664 0.5286882 0.5321468 0.4427013 0.4878232
52  0.4852641 0.5085072 0.4725842 0.4998472 0.5265352 0.4974551 0.5059310
53  0.5576873 0.5160255 0.4733863 0.4670930 0.4771060 0.5273342 0.5269702
54  0.5619432 0.5992589 0.4501598 0.4938317 0.4513753 0.5198356 0.5464808
55  0.5248477 0.5140100 0.4678038 0.5345637 0.4957248 0.4104633 0.4695386
56  0.5682793 0.5160530 0.5289726 0.5011637 0.4662155 0.4102125 0.4568435
57  0.4827746 0.4873870 0.4578923 0.5247811 0.5096024 0.4953425 0.5595496
58  0.5623843 0.5196480 0.4786559 0.5046686 0.4586252 0.5198193 0.4603510
59  0.5814632 0.4967230 0.5037336 0.4286667 0.5111527 0.4689203 0.5405966
60  0.4591576 0.5586869 0.4523423 0.4939386 0.4427641 0.5539482 0.5255110
61  0.5285996 0.5207415 0.5099194 0.4475034 0.4234439 0.5540476 0.4547077
62  0.5634298 0.5335155 0.4898371 0.4389963 0.4901387 0.5168740 0.5006454
63  0.5291145 0.5637897 0.5280188 0.5561173 0.5324937 0.5080185 0.5337433
64  0.4626737 0.4970197 0.4584868 0.4941964 0.4895757 0.4657323 0.4410824
65  0.5055139 0.4977539 0.5130968 0.4675177 0.4472821 0.4665314 0.5332599
66  0.4747309 0.5194090 0.4731045 0.5107214 0.4300612 0.5769190 0.5536624
67  0.4004760 0.4787583 0.5662345 0.4676218 0.5972299 0.5057595 0.4929717
68  0.5899140 0.5838738 0.5090273 0.5012798 0.4760582 0.5540391 0.4614937
69  0.5764458 0.4467131 0.5621985 0.5171500 0.4957535 0.4585839 0.5556389
70  0.5517234 0.4949880 0.4798179 0.5276835 0.4955250 0.5535997 0.5134346
71  0.5298780 0.5734157 0.5122137 0.4759392 0.4994940 0.5359008 0.4491161
72  0.4258301 0.5599959 0.4333938 0.5164360 0.5012669 0.5265176 0.4507416
73  0.4691791 0.5275764 0.4783426 0.4117217 0.4579936 0.5790254 0.5150746
74  0.5686437 0.5504970 0.5682003 0.4731631 0.4802518 0.4549132 0.4954882
75  0.4915969 0.4704291 0.4910512 0.4510392 0.4661607 0.4476144 0.5048317
76  0.4378248 0.4407967 0.4965202 0.5087244 0.5412621 0.4143234 0.5631665
77  0.5285500 0.4856424 0.4963019 0.5729690 0.4424081 0.5191564 0.4180260
78  0.4949518 0.5791235 0.5207226 0.4358551 0.5361669 0.5604920 0.5175237
79  0.4888183 0.4732210 0.5251454 0.5772895 0.4761451 0.4534194 0.5514121
80  0.4677864 0.5080149 0.4228534 0.4347649 0.4419797 0.5087936 0.4844627
81  0.4835626 0.5545504 0.5112734 0.5136519 0.4476677 0.5580490 0.4880681
82  0.5069127 0.4654139 0.5758202 0.4510172 0.4667347 0.4789567 0.4743673
83  0.5521728 0.5841890 0.4490722 0.5763637 0.4786180 0.5378297 0.5217432
84  0.4907343 0.4676678 0.5612047 0.5320845 0.4548366 0.5251734 0.5270685
85  0.5243234 0.5004745 0.5893027 0.5245044 0.5104365 0.5234435 0.5168192
86  0.5193027 0.4673008 0.5408406 0.4435541 0.4769944 0.4201409 0.4515349
87  0.4818351 0.4571186 0.5049894 0.4748265 0.4924737 0.5028328 0.4540287
88  0.5304727 0.5541403 0.4578837 0.5821444 0.4565729 0.4222042 0.4164593
89  0.5768929 0.4670246 0.5197525 0.5483798 0.5026577 0.5775938 0.5178400
90  0.4892794 0.5380948 0.5292772 0.4370896 0.5309087 0.5947998 0.5723320
91  0.5193557 0.5974230 0.5622723 0.4797954 0.4358561 0.5154163 0.5119929
92  0.5201982 0.5507259 0.5037492 0.5375013 0.5213036 0.4950696 0.5564491
93  0.5300826 0.4540162 0.5419723 0.5178133 0.5235764 0.4645341 0.5246739
94  0.4752086 0.5328049 0.5378189 0.5112556 0.4557587 0.5159731 0.5098494
95  0.5398597 0.4835548 0.5210872 0.5115213 0.4660114 0.4795908 0.5264413
96  0.5050365 0.4679324 0.4574310 0.4970990 0.4697897 0.5583767 0.5050828
97  0.4739299 0.5231101 0.4598507 0.4536605 0.5161787 0.4346610 0.4914231
98  0.4308364 0.5198320 0.4793397 0.4668961 0.4454029 0.5803261 0.5695142
99  0.4459016 0.4823741 0.5612796 0.4979911 0.4307566 0.4549148 0.4751038
100 0.5178964 0.5718396 0.4296635 0.5616071 0.5068908 0.4337302 0.5113783
101 0.5920581 0.4725055 0.5211031 0.5565077 0.4565024 0.5164286 0.4886460
102 0.4481710 0.5407805 0.4760262 0.4540812 0.5029896 0.5282963 0.5600142
103 0.5607177 0.4395596 0.5618332 0.5368458 0.4901887 0.4892644 0.5499885
104 0.5125919 0.4974019 0.5496438 0.5108799 0.4702922 0.5323743 0.5773121
105 0.4381834 0.4879084 0.4301967 0.4739344 0.4780814 0.4303930 0.5318018
106 0.4410793 0.5007265 0.5111058 0.4865486 0.5350670 0.4742912 0.4601354
107 0.5537325 0.5946610 0.5322976 0.5185788 0.4509513 0.4748719 0.4156911
108 0.4584661 0.5148525 0.4767944 0.5214374 0.4683118 0.5894663 0.4952239
109 0.4971716 0.5068461 0.5425682 0.4519786 0.4840762 0.4827726 0.4806280
110 0.5922673 0.5353061 0.5759943 0.4804096 0.5519314 0.5487572 0.5152777
111 0.4940795 0.5233380 0.5691688 0.5129031 0.5555896 0.5365671 0.5237250
112 0.5276814 0.4657537 0.4481656 0.5642213 0.4839461 0.5655179 0.5039705
113 0.4847487 0.4307798 0.4690832 0.4409182 0.4949970 0.4987849 0.4798305
114 0.5372022 0.5161273 0.4881874 0.4407717 0.4804770 0.5570726 0.4810828
115 0.4911816 0.5305407 0.4699751 0.4935827 0.4827081 0.4267037 0.5258086
116 0.4568738 0.4929177 0.4846580 0.4785204 0.4363302 0.5124670 0.4722115
117 0.5358998 0.4430937 0.5278329 0.5154722 0.4686557 0.4263449 0.5874335
118 0.5580599 0.4577958 0.5115857 0.4555083 0.5413948 0.4735568 0.5085372
119 0.5157686 0.5990101 0.4920484 0.4344005 0.4883510 0.5393258 0.5246826
120 0.4223522 0.4943470 0.4856936 0.5320803 0.4565226 0.4932323 0.5704951
121 0.4693664 0.5150169 0.5810701 0.5567159 0.5567640 0.5448681 0.4604168
122 0.4595680 0.5822517 0.4308363 0.5648946 0.5806727 0.5277750 0.4762082
123 0.5703674 0.4705283 0.4389272 0.4415908 0.5003542 0.5387895 0.4874778
124 0.5214364 0.5170696 0.4750938 0.5307605 0.4343331 0.4594383 0.4596386
125 0.5144738 0.4867216 0.4833553 0.4010722 0.5596268 0.4709659 0.4257285
126 0.5889735 0.4423667 0.5740022 0.4265076 0.4684834 0.5307075 0.4818874
127 0.4806515 0.4983013 0.4736649 0.5114372 0.5278610 0.4586881 0.4586646
128 0.5082822 0.4892920 0.5574753 0.5145896 0.4297737 0.4676278 0.4397992
129 0.5458900 0.4694266 0.5232242 0.4828875 0.5489327 0.4864989 0.4795025
130 0.4606870 0.4627657 0.4511541 0.5782747 0.5296008 0.4931099 0.5116318
131 0.5946521 0.5491180 0.4862178 0.4916640 0.5160739 0.5676308 0.4930266
132 0.5604475 0.4962415 0.4924108 0.4241972 0.5133414 0.5196227 0.5780882
133 0.5129740 0.4962925 0.4852754 0.4392186 0.4867211 0.5449017 0.5065782
134 0.5100597 0.4804049 0.5012933 0.4792381 0.4837041 0.4950162 0.4630253
135 0.4348500 0.5126392 0.4452294 0.4422956 0.4864139 0.4140535 0.4469465
136 0.4715682 0.5299153 0.4852770 0.4711129 0.5038649 0.5514584 0.5021349
137 0.5387261 0.5357232 0.5022081 0.5401738 0.5165875 0.5266926 0.4537737
138 0.4869571 0.4602818 0.4425079 0.4534315 0.4687394 0.4830212 0.4267868
139 0.5084688 0.4316386 0.4679985 0.5111773 0.5432160 0.5172702 0.5020322
140 0.5414518 0.4758635 0.4464462 0.4779384 0.5277040 0.4796728 0.5641781
141 0.4679042 0.4730716 0.4969306 0.4757460 0.4500993 0.5232036 0.5418203
142 0.4738227 0.5177164 0.4886139 0.5453665 0.5092473 0.4770146 0.5127640
143 0.5345434 0.4273265 0.5058706 0.4598613 0.5469665 0.4793992 0.4259719
144 0.5232280 0.4763140 0.4724692 0.5159065 0.4400568 0.5497899 0.5125986
145 0.4575905 0.5771461 0.5778434 0.4982270 0.5236464 0.5785291 0.5224537
146 0.4817367 0.5203525 0.4338300 0.4891830 0.5097060 0.5515053 0.4582017
147 0.4894603 0.5521973 0.4532386 0.4863983 0.5692979 0.4047091 0.4231922
148 0.4560922 0.4883195 0.4776802 0.4191028 0.5707319 0.5638736 0.4857645
149 0.5264757 0.4386217 0.5568214 0.4690834 0.5239361 0.5621415 0.4887014
150 0.5400758 0.4920305 0.5066976 0.5587292 0.4829223 0.4897073 0.5046763
151 0.4609936 0.5168620 0.4830154 0.5429884 0.5753771 0.4805518 0.4970595
152 0.4883363 0.5283680 0.4729927 0.5187233 0.5074243 0.5121457 0.5319593
153 0.5009154 0.4576442 0.4936138 0.5054701 0.5204966 0.5949579 0.5600062
154 0.4838303 0.4805990 0.5592020 0.4577362 0.5381890 0.5160818 0.4977356
155 0.5182469 0.4928965 0.4396448 0.5412948 0.4733881 0.4909574 0.4385324
156 0.4784907 0.5280551 0.5463345 0.5246744 0.5484016 0.4145820 0.4192215
157 0.5950685 0.5153450 0.4915592 0.5327745 0.5842327 0.4891854 0.5189316
158 0.4783318 0.4612470 0.4416132 0.5242425 0.4511548 0.5518770 0.5056035
159 0.5132738 0.5315851 0.4995580 0.4983859 0.5569940 0.5708888 0.4650907
160 0.4896608 0.5240481 0.5597290 0.4852601 0.4998663 0.5677647 0.5057850
161 0.5147100 0.5294311 0.4998533 0.4573459 0.5061573 0.4524586 0.5774921
162 0.5362940 0.5302604 0.5135795 0.5101189 0.5661732 0.5003794 0.4811123
163 0.4485613 0.5215632 0.4078513 0.5647317 0.5407401 0.4356585 0.4487207
164 0.5104283 0.5647900 0.5073840 0.5071715 0.5340394 0.5153141 0.4768939
165 0.5288494 0.5468429 0.5129162 0.4585061 0.5809519 0.5441793 0.5083383
166 0.4788496 0.4999579 0.5453092 0.5360684 0.4960450 0.5507278 0.4341933
167 0.5246103 0.5570577 0.4861141 0.4462404 0.5695401 0.4296725 0.4222389
168 0.4706675 0.4795704 0.4306851 0.5262971 0.4757064 0.4982607 0.4973955
169 0.5191164 0.4381743 0.5251834 0.5392535 0.4404815 0.4790081 0.4573619
170 0.4750586 0.5093179 0.5166958 0.4488930 0.4749483 0.4423539 0.5307563
171 0.4412696 0.5000618 0.5596757 0.4218767 0.5631491 0.5117138 0.4954475
172 0.5747525 0.5632775 0.4864810 0.5102776 0.4390679 0.4982387 0.5683201
173 0.4374604 0.4624197 0.4701688 0.4725449 0.4931496 0.5075569 0.4613958
174 0.5469139 0.5544382 0.4668435 0.4980832 0.5976522 0.5329547 0.4841116
175 0.5330359 0.4810657 0.5383627 0.5358177 0.4835563 0.5502526 0.4242804
176 0.4811255 0.5715509 0.4527839 0.4972984 0.5252039 0.5801443 0.5953963
177 0.5616852 0.4666176 0.5214956 0.4859003 0.4398224 0.4348655 0.4231335
178 0.5494417 0.5478924 0.5175739 0.5489000 0.4662396 0.4814685 0.4663505
179 0.5309107 0.4371132 0.4830475 0.5480027 0.5286673 0.4931852 0.4569893
180 0.4569924 0.4376194 0.5677659 0.5247186 0.4947130 0.4915346 0.5344412
181 0.5140637 0.4817993 0.4500291 0.5351931 0.5790322 0.4942287 0.5343595
182 0.4800107 0.4464860 0.4858219 0.4679092 0.4584981 0.4937971 0.4456564
183 0.4941757 0.4791130 0.5562980 0.5263955 0.5804562 0.4504775 0.5200720
184 0.4904145 0.5279306 0.4874925 0.4987594 0.4174636 0.5945005 0.4927220
185 0.4843010 0.5054949 0.5236418 0.5151604 0.5351600 0.5179893 0.4514178
186 0.5222197 0.5287679 0.4573651 0.4980125 0.5092772 0.5209698 0.5116476
187 0.4213420 0.5736905 0.5487291 0.4489036 0.4723391 0.4175158 0.4796826
188 0.5208400 0.4188125 0.5243287 0.4163312 0.5774555 0.4290958 0.4852040
189 0.4515252 0.5548677 0.5097439 0.4896732 0.4862518 0.4201239 0.4538710
190 0.4457968 0.4976193 0.4822694 0.4902788 0.5083460 0.4434564 0.4776626
191 0.4686194 0.5011080 0.4221794 0.4818485 0.4796685 0.4749746 0.4993102
192 0.4890436 0.5397892 0.4625189 0.4852857 0.4339605 0.4210870 0.5665826
193 0.5498767 0.4172089 0.5586766 0.4824244 0.4250780 0.5202730 0.4743676
194 0.5597594 0.4957767 0.5139510 0.4557864 0.5483543 0.5613627 0.4116300
195 0.4687479 0.5004669 0.5119511 0.5249473 0.4882718 0.4477965 0.4360517
196 0.4380283 0.4912731 0.4938476 0.4596390 0.4784466 0.5097707 0.4598863
197 0.4172681 0.4941289 0.5268408 0.5404058 0.5360551 0.4091272 0.4636942
198 0.5111105 0.5048423 0.4971332 0.4726551 0.5224682 0.5058273 0.4734736
199 0.5103587 0.5051002 0.5748865 0.4877221 0.5595838 0.4887626 0.5102686
200 0.4514771 0.5152724 0.5040476 0.4662086 0.4018462 0.5011481 0.4231949
201 0.4775928 0.4613756 0.5155035 0.5354753 0.5331847 0.4667871 0.5989202
202 0.5237647 0.5506553 0.5624244 0.4965670 0.4971545 0.5329011 0.5260313
203 0.5268574 0.4860935 0.5454770 0.5345814 0.4431855 0.4738648 0.5452933
204 0.5013809 0.4871561 0.5057684 0.5072831 0.4605094 0.5321240 0.4185864
205 0.4441285 0.4989887 0.5034395 0.5056477 0.4769228 0.5230114 0.5622900
206 0.5623911 0.5429283 0.5109493 0.4180277 0.5252035 0.5154057 0.4891207
207 0.4466894 0.5426510 0.5037959 0.4546613 0.4920854 0.5213591 0.4595213
208 0.5132004 0.5245676 0.4897632 0.4351857 0.5327445 0.4317441 0.5346713
209 0.4464041 0.5418479 0.4355557 0.5194122 0.5620542 0.5534975 0.4879574
210 0.4261185 0.4602221 0.5124534 0.4359891 0.4498740 0.5043461 0.4640449
211 0.4713535 0.4818780 0.5090846 0.4155658 0.4446055 0.4521346 0.4354726
212 0.4129755 0.4303051 0.4825940 0.4832046 0.5882172 0.4800066 0.4655952
213 0.4975672 0.5386754 0.4399784 0.4729595 0.4587034 0.5032435 0.5908114
214 0.5385269 0.5328817 0.5026964 0.4945077 0.5257644 0.4889681 0.5338949
215 0.4310751 0.5255946 0.4421349 0.5399976 0.5183646 0.4946903 0.4429303
216 0.4894507 0.4147091 0.5409663 0.5761904 0.5268693 0.5106417 0.4814042
217 0.4543487 0.4424485 0.5049413 0.4753165 0.4475527 0.4893073 0.5586313
218 0.4604286 0.5443469 0.5400022 0.5456488 0.5975835 0.4725646 0.4814288
219 0.5306570 0.4943110 0.4266319 0.5040110 0.4963036 0.4938362 0.4826547
220 0.5129843 0.4178853 0.5441032 0.4108231 0.4913218 0.5290085 0.4667287
221 0.4461446 0.5760823 0.5598757 0.5118373 0.5570665 0.5189773 0.5271101
222 0.4452434 0.5528604 0.4638201 0.4567153 0.5874219 0.4833848 0.5856955
223 0.5701702 0.5815057 0.4684641 0.5977111 0.5462640 0.4981406 0.5006190
224 0.5879853 0.4351447 0.5283912 0.4160620 0.4948771 0.5230901 0.5063713
225 0.5157880 0.4806621 0.5162475 0.4696400 0.4627819 0.4627064 0.5258231
226 0.5170744 0.5074688 0.5431010 0.5292570 0.5648088 0.5735150 0.5541115
227 0.4401871 0.4914470 0.4636976 0.5156375 0.4129096 0.4637021 0.4899915
228 0.4845751 0.5517126 0.5344096 0.4491809 0.4732427 0.5187806 0.4217670
229 0.4506374 0.5971941 0.5274733 0.5157867 0.5078223 0.5111744 0.4372336
230 0.4474925 0.5047499 0.4540717 0.5367986 0.5164692 0.5087816 0.5589299
231 0.4263824 0.5371861 0.4387690 0.4397701 0.4481649 0.5459618 0.5554729
232 0.4575778 0.5344797 0.4932748 0.5054556 0.5469134 0.5174137 0.4691295
233 0.5646033 0.5245612 0.4705104 0.4792069 0.4447257 0.4583224 0.4490103
234 0.4316335 0.4423067 0.5215346 0.5063533 0.4676458 0.4744171 0.5044381
235 0.4663591 0.4645353 0.4593164 0.4828560 0.4681418 0.5124554 0.5509610
236 0.5400231 0.4732294 0.4539384 0.4349246 0.5909167 0.4771318 0.4580448
237 0.4872505 0.4843338 0.5529570 0.4736731 0.5138230 0.5487463 0.5915593
238 0.4905988 0.5817869 0.4983230 0.4844997 0.5559393 0.5256144 0.4931655
239 0.4552226 0.4982491 0.5031897 0.4996333 0.4733052 0.4763391 0.5122241
240 0.5651218 0.4753479 0.4720550 0.5531829 0.4920838 0.5378809 0.5820128
241 0.5079985 0.5220578 0.4792040 0.5386368 0.4002150 0.4681059 0.4468912
242 0.4415047 0.4755198 0.4912439 0.4686705 0.4882345 0.4955198 0.4560766
243 0.5144360 0.4902771 0.4566342 0.4973341 0.4470616 0.5148073 0.5899117
244 0.5307294 0.5146341 0.4812391 0.4357069 0.5032320 0.4552458 0.5214791
245 0.4965323 0.4494758 0.4888536 0.5721740 0.4964274 0.5333541 0.5082746
246 0.5092170 0.5509770 0.4277948 0.5597303 0.4870951 0.5001071 0.5772238
247 0.5293025 0.5229329 0.4171159 0.4384379 0.5530334 0.5178222 0.5444442
          X22       X23       X24       X25       X26       X27       X28
1   0.4815272 0.5586676 0.4500824 0.5832715 0.5318447 0.5117314 0.5472547
2   0.5464058 0.5200976 0.5155736 0.5611311 0.4316109 0.5847908 0.4842653
3   0.5245335 0.5741116 0.5332972 0.5287989 0.5327723 0.4694638 0.4214561
4   0.4154077 0.5168094 0.4946047 0.5061126 0.4811057 0.5174698 0.5835993
5   0.5107473 0.5078010 0.4358275 0.4934266 0.4980592 0.4760520 0.5594725
6   0.4165544 0.4943162 0.4826972 0.4538986 0.5288724 0.4474252 0.4735041
7   0.4613002 0.5153345 0.5686116 0.5457642 0.5605778 0.4243334 0.4651750
8   0.4089740 0.5254284 0.4873724 0.4519118 0.4370603 0.5037073 0.4316300
9   0.5056194 0.5006990 0.5520561 0.4433548 0.4503541 0.4602503 0.5155682
10  0.5276171 0.5357312 0.5862564 0.5269969 0.4513835 0.4447277 0.5258206
11  0.4333261 0.5207645 0.5042412 0.4787522 0.5020773 0.5619790 0.5498356
12  0.5404627 0.5809485 0.4849389 0.5179532 0.5230789 0.5970747 0.4748469
13  0.5288058 0.4727296 0.4797238 0.4563711 0.4467641 0.5600833 0.4155885
14  0.5190928 0.5816379 0.5709478 0.4958282 0.5178063 0.4540828 0.5722552
15  0.4227410 0.4668324 0.5483205 0.5387540 0.4848664 0.4745376 0.5038893
16  0.5216330 0.4869443 0.5241369 0.4697841 0.4873008 0.5020846 0.4834270
17  0.4084142 0.5624973 0.5330597 0.4640363 0.5792821 0.5154322 0.5557746
18  0.5280796 0.5600147 0.5375214 0.5523420 0.4755880 0.4871811 0.4621853
19  0.4144697 0.4626760 0.5403766 0.4485899 0.4569068 0.5023609 0.5065593
20  0.5266456 0.4049560 0.5085195 0.4772332 0.5135899 0.5367575 0.5621823
21  0.5791456 0.5423605 0.5322203 0.4694147 0.4401342 0.5061095 0.5546859
22  0.5884195 0.5154113 0.5563749 0.4753652 0.4609906 0.4753501 0.5375113
23  0.4727118 0.5510651 0.5159671 0.5148713 0.4709457 0.4772954 0.5589442
24  0.5459764 0.5228691 0.5385721 0.4719379 0.5694804 0.4614263 0.4624818
25  0.4737176 0.5960499 0.5005087 0.5132375 0.4358137 0.4534157 0.5188058
26  0.5124339 0.4408014 0.5108180 0.4499909 0.5418790 0.5028691 0.4966162
27  0.4584705 0.5281030 0.4690214 0.4560516 0.5474591 0.4920522 0.4831834
28  0.4880556 0.5153384 0.5336931 0.5367887 0.5400997 0.4360003 0.4679613
29  0.5062084 0.5062490 0.5233630 0.5278178 0.4777372 0.5416018 0.4390941
30  0.5263485 0.5775374 0.5301069 0.5096414 0.4091495 0.4502776 0.5153852
31  0.4987034 0.4366835 0.5198817 0.4555798 0.4786519 0.4913303 0.4524904
32  0.5434123 0.5833960 0.4519172 0.4965612 0.4267609 0.5420820 0.4607915
33  0.4772017 0.4714327 0.5272967 0.4235593 0.5256657 0.5147143 0.4974693
34  0.4286407 0.4771992 0.4962957 0.4100861 0.4900788 0.5304172 0.4931898
35  0.4727500 0.5494630 0.5612121 0.4349549 0.4862139 0.5028599 0.5207116
36  0.5010594 0.5101564 0.5321779 0.4084849 0.5520125 0.4791377 0.5141546
37  0.4132858 0.5115071 0.5107207 0.5389038 0.5107063 0.4761445 0.4514783
38  0.4826567 0.4724048 0.4813269 0.5053031 0.5239424 0.5231103 0.5572031
39  0.4690718 0.4627769 0.5946576 0.5678852 0.4664297 0.5273979 0.4931827
40  0.4890330 0.5206906 0.4717476 0.4400680 0.4759494 0.5686107 0.4489490
41  0.5433956 0.5214908 0.5768649 0.5044252 0.5500419 0.5256558 0.5939567
42  0.4765286 0.5445123 0.5485673 0.5316129 0.5436403 0.4372805 0.4536911
43  0.5523295 0.4902377 0.4985196 0.5165654 0.5579189 0.5482698 0.5382287
44  0.5401777 0.5241254 0.4965541 0.4575690 0.4693249 0.5002768 0.5244365
45  0.5346537 0.4819025 0.4613471 0.4352675 0.5611207 0.5349608 0.4541402
46  0.5584603 0.5025714 0.4832412 0.4726513 0.4745258 0.4836236 0.5001061
47  0.5184619 0.4164171 0.5082393 0.4641139 0.4905113 0.4177639 0.5284057
48  0.4253372 0.4992622 0.4245225 0.4734015 0.4800573 0.5440792 0.4490171
49  0.5414956 0.4156271 0.5288896 0.5405376 0.5332610 0.4015706 0.4940290
50  0.4895942 0.5287212 0.4415682 0.5518955 0.5344624 0.5529293 0.5296556
51  0.4866206 0.4551410 0.5110447 0.5429646 0.5180371 0.4215279 0.5368719
52  0.5533575 0.5413645 0.5625628 0.4984403 0.4719207 0.5092359 0.4596542
53  0.5040177 0.5278270 0.5459766 0.4797904 0.5549393 0.5571877 0.4871958
54  0.5011181 0.4088013 0.4271458 0.4472766 0.4922957 0.5335336 0.5155781
55  0.5394696 0.4353239 0.5088002 0.5014998 0.4755939 0.4824925 0.4493243
56  0.4557571 0.5873042 0.5292615 0.4816748 0.4661684 0.4320873 0.4992473
57  0.5438571 0.5022367 0.5384310 0.4631743 0.5676159 0.5517007 0.5021352
58  0.4689863 0.4615403 0.4213568 0.4829298 0.5448141 0.4128896 0.5871344
59  0.4892583 0.4639594 0.4429491 0.5538242 0.5541225 0.5702140 0.4309845
60  0.5646846 0.4984353 0.5753077 0.4878769 0.5309573 0.5055001 0.4921158
61  0.5760912 0.5312219 0.5500543 0.5104511 0.4071127 0.4226543 0.4366566
62  0.4132581 0.4260944 0.4922798 0.5242193 0.5244084 0.5838967 0.4720722
63  0.4596332 0.4355861 0.5090787 0.4904369 0.5377816 0.5343327 0.4773250
64  0.4936486 0.4626234 0.5022646 0.5810412 0.4764031 0.4113697 0.5462956
65  0.5152415 0.5576226 0.5331440 0.5392511 0.5058540 0.5316920 0.5142448
66  0.5809555 0.5381852 0.4747280 0.5466974 0.5135927 0.5226477 0.5454634
67  0.5144356 0.4013405 0.5146673 0.5263689 0.5720072 0.4315343 0.4987071
68  0.5555531 0.4995285 0.5762646 0.4488021 0.4652376 0.4321401 0.4469966
69  0.4760128 0.4296478 0.5822527 0.5678161 0.5075725 0.5338763 0.4786852
70  0.4192653 0.5225635 0.5077652 0.4174260 0.4684238 0.4427289 0.4581712
71  0.4708412 0.5423596 0.4814307 0.5188125 0.4220484 0.4165515 0.4542457
72  0.4576712 0.5904397 0.4898954 0.4174288 0.4892255 0.5642603 0.4883681
73  0.5301410 0.5553042 0.5610118 0.4998749 0.4426938 0.4705363 0.4834633
74  0.4854081 0.5289004 0.4725922 0.5181087 0.5449106 0.4609356 0.4928329
75  0.4837046 0.5000558 0.5243229 0.5114847 0.4423947 0.5773659 0.5043402
76  0.4556426 0.5736703 0.4595623 0.4190413 0.4876304 0.4694947 0.4809855
77  0.4989191 0.5756328 0.4935905 0.4515873 0.4177547 0.5054115 0.5337443
78  0.5254986 0.4778276 0.5299086 0.4628826 0.4568737 0.4390582 0.5994700
79  0.4795791 0.4943641 0.5382589 0.5777456 0.5692001 0.5084243 0.5197520
80  0.4628576 0.5387704 0.5685155 0.5147301 0.4843854 0.4593617 0.5951249
81  0.4764729 0.4988384 0.5625614 0.4520581 0.5736637 0.5155244 0.5668459
82  0.4939387 0.4938123 0.5330845 0.4716717 0.5527459 0.4483794 0.5069606
83  0.4847076 0.4539687 0.5248500 0.5052239 0.4704517 0.5035908 0.5382946
84  0.5019902 0.4737273 0.4411068 0.5655493 0.4818116 0.4677957 0.4732504
85  0.4760196 0.4714917 0.5726520 0.5503430 0.5478378 0.5020216 0.5250522
86  0.4615140 0.5394289 0.5120679 0.4724521 0.5502634 0.4733738 0.4941512
87  0.4270182 0.4105156 0.5532226 0.5423536 0.4593134 0.4411160 0.5221174
88  0.4194913 0.5459266 0.4809034 0.4201801 0.4945596 0.4812411 0.5397339
89  0.5275949 0.4886017 0.4956367 0.4657934 0.4692685 0.5110128 0.4926081
90  0.4696024 0.5593421 0.4753714 0.4982930 0.4043293 0.4345146 0.5382541
91  0.4284231 0.5121191 0.5388115 0.4870148 0.4869558 0.4489895 0.5545318
92  0.5395554 0.4862743 0.4929058 0.5961606 0.5386875 0.4914467 0.4622773
93  0.5549157 0.5107597 0.4908956 0.4879580 0.4358742 0.4465267 0.4150356
94  0.5144044 0.4023270 0.5592091 0.4454231 0.5477417 0.5165038 0.5925984
95  0.5172275 0.5151094 0.4935183 0.5069864 0.4885368 0.5045862 0.4786229
96  0.5140433 0.4899466 0.4871784 0.5434628 0.4424767 0.4533278 0.4707125
97  0.4504157 0.5268906 0.5060748 0.5197814 0.5997936 0.4839456 0.5421424
98  0.4621731 0.5636110 0.5222723 0.4722654 0.5215109 0.4947994 0.5082993
99  0.5220798 0.5766765 0.5169117 0.5571598 0.5199541 0.4994298 0.4847911
100 0.4585713 0.4834391 0.5060720 0.4554124 0.4980970 0.5395148 0.5622732
101 0.5523925 0.5256151 0.5181894 0.5757850 0.4891320 0.5957619 0.4380782
102 0.5718596 0.4761757 0.5056090 0.5122011 0.4263804 0.5372637 0.4483152
103 0.4380091 0.4780498 0.5141105 0.4674825 0.4922602 0.5468583 0.5261569
104 0.4983325 0.5504129 0.5378543 0.4520279 0.4030980 0.5222516 0.5496049
105 0.4192190 0.5425646 0.5136266 0.4669647 0.5296178 0.5114621 0.4268196
106 0.5164382 0.5221385 0.4738247 0.5399585 0.4920354 0.5147719 0.5196055
107 0.4248815 0.4520719 0.5128239 0.5812461 0.4890621 0.4388428 0.4753832
108 0.4746534 0.4962147 0.5092640 0.4898863 0.5096798 0.4931762 0.5641563
109 0.4457792 0.5356201 0.5768021 0.5010006 0.4405419 0.4527376 0.4942350
110 0.4873115 0.5045705 0.4669214 0.5666332 0.4866507 0.5596590 0.5191193
111 0.5151784 0.5310091 0.4770987 0.4406451 0.4590364 0.4546978 0.5468204
112 0.4998368 0.5830944 0.5307788 0.4149578 0.4870779 0.4341731 0.4491065
113 0.5132291 0.4689855 0.4723736 0.5381351 0.4827463 0.4855934 0.5499769
114 0.5398323 0.5164900 0.4309138 0.5730438 0.4965079 0.5238756 0.5661590
115 0.4942851 0.4800404 0.4875890 0.4878203 0.5823228 0.4315112 0.4582076
116 0.5310438 0.4450376 0.5284584 0.4728497 0.4609402 0.5011445 0.4806595
117 0.5341939 0.5656906 0.4355484 0.4713345 0.4554249 0.4738654 0.5192970
118 0.5612406 0.5522075 0.5495638 0.4403594 0.4894069 0.5460498 0.4994068
119 0.4783852 0.5400800 0.4684832 0.4896131 0.5865494 0.4773851 0.5204366
120 0.5862763 0.4571942 0.5066028 0.5452865 0.5311075 0.4884851 0.5088267
121 0.5050056 0.5406599 0.4610722 0.5463167 0.5921883 0.4848543 0.5592275
122 0.5131642 0.5016899 0.5082961 0.5008959 0.4945486 0.5089766 0.4068148
123 0.5450366 0.4663093 0.5208267 0.5154733 0.5991644 0.5364921 0.4747856
124 0.4748631 0.4988469 0.5198378 0.5387898 0.5729984 0.5314563 0.5197113
125 0.4469216 0.4766256 0.5131908 0.4891288 0.5130968 0.4813177 0.4883728
126 0.4891940 0.5227368 0.5423756 0.5190233 0.4908072 0.5058491 0.4573446
127 0.4736908 0.4064858 0.5595914 0.4315797 0.5314574 0.5337033 0.5110132
128 0.5682574 0.4847035 0.4886584 0.4783471 0.4805412 0.5989798 0.5068074
129 0.4734550 0.4679405 0.4770027 0.4628410 0.5668804 0.5356659 0.5070324
130 0.4434184 0.5335626 0.4484969 0.5000316 0.5091007 0.5487951 0.4766596
131 0.4988926 0.4450849 0.4879590 0.5961149 0.4415979 0.4588192 0.5789493
132 0.4142125 0.4977519 0.5583690 0.5169600 0.4943085 0.4577297 0.4982953
133 0.4347187 0.5455880 0.4772394 0.5155651 0.5124271 0.5284518 0.5013475
134 0.4785097 0.4937962 0.4183657 0.4285337 0.5452778 0.4730744 0.5322212
135 0.5767380 0.5546860 0.4788728 0.5514627 0.4635181 0.4502487 0.4598169
136 0.4835092 0.4431574 0.4654776 0.5233303 0.5330848 0.4406305 0.5042572
137 0.5502049 0.4221648 0.4963437 0.4664401 0.5096398 0.5239889 0.4494270
138 0.5150556 0.4819566 0.5364427 0.4740071 0.4483570 0.4923669 0.5177302
139 0.5975329 0.4057299 0.5215747 0.4723209 0.4966431 0.4276997 0.4985654
140 0.5304636 0.4958042 0.4370164 0.4822785 0.4027736 0.5564347 0.4771552
141 0.5490850 0.5072268 0.4695654 0.5011551 0.4462439 0.4915007 0.4883024
142 0.5492464 0.5554199 0.5210250 0.4384739 0.4744363 0.5272116 0.4769655
143 0.5198611 0.4785457 0.4461748 0.5252954 0.5251514 0.4353916 0.4396463
144 0.5520921 0.5525946 0.4625342 0.4780019 0.5671594 0.5035279 0.4037588
145 0.4062244 0.4458720 0.4704177 0.5594633 0.4624215 0.4789775 0.4663038
146 0.4891574 0.4775047 0.5375713 0.4631785 0.4999462 0.5551297 0.5256227
147 0.4610544 0.4940821 0.5511931 0.4954764 0.5330808 0.4433800 0.4047992
148 0.5244393 0.4943885 0.4914826 0.4839689 0.5395312 0.4977046 0.5045033
149 0.4722629 0.4589855 0.4573179 0.5334200 0.5326029 0.4993997 0.5051315
150 0.4446664 0.5306226 0.5782736 0.4558091 0.5655710 0.5567737 0.4399119
151 0.5391757 0.5178922 0.4213061 0.5521821 0.4214817 0.5246982 0.5228159
152 0.4767014 0.5031567 0.4553164 0.5175593 0.5326562 0.5209964 0.5531946
153 0.5505548 0.5372822 0.5320003 0.4576041 0.4222804 0.5950944 0.4792992
154 0.4310762 0.4480244 0.5316603 0.4999458 0.5306545 0.4472882 0.5076639
155 0.5225044 0.4734115 0.4856736 0.4978557 0.4693586 0.4957239 0.4366157
156 0.5369550 0.4265197 0.4973226 0.4554083 0.4638918 0.5647058 0.5331309
157 0.4172295 0.4387708 0.5650868 0.5559543 0.4642927 0.4436380 0.4465406
158 0.5257181 0.5086123 0.5387965 0.5197963 0.5562031 0.5046952 0.5050445
159 0.4916571 0.4753450 0.5232321 0.5571051 0.4380654 0.5099357 0.5540334
160 0.5351603 0.4023772 0.4575197 0.4604755 0.5800175 0.4974154 0.5407242
161 0.4503869 0.4787374 0.4394629 0.5094770 0.4523163 0.5137711 0.5202119
162 0.4778899 0.5394120 0.4709567 0.4958951 0.4928022 0.4835007 0.4074090
163 0.5160662 0.4889272 0.4735322 0.5632933 0.4324215 0.4941718 0.4816251
164 0.5271521 0.4845436 0.5624846 0.5448294 0.4927353 0.5175450 0.4954820
165 0.4433057 0.4857526 0.5358247 0.4833751 0.5186279 0.5101231 0.4673673
166 0.4522260 0.5065919 0.4263218 0.5473004 0.4912759 0.4927846 0.5624700
167 0.4475333 0.5564901 0.4751693 0.4389262 0.4890508 0.4768684 0.4973117
168 0.4630274 0.5256092 0.4721232 0.4766100 0.5245902 0.5542105 0.4724266
169 0.4483529 0.5048730 0.5723996 0.4408303 0.5037124 0.5142757 0.4347475
170 0.5712568 0.4469971 0.4212828 0.4947113 0.4923878 0.4577483 0.4783907
171 0.5820137 0.5133644 0.4449898 0.4480064 0.5145576 0.4867592 0.5817038
172 0.5265205 0.4753148 0.4470572 0.5784354 0.5233498 0.4772347 0.4810450
173 0.4201894 0.5326726 0.4210547 0.5409264 0.5506633 0.4300470 0.5541746
174 0.4555075 0.5079552 0.4920784 0.5337721 0.4986581 0.4943296 0.4859486
175 0.4707959 0.4942774 0.4739441 0.5316687 0.5940140 0.5009793 0.4029949
176 0.4840857 0.4293775 0.4788507 0.4564396 0.4090215 0.4117785 0.4532202
177 0.4593246 0.4842501 0.5057708 0.5146825 0.5467973 0.5071622 0.4331999
178 0.5030637 0.4888424 0.5173241 0.5734683 0.5765541 0.4736953 0.5413084
179 0.5367432 0.5658595 0.4504825 0.4620423 0.5243374 0.4777627 0.4928971
180 0.4280689 0.4812110 0.5170654 0.5047684 0.4684136 0.5481246 0.4636106
181 0.4272277 0.4178497 0.5253562 0.5335413 0.4795681 0.5762976 0.5420291
182 0.4948402 0.4937009 0.5176630 0.5701246 0.4710801 0.4600035 0.4239069
183 0.5443240 0.4718691 0.4586854 0.5058974 0.5680024 0.5503496 0.5685387
184 0.4900681 0.4219173 0.5056092 0.5874839 0.4637913 0.4404423 0.5331822
185 0.4534174 0.4579428 0.4039767 0.5268296 0.5522557 0.5186793 0.5279596
186 0.4332879 0.4934681 0.5140795 0.5186465 0.4703295 0.5892489 0.5593769
187 0.5090074 0.4907714 0.5179285 0.4797318 0.5525080 0.4560821 0.5194432
188 0.4970490 0.4802437 0.5006615 0.4562521 0.4423155 0.5069093 0.4640050
189 0.5549464 0.5298600 0.4565214 0.4501803 0.5170869 0.5387366 0.4076726
190 0.4832408 0.4544259 0.4842322 0.4887847 0.5249879 0.4511448 0.5136462
191 0.4738746 0.4729645 0.4942919 0.4414844 0.5269439 0.5009039 0.5645852
192 0.4830788 0.4452112 0.4698682 0.5017159 0.5275103 0.4574502 0.4739800
193 0.5504592 0.4901900 0.4637178 0.4921908 0.4618658 0.5101533 0.5471465
194 0.4689384 0.4317655 0.4749996 0.5268902 0.5324635 0.5356755 0.5479769
195 0.4888018 0.4904129 0.4930721 0.4929599 0.5752016 0.5086271 0.5156352
196 0.5582618 0.4955743 0.5619824 0.5339063 0.5198766 0.4914408 0.5204931
197 0.5394571 0.5183119 0.4862698 0.4032040 0.5424162 0.5197346 0.4824956
198 0.4552973 0.5272490 0.4796316 0.4650919 0.5478192 0.5254711 0.5074260
199 0.4230293 0.5011292 0.5077456 0.4605961 0.4926673 0.4316986 0.5021456
200 0.4744605 0.4779901 0.4989432 0.5177917 0.5201048 0.5525801 0.4674855
201 0.4899333 0.4026285 0.4136010 0.5100701 0.4199747 0.5882549 0.4514423
202 0.5063288 0.4702672 0.4865840 0.4914604 0.4938724 0.4985346 0.5573005
203 0.5115938 0.4029910 0.4711358 0.5763060 0.5301536 0.5670259 0.5100856
204 0.5264607 0.4801508 0.5718383 0.4583224 0.5128955 0.4384027 0.4924547
205 0.4207749 0.5592074 0.4667632 0.4537563 0.4462554 0.5254271 0.5172376
206 0.5948681 0.5854669 0.4866769 0.4415220 0.4863727 0.5444163 0.5281045
207 0.5083090 0.5247614 0.4835033 0.4954689 0.4299623 0.4511519 0.4221814
208 0.4519203 0.5055880 0.4400155 0.5383331 0.4963612 0.4540110 0.4533995
209 0.4965487 0.4694319 0.5683332 0.5092797 0.5036533 0.5207172 0.5408279
210 0.5527015 0.4576740 0.4803538 0.4383443 0.4545538 0.4355247 0.5295881
211 0.5914439 0.5895645 0.4708374 0.5081388 0.5735917 0.5528917 0.4550183
212 0.5089770 0.4849706 0.5920822 0.4652610 0.4467189 0.5567672 0.4358877
213 0.5274953 0.5952248 0.5463273 0.4748429 0.4924318 0.4694882 0.4634859
214 0.4925378 0.5285483 0.4110840 0.5278039 0.5370224 0.4815974 0.4493959
215 0.4482513 0.4945665 0.4986030 0.5478183 0.4953968 0.5249867 0.5412608
216 0.5073628 0.5823908 0.4099713 0.5321968 0.4603824 0.5002453 0.5740876
217 0.5532733 0.4424656 0.4524362 0.4798283 0.4977949 0.5095145 0.4788408
218 0.5634849 0.4970348 0.4856021 0.5152429 0.4459913 0.5212558 0.5496073
219 0.4764425 0.4405470 0.5280774 0.4639259 0.4737221 0.4700428 0.5073208
220 0.5681007 0.5304700 0.5893791 0.4906178 0.5671993 0.4288535 0.5167947
221 0.5283515 0.4857716 0.4197030 0.4914832 0.4811162 0.4313136 0.4833986
222 0.4693174 0.5590586 0.5247313 0.5994637 0.5050062 0.5192763 0.4922043
223 0.4590561 0.5287688 0.4602038 0.4687364 0.4481340 0.5420470 0.5573216
224 0.5992209 0.5528632 0.4960585 0.4093927 0.5353620 0.4933061 0.4482872
225 0.5981759 0.4543241 0.5360175 0.4729664 0.4463225 0.5495941 0.4297941
226 0.5894891 0.4923661 0.4904017 0.4813706 0.5111290 0.5159307 0.5268368
227 0.4846921 0.5964257 0.5133008 0.4991868 0.5651466 0.4761529 0.4582763
228 0.5207157 0.4581922 0.5063842 0.5222734 0.5334200 0.4770810 0.4680079
229 0.4832124 0.4362467 0.5337861 0.5117126 0.5032806 0.5578490 0.5990406
230 0.4616161 0.4513865 0.4975404 0.5773368 0.4749942 0.4100672 0.4968732
231 0.5713216 0.5030820 0.4751025 0.4543395 0.5139019 0.5249855 0.4982383
232 0.5431989 0.5484206 0.5577743 0.5163090 0.4586576 0.5571402 0.4768237
233 0.5355035 0.5254912 0.4497868 0.5653891 0.4206200 0.5022261 0.4604878
234 0.5478708 0.4505009 0.5727991 0.4805788 0.5877573 0.5725540 0.5026594
235 0.5127427 0.4141984 0.5649338 0.4948443 0.4646918 0.5110576 0.5561431
236 0.4807877 0.4701430 0.4741201 0.5816846 0.4878731 0.5740924 0.5387937
237 0.5050371 0.5069325 0.4801515 0.5285025 0.5267569 0.4916964 0.5328219
238 0.5520069 0.4682553 0.5341775 0.5236736 0.4999414 0.5418317 0.5560807
239 0.4906534 0.5485552 0.5847073 0.5717761 0.4492713 0.5184416 0.5144272
240 0.4996351 0.5014299 0.4601430 0.5267466 0.4789817 0.5372928 0.5075395
241 0.4686295 0.4518893 0.4920864 0.5086040 0.4924507 0.4431439 0.4368976
242 0.4528644 0.4847650 0.5165365 0.5510299 0.5183036 0.4281775 0.5014038
243 0.5065282 0.5243004 0.5727702 0.5403992 0.5134599 0.4261560 0.5366753
244 0.4156835 0.5815746 0.5005480 0.4435224 0.5376482 0.4878727 0.5270093
245 0.5822499 0.4069684 0.4595356 0.4736433 0.5243402 0.4921203 0.4443856
246 0.5216216 0.5367673 0.5015239 0.4673213 0.5247713 0.4948594 0.5264682
247 0.5830226 0.4609531 0.5259689 0.4999324 0.4588011 0.4480799 0.4117248
          X29       X30
1   0.4862677 0.5716898
2   0.5643366 0.5494627
3   0.5265234 0.5380485
4   0.5570843 0.5783482
5   0.5197094 0.5069641
6   0.5460060 0.5064185
7   0.4717101 0.5850295
8   0.4749207 0.4421816
9   0.5337794 0.4833475
10  0.5438393 0.4933747
11  0.4955452 0.4815237
12  0.5065732 0.4577292
13  0.5809866 0.5553937
14  0.4838344 0.4523952
15  0.5307871 0.4817517
16  0.4583511 0.4502213
17  0.4675061 0.4341882
18  0.5400090 0.5531604
19  0.4613491 0.4489714
20  0.5020830 0.5118600
21  0.4656906 0.5019370
22  0.5941633 0.4745570
23  0.5010235 0.4864467
24  0.4607330 0.5040148
25  0.5210646 0.4456702
26  0.4833468 0.5701018
27  0.4611477 0.4756159
28  0.4939562 0.4630912
29  0.4296536 0.5085887
30  0.4724275 0.4764149
31  0.4813395 0.5365785
32  0.5043723 0.4041086
33  0.4672403 0.4303200
34  0.5266608 0.5260462
35  0.5502190 0.4900451
36  0.4337195 0.5696654
37  0.4716702 0.4405946
38  0.5112687 0.4581262
39  0.5232361 0.4917260
40  0.4466523 0.5033667
41  0.4540639 0.4304208
42  0.5005094 0.4508396
43  0.4662077 0.5187633
44  0.5610085 0.4977559
45  0.4930224 0.5355092
46  0.5413300 0.4123769
47  0.4528022 0.4647072
48  0.4599364 0.4793757
49  0.4643703 0.4255261
50  0.4629425 0.4754761
51  0.5050239 0.5072150
52  0.5211830 0.4247582
53  0.5246886 0.5610499
54  0.4763577 0.5074572
55  0.5452236 0.5556234
56  0.5183639 0.5154078
57  0.5290699 0.5809353
58  0.5812847 0.5349323
59  0.5572253 0.5048209
60  0.5329546 0.5546629
61  0.5601421 0.4765520
62  0.5593099 0.5269981
63  0.5854569 0.5751019
64  0.5243849 0.5207048
65  0.4226471 0.4140102
66  0.5372729 0.5125239
67  0.4974526 0.4780854
68  0.4069753 0.4747693
69  0.4938370 0.4729615
70  0.4866394 0.5198271
71  0.5368220 0.5540665
72  0.4887407 0.5252322
73  0.5585720 0.4941142
74  0.4102255 0.5326105
75  0.4457151 0.4987700
76  0.5871034 0.5179991
77  0.4749960 0.5644728
78  0.4261001 0.5386635
79  0.5132060 0.4045445
80  0.4672322 0.5815804
81  0.5223808 0.5126615
82  0.5089547 0.4830960
83  0.5206063 0.4856072
84  0.5791599 0.5419283
85  0.4954832 0.5193304
86  0.4335211 0.4957183
87  0.4465874 0.5017836
88  0.5498098 0.4050283
89  0.4922847 0.5008092
90  0.4966824 0.5181339
91  0.4046358 0.5096864
92  0.4987664 0.5731081
93  0.5711975 0.5259090
94  0.5532616 0.5132090
95  0.4181851 0.4333559
96  0.4824073 0.4951343
97  0.5247091 0.5342803
98  0.5043086 0.5118534
99  0.4652667 0.4731674
100 0.5574372 0.4867845
101 0.5315213 0.5128618
102 0.5163516 0.4743823
103 0.5031318 0.5132704
104 0.5067398 0.4424233
105 0.4044656 0.5205952
106 0.5220246 0.4737463
107 0.5358219 0.5352992
108 0.4682030 0.4483097
109 0.4292721 0.5082349
110 0.5797326 0.4721142
111 0.5357580 0.4853502
112 0.4654027 0.5890901
113 0.4950045 0.4693665
114 0.4815473 0.5320150
115 0.5563327 0.4910613
116 0.4999890 0.5499632
117 0.4319772 0.5087308
118 0.4978394 0.4658316
119 0.5427136 0.4601338
120 0.5695104 0.5094584
121 0.4759390 0.5031464
122 0.5118263 0.5389841
123 0.5232955 0.5957042
124 0.4889609 0.5817148
125 0.5828327 0.5629682
126 0.4248145 0.5339971
127 0.4731200 0.4120677
128 0.5600330 0.5721149
129 0.4295628 0.5130006
130 0.4298252 0.4886126
131 0.5230601 0.4964264
132 0.5361583 0.5098616
133 0.4977575 0.4002551
134 0.4529832 0.5187101
135 0.4848948 0.4521546
136 0.5816482 0.5147413
137 0.4357110 0.5274562
138 0.5478520 0.4414013
139 0.5281386 0.5300591
140 0.5613433 0.5129717
141 0.4665333 0.4807609
142 0.5347202 0.4998396
143 0.4829037 0.5036553
144 0.5577108 0.5118237
145 0.4896953 0.5222686
146 0.5368243 0.5076824
147 0.5157109 0.5853368
148 0.5073150 0.5346140
149 0.5423308 0.4822311
150 0.4934875 0.4510878
151 0.4846634 0.5478036
152 0.5107612 0.4728673
153 0.4352145 0.5491670
154 0.5589482 0.4898727
155 0.4724461 0.5485192
156 0.5937421 0.5277676
157 0.4997459 0.4678742
158 0.5213707 0.5080604
159 0.4974307 0.4903891
160 0.4606902 0.4950235
161 0.4612750 0.5292794
162 0.5614975 0.5634882
163 0.4604166 0.4988013
164 0.4251926 0.5409355
165 0.5243052 0.4677597
166 0.5086962 0.5110576
167 0.4908472 0.4470121
168 0.4415942 0.5720325
169 0.5195348 0.5606926
170 0.5775128 0.5271093
171 0.5669663 0.4499956
172 0.5290184 0.5353381
173 0.5165325 0.4618725
174 0.5387846 0.4995045
175 0.4656785 0.5159226
176 0.5567077 0.5043012
177 0.5784772 0.4957104
178 0.5359922 0.5974278
179 0.4810590 0.5390056
180 0.5152147 0.4921770
181 0.4424519 0.5641120
182 0.5884472 0.5075548
183 0.5637829 0.4959409
184 0.5034954 0.5166437
185 0.4639454 0.4730166
186 0.4802252 0.5474986
187 0.5649515 0.4475708
188 0.5756711 0.4536859
189 0.5055262 0.4702040
190 0.4190516 0.5836445
191 0.5018639 0.4859983
192 0.4515421 0.5981639
193 0.5109077 0.4906682
194 0.4841579 0.5388707
195 0.4834507 0.4684030
196 0.5229890 0.4440858
197 0.5222374 0.5328521
198 0.5188638 0.4818378
199 0.5629765 0.5355802
200 0.5002958 0.4509449
201 0.5221845 0.5889781
202 0.4555060 0.4815071
203 0.4712637 0.5296303
204 0.4573055 0.5449608
205 0.4892729 0.5764124
206 0.5482596 0.4994098
207 0.5871329 0.4409245
208 0.5734060 0.5095932
209 0.4858738 0.5491125
210 0.5619132 0.5740341
211 0.4855785 0.5274641
212 0.5230253 0.5826183
213 0.4418641 0.5209504
214 0.5611808 0.4703506
215 0.5015235 0.5644305
216 0.5846366 0.5008261
217 0.4416349 0.5056778
218 0.4916477 0.4342597
219 0.5828891 0.5525767
220 0.4783306 0.5113609
221 0.5085949 0.4254012
222 0.4856380 0.4615042
223 0.5789582 0.4611618
224 0.5203359 0.4920487
225 0.4944175 0.4173615
226 0.4999649 0.4649396
227 0.4936063 0.4814820
228 0.5215271 0.4535542
229 0.4638218 0.4214991
230 0.4800745 0.5787312
231 0.4504532 0.5267877
232 0.4852411 0.4403831
233 0.4915995 0.4764816
234 0.5373245 0.4992732
235 0.5077572 0.5519154
236 0.4947986 0.5539556
237 0.5581176 0.5450992
238 0.4924002 0.5139594
239 0.5678656 0.5304808
240 0.4216611 0.5011149
241 0.5137434 0.5083537
242 0.5371345 0.5227472
243 0.4887428 0.5049493
244 0.4025393 0.4849103
245 0.4956158 0.5380606
246 0.4129493 0.4330687
247 0.4359762 0.4629777
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL

comparison_samples <- 10000
samples <- 1000
factors <- 8

comparison_sample <- data.frame(replicate(factors, runif(comparison_samples)))
names(comparison_sample) <- paste("X", 1:factors, sep = "")

extra_experiments <- 1

runs <- 100

model_formula <- formula(paste("~ ", paste(names(comparison_sample), collapse = "+")))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$absDeff <- eval.design(model_formula, design, X = bind_rows(comparison_sample, design))$Deffbound
    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(lin_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$absDeff <- eval.design(model_formula, design, X = bind_rows(comparison_sample, design))$Deffbound
    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^2)+ ",
#                                     sep = ""),
#                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design

    design$absDeff <- eval.design(model_formula, design, X = bind_rows(comparison_sample, design))$Deffbound
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^3)+",
#                                     sep = ""),
#                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design

    design$absDeff <- eval.design(model_formula, design, X = bind_rows(comparison_sample, design))$Deffbound
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^2)+ ",
#                                     sep = ""),
#                               " ^2)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(quad_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, biased_sample, nTrials = (2 * factors) + extra_experiments)
    design <- output$design

    design$absDeff <- eval.design(model_formula, design, X = bind_rows(comparison_sample, design))$Deffbound
    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(names(uniform_sample),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     names(uniform_sample),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

#model_formula <- formula(paste("~ ",
#                               paste("I(",
#                                     names(uniform_sample),
#                                     collapse = "^3)+",
#                                     sep = ""),
#                               "^3)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(cube_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    output <- optFederov(model_formula, biased_sample, nTrials = (3 * factors) + extra_experiments)
    design <- output$design

    design$absDeff <- eval.design(model_formula, design, X = bind_rows(comparison_sample, design))$Deffbound
    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}
#+END_SRC

#+RESULTS:
****** Sample Sanity Test
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

#for (i in 1:1) {
#    sample <- NULL
#    current_size <- 0
#
#    while (current_size < samples) {
#        new_sample <- data.frame(replicate(factors, runif(samples)))
#        names(new_sample) <- paste("X", 1:factors, sep = "")
#
#        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.25 & . <= 0.35) | (. >= 0.65 & . <= 0.75)))
#
#        if (is.null(sample)) {
#            sample <- new_sample
#        } else {
#            sample <- bind_rows(sample, new_sample)
#        }
#
#        current_size <- nrow(sample)
#    }
#
#    sample <- sample[1:samples, ]
#}

ggplot(biased_sample) + geom_point(aes(x = X1, y = X4))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureYypU31.png]]
***** Plots
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv': No such file or directory

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = X1, y = ..count.., fill = id)) +
    facet_wrap(model ~ ., ncol = 2, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureezGAZf.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = absDeff)) +
       facet_wrap(model ~ ., ncol = 2) +
       ylim(c(0,1)) +
       scale_x_discrete(name = "Jittered Experiments") +
       geom_jitter(alpha = 0.5, height = 0.0) +
       #coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureVohpxM.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_10k_comparison_1000_samples_4_factors.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
**** Screening
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(FrF2)
library(AlgDesign)

design <- pb(8)

m <- as.matrix(as.data.frame(model.matrix(~., data.frame(replicate(7,
                                                                   sample(x = c(-1, 1),
                                                                          size = 30,
                                                                          replace = T))))))

det((t(m) %*% m) / 30)
#+END_SRC

#+RESULTS:
:
: Warning message:
: In pb(8) :
:   Plackett-Burman designs in 8 runs coincide with regular fractional factorials.
:           For screening more than four factors, you may want to consider increasing the number of runs to 12.
:           Make sure to take the alias structure into account for interpretation!
:
: [1] 0.3720999
*** [2018-12-06 Thu]
**** DoE Example for Adrien
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- expand.grid(seq(0, 100, 10), seq(0, 100, 10))

design <- optFederov(~ ., data, nTrials = 4)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
design$design
#+END_SRC

#+RESULTS:
:     Var1 Var2
: 1      0    0
: 11   100    0
: 111    0  100
: 121  100  100

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)

y <- function(x1, x2) {
    return(x1 + x2)
}


design$design <- design$design %>% mutate(y_var = y(Var1, Var2))
design$design
#+END_SRC

#+RESULTS:
:
:   Var1 Var2 y_var
: 1    0    0     0
: 2  100    0   100
: 3    0  100   100
: 4  100  100   200

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
summary.aov(aov(y_var ~ ., design$design))
#+END_SRC

#+RESULTS:
:             Df Sum Sq Mean Sq   F value Pr(>F)
: Var1         1  10000   10000 1.238e+31 <2e-16 ***
: Var2         1  10000   10000 1.238e+31 <2e-16 ***
: Residuals    1      0       0
: ---
: codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
**** /D Criterion/ for a Linear Model with Numerical Factors
We want to better understand the designs generated for set of numerical factors,
and assuming a linear model. I will first establish that Plackett-Burman designs
and orthogonal designs have a value of $1.0$ for the /D criterion/. I decided to
focus on the D criterion because it is simpler to compute that the D-Efficiency,
and also because it does not need a candidate set to be computed.

I'll look at the designs generated by =runif= and by my /ad hoc/ CDF for linear
models. Later, I will look at the designs found by =optFederov= starting from
different candidate sets. We expect this will give us insights that can be applied to the
quadratic and cubic cases.

First, we look at Plackett-Burman designs to be sure our determinant
computations are correct:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
options(warn = -1)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(FrF2)
library(dplyr)

convert_pb_dataframe <- function(design) {
    design <- design %>% as.matrix() %>%
                         as.data.frame() %>%
                         mutate_all(as.character) %>%
                         mutate_all(as.numeric)
    return(design)
}

compute_D <- function(formula, data) {
    model_matrix <- as.matrix(as.data.frame(model.matrix(formula, data)))
    return(det((t(model_matrix) %*% model_matrix) / nrow(data)) ^ (1 / ncol(model_matrix)))
}

samples <- 8
factors <- samples - 1

plackett_burman <- convert_pb_dataframe(pb(samples))
names(plackett_burman) <- paste("X", 1:factors, sep = "")

formula <- formula(paste("~ ", paste("X", 1:factors, sep = "", collapse = " + ")))

plackett_burman$D <- compute_D(formula, plackett_burman)
plackett_burman$ad_D <- eval.design(formula, plackett_burman)$determinant

plackett_burman
#+END_SRC

#+RESULTS:
#+begin_example

  X1 X2 X3 X4 X5 X6 X7 D ad_D
1  1  1 -1  1 -1 -1  1 1    1
2  1 -1 -1  1  1  1 -1 1    1
3  1 -1  1 -1 -1  1  1 1    1
4 -1 -1 -1 -1 -1 -1 -1 1    1
5  1  1  1 -1  1 -1 -1 1    1
6 -1 -1  1  1  1 -1  1 1    1
7 -1  1 -1 -1  1  1  1 1    1
8 -1  1  1  1 -1  1 -1 1    1
#+end_example

I implemented my own function to compute the D criterion to be sure that I got
the underlying formula right. Now we can see what happens in other
circumstances, for example when we change one or more lines from a PB design.
First I'll convert the design to a data frame and get rid of the factor
encoding. I do that because I want to be able to generate simple numerical
vectors later.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
plackett_burman <- convert_pb_dataframe(pb(samples))
names(plackett_burman) <- paste("X", 1:factors, sep = "")

formula <- formula(paste("~ ", paste("X", 1:factors, sep = "", collapse = " + ")))

str(plackett_burman)

compute_D(formula, plackett_burman)
eval.design(formula, plackett_burman)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

'data.frame':	8 obs. of  7 variables:
 $ X1: num  1 -1 -1 -1 1 1 1 -1
 $ X2: num  -1 -1 1 -1 1 1 -1 1
 $ X3: num  1 1 1 -1 1 -1 -1 -1
 $ X4: num  -1 1 1 -1 -1 1 1 -1
 $ X5: num  -1 1 -1 -1 1 -1 1 1
 $ X6: num  1 -1 1 -1 -1 -1 1 1
 $ X7: num  1 1 -1 -1 -1 1 -1 1

[1] 1

[1] 1
#+end_example

Now we swap a random line for a line generated by =runif=. We see that a single
swapped line changes the D criterion of the design.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
new_line <- sample(c(-1, 1), factors, replace = TRUE)
new_line

plackett_burman
plackett_burman[sample(1:nrow(plackett_burman), 1), ] <- new_line
plackett_burman

compute_D(formula, plackett_burman)
eval.design(formula, plackett_burman)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

[1]  1 -1 -1 -1  1  1  1

  X1 X2 X3 X4 X5 X6 X7
1  1 -1  1 -1 -1  1  1
2 -1 -1  1  1  1 -1  1
3 -1  1  1  1 -1  1 -1
4 -1 -1 -1 -1 -1 -1 -1
5  1  1  1 -1  1 -1 -1
6  1  1 -1  1 -1 -1  1
7  1 -1 -1  1  1  1 -1
8 -1  1 -1 -1  1  1  1

  X1 X2 X3 X4 X5 X6 X7
1  1 -1  1 -1 -1  1  1
2 -1 -1  1  1  1 -1  1
3 -1  1  1  1 -1  1 -1
4 -1 -1 -1 -1 -1 -1 -1
5  1  1  1 -1  1 -1 -1
6  1  1 -1  1 -1 -1  1
7  1 -1 -1 -1  1  1  1
8 -1  1 -1 -1  1  1  1

[1] 0.8408964

[1] 0.8408964
#+end_example

We can also look at the de D criterion for designs generated at random:

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
samples <- 10
factors <- 7

design <- data.frame(replicate(factors, sample(c(-1, 1),
                                               samples,
                                               replace = TRUE)))

design
compute_D(formula, design)
eval.design(formula, design)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

   X1 X2 X3 X4 X5 X6 X7
1  -1  1 -1  1 -1 -1 -1
2  -1  1  1 -1 -1  1 -1
3  -1 -1 -1 -1 -1 -1  1
4   1  1  1 -1  1  1  1
5  -1  1  1  1  1  1  1
6   1 -1  1 -1 -1 -1 -1
7  -1 -1  1  1  1 -1  1
8   1  1  1  1 -1  1  1
9  -1  1 -1 -1 -1 -1 -1
10  1  1 -1  1 -1  1  1

[1] 0.6010791

[1] 0.6010791
#+end_example

We now change to smaller number of factors, to explore orthogonality and other
properties. Let's start with 2 numerical factors. With this experiment I
understood that we must encode numerical factors properly, so that they fit in
the $[-1, 1]$ interval. If we don't, the D criterion will not be between $[0,
1]$. In the cases I've looked at before, this is why the D values were so far
from 1. I still have to understand better how to adapt this for other kinds of
factors and quadratic and cubic model terms. I do not believe the encoding
affects the relationships between the D criterions of designs, only the
magnitude of the D criterion.

We will then need to encode those factors using the =rsm= package, so that we
can move the actual parameter intervals, in this case $[0, 1]$, to the intervals
accepted by =eval.design= and =model.matrix=, which is $[-1, 1]$.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
samples <- 4
factors <- 2

design_codings <- list()

for (i in 1:factors) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(data.frame(replicate(factors, runif(samples))),
                     formulas = design_codings)

formula <- formula(paste("~ ", paste(names(design), sep = "", collapse = " + ")))

design

compute_D(formula, design)
eval.design(formula, design)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

         X1        X2
1 0.5084806 0.6347440
2 0.6514318 0.7666411
3 0.6340301 0.9113029
4 0.2916568 0.6242384

Data are stored in coded form using these coding formulas ...
x1 ~ (X1 - 0.5)/0.5
x2 ~ (X2 - 0.5)/0.5

[1] 0.1242581

[1] 0.1242581
#+end_example

We can look at the points in the generated design:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureW0DXOS.png]]

Now that I know the D criterion is properly scaled, I will attempt to select the
experiments that have D closest to $1.0$ by picking orthogonal rows. I will also
compute the D criterion for the decoded design, that is, a design whose
numerical factors are not in the interval $[-1, 1]$, but on the original $[0,
1]$ interval. We see that this ideal design has a much lower D value when
decoded. The results I've shown before for numerical factors where obtained
using only the decoded design matrices.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(rsm)

design <- data.frame(X1 = c(0.0, 0.0, 1.0, 1.0),
                     X2 = c(0.0, 1.0, 1.0, 0.0))

design

design_codings <- list()

for (i in 1:ncol(design)) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(design, formulas = design_codings)

formula <- formula(paste("~ ", paste(names(design), sep = "", collapse = " + ")))

compute_D(formula, design)
eval.design(formula, design)$determinant

decoded_formula <- formula(paste("~ ", paste(names(decode.data(design)), sep = "", collapse = " + ")))

compute_D(decoded_formula, decode.data(design))
eval.design(decoded_formula, decode.data(design))$determinant
#+END_SRC

#+RESULTS:
#+begin_example

  X1 X2
1  0  0
2  0  1
3  1  1
4  1  0

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

[1] 1

[1] 1

[1] 0.3968503

[1] 0.3968503
#+end_example

Looking at the design, we get:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figure2YXE3C.png]]

***** Generating Designs
I will now look at the D criterion for designs generated using =runif= and the
custom CDF I created, which looks like this:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
options(warn = 0)
#+END_SRC

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
ggplot(lin_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureHUJ2Vx.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 4
samples <- factors + 2

data <- NULL

runs <- 300

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ", paste(names(design), sep = "", collapse = " + ")))

    decoded_formula <- formula(paste("~ ", paste(names(decode.data(design)), sep = "", collapse = " + ")))

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant,
                                       eval.design(decoded_formula, decode.data(design))$determinant,
                                       eval.design(decoded_formula, decode.data(biased_design))$determinant),
                                 names = c("runif", "biased", "runif_decoded", "biased_decoded"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

The next figure shows 4 experiments that generated 1000 samples of designs of
size 4, for 2 numerical parameters. The /biased/ experiment uses my CDF and the
/runif/ experiment uses =runif=. The /decoded/ experiments correspond to
computing the D criterion for parameters in the $[0, 1]$ interval, while the
other experiments use parameters in the $[-1, 1]$ interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureFueQ8n.png]]

We see that the biased CDF avoids a region of intermediary values of D, but
still samples from a bad region, although it looks like there are more samples
for higher values of D. I believe it could be possible to avoid this worse
region by dinamically selecting CDFs based on values of previous lines in the
design.

The next Figure shows that, for this case at least, =optFederov= can find
designs with very good D. An intial candidate set of 1000 experiments for 2
numerical factors was given to =optFederov=, which ran in this initial set and
found the design whose D value is reported below. This experiment was also
repeated 1000 times. We see that =optFederov= always finds designs with D close
to $1.0$. It was interesting to see that the best value from the biased sample
is also close to $1.0$, with the benefit of not needing to run the
column-swapping algorithm from Federov's method.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 4
samples <- factors + 2
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(decoded_formula, decode.data(federov_design))$determinant),
                                 names = c("federov", "federov_decoded"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureNWwVao.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 32
samples <- factors + 7

federov_multiplier <- ceiling(1000 / samples)
federov_samples <- samples * federov_multiplier

comparison_data <- NULL

runs <- 30

for (i in 1:runs) {
    if (i %% 5 == 0) {
        print(i)
    }

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    formula <- formula(paste("~ ", paste(names(federov_sample),
                                         sep = "",
                                         collapse = " + ")))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    best_sampled_D <- -1
    best_biased_design <- NULL

    for (j in 1:federov_multiplier) {
        biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                     sample_factor(lin_cdf,
                                                                                   samples))))

        sampled_D <- eval.design(formula, biased_design)$determinant

        if (sampled_D > best_sampled_D) {
            best_sampled_D <- sampled_D
            best_biased_design <- biased_design
        }
    }

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       best_sampled_D),
                                 names = c("federov", "biased"),
                                 id = i)

    if (is.null(comparison_data)) {
        comparison_data <- new_experiment
    } else {
        comparison_data <- bind_rows(comparison_data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: [1] 5
: [1] 10
: [1] 15
: [1] 20
: [1] 25
: [1] 30

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
write.csv(comparison_data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_1000_repetitions_3_factors.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

Motivated by that, I ran 1000 repetitions of an experiment where I sampled 250
designs with 4 experiments, resulting in 1000 samples per iteration, from the
biased CDF and picked the design with the best D criterion. I compared this with
the case where I ran =optFederov= for 1000 times on an initial candidate set of
size 1000.

The Figure below shows the comparison of the two methods, and shows that
picking the best design sampled from the biased distribution seems close to
the results of =optFederov= for this size of design and amount of parameters.

What I intend to do next is to look at this behaviour for higher numbers of
parameters and more experiments. I will also start to apply this encoding method
to numerical parameters in the case where we use quadratic and cubic models.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
comparison_data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_1000_repetitions_2_factors.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(comparison_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figure71PmUj.png]]

**** Experiments with Sampling and =optFederov=, Using the Right Encoding
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Load Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL
data <- read.csv("dopt_sampling_tests/federov_results_biased_1000_samples_1_factor.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
***** Generate Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
lin_data <- factor_cdf(linear_function, name = "Linear Factors")

cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- NULL

samples <- 300
factors <- 16

extra_experiments <- 4

uniform_sample <- data.frame(replicate(factors, runif(samples)))
names(uniform_sample) <- paste("X", 1:factors, sep = "")

runs <- 10

model_formula <- formula(paste("~ ",
                               paste("x", 1:factors, sep = "", collapse = " + "),
                               sep = ""))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- as.data.frame(get_runif_coded_design(uniform_sample))

    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples / 10)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    normal_sample <- data.frame(replicate(factors, rnorm((9 * samples) / 10, mean = 0.5, sd = 0.02)))
    names(normal_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- bind_rows(uniform_sample, normal_sample)

    uniform_sample <- as.data.frame(get_runif_coded_design(uniform_sample))

    output <- optFederov(model_formula, uniform_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_normal"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(lin_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    biased_sample <- as.data.frame(get_runif_coded_design(biased_sample))

    output <- optFederov(model_formula, biased_sample, nTrials = factors + extra_experiments)
    design <- output$design

    design$model <- "linear_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(paste("x", 1:factors, sep = ""),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     paste("x", 1:factors, sep = ""),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- as.data.frame(get_runif_coded_design(uniform_sample))

    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 2) + extra_experiments)
    design <- output$design
    design$model <- "quadratic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(paste("x", 1:factors, sep = ""),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     paste("x", 1:factors, sep = ""),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     paste("x", 1:factors, sep = ""),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- as.data.frame(get_runif_coded_design(uniform_sample))

    output <- optFederov(model_formula, uniform_sample, nTrials = (factors * 3) + extra_experiments)
    design <- output$design
    design$model <- "cubic_unif"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D
    data <- bind_rows(data, design)
}

model_formula <- formula(paste("~ ",
                               paste(paste(paste("x", 1:factors, sep = ""),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     paste("x", 1:factors, sep = ""),
                                     collapse = "^2)+ ",
                                     sep = ""),
                               " ^2)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(quad_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    biased_sample <- as.data.frame(get_runif_coded_design(biased_sample))

    output <- optFederov(model_formula, biased_sample, nTrials = (2 * factors) + extra_experiments)
    design <- output$design

    design$model <- "quadratic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples / 10)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    normal_sample <- data.frame(replicate(factors, rnorm((9 * samples) / 10, mean = 0.5, sd = 0.02)))
    names(normal_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- bind_rows(uniform_sample, normal_sample)

    uniform_sample <- as.data.frame(get_runif_coded_design(uniform_sample))

    output <- optFederov(model_formula, biased_sample, nTrials = (2 * factors) + extra_experiments)
    design <- output$design

    design$model <- "quadratic_normal"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

model_formula <- formula(paste("~ ",
                               paste(paste(paste("x", 1:factors, sep = ""),
                                           collapse = "+"),
                                     "+"),
                               paste("I(",
                                     paste("x", 1:factors, sep = ""),
                                     collapse = "^2)+",
                                     sep = ""),
                               "^2)+",
                               paste("I(",
                                     paste("x", 1:factors, sep = ""),
                                     collapse = "^3)+",
                                     sep = ""),
                               "^3)"))

for (i in 1:runs) {
    biased_sample <- data.frame(replicate(factors, sample_factor(cube_cdf, samples)))
    names(biased_sample) <- paste("X", 1:factors, sep = "")

    biased_sample <- as.data.frame(get_runif_coded_design(biased_sample))

    output <- optFederov(model_formula, biased_sample, nTrials = (3 * factors) + extra_experiments)
    design <- output$design

    design$model <- "cubic_biased"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}

for (i in 1:runs) {
    uniform_sample <- data.frame(replicate(factors, runif(samples / 10)))
    names(uniform_sample) <- paste("X", 1:factors, sep = "")

    normal_sample <- data.frame(replicate(factors, rnorm((9 * samples) / 10, mean = 0.5, sd = 0.02)))
    names(normal_sample) <- paste("X", 1:factors, sep = "")

    uniform_sample <- bind_rows(uniform_sample, normal_sample)

    uniform_sample <- as.data.frame(get_runif_coded_design(uniform_sample))

    output <- optFederov(model_formula, biased_sample, nTrials = (3 * factors) + extra_experiments)
    design <- output$design

    design$model <- "cubic_normal"
    design$id <- i
    design$deff <- output$Dea
    design$d<- output$D

    if (is.null(data)) {
        data <- design
    } else {
        data <- bind_rows(data, design)
    }
}
#+END_SRC

#+RESULTS:
****** Sample Sanity Test
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)

#for (i in 1:1) {
#    sample <- NULL
#    current_size <- 0
#
#    while (current_size < samples) {
#        new_sample <- data.frame(replicate(factors, runif(samples)))
#        names(new_sample) <- paste("X", 1:factors, sep = "")
#
#        new_sample <- filter_all(new_sample, all_vars(. <= 0.1 | . >= 0.9 | (. >= 0.25 & . <= 0.35) | (. >= 0.65 & . <= 0.75)))
#
#        if (is.null(sample)) {
#            sample <- new_sample
#        } else {
#            sample <- bind_rows(sample, new_sample)
#        }
#
#        current_size <- nrow(sample)
#    }
#
#    sample <- sample[1:samples, ]
#}

ggplot(decode.data(uniform_sample)) + geom_point(aes(x = X1, y = X2))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurehkxiBZ.png]]
***** Plots
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv", header = TRUE, sep = ",")
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/federov_results_biased_1200_samples_15_factors_rnorm.csv': No such file or directory

The Figure below shows the distribution of parameter values selected by 100
repetitions of =optFederov= for a model with a single linear term, corresponding
to a single numerical factor, using 3 different starting samples of size 1000.
The =linear_biased_region= sample was taken uniformly in a region around the
"extremes" of factor levels, the =linear_extremes= sample contains only the
"extreme" factor levels, and the =linear_unif= sample was taken uniformly in the
interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = x1, y = ..count.., fill = id)) +
    facet_wrap(model ~ ., ncol = 3, scale = "free_y") +
    #facet_wrap(model ~ ., ncol = 2) +
    geom_histogram(bins = 120) +
    theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureMK0pe1.png]]

The next Figure shows the /D-Efficiency/ values obtained for the 3 starting
samples, over the 100 repetitions.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
plot_data <- data[!duplicated(data[, c("id", "model")]), ]
ggplot(plot_data, aes(x = 1, y = d)) +
       facet_wrap(model ~ ., ncol = 3) +
       ylim(c(0,1)) +
       scale_x_discrete(name = "Jittered Experiments") +
       geom_jitter(alpha = 0.5, height = 0.0) +
       #coord_flip() +
       #geom_point(alpha = 0.5) +
       theme_bw(base_size = 18) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurefvOWr3.png]]

***** Save Latest Data
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/federov_results_1000_samples_16_factors_normal.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
*** [2018-12-10 Mon]
**** /D Criterion/ for a Quadratic Model with Numerical Factors
We want to better understand the designs generated for set of numerical factors,
and assuming a linear model. I will first establish that Plackett-Burman designs
and orthogonal designs have a value of $1.0$ for the /D criterion/. I decided to
focus on the D criterion because it is simpler to compute that the D-Efficiency,
and also because it does not need a candidate set to be computed.

I'll look at the designs generated by =runif= and by my /ad hoc/ CDF for quadratic
models. Later, I will look at the designs found by =optFederov= starting from
different candidate sets. We expect this will give us insights that can be
applied to the cubic case.

We will generate and encode small numbers of factors using the =rsm= package, so
that we can move the actual parameter intervals, in this case $[0, 1]$, to the
intervals accepted by =eval.design= and =model.matrix=, which is $[-1, 1]$.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
samples <- 8
factors <- 2

design_codings <- list()

for (i in 1:factors) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(data.frame(replicate(factors, runif(samples))),
                     formulas = design_codings)

formula <- formula(paste("~ ",
                         paste(names(design),
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^2)",
                               sep = "",
                               collapse = " + "),
                         sep = ""))

design

eval.design(formula, design)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

          X1        X2
1 0.91601507 0.3434803
2 0.11988085 0.2654639
3 0.06312076 0.1846887
4 0.99134216 0.5806919
5 0.18524689 0.9091160
6 0.37951788 0.2755051
7 0.38962784 0.7707773
8 0.84681740 0.1817112

Data are stored in coded form using these coding formulas ...
x1 ~ (X1 - 0.5)/0.5
x2 ~ (X2 - 0.5)/0.5

[1] 0.1865226
#+end_example

We can look at the points in the generated design:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureDffI9J.png]]

Now that I know the D criterion is properly scaled, I will attempt to select the
experiments that have D closest to $1.0$ by picking orthogonal rows. I will also
compute the D criterion for the decoded design, that is, a design whose
numerical factors are not in the interval $[-1, 1]$, but on the original $[0,
1]$ interval. We see that this ideal design has a much lower D value when
decoded. The results I've shown before for numerical factors where obtained
using only the decoded design matrices.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(rsm)

design <- data.frame(X1 = c(0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5),
                     X2 = c(0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5))

design

design_codings <- list()

for (i in 1:ncol(design)) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(design, formulas = design_codings)

formula <- formula(paste("~ ",
                         paste(names(design),
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^2)",
                               sep = "",
                               collapse = " + "),
                         sep = ""))

eval.design(formula, design)$determinant

decoded_formula <- formula(paste("~ ",
                                 paste(names(decode.data(design)),
                                       sep = "",
                                       collapse = " + "),
                                 " + ",
                                 paste("I(",
                                       names(decode.data(design)),
                                       "^2)",
                                       sep = "",
                                       collapse = " + "),
                                 sep = ""))

eval.design(decoded_formula, decode.data(design))$determinant
#+END_SRC

#+RESULTS:
#+begin_example

   X1  X2
1 0.0 0.0
2 0.0 1.0
3 1.0 1.0
4 1.0 0.0
5 0.5 1.0
6 1.0 0.5
7 0.5 0.5

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

[1] 0.427508

[1] 0.08099761
#+end_example

Looking at the design, we get:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurenArsDh.png]]

***** Generating Designs
I will now look at the D criterion for designs generated using =runif= and the
custom CDF I created, which looks like this:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
options(warn = 0)
#+END_SRC

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
ggplot(quad_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureLFnCUx.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 2
samples <- (2 * factors) + 2

data <- NULL

runs <- 1000

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

    decoded_formula <- formula(paste("~ ",
                                    paste(names(decode.data(design)),
                                          sep = "",
                                          collapse = " + "),
                                    " + ",
                                    paste("I(",
                                          names(decode.data(design)),
                                          "^2)",
                                          sep = "",
                                          collapse = " + "),
                                    sep = ""))

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant,
                                       eval.design(decoded_formula, decode.data(design))$determinant,
                                       eval.design(decoded_formula, decode.data(biased_design))$determinant),
                                 names = c("runif", "biased", "runif_decoded", "biased_decoded"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

The next figure shows 4 experiments that generated 1000 samples of designs of
size 4, for 2 numerical parameters. The /biased/ experiment uses my CDF and the
/runif/ experiment uses =runif=. The /decoded/ experiments correspond to
computing the D criterion for parameters in the $[0, 1]$ interval, while the
other experiments use parameters in the $[-1, 1]$ interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figuren1AGo0.png]]

We see that the biased CDF avoids a region of intermediary values of D, but
still samples from a bad region, although it looks like there are more samples
for higher values of D. I believe it could be possible to avoid this worse
region by dinamically selecting CDFs based on values of previous lines in the
design.

The next Figure shows that, for this case at least, =optFederov= can find
designs with very good D. An intial candidate set of 1000 experiments for 2
numerical factors was given to =optFederov=, which ran in this initial set and
found the design whose D value is reported below. This experiment was also
repeated 1000 times. We see that =optFederov= always finds designs with D close
to $1.0$. It was interesting to see that the best value from the biased sample
is also close to $1.0$, with the benefit of not needing to run the
column-swapping algorithm from Federov's method.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 2
samples <- (2 * factors) + 2
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(decoded_formula, decode.data(federov_design))$determinant),
                                 names = c("federov", "federov_decoded"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figure2lbEiZ.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 32
samples <- factors + 7

federov_multiplier <- ceiling(1000 / samples)
federov_samples <- samples * federov_multiplier

comparison_data <- NULL

runs <- 30

for (i in 1:runs) {
    if (i %% 5 == 0) {
        print(i)
    }

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    formula <- formula(paste("~ ", paste(names(federov_sample),
                                         sep = "",
                                         collapse = " + ")))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    best_sampled_D <- -1
    best_biased_design <- NULL

    for (j in 1:federov_multiplier) {
        biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                     sample_factor(lin_cdf,
                                                                                   samples))))

        sampled_D <- eval.design(formula, biased_design)$determinant

        if (sampled_D > best_sampled_D) {
            best_sampled_D <- sampled_D
            best_biased_design <- biased_design
        }
    }

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       best_sampled_D),
                                 names = c("federov", "biased"),
                                 id = i)

    if (is.null(comparison_data)) {
        comparison_data <- new_experiment
    } else {
        comparison_data <- bind_rows(comparison_data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: [1] 5
: [1] 10
: [1] 15
: [1] 20
: [1] 25
: [1] 30

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
write.csv(comparison_data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_1000_repetitions_3_factors.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

Motivated by that, I ran 1000 repetitions of an experiment where I sampled 250
designs with 4 experiments, resulting in 1000 samples per iteration, from the
biased CDF and picked the design with the best D criterion. I compared this with
the case where I ran =optFederov= for 1000 times on an initial candidate set of
size 1000.

The Figure below shows the comparison of the two methods, and shows that
picking the best design sampled from the biased distribution seems close to
the results of =optFederov= for this size of design and amount of parameters.

What I intend to do next is to look at this behaviour for higher numbers of
parameters and more experiments. I will also start to apply this encoding method
to numerical parameters in the case where we use quadratic and cubic models.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
comparison_data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_1000_repetitions_2_factors.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(comparison_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figurekupliy.png]]
*** [2018-12-11 Tue]
**** /D Criterion/ for a Linear Model with Numerical Factors, using other CDFs
The past experiments showed that we get designs with similar D if we
sample from one of the 3 biased Cumulative Distribution Functions. To
better understand the impact of CDFs on the final result, I wanted to
compare the results of the other CDFs with =optFederov=.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

The CDFs I selected look like this:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf, cube_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5yMLvd/figurej5CCIB.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- factors + 5

data <- NULL

runs <- 30

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ", paste(names(design), sep = "", collapse = " + ")))

    lin_biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                     sample_factor(lin_cdf,
                                                                                   samples))))

    quad_biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                      sample_factor(quad_cdf,
                                                                                    samples))))

    cube_biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                      sample_factor(cube_cdf,
                                                                                    samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, lin_biased_design)$determinant,
                                       eval.design(formula, quad_biased_design)$determinant,
                                       eval.design(formula, cube_biased_design)$determinant),
                                 names = c("runif", "lin_biased", "quad_biased", "cube_biased"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

The next figure shows 4 experiments that generated 1000 samples of designs of
size 35, for 30 numerical factors. The /biased/ experiments use my CDFs and the
/runif/ experiment uses =runif=. We see that the ``linear'' CDF performed best
among the CDFs.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figuredRZfJe.png]]

We see that the CDF for the ``linear'' case obtained small values of D,
close to 0.5. Those values are higher than the ones found by other CDFs
and by =runif=.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- factors + 5
federov_samples <- 1000

runs <- 30

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, biased_federov_design)$determinant,
                                       eval.design(formula, federov_design)$determinant),
                                 names = c("biased_federov", "federov"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 32 warnings (use warnings() to see them)

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gjs27r/figureewZDXl.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- factors + 6

desired_samples <- 1000
federov_multiplier <- ceiling(desired_samples / samples)
federov_samples <- samples * federov_multiplier

comparison_data <- NULL

runs <- 30

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    formula <- formula(paste("~ ", paste(names(federov_sample),
                                         sep = "",
                                         collapse = " + ")))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    best_sampled_D <- -1
    best_biased_design <- NULL

    for (j in 1:federov_multiplier) {
        biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                     sample_factor(lin_cdf,
                                                                                   samples))))

        sampled_D <- eval.design(formula, biased_design)$determinant

        if (sampled_D > best_sampled_D) {
            best_sampled_D <- sampled_D
            best_biased_design <- biased_design
        }
    }

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       best_sampled_D),
                                 names = c("federov", "biased"),
                                 id = i)

    if (is.null(comparison_data)) {
        comparison_data <- new_experiment
    } else {
        comparison_data <- bind_rows(comparison_data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: [1] 5
: [1] 10
: [1] 15
: [1] 20
: [1] 25
: [1] 30

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
write.csv(comparison_data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_30_repetitions_30_factors_36_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

Motivated by that, I ran 1000 repetitions of an experiment where I sampled 250
designs with 4 experiments, resulting in 1000 samples per iteration, from the
biased CDF and picked the design with the best D criterion. I compared this with
the case where I ran =optFederov= for 1000 times on an initial candidate set of
size 1000.

The Figure below shows the comparison of the two methods, and shows that
picking the best design sampled from the biased distribution seems close to
the results of =optFederov= for this size of design and amount of parameters.

What I intend to do next is to look at this behaviour for higher numbers of
parameters and more experiments. I will also start to apply this encoding method
to numerical parameters in the case where we use quadratic and cubic models.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
comparison_data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_36_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(comparison_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figurehT3LG2.png]]
*** [2018-12-13 Thu]
**** /D Criterion/ for a Quadratic Model with Numerical Factors II
I'll look at the designs generated by =runif= and by my /ad hoc/ CDF for
quadratic models. Later, I will look at the designs found by =optFederov=
starting from different candidate sets. We expect this will give us insights
that can be applied to the cubic case.

We will generate and encode small numbers of factors using the =rsm= package, so
that we can move the actual parameter intervals, in this case $[0, 1]$, to the
intervals accepted by =eval.design= and =model.matrix=, which is $[-1, 1]$.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factors <- 2
samples <- (factors * 2) + 1

design_codings <- list()

for (i in 1:factors) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(data.frame(replicate(factors, runif(samples))),
                     formulas = design_codings)

formula <- formula(paste("~ ",
                         paste(names(design),
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^2)",
                               sep = "",
                               collapse = " + "),
                         sep = ""))

design

eval.design(formula, design)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

          X1        X2
1 0.04853696 0.5165916
2 0.28785364 0.6475220
3 0.88326033 0.7001497
4 0.26150286 0.8158525
5 0.60348235 0.5417446

Data are stored in coded form using these coding formulas ...
x1 ~ (X1 - 0.5)/0.5
x2 ~ (X2 - 0.5)/0.5

[1] 0.06564664
#+end_example

We can look at the points in the generated design:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-0SJBEn/figure6GOI0Y.png]]

Now that I know the D criterion is properly scaled, I will attempt to select the
experiments that have D closest to $1.0$ by picking orthogonal rows. I will also
compute the D criterion for the decoded design, that is, a design whose
numerical factors are not in the interval $[-1, 1]$, but on the original $[0,
1]$ interval. We see that this ideal design has a much lower D value when
decoded. The results I've shown before for numerical factors where obtained
using only the decoded design matrices.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(rsm)

design <- data.frame(X1 = c(0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5),
                     X2 = c(0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5))

design

design_codings <- list()

for (i in 1:ncol(design)) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(design, formulas = design_codings)

formula <- formula(paste("~ ",
                         paste(names(design),
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^2)",
                               sep = "",
                               collapse = " + "),
                         sep = ""))

eval.design(formula, design)$determinant

decoded_formula <- formula(paste("~ ",
                                 paste(names(decode.data(design)),
                                       sep = "",
                                       collapse = " + "),
                                 " + ",
                                 paste("I(",
                                       names(decode.data(design)),
                                       "^2)",
                                       sep = "",
                                       collapse = " + "),
                                 sep = ""))

eval.design(decoded_formula, decode.data(design))$determinant
#+END_SRC

#+RESULTS:
#+begin_example

   X1  X2
1 0.0 0.0
2 0.0 1.0
3 1.0 1.0
4 1.0 0.0
5 0.5 1.0
6 1.0 0.5
7 0.5 0.5

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

[1] 0.427508

[1] 0.08099761
#+end_example

Looking at the design, we get:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-0SJBEn/figureGY0WhM.png]]

I will now look at the D criterion for designs generated using =runif= and the
custom CDF I created, which looks like this:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
options(warn = 0)
#+END_SRC

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
ggplot(quad_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-EuMp0l/figureP603vu.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 5

data <- NULL

runs <- 300

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(quad_cdf,
                                                                               samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

The next figure shows 4 experiments that generated 1000 samples of designs of
size 4, for 2 numerical parameters. The /biased/ experiment uses my CDF and the
/runif/ experiment uses =runif=. The /decoded/ experiments correspond to
computing the D criterion for parameters in the $[0, 1]$ interval, while the
other experiments use parameters in the $[-1, 1]$ interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-EuMp0l/figureuX65Xs.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 5
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(quad_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureBcXYL5.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_65_exp.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", row.names = FALSE)
#+END_SRC

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_65_exp.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figure4wWtwD.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figure3mXGdl.png]]

**** Tests with the D Criterion for Quadratic Models
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(rsm)

compute_D <- function(formula, data) {
    model_matrix <- as.matrix(as.data.frame(model.matrix(formula, data)))
    return(det((t(model_matrix) %*% model_matrix) / nrow(data)) ^ (1 / ncol(model_matrix)))
}

design <- data.frame(X1 = c(0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.5),
                     X2 = c(0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 0.0))

design$X1_2 <- design$X1 * design$X1
design$X2_2 <- design$X2 * design$X2

quadratic_formula = formula(~ x1 + x2 + x3 + x4)
decoded_quadratic_formula = formula(~ X1 + X2 + X1_2 + X2_2)

coded_design <- coded.data(design)

design

as.data.frame(coded_design)

compute_D(quadratic_formula, coded_design)
compute_D(decoded_quadratic_formula, design)
#+END_SRC

#+RESULTS:
#+begin_example

Warning message:
In coded.data(design) :
  Automatic codings created -- may not be what you want

   X1  X2 X1_2 X2_2
1 0.0 0.0 0.00 0.00
2 1.0 1.0 1.00 1.00
3 0.0 1.0 0.00 1.00
4 1.0 0.0 1.00 0.00
5 0.5 0.5 0.25 0.25
6 1.0 0.5 1.00 0.25
7 0.5 1.0 0.25 1.00
8 0.0 0.5 0.00 0.25
9 0.5 0.0 0.25 0.00

  x1 x2   x3   x4
1 -1 -1 -1.0 -1.0
2  1  1  1.0  1.0
3 -1  1 -1.0  1.0
4  1 -1  1.0 -1.0
5  0  0 -0.5 -0.5
6  1  0  1.0 -0.5
7  0  1 -0.5  1.0
8 -1  0 -1.0 -0.5
9  0 -1 -0.5 -1.0

[1] 0.2675805

[1] 0.08826865
#+end_example
*** [2018-12-19 Wed]
**** /D Criterion/ for a Cubic Model with Numerical Factors
#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factors <- 2
samples <- (factors * 3) + 1

design_codings <- list()

for (i in 1:factors) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(data.frame(replicate(factors, runif(samples))),
                     formulas = design_codings)

formula <- formula(paste("~ ",
                         paste(names(design),
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^2)",
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^3)",
                               sep = "",
                               collapse = " + "),
                         sep = ""))

design

eval.design(formula, design)$determinant
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following object is masked from â€˜package:GGallyâ€™:

    nasa

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

         X1         X2
1 0.9335433 0.04080147
2 0.9794172 0.63624393
3 0.5776782 0.29871166
4 0.1608125 0.89803648
5 0.1628355 0.90490132
6 0.5684842 0.44253111
7 0.5737806 0.65747989

Data are stored in coded form using these coding formulas ...
x1 ~ (X1 - 0.5)/0.5
x2 ~ (X2 - 0.5)/0.5

[1] 0.01345616
#+end_example

We can look at the points in the generated design:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureoficdw.png]]

Now that I know the D criterion is properly scaled, I will attempt to select the
experiments that have D closest to $1.0$ by picking orthogonal rows. I will also
compute the D criterion for the decoded design, that is, a design whose
numerical factors are not in the interval $[-1, 1]$, but on the original $[0,
1]$ interval. We see that this ideal design has a much lower D value when
decoded. The results I've shown before for numerical factors where obtained
using only the decoded design matrices.

#+HEADER: :results output :session *R* :exports both :eval no-export
#+BEGIN_SRC R
library(rsm)

design <- data.frame(X1 = c(0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.3, 0.3),
                     X2 = c(0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.3, 0.4))

design

design_codings <- list()

for (i in 1:ncol(design)) {
    design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                               paste("(X", i, "-0.5)/0.5", sep = ""),
                                                               sep = ""))
}

design_codings

design <- coded.data(design, formulas = design_codings)

formula <- formula(paste("~ ",
                         paste(names(design),
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^2)",
                               sep = "",
                               collapse = " + "),
                         " + ",
                         paste("I(",
                               names(design),
                               "^3)",
                               sep = "",
                               collapse = " + "),
                         sep = ""))

eval.design(formula, design)$determinant

decoded_formula <- formula(paste("~ ",
                                 paste(names(decode.data(design)),
                                       sep = "",
                                       collapse = " + "),
                                 " + ",
                                 paste("I(",
                                       names(decode.data(design)),
                                       "^2)",
                                       sep = "",
                                       collapse = " + "),
                                 " + ",
                                 paste("I(",
                                       names(decode.data(design)),
                                       "^3)",
                                       sep = "",
                                       collapse = " + "),
                                 sep = ""))

eval.design(decoded_formula, decode.data(design))$determinant
#+END_SRC

#+RESULTS:
#+begin_example

   X1  X2
1 0.0 0.0
2 0.0 1.0
3 1.0 1.0
4 1.0 0.0
5 0.5 1.0
6 1.0 0.5
7 0.5 0.5
8 0.3 0.3
9 0.3 0.4

$x1
x1 ~ (X1 - 0.5)/0.5

$x2
x2 ~ (X2 - 0.5)/0.5

[1] 0.1023265

[1] 0.009503538
#+end_example

Looking at the design, we get:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)

ggplot(decode.data(design), aes(x = X1, y = X2)) +
       xlim(c(0, 1)) +
       ylim(c(0, 1)) +
       geom_point(size = 3) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figure1o79uY.png]]

I will now look at the D criterion for designs generated using =runif= and the
custom CDF I created, which looks like this:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
options(warn = 0)
#+END_SRC

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
ggplot(cube_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figuregb0vna.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (3 * factors) + 10

data <- NULL

runs <- 10

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^3)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(cube_cdf,
                                                                               samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)

The next figure shows 4 experiments that generated 1000 samples of designs of
size 4, for 2 numerical parameters. The /biased/ experiment uses my CDF and the
/runif/ experiment uses =runif=. The /decoded/ experiments correspond to
computing the D criterion for parameters in the $[0, 1]$ interval, while the
other experiments use parameters in the $[-1, 1]$ interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureIlxumC.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (3 * factors) + 10
federov_samples <- 1000

runs <- 10

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(cube_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_10_repetitions_30_factors_100_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_10_repetitions_30_factors_100_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figure6rgpM6.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_10_repetitions_30_factors_100_exp.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_10_repetitions_30_factors_100_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_10_repetitions_30_factors_100_exp.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_10_repetitions_30_factors_100_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figurec2Y7fd.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figurerKrFGP.png]]
*** [2018-12-20 Thu]
**** Implementing Exchange Algorithms in Julia: Computing Variance Predictions
#+HEADER: :results output :session *Julia*
#+BEGIN_SRC julia
using DataFrames, StatsModels, Random

function linear_model(data::DataFrame)
    linear_formula = Expr(:call)
    linear_formula.args = vcat(:+, names(data))
    @eval @formula(_ ~ 1 + $linear_formula)
end

function compute_variance_prediction(dispersion_matrix::Array{Float64, 2},
                                     model_matrix::Array{Float64, 2},
                                     candidate_set::Array{Float64, 2},
                                     design_variance::Array{Float64, 1},
                                     candidate_variance::Array{Float64, 1},
                                     predictions::Array{Float64, 2},
                                     deltas::Array{Float64, 2})
    @inbounds @simd for i = 1:size(model_matrix, 1)
        design_variance[i] = model_matrix[i, :]' * dispersion_matrix * model_matrix[i, :]
    end

    @inbounds @simd for i = 1:size(candidate_set, 1)
        candidate_variance[i] = candidate_set[i, :]' * dispersion_matrix * candidate_set[i, :]
    end

    Threads.@threads for i = 1:size(predictions, 1)
        @inbounds for j = 1:size(predictions, 2)
            predictions[i, j] = model_matrix[i, :]' * dispersion_matrix * candidate_set[j, :]
        end
    end

    Threads.@threads for i = 1:size(deltas, 1)
        @inbounds for j = 1:size(deltas, 2)
            deltas[i, j] = candidate_variance[j] -
                           (design_variance[i] * candidate_variance[j] -
                            (predictions[i, j] * predictions[i, j])) -
                           design_variance[i]
        end
    end

    return (design_variance, candidate_variance, predictions, deltas)
end

function variance_prediction(formula::Formula,
                             design::DataFrame,
                             candidate_set::DataFrame)
    model_matrix = ModelMatrix(ModelFrame(formula, design))
    candidate_model_matrix = ModelMatrix(ModelFrame(formula, candidate_set))

    response_index = findfirst(x -> x == :_, names(design))
    removed_column = findfirst(x -> x == response_index, model_matrix.assign)

    clean_columns = trues(size(model_matrix.m, 2))
    clean_columns[removed_column] = false

    clean_model_matrix = model_matrix.m[:, clean_columns]
    clean_candidate_model_matrix = candidate_model_matrix.m[:, clean_columns]


    design_variance = similar(clean_model_matrix[:, 1])
    candidate_variance = similar(clean_candidate_model_matrix[:, 1])

    predictions = similar(clean_candidate_model_matrix[:, 1],
                          (size(clean_model_matrix, 1),
                           size(clean_candidate_model_matrix, 1)))

    deltas = similar(clean_candidate_model_matrix[:, 1],
                     (size(clean_model_matrix, 1),
                      size(clean_candidate_model_matrix, 1)))

    dispersion_matrix = inv(clean_model_matrix' * clean_model_matrix)

    compute_variance_prediction(dispersion_matrix,
                                clean_model_matrix,
                                clean_candidate_model_matrix,
                                design_variance,
                                candidate_variance,
                                predictions,
                                deltas)
end

function run()
    Random.seed!(1234)

    factors = 30
    experiments = 35
    candidate_set_size = 10000

    data = Dict("_" => ones(experiments))

    for i in 1:factors
        data["X$i"] = rand(experiments)
    end

    design = DataFrame(data)

    data = Dict("_" => ones(candidate_set_size))

    for i in 1:factors
        data["X$i"] = rand(candidate_set_size)
    end

    candidate_set = DataFrame(data)

    variance_prediction(linear_model(design), design, candidate_set)
end

@time res = run()
#+END_SRC

#+RESULTS:
#+begin_example


linear_model (generic function with 1 method)

compute_variance_prediction (generic function with 2 methods)

variance_prediction (generic function with 2 methods)

run (generic function with 1 method)

  1.239840 seconds (1.45 M allocations: 445.030 MiB, 3.27% gc time)
([0.908093, 0.874791, 0.995594, 0.831536, 0.884585, 0.863678, 0.869294, 0.920054, 0.803498, 0.878221  â€¦  0.872946, 0.925484, 0.970307, 0.943129, 0.731105, 0.960452, 0.827948, 0.793881, 0.937411, 0.883036], [4.124, 2.63609, 27.5733, 17.517, 4.73112, 4.582, 2.88025, 8.76361, 5.99648, 20.0881  â€¦  2.3649, 11.4156, 13.731, 12.5973, 5.40583, 3.6751, 5.7942, 4.53855, 15.272, 5.28235], [0.459142 -0.485844 â€¦ 0.984948 0.530271; -0.0210519 -0.359596 â€¦ 0.0536111 0.195842; â€¦ ; 0.330644 -0.385013 â€¦ 0.125994 -0.228165; 0.585736 -0.0126345 â€¦ 0.0583541 0.08342], [-0.318255 -0.429772 â€¦ 1.46564 -0.141419; -0.357986 -0.41542 â€¦ 1.04027 -0.175041; â€¦ ; -0.569968 -0.624186 â€¦ 0.034322 -0.554735; -0.0575896 -0.574549 â€¦ 0.906642 -0.258233])
#+end_example
* 2019
** January
*** [2019-01-20 Sun]
**** /D Criterion/ for a Linear Model with Numerical Factors
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
ggplot(lin_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figuregx0X8Q.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (1 * factors) + 10

data <- NULL

runs <- 300

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + ")))

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

The next figure shows 4 experiments that generated 1000 samples of designs of
size 4, for 2 numerical parameters. The /biased/ experiment uses my CDF and the
/runif/ experiment uses =runif=. The /decoded/ experiments correspond to
computing the D criterion for parameters in the $[0, 1]$ interval, while the
other experiments use parameters in the $[-1, 1]$ interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureCvpHqd.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (1 * factors) + 10
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_10_repetitions_30_factors_100_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureCiXrVb.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureGCoV81.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figuretYc4w9.png]]
**** /D Criterion/ for a Quadratic Model with Numerical Factors III
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
ggplot(quad_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figure8LRkEJ.png]]
***** Generate Data

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 10

data <- NULL

runs <- 300

for (i in 1:runs) {
    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))
    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(quad_cdf,
                                                                               samples))))

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:

The next figure shows 4 experiments that generated 1000 samples of designs of
size 4, for 2 numerical parameters. The /biased/ experiment uses my CDF and the
/runif/ experiment uses =runif=. The /decoded/ experiments correspond to
computing the D criterion for parameters in the $[0, 1]$ interval, while the
other experiments use parameters in the $[-1, 1]$ interval.

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-cohRUt/figureA5dw4L.png]]

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 10
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(quad_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: There were 50 or more warnings (use warnings() to see the first 50)

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 10
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(quad_cdf,
                                                                                7 * (federov_samples / 10)))))

    federov_sample <- bind_rows(federov_sample, get_runif_coded_design(data.frame(replicate(factors,
                                                                                            rnorm(3 * (federov_samples / 10),
                                                                                                  mean = 0.5, sd = 0.04)))))

    normal_biased_federov_design <- optFederov(formula,
                                               federov_sample,
                                               nTrials = samples)$design

    new_experiment <- data.frame(D = c(eval.design(formula, normal_biased_federov_design)$determinant),
                                 names = c("Federov with Normal + Uniform Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
[1] 110
[1] 120
[1] 130
[1] 140
[1] 150
[1] 160
[1] 170
[1] 180
[1] 190
[1] 200
[1] 210
[1] 220
[1] 230
[1] 240
[1] 250
[1] 260
[1] 270
[1] 280
[1] 290
[1] 300
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_normal_30p_1000_samples_300_repetitions_30_factors_70_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
***** Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_normal_30p_1000_samples_300_repetitions_30_factors_70_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureK12eRY.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_65_exp.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", row.names = FALSE)
write.csv(normal_biased_federov_design, "dopt_sampling_tests/federov_normal_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_65_exp.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", header = TRUE)
normal_biased_federov_design <- read.csv("dopt_sampling_tests/federov_normal_biased_1000_samples_300_repetitions_30_factors_65_exp.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
str(filter_all(federov_sample, all_vars(. > -0.15 & . < 0.15)))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	60 obs. of  30 variables:
 $ x1 : num  -0.08608 0.02675 -0.00236 -0.04267 0.0535 ...
 $ x2 : num  0.0514 0.1378 0.0264 -0.0618 -0.1003 ...
 $ x3 : num  0.045 -0.0923 0.0501 -0.0253 -0.0922 ...
 $ x4 : num  0.0253 0.0339 -0.0822 -0.0707 0.0365 ...
 $ x5 : num  -0.1248 -0.0441 -0.1015 0.0114 -0.1171 ...
 $ x6 : num  0.1325 -0.0418 0.0157 -0.1108 -0.0294 ...
 $ x7 : num  0.0315 -0.064 0.023 -0.1463 0.0736 ...
 $ x8 : num  0.0229 -0.0829 0.0402 0.1271 -0.0116 ...
 $ x9 : num  0.0826 0.0276 -0.1217 -0.0968 -0.0781 ...
 $ x10: num  0.02926 0.00573 -0.05965 -0.04871 0.11438 ...
 $ x11: num  -0.04048 -0.06759 0.00846 -0.08865 0.00447 ...
 $ x12: num  -0.0675 -0.0374 -0.048 -0.1097 -0.012 ...
 $ x13: num  0.01585 -0.06534 0.08267 0.00883 0.14171 ...
 $ x14: num  0.0669 0.0825 -0.0431 0.0324 -0.0511 ...
 $ x15: num  0.04077 0.05535 0.05811 -0.00504 0.02461 ...
 $ x16: num  -0.0723 0.054 0.1176 0.0588 0.074 ...
 $ x17: num  0.087073 -0.000871 0.011199 -0.046693 -0.021858 ...
 $ x18: num  0.0406 -0.0051 0.0877 0.0424 -0.0133 ...
 $ x19: num  0.02284 0.03481 -0.00605 0.04454 0.00756 ...
 $ x20: num  0.00147 -0.09211 -0.03126 0.03029 -0.02169 ...
 $ x21: num  0.0116 -0.00123 -0.00714 -0.04002 0.06908 ...
 $ x22: num  0.0256 0.0565 -0.0189 -0.0382 0.014 ...
 $ x23: num  0.11389 0.10539 0.00651 -0.06157 -0.14051 ...
 $ x24: num  -0.0856 -0.1243 -0.0142 -0.1042 0.0426 ...
 $ x25: num  0.01796 -0.01925 0.00419 0.0124 -0.08893 ...
 $ x26: num  -0.0428 0.0631 -0.0249 -0.137 0.0564 ...
 $ x27: num  0.1163 -0.0224 -0.1297 -0.0719 -0.0127 ...
 $ x28: num  0.0303 -0.0733 0.0745 -0.0204 -0.0667 ...
 $ x29: num  -0.06021 0.0367 -0.05284 -0.00511 -0.07654 ...
 $ x30: num  -0.01683 -0.0159 0.04491 0.00642 0.04917 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(normal_biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure9fqh7A.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureqvBhPb.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureZLhZC6.png]]
*** [2019-01-21 Mon]
**** /D Criterion/ & /Practical Efficiency/ for a Linear Model with Numerical Factors
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
ggplot(lin_cdf, aes(x = x, y = y)) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gCuM81/figureVUp0bq.png]]

***** Generating Data
How to compare two sets of coefficients?

[[https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions][This CrossValidated question]] suggests comparing coefficients using their
standard errors.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)
library(AlgDesign)

compute_linear_response <- function(coefficients, experiment) {
    # Computing response with a linear model
    return((unname(drop(as.matrix(experiment))) %*% coefficients[2:length(coefficients)] + coefficients[1]))
}

add_noise <- function(design, sd = 1) {
    return(design$Y + rnorm(nrow(design), mean = 0, sd = sd))
}

compare_linear_fit <- function(coefficients, sample, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_linear_response(coefficients, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(Y ~ . , design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_linear_response(coefficients, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(Y ~ . , design_b),
                               sample)) ^ 2))))
}

# https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions
compare_linear_coefficients <- function(coefficients, design) {
    #return(sum((coefficients - unname(coef(lm(Y ~., design)))) / sqrt((data.frame(summary(lm(Y ~., design))["coefficients"])[, 2] ^ 2))))
    return(sqrt(sum((coefficients - unname(coef(lm(Y ~ . , design)))) ^ 2)))
}
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 30
samples <- (1 * factors) + 10
runs <- 300

coefficient_variability = 10
noise_sd <- 2
fit_comparison_sample_size <- 600

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- runif(factors + 1,
                          min = -coefficient_variability,
                          max = coefficient_variability)

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))

    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + ")))

    design$Y <- compute_linear_response(coefficients, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))
    biased_design$Y <- compute_linear_response(coefficients, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 coefficient_distance = c(compare_linear_coefficients(coefficients, design),
                                                          compare_linear_coefficients(coefficients, biased_design)),
                                 linear_fit_distance = compare_linear_fit(coefficients,
                                                                          fit_comparison_sample,
                                                                          design,
                                                                          biased_design,
                                                                          sample_size = fit_comparison_sample_size,
                                                                          sd = noise_sd),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
[1] 110
[1] 120
[1] 130
[1] 140
[1] 150
[1] 160
[1] 170
[1] 180
[1] 190
[1] 200
[1] 210
[1] 220
[1] 230
[1] 240
[1] 250
[1] 260
[1] 270
[1] 280
[1] 290
[1] 300
#+end_example

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurerGj2fc.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Uniform Sample")$coefficient_distance /
                                                               filter(data, names == "Biased Sample")$coefficient_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Diff. Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureAoWZJT.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Uniform Sample")$linear_fit_distance /
                                                              filter(data, names == "Biased Sample")$linear_fit_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Difference Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figures3nW0F.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureDw2Opf.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figuren7pu3t.png]]


#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (1 * factors) + 10
federov_samples <- 1000

runs <- 300

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- runif(factors + 1,
                          min = -coefficient_variability,
                          max = coefficient_variability)

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_linear_response(coefficients, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_linear_response(coefficients, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 coefficient_distance = c(compare_linear_coefficients(coefficients, federov_design),
                                                          compare_linear_coefficients(coefficients, biased_federov_design)),
                                 linear_fit_distance = compare_linear_fit(coefficients,
                                                                          fit_comparison_sample,
                                                                          federov_design,
                                                                          biased_federov_design,
                                                                          sample_size = fit_comparison_sample_size,
                                                                          sd = noise_sd),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
[1] 110
[1] 120
[1] 130
[1] 140
[1] 150
[1] 160
[1] 170
[1] 180
[1] 190
[1] 200
[1] 210
[1] 220
[1] 230
[1] 240
[1] 250
[1] 260
[1] 270
[1] 280
[1] 290
[1] 300
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

***** Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

The results below show 400 repetitions of the experiments described below. Each
experiment consists of generating designs of size 40 for 30 numerical factors in
the interval $[-1, 1]$, considering a linear model.

| Name                      | Experiment                                                   |
|---------------------------+--------------------------------------------------------------|
| Uniform Sample            | Pick 40 experiments from a uniform distribution              |
| Biased Sample             | Pick 40 experiments from a biased distribution               |
| Federov w/ Uniform Sample | Generate uniformly a set of size 1000 exp.; pick 40 w.r.t. D |
| Federov w/ Biased Sample  | Generate biasedly a set of size 1000 exp.; pick 40 w.r.t. D  |

The figures below compare the =coefficient_distance=, the =linear_fit_distance= and the =D=
values of the 400 repetitions. These values are computed as follows:

| Value                  | Computation                                                 |
|------------------------+-------------------------------------------------------------|
| =coefficient_distance= | Euclidean distance between fitted & real model coefficients |
| =linear_fit_distance=  | Euclidean distance between predicted & real response        |
| =D=                    | Computed as usual by =AlgDesign::eval.design()=             |

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureVeWflC.png]]


#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$coefficient_distance /
                                                               filter(data, names == "Federov with Biased Sample")$coefficient_distance),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Ratios (Federov Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureyecAn6.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$linear_fit_distance /
                                                              filter(data, names == "Federov with Biased Sample")$linear_fit_distance),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Dist. Ratio (Federov Uniform - Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureUuKQrT.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figuresp1tFI.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurecvsiuW.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureJNyztD.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure7o8lPz.png]]
*** [2019-01-25 Fri]
**** /D Criterion/ & /Practical Efficiency/ for a Linear Model with Numerical Factors
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf, cube_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurelHjQBb.png]]

***** Generating Data
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

How to compare two sets of coefficients?

[[https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions][This CrossValidated question]] suggests comparing coefficients using their
standard errors.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)
library(AlgDesign)

compute_linear_response <- function(coefficients, experiment) {
    # Computing response with a linear model
    return((unname(drop(as.matrix(experiment))) %*% coefficients[2:length(coefficients)] + coefficients[1]))
}

add_noise <- function(design, sd = 1) {
    return(design$Y + rnorm(nrow(design), mean = 0, sd = sd))
}

compare_linear_fit <- function(coefficients, sample, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_linear_response(coefficients, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(Y ~ . , design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_linear_response(coefficients, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(Y ~ . , design_b),
                               sample)) ^ 2))))
}

# https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions
compare_linear_coefficients <- function(coefficients, design) {
    #return(sum((coefficients - unname(coef(lm(Y ~., design)))) / sqrt((data.frame(summary(lm(Y ~., design))["coefficients"])[, 2] ^ 2))))
    return(sqrt(sum((coefficients - unname(coef(lm(Y ~ . , design)))) ^ 2)))
}
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 30
samples <- (1 * factors) + 10
runs <- 100

coefficient_variability = 10
coefficient_noise = 0.4
noise_sd <- 1
fit_comparison_sample_size <- 600

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(factors + 1,
                              ifelse(runif(1) > 0.8,
                                     runif(1,
                                           min = -coefficient_variability,
                                           max = coefficient_variability),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))

    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + ")))

    design$Y <- compute_linear_response(coefficients, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))
    biased_design$Y <- compute_linear_response(coefficients, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 coefficient_distance = c(compare_linear_coefficients(coefficients, design),
                                                          compare_linear_coefficients(coefficients, biased_design)),
                                 linear_fit_distance = compare_linear_fit(coefficients,
                                                                          fit_comparison_sample,
                                                                          design,
                                                                          biased_design,
                                                                          sample_size = fit_comparison_sample_size,
                                                                          sd = noise_sd),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
#+end_example

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurevxLvDm.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Uniform Sample")$coefficient_distance /
                                                               filter(data, names == "Biased Sample")$coefficient_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Diff. Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureSGyxYr.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Uniform Sample")$linear_fit_distance /
                                                              filter(data, names == "Biased Sample")$linear_fit_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Difference Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure6DjZew.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureylC3NF.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureRrqfN6.png]]


#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (1 * factors) + 10
federov_samples <- 1000

runs <- 100

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(factors + 1,
                              ifelse(runif(1) > 0.8,
                                     runif(1,
                                           min = -coefficient_variability,
                                           max = coefficient_variability),
                                     0.0))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_linear_response(coefficients, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_linear_response(coefficients, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 coefficient_distance = c(compare_linear_coefficients(coefficients, federov_design),
                                                          compare_linear_coefficients(coefficients, biased_federov_design)),
                                 linear_fit_distance = compare_linear_fit(coefficients,
                                                                          fit_comparison_sample,
                                                                          federov_design,
                                                                          biased_federov_design,
                                                                          sample_size = fit_comparison_sample_size,
                                                                          sd = noise_sd),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

***** Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

The following results show 300 repetitions of the experiments described below.
The first step of each iteration is selecting model coefficients in the interval
$[-10.0, 10.0]$ from a uniform distribution, and a set of experiments for
comparing the model fits. Then, each iteration generates designs of size 40 for
30 numerical factors in the interval $[-1.0, 1.0]$, using the strategies listed
below:

| Strategy                  | Description                                                  |
|---------------------------+--------------------------------------------------------------|
| Uniform Sample            | Pick 40 experiments from a uniform distribution              |
| Biased Sample             | Pick 40 experiments from a biased distribution               |
| Federov w/ Uniform Sample | Generate uniformly a set of size 1000 exp.; pick 40 w.r.t. D |
| Federov w/ Biased Sample  | Generate biasedly a set of size 1000 exp.; pick 40 w.r.t. D  |

The designs are evaluated using the generated model, with noise ($sd = 2$), and
the results are used to fit linear models, which are compared with the actual
model generated for that iteration using 2 model metrics and 1 design metric.
The figures below compare the =coefficient_distance=, the =linear_fit_distance=
and the =D= values of the 300 repetitions. These values are computed as follows:

| Value                  | Computation                                                 |
|------------------------+-------------------------------------------------------------|
| =coefficient_distance= | Euclidean distance between fitted & real model coefficients |
| =linear_fit_distance=  | Euclidean distance between predicted & real response        |
| =D=                    | Computed as usual by =AlgDesign::eval.design()=             |

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure6qU3HP.png]]


#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = (filter(data, names == "Federov with Uniform Sample")$coefficient_distance /
                                                           filter(data, names == "Federov with Biased Sample")$coefficient_distance),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Coeff. Dist. Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureUd8lnG.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = (filter(data, names == "Federov with Uniform Sample")$linear_fit_distance /
                                                          filter(data, names == "Federov with Biased Sample")$linear_fit_distance),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Fit Distance Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureHuCFCA.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureN9Pm0e.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureV3qrKI.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureJNyztD.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure7o8lPz.png]]
*** [2019-01-29 Tue]
**** /Practical Efficiency/ for a Linear Model with Numerical Factors, Few Significant Factors
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf, cube_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurelHjQBb.png]]

***** Generating Data
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

How to compare two sets of coefficients?

[[https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions][This CrossValidated question]] suggests comparing coefficients using their
standard errors.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)
library(AlgDesign)

compute_linear_response <- function(coefficients, experiment) {
    # Computing response with a linear model
    return((unname(drop(as.matrix(experiment))) %*% coefficients[2:length(coefficients)] + coefficients[1]))
}

add_noise <- function(design, sd = 1) {
    return(design$Y + rnorm(nrow(design), mean = 0, sd = sd))
}

compare_linear_fit <- function(coefficients, sample, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_linear_response(coefficients, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(Y ~ . , design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_linear_response(coefficients, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(Y ~ . , design_b),
                               sample)) ^ 2))))
}

# https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions
compare_linear_coefficients <- function(coefficients, design) {
    #return(sum((coefficients - unname(coef(lm(Y ~., design)))) / sqrt((data.frame(summary(lm(Y ~., design))["coefficients"])[, 2] ^ 2))))
    return(sqrt(sum((coefficients - unname(coef(lm(Y ~ . , design)))) ^ 2)))
}

compare_linear_real_coefficients <- function(coefficients, coefficient_limits, design) {
    prf_values <- unname(summary(lm(Y ~., design))[4][["coefficients"]][,4])
    return(c(sum(prf_values[abs(coefficients) > coefficient_limits$min])))
}

compare_linear_noise_coefficients <- function(coefficients, coefficient_limits, design) {
    prf_values <- unname(summary(lm(Y ~., design))[4][["coefficients"]][,4])
    return(sum(prf_values[abs(coefficients) <= coefficient_limits$min]))
}
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 30
samples <- (1 * factors) + 10
runs <- 100

coefficient_variability = list(max = 5, min = 3)
coefficient_noise = 0.2
noise_sd <- 1
fit_comparison_sample_size <- 600

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(factors + 1,
                              ifelse(runif(1) > 0.8,
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))

    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + ")))

    design$Y <- compute_linear_response(coefficients, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))
    biased_design$Y <- compute_linear_response(coefficients, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_linear_real_coefficients(coefficients, coefficient_variability, design),
                                                               compare_linear_real_coefficients(coefficients, coefficient_variability, biased_design)),
                                 noise_coefficients_prf_sum = c(compare_linear_noise_coefficients(coefficients, coefficient_variability, design),
                                                                compare_linear_noise_coefficients(coefficients, coefficient_variability, biased_design)),
                                 coefficient_distance = c(compare_linear_coefficients(coefficients, design),
                                                          compare_linear_coefficients(coefficients, biased_design)),
                                 linear_fit_distance = compare_linear_fit(coefficients,
                                                                          fit_comparison_sample,
                                                                          design,
                                                                          biased_design,
                                                                          sample_size = fit_comparison_sample_size,
                                                                          sd = noise_sd),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficients = abs(coefficients))

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureiwsazS.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurelDVQjF.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure1n5u64.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureOylyD1.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Uniform Sample")$coefficient_distance /
                                                               filter(data, names == "Biased Sample")$coefficient_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Diff. Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure8msXDr.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Uniform Sample")$linear_fit_distance /
                                                              filter(data, names == "Biased Sample")$linear_fit_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Difference Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure5FuwWK.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureYLuok0.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurec5hOLo.png]]


#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (1 * factors) + 10
federov_samples <- 1000

runs <- 100

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(factors + 1,
                              ifelse(runif(1) > 0.8,
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_linear_response(coefficients, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_linear_response(coefficients, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_linear_real_coefficients(coefficients, coefficient_variability, federov_design),
                                                               compare_linear_real_coefficients(coefficients, coefficient_variability, biased_federov_design)),
                                 noise_coefficients_prf_sum = c(compare_linear_noise_coefficients(coefficients, coefficient_variability, federov_design),
                                                                compare_linear_noise_coefficients(coefficients, coefficient_variability, biased_federov_design)),
                                 coefficient_distance = c(compare_linear_coefficients(coefficients, federov_design),
                                                          compare_linear_coefficients(coefficients, biased_federov_design)),
                                 linear_fit_distance = compare_linear_fit(coefficients,
                                                                          fit_comparison_sample,
                                                                          federov_design,
                                                                          biased_federov_design,
                                                                          sample_size = fit_comparison_sample_size,
                                                                          sd = noise_sd),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/comparison_federov_biased_1000_samples_100_repetitions_30_factors_40_exp_random_coeff_few_sig.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

***** Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_100_repetitions_30_factors_40_exp_random_coeff_few_sig.csv", header = TRUE)
#+END_SRC

#+RESULTS:

The following results show 100 repetitions of the experiments described below.
The first step of each iteration is generating model coefficients. Since it
better mirrors our targeted use case, we decided to generate coefficient sets
where only a small amount of coefficients is allowed to be significant, 20% in
this case. Significant coefficients are picked from an uniform distribution in
the interval $[-5, -3] \cup [3, 5]$, and unsignificant coefficients are picked
from a normal distribution in the interval $[-0.2, 0.2]$.

| Parameter                       | Interval                          |
|---------------------------------+-----------------------------------|
| Coefficients                    | 30 + Intercept                    |
| Significant Coefficients        | 20%                               |
| Significant Coefficient Range   | Uniform in $[-5, -3] \cup [3, 5]$ |
| Unsignificant Coefficient Range | Normal in $[-0.2, 0.2]$           |

After generating the coefficients, we generate a set of experiments for
comparing model fits. Then, each iteration generates designs of size 40 for 30
numerical factors in the interval $[-1.0, 1.0]$, using the strategies listed
below:

| Strategy                  | Description                                                  |
|---------------------------+--------------------------------------------------------------|
| Uniform Sample            | Pick 40 experiments from a uniform distribution              |
| Biased Sample             | Pick 40 experiments from a biased distribution               |
| Federov w/ Uniform Sample | Generate uniformly a set of size 1000 exp.; pick 40 w.r.t. D |
| Federov w/ Biased Sample  | Generate biasedly a set of size 1000 exp.; pick 40 w.r.t. D  |

Responses are computed using the generated coefficient set, with added noise
from a normal distribution with standard deviation $sd = 2$ and mean zero. The
results are used to fit linear models, which are compared with the actual model
generated for that iteration using 2 model metrics and 1 design metric. The
figures below compare the =coefficient_distance=, the =linear_fit_distance= and
the =D= values of the 100 repetitions. These values are computed as follows:

| Value                  | Computation                                                 |
|------------------------+-------------------------------------------------------------|
| =coefficient_distance= | Euclidean distance between fitted & real model coefficients |
| =linear_fit_distance=  | Euclidean distance between predicted & real response        |
| =D=                    | Computed as usual by =AlgDesign::eval.design()=             |

I've also computed the distance between the following two metrics, in an attempt
to detect the model's hability to identify significant coefficients in a set
were most coefficients do not impact the response.

| Value                        | Computation                                                |
|------------------------------+------------------------------------------------------------|
| =real_coefficients_prf_sum=  | Sum of the fitted model's Pr(>F) values for actual coeffs. |
| =noise_coefficients_prf_sum= | Sum of the fitted model's Pr(>F) values for noise coeffs.  |

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figureB9wjIl.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

factors <- 30
coefficient_variability = list(max = 5, min = 3)
coefficient_noise = 0.2

coefficients <- replicate(factors + 1,
                          ifelse(runif(1) > 0.8,
                                  ifelse(runif(1) >= 0.5,
                                        runif(1,
                                              min = -coefficient_variability$max,
                                              max = -coefficient_variability$min),
                                        runif(1,
                                              min = coefficient_variability$min,
                                              max = coefficient_variability$max)),
                                  rnorm(1, mean = 0, sd = coefficient_noise)))

plot_data <- data.frame(coefficients = coefficients)

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurerYFiCo.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 0.001)) +
       scale_x_continuous(name = "Experiment") +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureL11b5s.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Experiment") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureoEtTvV.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = (sort(filter(data, names == "Federov with Uniform Sample")$coefficient_distance /
                                                                filter(data, names == "Federov with Biased Sample")$coefficient_distance)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurexbKYPd.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = (sort(filter(data, names == "Federov with Uniform Sample")$linear_fit_distance /
                                                               filter(data, names == "Federov with Biased Sample")$linear_fit_distance)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Fit Distance Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureCsuJpN.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureSKQFNO.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurembQtSV.png]]

****** Looking at Sample Designs

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurecZsbSP.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure7o8lPz.png]]

*** [2019-01-30 Wed]
**** /Practical Efficiency/ for a Quadratic Model with Numerical Factors, Few Significant Factors
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf, cube_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureeWJKFb.png]]

***** Generating Data
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_30_factors_70_exp_random_coeff_few_sig.csv", header = TRUE)
#+END_SRC

#+RESULTS:

How to compare two sets of coefficients?

[[https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions][This CrossValidated question]] suggests comparing coefficients using their
standard errors.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)
library(AlgDesign)

compute_response <- function(coefficients, formula, experiment) {
    return(model.matrix(formula, experiment) %*% coefficients)
}

add_noise <- function(design, sd = 1) {
    return(design$Y + rnorm(nrow(design), mean = 0, sd = sd))
}

compare_fit <- function(coefficients, sample, formula, response_formula, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_response(coefficients, formula, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(response_formula, design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_response(coefficients, formula, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(response_formula, design_b),
                               sample)) ^ 2))))
}

# https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions
#return(sum((coefficients - unname(coef(lm(Y ~., design)))) / sqrt((data.frame(summary(lm(Y ~., design))["coefficients"])[, 2] ^ 2))))
compare_coefficients <- function(coefficients, formula, design) {
    return(sqrt(sum((coefficients - unname(coef(lm(formula, design)))) ^ 2)))
}

compare_real_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(c(sum(prf_values[abs(coefficients) > coefficient_limits$min])))
}

compare_noise_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(sum(prf_values[abs(coefficients) <= coefficient_limits$min]))
}
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 30
samples <- (2 * factors) + 10
runs <- 100

coefficient_variability = list(max = 5, min = 3)
coefficient_noise = 0.2
noise_sd <- 1
fit_comparison_sample_size <- 600

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate((2 * factors) + 1,
                              ifelse(runif(1) > 0.8,
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))

    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

    response_formula <- formula(paste("Y ~ ",
                                paste(names(design),
                                      sep = "",
                                      collapse = " + "),
                                " + ",
                                paste("I(",
                                      names(design),
                                      "^2)",
                                      sep = "",
                                      collapse = " + "),
                                sep = ""))

    design$Y <- compute_response(coefficients, formula, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(quad_cdf,
                                                                               samples))))

    biased_design$Y <- compute_response(coefficients, formula, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients, coefficient_variability, response_formula, design),
                                                               compare_real_coefficients(coefficients, coefficient_variability, response_formula, biased_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients, coefficient_variability, response_formula, design),
                                                                compare_noise_coefficients(coefficients, coefficient_variability, response_formula, biased_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients, response_formula, design),
                                                          compare_coefficients(coefficients, response_formula, biased_design)),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   design,
                                                                   biased_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficients = abs(coefficients))

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureIrw7w5.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 0.2)) +
       scale_x_continuous(name = "Jittered Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurej8CEiU.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figure5DY3LP.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureCkYRAC.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Uniform Sample")$coefficient_distance /
                                                               filter(data, names == "Biased Sample")$coefficient_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Diff. Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureN9eoDp.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Uniform Sample")$linear_fit_distance /
                                                              filter(data, names == "Biased Sample")$linear_fit_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Difference Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurePa6xQE.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureFrHkNH.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureJQLayX.png]]


#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 10
federov_samples <- 1000

runs <- 40

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate((2 * factors) + 1,
                              ifelse(runif(1) > 0.8,
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_response(coefficients, formula, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(quad_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_response(coefficients, formula, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients, coefficient_variability, response_formula, federov_design),
                                                               compare_real_coefficients(coefficients, coefficient_variability, response_formula, biased_federov_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients, coefficient_variability, response_formula, federov_design),
                                                                compare_noise_coefficients(coefficients, coefficient_variability, response_formula, biased_federov_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients, response_formula, federov_design),
                                                          compare_coefficients(coefficients, response_formula, biased_federov_design)),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   federov_design,
                                                                   biased_federov_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
:
: [1] 10
: [1] 20
: [1] 30
: [1] 40
: There were 42 warnings (use warnings() to see them)

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_30_factors_70_exp_random_coeff_few_sig.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

***** Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_30_factors_70_exp_random_coeff_few_sig.csv", header = TRUE)
#+END_SRC

#+RESULTS:

The following results show 100 repetitions of the experiments described below.
The first step of each iteration is generating model coefficients. Since it
better mirrors our targeted use case, we decided to generate coefficient sets
where only a small amount of coefficients is allowed to be significant, 20% in
this case. Significant coefficients are picked from an uniform distribution in
the interval $[-5, -3] \cup [3, 5]$, and unsignificant coefficients are picked
from a normal distribution in the interval $[-0.2, 0.2]$.

After generating the coefficients, we generate a set of experiments for
comparing model fits. Then, each iteration generates designs of size 70 for 30
numerical factors in the interval $[-1.0, 1.0]$, using the strategies listed
below:

| Strategy                  | Description                                                  |
|---------------------------+--------------------------------------------------------------|
| Uniform Sample            | Pick 70 experiments from a uniform distribution              |
| Biased Sample             | Pick 70 experiments from a biased distribution               |
| Federov w/ Uniform Sample | Generate uniformly a set of size 1000 exp.; pick 70 w.r.t. D |
| Federov w/ Biased Sample  | Generate biasedly a set of size 1000 exp.; pick 70 w.r.t. D  |

Responses are computed using the generated coefficient set, with added noise
from a normal distribution with standard deviation $sd = 2$ and mean zero. The
results are used to fit linear models, which are compared with the actual model
generated for that iteration using 2 model metrics and 1 design metric. The
figures below compare the =coefficient_distance=, the =linear_fit_distance= and
the =D= values of the 100 repetitions. These values are computed as follows:

| Value                  | Computation                                                 |
|------------------------+-------------------------------------------------------------|
| =coefficient_distance= | Euclidean distance between fitted & real model coefficients |
| =linear_fit_distance=  | Euclidean distance between predicted & real response        |
| =D=                    | Computed as usual by =AlgDesign::eval.design()=             |

I've also computed the distance between the following two metrics, in an attempt
to detect the model's hability to identify significant coefficients in a set
were most coefficients do not impact the response.

| Value                        | Computation                                                |
|------------------------------+------------------------------------------------------------|
| =real_coefficients_prf_sum=  | Sum of the fitted model's Pr(>F) values for actual coeffs. |
| =noise_coefficients_prf_sum= | Sum of the fitted model's Pr(>F) values for noise coeffs.  |

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))
ggplot(plot_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureo3LhwF.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(real_coefficient_prf_sum_difference = (filter(data, names == "Federov with Uniform Sample")$real_coefficients_prf_sum /
                                                               filter(data, names == "Federov with Biased Sample")$real_coefficients_prf_sum),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = real_coefficient_prf_sum_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Real Coeff. Prf Sum Ratio (Federov Uniform / Biased)") +
       #ylim(c(0, 100)) +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureM36cP9.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 0.001)) +
       scale_x_continuous(name = "Experiment") +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurecdN0xb.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Experiment") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figure0NWhEh.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = (sort(filter(data, names == "Federov with Uniform Sample")$coefficient_distance /
                                                                filter(data, names == "Federov with Biased Sample")$coefficient_distance)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureHWB2cC.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = (sort(filter(data, names == "Federov with Uniform Sample")$linear_fit_distance /
                                                               filter(data, names == "Federov with Biased Sample")$linear_fit_distance)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Fit Distance Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureV5IKqN.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureichTIk.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figuredmWZRS.png]]

****** Looking at Sample Designs

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figurecZsbSP.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figure7o8lPz.png]]
*** [2019-01-31 Thu]
**** /Practical Efficiency/ for a Quadratic Model with Numerical Factors, Few Significant Factors + Normal Dist.
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf, cube_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6yUrit/figureeWJKFb.png]]

***** Generating Data
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

How to compare two sets of coefficients?

[[https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions][This CrossValidated question]] suggests comparing coefficients using their
standard errors.

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)
library(AlgDesign)

compute_response <- function(coefficients, formula, experiment) {
    return(model.matrix(formula, experiment) %*% coefficients)
}

add_noise <- function(design, sd = 1) {
    return(design$Y + rnorm(nrow(design), mean = 0, sd = sd))
}

compare_fit <- function(coefficients, sample, formula, response_formula, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_response(coefficients, formula, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(response_formula, design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_response(coefficients, formula, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(response_formula, design_b),
                               sample)) ^ 2))))
}

# https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions
#return(sum((coefficients - unname(coef(lm(Y ~., design)))) / sqrt((data.frame(summary(lm(Y ~., design))["coefficients"])[, 2] ^ 2))))
compare_coefficients <- function(coefficients, formula, design) {
    return(sqrt(sum((coefficients - unname(coef(lm(formula, design)))) ^ 2)))
}

compare_real_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(c(sum(prf_values[abs(coefficients) > coefficient_limits$min])))
}

compare_noise_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(sum(prf_values[abs(coefficients) <= coefficient_limits$min]))
}
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 30
samples <- (2 * factors) + 10
runs <- 100

coefficient_variability = list(max = 5, min = 3)
coefficient_noise = 0.2
noise_sd <- 1
fit_comparison_sample_size <- 600

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate((2 * factors) + 1,
                              ifelse(runif(1) > 0.8,
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))

    formula <- formula(paste("~ ",
                            paste(names(design),
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  names(design),
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

    response_formula <- formula(paste("Y ~ ",
                                paste(names(design),
                                      sep = "",
                                      collapse = " + "),
                                " + ",
                                paste("I(",
                                      names(design),
                                      "^2)",
                                      sep = "",
                                      collapse = " + "),
                                sep = ""))

    design$Y <- compute_response(coefficients, formula, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(quad_cdf,
                                                                               8 * (samples / 10)))))

    biased_design <- bind_rows(biased_design, get_runif_coded_design(data.frame(replicate(factors,
                                                                                          rnorm(2 * (samples / 10),
                                                                                                mean = 0.5, sd = 0.04)))))

    biased_design$Y <- compute_response(coefficients, formula, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients, coefficient_variability, response_formula, design),
                                                               compare_real_coefficients(coefficients, coefficient_variability, response_formula, biased_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients, coefficient_variability, response_formula, design),
                                                                compare_noise_coefficients(coefficients, coefficient_variability, response_formula, biased_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients, response_formula, design),
                                                          compare_coefficients(coefficients, response_formula, biased_design)),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   design,
                                                                   biased_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficients = abs(coefficients))

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureIrw7w5.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 0.2)) +
       scale_x_continuous(name = "Jittered Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurep2HyUB.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureb1dQB9.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureCyRGme.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Uniform Sample")$coefficient_distance /
                                                               filter(data, names == "Biased Sample")$coefficient_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Diff. Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureRNE681.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Uniform Sample")$linear_fit_distance /
                                                              filter(data, names == "Biased Sample")$linear_fit_distance),
                        id = seq(1:(nrow(data) / 2)))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Difference Ratio (Uniform / Biased)") +
       geom_point(alpha = 0.5) +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figure7upq1R.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figurezrzRwJ.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureZyOV5C.png]]


#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 30
samples <- (2 * factors) + 10
federov_samples <- 1000

runs <- 100

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate((2 * factors) + 1,
                              ifelse(runif(1) > 0.8,
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_response(coefficients, formula, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(quad_cdf,
                                                                                7 * (federov_samples / 10)))))

    federov_sample <- bind_rows(federov_sample, get_runif_coded_design(data.frame(replicate(factors,
                                                                                            rnorm(3 * (federov_samples / 10),
                                                                                                  mean = 0.5, sd = 0.04)))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_response(coefficients, formula, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients, coefficient_variability, response_formula, federov_design),
                                                               compare_real_coefficients(coefficients, coefficient_variability, response_formula, biased_federov_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients, coefficient_variability, response_formula, federov_design),
                                                                compare_noise_coefficients(coefficients, coefficient_variability, response_formula, biased_federov_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients, response_formula, federov_design),
                                                          compare_coefficients(coefficients, response_formula, biased_federov_design)),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   federov_design,
                                                                   biased_federov_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- bind_rows(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_30_factors_70_exp_random_coeff_few_sig_norm.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

***** Results
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_30_factors_70_exp_random_coeff_few_sig_norm.csv", header = TRUE)
#+END_SRC

#+RESULTS:

The following results show 100 repetitions of the experiments described below.
The first step of each iteration is generating model coefficients. Since it
better mirrors our targeted use case, we decided to generate coefficient sets
where only a small amount of coefficients is allowed to be significant, 20% in
this case. Significant coefficients are picked from an uniform distribution in
the interval $[-5, -3] \cup [3, 5]$, and unsignificant coefficients are picked
from a normal distribution in the interval $[-0.2, 0.2]$.

After generating the coefficients, we generate a set of experiments for
comparing model fits. Then, each iteration generates designs of size 70 for 30
numerical factors in the interval $[-1.0, 1.0]$, using the strategies listed
below:

| Strategy                  | Description                                                  |
|---------------------------+--------------------------------------------------------------|
| Uniform Sample            | Pick 70 experiments from a uniform distribution              |
| Biased Sample             | Pick 70 experiments from a biased distribution               |
| Federov w/ Uniform Sample | Generate uniformly a set of size 1000 exp.; pick 70 w.r.t. D |
| Federov w/ Biased Sample  | Generate biasedly a set of size 1000 exp.; pick 70 w.r.t. D  |

Responses are computed using the generated coefficient set, with added noise
from a normal distribution with standard deviation $sd = 2$ and mean zero. The
results are used to fit linear models, which are compared with the actual model
generated for that iteration using 2 model metrics and 1 design metric. The
figures below compare the =coefficient_distance=, the =linear_fit_distance= and
the =D= values of the 100 repetitions. These values are computed as follows:

| Value                  | Computation                                                 |
|------------------------+-------------------------------------------------------------|
| =coefficient_distance= | Euclidean distance between fitted & real model coefficients |
| =linear_fit_distance=  | Euclidean distance between predicted & real response        |
| =D=                    | Computed as usual by =AlgDesign::eval.design()=             |

I've also computed the distance between the following two metrics, in an attempt
to detect the model's hability to identify significant coefficients in a set
were most coefficients do not impact the response.

| Value                        | Computation                                                |
|------------------------------+------------------------------------------------------------|
| =real_coefficients_prf_sum=  | Sum of the fitted model's Pr(>F) values for actual coeffs. |
| =noise_coefficients_prf_sum= | Sum of the fitted model's Pr(>F) values for noise coeffs.  |

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))
ggplot(plot_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureaiOagD.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficients = abs(coefficients))

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureEdGzmF.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(real_coefficient_prf_sum_difference = (sort(filter(data, names == "Federov with Uniform Sample")$real_coefficients_prf_sum /
                                                                    filter(data, names == "Federov with Biased Sample")$real_coefficients_prf_sum)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = real_coefficient_prf_sum_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Real Coeff. Prf Sum Ratio (Federov Uniform / Biased)") +
       #ylim(c(0, 100)) +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureHAVAqg.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 0.001)) +
       scale_x_continuous(name = "Experiment") +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figure8PYXHn.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Experiment") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureTT4kGC.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = (sort(filter(data, names == "Federov with Uniform Sample")$coefficient_distance /
                                                                filter(data, names == "Federov with Biased Sample")$coefficient_distance)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureIzKgLf.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = (sort(filter(data, names == "Federov with Uniform Sample")$linear_fit_distance /
                                                               filter(data, names == "Federov with Biased Sample")$linear_fit_distance)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Fit Distance Ratio (Federov Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureA2BXM3.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = coefficient_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureub6SH8.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureAjfeHv.png]]

****** Looking at Sample Designs

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("x1", "x5", "x10", "x15", "x20", "x25", "x30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figuremsHNkw.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("X1", "X5", "X10", "X15", "X20", "X25", "X30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-hIxYUy/figureAcd7yv.png]]
** February
*** [2019-02-01 Fri]
**** Sampling from Multivariate Distributions in =R=
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(MASS)
library(GGally)
library(ggplot2)

means <- c(0, -5, 2, 1, 10)
variances <- c(1, 10, 3, 5, 8)
correlations <- c(0)

sigma_m <- matrix(rep(correlations[1]), length(means), length(means))

for(i in 1:length(means)) {
    sigma_m[i, i] <- variances[i]
}

sample <- data.frame(mvrnorm(n = 100, means, sigma_m))

ggpairs(sample) +
        theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-7hEm7H/figureYdbURn.png]]
*** [2019-02-04 Mon]
**** Gaussian Processes in R
Adapted from the example from [[https://cran.r-project.org/web/packages/kergp/kergp.pdf][kergp's documentation]] for the =gp= function:

#+begin_SRC R :results output :session *R*
library(kergp)
library(dplyr)
library(tidyr)

#set.seed(1234)

myCov <- covTS(inputs = c("Temp", "Humid"),
               dep = c(range = "input"),
               value = c(range = 0.4))

## change coefficients (variances)
coef(myCov) <- c(100, 10, 1, 50000)
d <- myCov@d
n <- 400

## design matrix
X <- expand.grid(seq(0, 1, length.out = 20),
                 seq(0, 1, length.out = 20))
colnames(X) <- inputNames(myCov)

## generate the GP realization
myGp <- gp(formula = y ~ 1, data = data.frame(y = rep(0, n), X),
           cov = myCov, estim = FALSE,
           beta = 0.0, varNoise = 0.05)

samples <- data.frame(simulate(myGp, nsim = 8, cond = FALSE)$sim)

samples$Temp <- data.frame(X)$Temp
samples$Humid <- data.frame(X)$Humid

samples <- gather(samples, id, Y, -Temp, -Humid)
#+end_SRC

#+RESULTS:
#+begin_example
Loading required package: Rcpp
Loading required package: testthat

Attaching package: â€˜testthatâ€™

The following object is masked from â€˜package:dplyrâ€™:

    matches

Loading required package: nloptr

Loading required package: MASS

Attaching package: â€˜MASSâ€™

The following object is masked from â€˜package:dplyrâ€™:

    select
#+end_example

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)

ggplot(samples[samples$id=="X3",], aes(x = Humid, y = Y,
                                       color = Temp,
                                       group = factor(Temp))) +
       geom_line() +
       theme_bw(base_size = 20)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figureO6j5wX.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)

ggplot(samples[samples$id=="X3",], aes(x = Temp, y = Y,
                                       color = Humid,
                                       group = factor(Humid))) +
       geom_line() +
       theme_bw(base_size = 20)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figureH5jBtR.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)

ggplot(samples[samples$id=="X3",], aes(fill = Y, x = Temp, z = Y, y = Humid)) +
       geom_point() +
       geom_contour() +
       geom_raster() +
       theme_bw(base_size = 20)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figurefzvpOG.png]]

#+begin_SRC R :results output :session *R*
samples_f1 <- filter(samples, id == "X1")
str(samples_f1)

regression_f1 <- lm(Y ~ Temp + Humid + I(Temp ^ 2) + I(Humid ^ 2), samples_f1)
summary.aov(regression_f1)
#+end_SRC

#+RESULTS:
#+begin_example

'data.frame':	400 obs. of  4 variables:
 $ Temp : num  0 0.0526 0.1053 0.1579 0.2105 ...
 $ Humid: num  0 0 0 0 0 0 0 0 0 0 ...
 $ id   : chr  "X1" "X1" "X1" "X1" ...
 $ Y    : num  64.1 64 64.1 64.1 63.7 ...

             Df Sum Sq Mean Sq   F value Pr(>F)
Temp          1      0       0 1.460e+00 0.2276
Humid         1  25431   25431 4.691e+05 <2e-16 ***
I(Temp^2)     1      0       0 3.562e+00 0.0598 .
I(Humid^2)    1    407     407 7.503e+03 <2e-16 ***
Residuals   395     21       0
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#+end_example
*** [2019-02-05 Tue]
**** Practical Efficiency with Numerical Factors & Functions Sampled from Gaussian Processes
***** Experiment Definition
1. Define factor number
   - High-dimensional spaces
   - Numerical factors
2. Generate samples for Gaussian Process
   - Testing set
   - =runif=
   - Biased distributions + =rnorm=
3. Generate Covariance Matrix
   - Choose kernel (=k1Matern5_2=, MatÃ©rn cov. Tensor Sum?)
   - Relative significances, kernel parameters
4. Sample function from Gaussian Process
5. Compute response for all samples
6. Fit models using strategies:
   - Random & Biased designs
   - D-Optimal with Random & Biased + =rnorm= designs
     - ANOVA to identify significant factors
     - Re-fit models using identified factors
7. Compare predictions with computed responses in test set
***** Clone Repository
Make sure you have the latest data:
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Setup
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

compute_response <- function(coefficients, formula, experiment) {
    return(model.matrix(formula, experiment) %*% coefficients)
}

add_noise <- function(design, sd = 1) {
    return(design$Y + rnorm(nrow(design), mean = 0, sd = sd))
}

compare_fit <- function(coefficients, sample, formula, response_formula, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_response(coefficients, formula, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(response_formula, design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_response(coefficients, formula, sample) + rnorm(sample_size, mean = 0, sd = sd)) -
                        predict(lm(response_formula, design_b),
                               sample)) ^ 2))))
}

compare_coefficients <- function(coefficients, formula, design) {
    return(sqrt(sum((coefficients - unname(coef(lm(formula, design)))) ^ 2)))
}

compare_real_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(c(sum(prf_values[abs(coefficients) > coefficient_limits$min])))
}

compare_noise_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(sum(prf_values[abs(coefficients) <= coefficient_limits$min]))
}

linear_data <- factor_cdf(linear_function, name = "Linear Factors")
linear_cdf <- subset(linear_data, name == "Linear Factors CDF")

quadratic_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quadratic_cdf <- subset(quadratic_data, name == "Quadratic Factors CDF")

cubic_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cubic_cdf <- subset(cubic_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(linear_cdf, quadratic_cdf, cubic_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-psN6ww/figurerLb68d.png]]

***** Generating Data
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+begin_SRC R :results output :session *R*
library(rsm)
library(dplyr)

data <- NULL

factors <- 31

testing_sample_size <- factors * 100
dopt_sample_size <- factors * 100

linear_design_size <- factors + 10
quadratic_design_size <- (2 * factors) + 10
cubic_design_size <- (3 * factors) + 10

quadratic_normal_ratio <- 0.2
quadratic_normal_sd <- 0.04

cubic_normal_ratio <- 0.2
cubic_normal_sd <- 0.04

significant_coefficient_ratio <- 0.2

# Generating all sample sets:

# Testing set to compare fit:

testing_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(testing_sample_size))))
testing_sample$name <- "testing_sample"

# DOPT runif set to sample from:

dopt_runif_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 runif(dopt_sample_size))))
dopt_runif_sample$name <- "dopt_runif_sample"

# Runif designs:

linear_runif_design <- get_runif_coded_design(data.frame(replicate(factors, runif(linear_design_size))))
linear_runif_design$name <- "linear_runif_design"

quadratic_runif_design <- get_runif_coded_design(data.frame(replicate(factors, runif(quadratic_design_size))))
quadratic_runif_design$name <- "quadratic_runif_design"

cubic_runif_design <- get_runif_coded_design(data.frame(replicate(factors, runif(cubic_design_size))))
cubic_runif_design$name <- "cubic_runif_design"

# Biased designs:

linear_biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                    sample_factor(linear_cdf,
                                                                                  linear_design_size))))
linear_biased_design$name <- "linear_biased_design"

quadratic_biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                       sample_factor(quadratic_cdf,
                                                                                     as.integer((1 - quadratic_normal_ratio) * quadratic_design_size)))))

quadratic_biased_design <- rbind(quadratic_biased_design, get_runif_coded_design(data.frame(replicate(factors,
                                                                                                      rnorm(as.integer(quadratic_normal_ratio * quadratic_design_size),
                                                                                                                       mean = 0.5, sd = quadratic_normal_sd)))))
quadratic_biased_design$name <- "quadratic_biased_design"

cubic_biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                   sample_factor(cubic_cdf,
                                                                                 as.integer((1 - cubic_normal_ratio) * cubic_design_size)))))

cubic_biased_design <- rbind(cubic_biased_design, get_runif_coded_design(data.frame(replicate(factors,
                                                                                              rnorm(as.integer(cubic_normal_ratio * cubic_design_size),
                                                                                                               mean = 0.5, sd = cubic_normal_sd)))))
cubic_biased_design$name <- "cubic_biased_design"

# DOPT biased sets to sample from:

dopt_linear_biased_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                         sample_factor(linear_cdf,
                                                                                       dopt_sample_size))))
dopt_linear_biased_sample$name <- "dopt_linear_biased_sample"

dopt_quadratic_biased_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                            sample_factor(quadratic_cdf,
                                                                                          as.integer((1 - quadratic_normal_ratio) * dopt_sample_size)))))

dopt_quadratic_biased_sample <- rbind(dopt_quadratic_biased_sample, get_runif_coded_design(data.frame(replicate(factors,
                                                                                                                rnorm(as.integer(quadratic_normal_ratio * dopt_sample_size),
                                                                                                                                 mean = 0.5, sd = quadratic_normal_sd)))))
dopt_quadratic_biased_sample$name <- "dopt_quadratic_biased_sample"

dopt_cubic_biased_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                        sample_factor(cubic_cdf,
                                                                                      as.integer((1 - cubic_normal_ratio) * dopt_sample_size)))))

dopt_cubic_biased_sample <- rbind(dopt_cubic_biased_sample, get_runif_coded_design(data.frame(replicate(factors,
                                                                                                        rnorm(as.integer(cubic_normal_ratio * dopt_sample_size),
                                                                                                                         mean = 0.5, sd = cubic_normal_sd)))))
dopt_cubic_biased_sample$name <- "dopt_cubic_biased_sample"

all_samples <- rbind(testing_sample, dopt_runif_sample, linear_runif_design,
                     quadratic_runif_design, cubic_runif_design, linear_biased_design,
                     quadratic_biased_design, cubic_biased_design, dopt_linear_biased_sample,
                     dopt_quadratic_biased_sample, dopt_cubic_biased_sample)
#+end_SRC

#+RESULTS:
*** [2019-02-07 Thu]
**** Comparing Sampling Strategies for Constructing D-Optimal Designs
Optimizing problems with a large number of configurable parameters, or
/factors/, is one of the main objectives of our transparent Design of
Experiments approach to autotuning. Despite the high dimensionality of these
problems, it is usually the case that only a \todo{add reference}small number of
factors impacts the measured performance metrics significantly. Our approach
aims to identify the most significant factors using an initial performance
model, D-Optimal designs, and Analysis of Variance.

Our experiments with SPAPT kernels obtained results similar to a uniform
sampling strategy, while using significantly less evaluations. Since it was
unfeasible to evaluate all possible configurations of each SPAPT kernel due to
their large search spaces, the experimental designs used in each iteration of
our iterative approach were selected from a limited sample of configurations.
The size of these samples were always small relative to the size of each search
space, due to the high dimension and number of values for each factor.

***** Clone Repository                                         :noexport:
Make sure you have the latest data:
#+BEGIN_SRC sh :results output :exports none :eval no-export
git clone https://github.com/phrb/dopt_sampling_tests.git || git -C dopt_sampling_tests pull
#+END_SRC

#+RESULTS:
***** Setup                                                    :noexport:
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(rsm)

factor_cdf <- function(f, name = "Generic Factors", interval = 0.0001) {
    x <- seq(0.0, 1.0, interval)
    y <- mapply(f, x)

    data <- data.frame(x = x, y = y, name = name)
    factor_pdf <- data.frame(x = x, y = data$y, name = paste(name, "PDF"))

    factor_pdf$y <- factor_pdf$y / sum(factor_pdf$y)
    data <- bind_rows(data, factor_pdf)

    y_cdf <- c()

    target_name <- paste(name, "PDF")

    pdf_data <- subset(data, name == target_name)$y

    for (i in 1:length(pdf_data)) {
        y_cdf <- c(y_cdf, sum(pdf_data[1:i]))
    }

    target_name <- paste(name, "CDF")
    data <- bind_rows(data,
                      data.frame(x = x, y = y_cdf, name = target_name))
    return(data)
}

sample_factor <- function(cdf_data, samples) {
    x_sampled <- c()

    for (i in 1:samples) {
        new_sample <- cdf_data$x[sum(cdf_data$y <= runif(1)) + 1]
        x_sampled <- c(x_sampled, new_sample)
    }

    return(x_sampled)
}

get_runif_design_codings <- function(design) {
    design_codings <- list()

    for (i in 1:ncol(design)) {
        design_codings[[paste("x", i, sep = "")]] <- formula(paste(paste("x", i, "~", sep = ""),
                                                                  paste("(X", i, "-0.5)/0.5", sep = ""),
                                                                  sep = ""))
    }

    return(design_codings)
}

get_runif_coded_design <- function(design) {
    design_codings <- get_runif_design_codings(design)
    return(coded.data(design, formulas = design_codings))
}

cubic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.35, 0.04) / 10) + (dnorm(x, 0.65, 0.04) / 10))
}

quadratic_function <- function(x) {
    return((64 * (0.5 - x) ** 6) + (dnorm(x, 0.5, 0.04) / 10))
}

linear_function <- function(x) {
    return((64 * (0.5 - x) ** 6))
}

compute_response <- function(coefficients, formula, experiment) {
    return(as.vector((model.matrix(formula, experiment) %*% coefficients)))
}

add_noise <- function(design, sd = 1) {
    return(as.vector(design$Y + rnorm(nrow(design), mean = 0, sd = sd)))
}

compare_fit <- function(coefficients, sample, formula, response_formula, design_a, design_b, sample_size = 100, sd = 1) {
    return(c(sqrt(sum(((compute_response(coefficients, formula, sample)) -
                        predict(lm(response_formula, design_a),
                               sample)) ^ 2)),
             sqrt(sum(((compute_response(coefficients, formula, sample)) -
                        predict(lm(response_formula, design_b),
                               sample)) ^ 2))))
}

compare_post_anova_fit <- function(coefficients, sample, formula, response_formula, design_a, design_b, sample_size = 100, sd = 1, prf_threshold = 0.0001) {
    regression_summary_a <- summary(aov(response_formula, design_a))

    prf_values_a <- as.data.frame(regression_summary_a[[1]])["Pr(>F)"]
    names(prf_values_a) <- c("values")

    refitted_formula_a <- formula(paste("Y ~", paste(rownames(subset(prf_values_a, values <= prf_threshold)), sep = "", collapse = " + ")))

    regression_summary_b <- summary(aov(response_formula, design_b))

    prf_values_b <- as.data.frame(regression_summary_b[[1]])["Pr(>F)"]
    names(prf_values_b) <- c("values")

    refitted_formula_b <- formula(paste("Y ~", paste(rownames(subset(prf_values_b, values <= prf_threshold)), sep = "", collapse = " + ")))

    return(c(sqrt(sum(((compute_response(coefficients, formula, sample)) -
                       predict(lm(refitted_formula_a, design_a),
                               sample)) ^ 2)),
            sqrt(sum(((compute_response(coefficients, formula, sample)) -
                      predict(lm(refitted_formula_b, design_b),
                              sample)) ^ 2))))
}

# https://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions
#return(sum((coefficients - unname(coef(lm(Y ~., design)))) / sqrt((data.frame(summary(lm(Y ~., design))["coefficients"])[, 2] ^ 2))))
compare_coefficients <- function(coefficients, formula, design) {
    return(sqrt(sum((coefficients - unname(coef(lm(formula, design)))) ^ 2)))
}

compare_post_anova_coefficients <- function(coefficients, formula, design_a, design_b, prf_threshold = 0.0001) {
    regression_summary_a <- summary(aov(formula, design_a))

    prf_values_a <- as.data.frame(regression_summary_a[[1]])["Pr(>F)"]
    names(prf_values_a) <- c("values")

    refitted_formula_a <- formula(paste("Y ~", paste(rownames(subset(prf_values_a, values <= prf_threshold)), sep = "", collapse = " + ")))

    regression_summary_b <- summary(aov(response_formula, design_b))

    prf_values_b <- as.data.frame(regression_summary_b[[1]])["Pr(>F)"]
    names(prf_values_b) <- c("values")

    refitted_formula_b <- formula(paste("Y ~", paste(rownames(subset(prf_values_b, values <= prf_threshold)), sep = "", collapse = " + ")))

    named_coefficients <- data.frame(t(coefficients))
    names(named_coefficients) <- names(coef(lm(formula, design_a)))

    named_refitted_coefficients_a <- data.frame(t(coef(lm(refitted_formula_a, design_a))))
    names(named_refitted_coefficients_a) <- names(coef(lm(refitted_formula_a, design_a)))

    named_refitted_coefficients_b <- data.frame(t(coef(lm(refitted_formula_b, design_b))))
    names(named_refitted_coefficients_b) <- names(coef(lm(refitted_formula_b, design_b)))

    coefficient_data <- bind_rows(named_coefficients,
                                  named_refitted_coefficients_a,
                                  named_refitted_coefficients_b)

    coefficient_data[is.na(coefficient_data)] <- 0.0

    #return(c(sqrt(sum((coefficients - unname(coef(lm(refitted_formula_a, design_a)))) ^ 2)),
    #         sqrt(sum((coefficients - unname(coef(lm(refitted_formula_b, design_b)))) ^ 2))))
    return(c(sqrt(sum((as.numeric(coefficient_data[1, ]) - as.numeric(coefficient_data[2, ])) ^ 2)),
             sqrt(sum((as.numeric(coefficient_data[1, ]) - as.numeric(coefficient_data[3, ])) ^ 2))))
}

compare_real_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(c(sum(prf_values[abs(coefficients) > coefficient_limits$min])))
}

compare_noise_coefficients <- function(coefficients, coefficient_limits, formula, design) {
    prf_values <- unname(summary(lm(formula, design))[4][["coefficients"]][,4])
    return(sum(prf_values[abs(coefficients) <= coefficient_limits$min]))
}

lin_data <- factor_cdf(linear_function, name = "Linear Factors")
lin_cdf <- subset(lin_data, name == "Linear Factors CDF")

quad_data <- factor_cdf(quadratic_function, name = "Quadratic Factors")
quad_cdf <- subset(quad_data, name == "Quadratic Factors CDF")

cube_data <- factor_cdf(cubic_function, name = "Cubic Factors")
cube_cdf <- subset(cube_data, name == "Cubic Factors CDF")
#+END_SRC

#+RESULTS:
#+begin_example

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector

Warning messages:
1: In bind_rows_(x, .id) : Unequal factor levels: coercing to character
2: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
3: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
4: In bind_rows_(x, .id) :
  binding character and factor vector, coercing into character vector
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf, cube_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       geom_line(size = 1.3) +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figureAI6GlZ.png]]

***** Generating Data                                          :noexport:
****** Linear Model
******* Generating Biased & Unbiased Samples
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R* :eval no-export :exports none
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 60
samples <- (1 * factors) + 10
runs <- 100
coefficient_number <- factors + 1

normal_samples_ratio <- 0.08

coefficient_probability <- 0.15
coefficient_variability <- list(max = 7, min = 1)
coefficient_noise = 0.01

noise_sd <- 0.3
fit_comparison_sample_size <- 1000

prf_threshold = 0.001

factor_names <- paste("x", 1:factors, sep = "")

formula <- formula(paste("~ ",
                        paste(factor_names,
                              sep = "",
                              collapse = " + "),
                        sep = ""))

response_formula <- formula(paste("Y ~ ",
                            paste(factor_names,
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(coefficient_number,
                              ifelse(runif(1) > (1.0 - coefficient_probability),
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))


    design$Y <- compute_response(coefficients, formula, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(lin_cdf,
                                                                               samples))))

    biased_design$Y <- compute_response(coefficients, formula, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         design),
                                                               compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         biased_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           design),
                                                                compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           biased_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               design),
                                                          compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               biased_design)),
                                 coefficient_refit_distance = compare_post_anova_coefficients(coefficients,
                                                                                              response_formula,
                                                                                              design,
                                                                                              biased_design,
                                                                                              prf_threshold = prf_threshold),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   design,
                                                                   biased_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 linear_refit_distance = compare_post_anova_fit(coefficients,
                                                                                fit_comparison_sample,
                                                                                formula,
                                                                                response_formula,
                                                                                design,
                                                                                biased_design,
                                                                                sample_size = fit_comparison_sample_size,
                                                                                sd = noise_sd,
                                                                                prf_threshold = prf_threshold),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- rbind(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
#+end_example

******* Generating Federov with Uniform & Biased Samples
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 60
samples <- (1 * factors) + 10
federov_samples <- 1000

runs <- 100

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(coefficient_number,
                              ifelse(runif(1) > (1.0 - coefficient_probability),
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_response(coefficients, formula, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(lin_cdf,
                                                                                federov_samples))))

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_response(coefficients, formula, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         federov_design),
                                                               compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         biased_federov_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           federov_design),
                                                                compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           biased_federov_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               federov_design),
                                                          compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               biased_federov_design)),
                                 coefficient_refit_distance = compare_post_anova_coefficients(coefficients,
                                                                                              response_formula,
                                                                                              federov_design,
                                                                                              biased_federov_design,
                                                                                              prf_threshold = prf_threshold),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   federov_design,
                                                                   biased_federov_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 linear_refit_distance = compare_post_anova_fit(coefficients,
                                                                                fit_comparison_sample,
                                                                                formula,
                                                                                response_formula,
                                                                                federov_design,
                                                                                biased_federov_design,
                                                                                sample_size = fit_comparison_sample_size,
                                                                                sd = noise_sd,
                                                                                prf_threshold = prf_threshold),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- rbind(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
#+end_example

****** Quadratic Model
******* Generating Biased & Unbiased Samples
You can load one of the datasets from the repository:

#+HEADER: :results output :session *R* :eval no-export :exports none
#+BEGIN_SRC R
data <- read.csv("dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:
: Error in file(file, "rt") : cannot open the connection
: In addition: Warning message:
: In file(file, "rt") :
:   cannot open file 'dopt_sampling_tests/comparison_federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv': No such file or directory

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

data <- NULL

factors <- 60
samples <- (2 * factors) + 10
runs <- 100

coefficient_number <- (2 * factors) + 1

normal_samples_ratio <- 0.08
#normal_samples_ratio <- 0

coefficient_probability <- 0.15
coefficient_variability <- list(max = 7, min = 1)
coefficient_noise = 0.01
noise_sd <- 0.3
fit_comparison_sample_size <- 1000

prf_threshold = 0.001

factor_names <- paste("x", 1:factors, sep = "")

formula <- formula(paste("~ ",
                        paste(factor_names,
                              sep = "",
                              collapse = " + "),
                        " + ",
                        paste("I(",
                              factor_names,
                              "^2)",
                              sep = "",
                              collapse = " + "),
                        sep = ""))

response_formula <- formula(paste("Y ~ ",
                            paste(factor_names,
                                  sep = "",
                                  collapse = " + "),
                            " + ",
                            paste("I(",
                                  factor_names,
                                  "^2)",
                                  sep = "",
                                  collapse = " + "),
                            sep = ""))

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(coefficient_number,
                              ifelse(runif(1) > (1.0 - coefficient_probability),
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                              runif(fit_comparison_sample_size))))

    design <- get_runif_coded_design(data.frame(replicate(factors, runif(samples))))


    design$Y <- compute_response(coefficients, formula, design)
    design$Y <- add_noise(design, sd = noise_sd)

    biased_design <- get_runif_coded_design(data.frame(replicate(factors,
                                                                 sample_factor(quad_cdf,
                                                                               as.integer((1 - normal_samples_ratio) * samples)))))

    if(normal_samples_ratio > 0) {
        biased_design <- rbind(biased_design, get_runif_coded_design(data.frame(replicate(factors,
                                                                                          rnorm(as.integer(normal_samples_ratio * samples),
                                                                                                mean = 0.5, sd = 0.04)))))
    }

    biased_design$Y <- compute_response(coefficients, formula, biased_design)
    biased_design$Y <- add_noise(biased_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, design)$determinant,
                                       eval.design(formula, biased_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         design),
                                                               compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         biased_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           design),
                                                                compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           biased_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               design),
                                                          compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               biased_design)),
                                 coefficient_refit_distance = compare_post_anova_coefficients(coefficients,
                                                                                              response_formula,
                                                                                              design,
                                                                                              biased_design,
                                                                                              prf_threshold = prf_threshold),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   design,
                                                                   biased_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 linear_refit_distance = compare_post_anova_fit(coefficients,
                                                                                fit_comparison_sample,
                                                                                formula,
                                                                                response_formula,
                                                                                design,
                                                                                biased_design,
                                                                                sample_size = fit_comparison_sample_size,
                                                                                sd = noise_sd,
                                                                                prf_threshold = prf_threshold),
                                 names = c("Uniform Sample", "Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- rbind(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

******* Generating Federov with Uniform & Biased Samples
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(rsm)
library(dplyr)

factors <- 60
samples <- (2 * factors) + 10
federov_samples <- 1000

runs <- 100

for (i in 1:runs) {
    if (i %% 10 == 0) {
        print(i)
    }

    coefficients <- replicate(coefficient_number,
                              ifelse(runif(1) > (1.0 - coefficient_probability),
                                     ifelse(runif(1) >= 0.5,
                                            runif(1,
                                                  min = -coefficient_variability$max,
                                                  max = -coefficient_variability$min),
                                            runif(1,
                                                  min = coefficient_variability$min,
                                                  max = coefficient_variability$max)),
                                     rnorm(1, mean = 0, sd = coefficient_noise)))

    fit_comparison_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                               runif(fit_comparison_sample_size))))


    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  runif(federov_samples))))

    federov_design <- optFederov(formula,
                                 federov_sample,
                                 nTrials = samples)$design

    federov_design$Y <- compute_response(coefficients, formula, federov_design)
    federov_design$Y <- add_noise(federov_design, sd = noise_sd)

    federov_sample <- get_runif_coded_design(data.frame(replicate(factors,
                                                                  sample_factor(quad_cdf,
                                                                                as.integer((1 - normal_samples_ratio) * federov_samples)))))

    if(normal_samples_ratio > 0) {
        federov_sample <- rbind(federov_sample, get_runif_coded_design(data.frame(replicate(factors,
                                                                                            rnorm(as.integer(normal_samples_ratio * federov_samples),
                                                                                                  mean = 0.5, sd = 0.04)))))
    }

    biased_federov_design <- optFederov(formula,
                                        federov_sample,
                                        nTrials = samples)$design

    biased_federov_design$Y <- compute_response(coefficients, formula, biased_federov_design)
    biased_federov_design$Y <- add_noise(biased_federov_design, sd = noise_sd)

    new_experiment <- data.frame(D = c(eval.design(formula, federov_design)$determinant,
                                       eval.design(formula, biased_federov_design)$determinant),
                                 real_coefficients_prf_sum = c(compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         federov_design),
                                                               compare_real_coefficients(coefficients,
                                                                                         coefficient_variability,
                                                                                         response_formula,
                                                                                         biased_federov_design)),
                                 noise_coefficients_prf_sum = c(compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           federov_design),
                                                                compare_noise_coefficients(coefficients,
                                                                                           coefficient_variability,
                                                                                           response_formula,
                                                                                           biased_federov_design)),
                                 coefficient_distance = c(compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               federov_design),
                                                          compare_coefficients(coefficients,
                                                                               response_formula,
                                                                               biased_federov_design)),
                                 coefficient_refit_distance = compare_post_anova_coefficients(coefficients,
                                                                                              response_formula,
                                                                                              federov_design,
                                                                                              biased_federov_design,
                                                                                              prf_threshold = prf_threshold),
                                 linear_fit_distance = compare_fit(coefficients,
                                                                   fit_comparison_sample,
                                                                   formula,
                                                                   response_formula,
                                                                   federov_design,
                                                                   biased_federov_design,
                                                                   sample_size = fit_comparison_sample_size,
                                                                   sd = noise_sd),
                                 linear_refit_distance = compare_post_anova_fit(coefficients,
                                                                                fit_comparison_sample,
                                                                                formula,
                                                                                response_formula,
                                                                                federov_design,
                                                                                biased_federov_design,
                                                                                sample_size = fit_comparison_sample_size,
                                                                                sd = noise_sd,
                                                                                prf_threshold = prf_threshold),
                                 names = c("Federov with Uniform Sample", "Federov with Biased Sample"),
                                 id = i)

    if (is.null(data)) {
        data <- new_experiment
    } else {
        data <- rbind(data, new_experiment)
    }
}
#+END_SRC

#+RESULTS:
#+begin_example

[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100
There were 50 or more warnings (use warnings() to see the first 50)
#+end_example

****** Test Plots
******* D-Criterion
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figurePioWHO.png]]

******* Coefficients
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :eval no-export :exports none
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficients = abs(coefficients))

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figureZGTGas.png]]

******* Pr(>F) Sums
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 0.2)) +
       scale_x_continuous(name = "Jittered Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figure52Hc47.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-sI8n2m/figuretIUiUc.png]]

******* Coefficient Euclidian Distance
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(coefficient_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$coefficient_refit_distance -
                                                               filter(data, names == "Federov with Biased Sample")$coefficient_refit_distance),
                        id = seq(1:(nrow(subset(data, names == "Uniform Sample")))))

ggplot(plot_data, aes(x = id, y = coefficient_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Dist. Diff. Ratio (Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figuresH4zQj.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = coefficient_refit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figureJUFBNj.png]]

******* Response Vector Euclidean Distance
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_fit_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$linear_refit_distance -
                                                              filter(data, names == "Federov with Biased Sample")$linear_refit_distance),
                        id = seq(1:(nrow(data) / 4)))

ggplot(plot_data, aes(x = id, y = linear_fit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Fit Difference Ratio (Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figureHwF7Gj.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_fit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figurelF51xM.png]]

******* Post-ANOVA+Refit Response Vector Euclidean Distance
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_refit_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$linear_refit_distance -
                                                                filter(data, names == "Federov with Biased Sample")$linear_refit_distance),
                        id = seq(1:nrow(filter(data, names == "Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_refit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Post-ANOVA-Fit Disance Difference (Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figure2hRjTT.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_refit_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$linear_refit_distance -
                                                                filter(data, names == "Federov with Biased Sample")$linear_refit_distance),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_refit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Post-ANOVA-Fit Disance Difference (Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-PE51oT/figureUGJoWE.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

ggplot(data, aes(x = 1, y = linear_refit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.4) +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figure7dS33H.png]]

****** Saving Data
#+HEADER: :results output :session *R* :eval no-export :exports none
#+BEGIN_SRC R
write.csv(data, "dopt_sampling_tests/quadratic/prft_0.001_noise_0.3.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

***** Sampling Strategies
A practical concern derived from the limitation on the size of samples to
choose a design from is the large amount of time it takes to evaluate a
relatively large number of candidate configurations. Each search space also had
several constraints on factor levels, reducing the valid search space and
slowing the process of finding candidates to test. Another concern is the
intuition that the \todo{improve, add reference} quality of the designs
constructed from uniformly sampled candidates should always be poorer than in
the case where designs are selected from complete search spaces. This property
should be more pronounced when certain types of performance models are selected,
such as quadratic models, due to the fact that practically none of the points
sampled uniformly from a high-dimensional search space would be in the central
region of the space.

#+NAME: tab:designs-models
#+ATTR_LATEX: :booktabs t :align ll
#+CAPTION: Design construction techniques and target performance models
| Sampling & Construction Strategy | Performance Models |
|----------------------------------+--------------------|
| Uniform                          | Linear & quadratic |
| Biased                           | Linear             |
| Biased + Normal                  | Quadratic          |
| Uniform + /Federov's/            | Linear & quadratic |
| Biased + /Federov's/             | Linear             |
| Biased + Normal + /Federov's/    | Quadratic          |

#+NAME: fig:cdfs
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 560 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
plot_data <- bind_rows(lin_cdf, quad_cdf)

ggplot(plot_data, aes(x = x, y = y)) +
       facet_wrap(name ~ ., ncol = 2) +
       ylab("P(X < x)") +
       geom_line(size = 1.3) +
       theme_bw(base_size = 28)
#+END_SRC
#+CAPTION: Cumulative Distribution Functions used for Biased Samples
#+LABEL: fig:cdfs
#+ATTR_LATEX: :width 0.55\textwidth
#+RESULTS: fig:cdfs
[[file:/tmp/babel-PE51oT/figureUmlGlZ.png]]

These concerns motivated the development of a methodology to evaluate the
quality of the designs produced by \todo{add reference} /Fedorov's algorithm/,
the chosen D-Optimal construction technique, in the cases where designs were
selected from samples with different properties. We studied design efficiency,
measured by the /D-criterion/, for designs constructed using the performance
models and construction techniques listed in Table\nbsp{}[[tab:designs-models]].
Sampling strategies consisted of obtaining samples using the /cumulative
distribution functions/ in Figure\nbsp{}[[fig:cdfs]], and adding samples from a
normal distribution to the quadratic model biased samples.

***** Comparing Sampling Strategies
In addition to the D-Criterion value, we also measured the quality of the
designs generated by each technique using the /euclidean distance/ between the
vector of responses generated by different models fitted using the results of a
design's experiments. We compared the results of a model using all factors and
of a re-fitted model using only significant factors identified with ANOVA. We
performed this comparison for all sampling strategies in
Table\nbsp{}[[tab:designs-models]].

We decided to sample new functions for each repetition of our experiments with
D-Criteria and model fits. This was initially done by sampling values for the
coefficients for each model term, generating a new function that was used to
compute the results corresponding to the factor values of each of a design's
experiments. Coefficient sampling is described in more detail in the following
section.

****** Sampling Function Coefficients
Consider the functions
$f_{\boldsymbol{\alpha},\boldsymbol{\beta}}\colon\mathbf{X}\in\mathbb{R}^n\to\mathbb{R}$
of the form

\[
f_{\boldsymbol{\alpha},\boldsymbol{\beta}}(\mathbf{X}) = \alpha_0 + \sum\limits^{n}_{i = 1}{\left(\alpha_{i}x_{i} + \beta_{i}x_{i}^{2}\right)} + \varepsilon\text{,}
\]

where $x \in [-1, 1] \; \forall x \in \mathbf{X}$, with /coefficients of linear
terms/ $\boldsymbol{\alpha} \in \mathbb{R}^{n + 1}$, /coefficients of quadratic
terms/ $\boldsymbol{\beta} \in \mathbb{R}^{n}$, and normally distributed error
$\varepsilon$. The procedure we used to sample new coeficients at each
experiment starts by choosing which of the $\alpha_i \in \boldsymbol{\alpha}$,
$\beta_i \in \boldsymbol{\beta}$ will be significant. With our target scenario
in mind, each coefficient had a 15% chance of being significant. A uniformly
sampled value in the interval $[-x_{\text{sup}}, -x_{\text{inf}}] \cup
[x_{\text{inf}}, x_{\text{sup}}]$ is chosen for each significant coefficient,
where $x_{\text{sup}},x_{\text{inf}} \in \mathbb{R}$,
$x_{\text{sup}}>x_{\text{inf}}>0$. Small normally distributed values with mean
zero and standard deviation $x_{\text{sd}}$ are assigned to the other factors.
Figure\nbsp{}[[fig:sampled-coeff]] shows one coefficient sample obtained by this
process, where $|\boldsymbol{\alpha}| + |\boldsymbol{\beta}| =
2|\boldsymbol{X}| + 1 = 121$, $x_{\text{inf}} = 1$, $x_{\text{sup}} = 7$, and
$x_{\text{sd}} = 0.04$.

#+NAME: fig:sampled-coeff
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

factors <- 60
coefficient_number <- (2 * factors) + 1
coefficient_probability <- 0.15
coefficient_variability <- list(max = 7, min = 1)
coefficient_noise = 0.04

coefficients <- replicate(coefficient_number,
                          ifelse(runif(1) > (1.0 - coefficient_probability),
                                  ifelse(runif(1) >= 0.5,
                                        runif(1,
                                              min = -coefficient_variability$max,
                                              max = -coefficient_variability$min),
                                        runif(1,
                                              min = coefficient_variability$min,
                                              max = coefficient_variability$max)),
                                  rnorm(1, mean = 0, sd = coefficient_noise)))

plot_data <- data.frame(coefficients = coefficients)

ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
       geom_point() +
       xlab("Coefficient") +
       ylab("Sampled Value") +
       xlim(0, 121) +
       theme_bw(base_size = 28)

#+END_SRC
#+CAPTION: Sampled coefficients
#+LABEL: fig:sampled-coeff
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS: fig:sampled-coeff
[[file:/tmp/babel-PE51oT/figureEXAy8N.png]]

****** Comparing Response Vectors
Consider a design $\xi_{n,k}$ with factors $x_{k1},\dots,x_{kn}$, experiments
$\boldsymbol{X}_1, \dots, \boldsymbol{X}_k$, and design matrix given by
#+begin_export latex
\[
\xi_{n,k} =
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{k1} & x_{k2} & x_{k3} & \dots  & x_{kn}
\end{bmatrix},
\]
#+end_export
where the $k$ experiments were chosen by Fedorov's algorithm from a relatively
large sample of size $K$.

Using a function $f_{\boldsymbol{\alpha},\boldsymbol{\beta}}$ sampled as
described in the previous section, we can compute the value or /response vector/
$\boldsymbol{y} = (y_1,\dots,y_k)$ for each experiment vector $\boldsymbol{X_i}
= (x_{i1},\dots,x_{in})$, obtaining the design $\xi^{\prime}_{n,k}$ with design
matrix given by
#+begin_export latex
\[
\xi^{\prime}_{n,k} =
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} & f_{\boldsymbol{\alpha},\boldsymbol{\beta}}(\boldsymbol{X}_1) \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} & f_{\boldsymbol{\alpha},\boldsymbol{\beta}}(\boldsymbol{X}_2)\\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    x_{k1} & x_{k2} & x_{k3} & \dots  & x_{kn} & f_{\boldsymbol{\alpha},\boldsymbol{\beta}}(\boldsymbol{X}_k)
\end{bmatrix}.
\]
#+end_export

We then use $\xi^{\prime}_{n,k}$ to perform a linear regression with model
\[
f_{\boldsymbol{\alpha},\boldsymbol{\beta}}(\boldsymbol{X}_i) = y_i = \alpha^{\prime}_0 +
\boldsymbol{X}_i^{\top}(\alpha^{\prime}_{1}, \dots, \alpha^{\prime}_{n}) +
\left(\boldsymbol{X}^{2}_i\right)^{\top}(\beta^{\prime}_{1}, \dots, \beta^{\prime}_{n}) +
\varepsilon\text{,}
\]
obtaining coefficients that define the function
$f_{\boldsymbol{\alpha}^{\prime},\boldsymbol{\beta}^{\prime}}$ which
approximates $f_{\boldsymbol{\alpha},\boldsymbol{\beta}}$.
Evaluating the fitted model in the original large sample of size $K$ gives
the response vector
\[
\boldsymbol{Y}^{\prime} = (f_{\boldsymbol{\alpha}^{\prime},\boldsymbol{\beta}^{\prime}}
(\boldsymbol{X}_1), \dots, f_{\boldsymbol{\alpha}^{\prime},\boldsymbol{\beta}^{\prime}}
(\boldsymbol{X}_K))\text{,}
\]
and evaluating the true sampled function gives the response vector
\[
\boldsymbol{Y} = (f_{\boldsymbol{\alpha},\boldsymbol{\beta}}
(\boldsymbol{X}_1), \dots, f_{\boldsymbol{\alpha},\boldsymbol{\beta}}
(\boldsymbol{X}_K))\text{.}
\]
The /response vector distance/ $d(\mathbf{Y}, \mathbf{Y}^{\prime}) =
\|\mathbf{Y} - \mathbf{Y}^{\prime}\|^{2}$ provides a measure of how well the
fitted model approximates the real function in the initial sample of size $K$,
and enables the comparison of two models for the same data.
****** Comparing D-Criteria
Consider a D-Optimal design $\xi_{n,k}$ constructed using Fedorov's algorithm,
with $n$ factors, experiments $\boldsymbol{X}_1, \dots, \boldsymbol{X}_k$ chosen
from a large candidate set of $K$ experiments, and design matrix given by
#+begin_export latex
\[
\xi_{n,k} =
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{k1} & x_{k2} & x_{k3} & \dots  & x_{kn}
\end{bmatrix}.
\]
#+end_export
The quadratic performance model used to construct this design was
#+begin_export latex
\[
\boldsymbol{y} = \boldsymbol{\beta{}M} + \varepsilon\text{,}
\]
#+end_export
where $\boldsymbol{\beta}$ is the vector of $2n + 1$ coefficients for linear,
quadratic, and intercept terms, $\varepsilon$ is the normally distributed error
term, and $\boldsymbol{M}$ is the /model matrix/, a $(2n + 1)\times{}k$ matrix
containing each factor level as it would appear in the performance model, given by
#+begin_export latex
\[
\boldsymbol{M} =
\begin{bmatrix}
    1 & x_{11} & \dots  & x_{1n} & x_{11}^{2} & \dots  & x_{1n}^2 \\
    1 & x_{21} & \dots  & x_{2n} & x_{21}^2 & \dots  & x_{2n}^2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{k1} & \dots  & x_{kn} & x_{k1}^2 & \dots  & x_{kn}^2
\end{bmatrix}.
\]
#+end_export

The model matrix $\boldsymbol{M}$ is used to compute the /D-Criterion/
$D(\boldsymbol{M}_{2n + 1, k}) \in [0,1]$. The D-Criterion of a design depends
on the performance model used to construct it, and is defined as
#+begin_export latex
\[
D(\boldsymbol{M}_{2n + 1, k}) = \left|\dfrac{\boldsymbol{M}^{\top}\boldsymbol{M}}{k}\right|^{\left(\dfrac{1}{2n + 1}\right)}\text{.}
\]
#+end_export
***** Experiments & Results
#+HEADER: :results output :session *R* :eval no-export :exports none
#+BEGIN_SRC R
#data_quadratic_big_prf<- read.csv("dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_60_factors_130_exp_prft_0.05_random_coeff_few_sig_post_anova.csv", header = TRUE)
#data_quadratic_small_prf<- read.csv("dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_60_factors_130_exp_prft_0.0001_random_coeff_few_sig_post_anova.csv", header = TRUE)

#data_quadratic_big_prf_normal <- read.csv("dopt_sampling_tests/quadratic_comparison_federov_biased_1000_samples_100_repetitions_60_factors_130_exp_prft_0.05_random_coeff_few_sig_norm_post_anova_fit_coef.csv", header = TRUE)
data_quadratic_small_prf_normal <- read.csv("dopt_sampling_tests/quadratic/prft_0.0001_noise_0.3.csv", header = TRUE)

#data_linear_big_prf <- read.csv("dopt_sampling_tests/linear_comparison_federov_biased_1000_samples_100_repetitions_60_factors_70_exp_prft_0.05_random_coeff_few_sig_post_anova_fit_coef.csv", header = TRUE)
data_linear_small_prf <- read.csv("dopt_sampling_tests/linear/prft_0.0001_noise_0.3.csv", header = TRUE)

#data_quadratic_big_prf$experiment <- "Quadratic Model"
#data_quadratic_small_prf$experiment <- "Quadratic Model"

#data_quadratic_big_prf_normal$experiment <- "Quadratic Model + Normal"
data_quadratic_small_prf_normal$experiment <- "Quadratic Model + Normal"

#data_linear_big_prf$experiment <- "Linear Model"
data_linear_small_prf$experiment <- "Linear Model"

data <- rbind(
  #data_quadratic_big_prf,
  #data_quadratic_small_prf,
  #data_quadratic_big_prf_normal,
  data_quadratic_small_prf_normal,
  #data_linear_big_prf,
  data_linear_small_prf
)
#+END_SRC

#+RESULTS:

The experiments described in this section use our Design of Experiments approach
to optimize sampled linear and quadratic functions by identifying their most
significant factors. First, we used the function coefficient sampling strategy
described in Section\nbsp{}[[Sampling Function Coefficients]] to generate target
objective functions with large numbers of factors, but with few significant
ones. Then, we used the biased sampling strategies described in
Section\nbsp{}[[Comparing Sampling Strategies]] to generate sets of candidate
points, which were used to construct D-Optimal designs using Fedorov's
algorithm. The response vectors of each design were evaluated using the
generated functions, which enabled running ANOVA tests to identify the most
significant factors. Finally, new models were fit to the design's data using
only the identified factors. The relevant parameters of each experiment are
described in Table\nbsp{}[[tab:exp-parameters]].

#+NAME: tab:exp-parameters
#+CAPTION: Parameters of experiments with different regression models
#+ATTR_LATEX: :booktabs t :align p{0.34\linewidth}p{0.22\linewidth}p{0.2\linewidth}
|------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------|
| Parameter                                            |                                                                 Linear Model |                                    Quadratic Model |
|------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------|
| Factors                                              |                                                                           60 |                                                 60 |
| \rowcolor{gray!25} Design Size                       |                                                                           70 |                                                130 |
| Fedorov Samples                                      |                                                                         1000 |                                               1000 |
| Testing Samples                                      |                                                                         1000 |                                               1000 |
| \rowcolor{gray!25} Normal Samples Ratio              |                                                                           -- |                                                 8% |
| \rowcolor{gray!25} Normal Samples Standard Deviation |                                                                           -- |                                               0.04 |
| \rowcolor{gray!25} Sampling Strategies               |                                       \mbox{Uniform, Biased}, \mbox{Fedorov} |     \mbox{Uniform, Biased}, \mbox{Normal}, Fedorov |
|------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------|
| \rowcolor{gray!25} Sampled Objective Function        | As in Section\nbsp{}[[Sampling Function Coefficients]], $\boldsymbol{\beta} = 0$ | As in Section\nbsp{}[[Sampling Function Coefficients]] |
| Noise Standard Deviation                             |                                                                          0.3 |                                                0.3 |
| \rowcolor{gray!25} Regression Model                  |                                                                       Linear |                                          Quadratic |
| $\text{P}_\text{r}(>\text{F})$ Threshold             |                                                                 0.0001, 0.05 |                                       0.0001, 0.05 |
|------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------|
| \rowcolor{gray!25} Coefficients                      |                                                                           61 |                                                121 |
| Coefficient Significance Probability                 |                                                                          15% |                                                15% |
| Coefficient Noise Standard Deviation                 |                                                                         0.01 |                                               0.01 |
| Coefficient Range                                    |                                                 $x_{inf} = 1$, $x_{sup} = 7$ |                       $x_{inf} = 1$, $x_{sup} = 7$ |
|------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------|
| Repetitions                                          |                                                                          100 |                                                100 |
|------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------|

****** D-Criterion
Since the evaluation of the D-Criterion does not depend on the actual response
values, we can group the experiments using the same regression model but
different values of $\text{P}_{\text{r}}(>\text{F})$, as well as the experiments
with and without added normal samples.

We see in Figures\nbsp{}[[fig:comparison-D-linear]] and [[fig:comparison-D-quadratic]]
that the D-Criteria was much higher for experiments using the linear regression
model, and that the biased samples given to Fedorov's algorithm help finding
higher values of D.

#+NAME: fig:comparison-D-linear
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- subset(data, experiment == "Linear Model")
plot_data$names <- factor(plot_data$names, levels = c(
                                             "Federov with Uniform Sample",
                                             "Federov with Biased Sample",
                                             "Uniform Sample",
                                             "Biased Sample"
                                           ))
ggplot(plot_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       ylab("D-Criterion") +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 28) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC
#+LABEL: fig:comparison-D-linear
#+ATTR_LATEX: :width 0.55\textwidth
#+CAPTION: Comparison of D-Criterion values for linear models
#+RESULTS: fig:comparison-D-linear
[[file:/tmp/babel-PE51oT/figureXgK50w.png]]

#+NAME: fig:comparison-D-quadratic
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- subset(data, experiment == "Quadratic Model + Normal")
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))
ggplot(plot_data, aes(x = 1, y = D)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, 1)) +
       ylab("D-Criterion") +
       scale_x_continuous(name = "Jittered Experiments") +
       geom_jitter(height = 0, alpha = 0.3) +
       theme_bw(base_size = 28) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC
#+LABEL: fig:comparison-D-quadratic
#+CAPTION: Comparison of D-Criterion values for quadratic models
#+ATTR_LATEX: :width 0.55\textwidth
#+RESULTS: fig:comparison-D-quadratic
[[file:/tmp/babel-PE51oT/figure7ZGIgg.png]]

****** Coefficients                                           :noexport:
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
  library(ggplot2)

  factors <- 60
  coefficient_number <- (2 * factors) + 1
  coefficient_probability <- 0.15
  coefficient_variability <- list(max = 7, min = 1)
  coefficient_noise = 0.04

  coefficients <- replicate(coefficient_number,
                            ifelse(runif(1) > (1.0 - coefficient_probability),
                                    ifelse(runif(1) >= 0.5,
                                          runif(1,
                                                min = -coefficient_variability$max,
                                                max = -coefficient_variability$min),
                                          runif(1,
                                                min = coefficient_variability$min,
                                                max = coefficient_variability$max)),
                                    rnorm(1, mean = 0, sd = coefficient_noise)))

  plot_data <- data.frame(coefficients = coefficients)

  ggplot(plot_data, aes(x = 1:length(coefficients), y = coefficients)) +
         geom_point() +
         theme_bw(base_size = 20)

#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figurexdRnet.png]]

****** $\text{P}_{\text{r}}(>\text{F})$ Sums                  :noexport:
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data <- data.frame(real_coefficient_prf_sum_difference = (sort(filter(data, names == "Federov with Uniform Sample")$real_coefficients_prf_sum /
                                                                    filter(data, names == "Federov with Biased Sample")$real_coefficients_prf_sum)),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = real_coefficient_prf_sum_difference)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Real Coeff. Prf Sum Ratio (Federov Uniform / Biased)") +
       #ylim(c(0, 100)) +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figureQ2ItXA.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = real_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 0.001)) +
       scale_x_continuous(name = "Experiment") +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-sI8n2m/figureWtnK6f.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = noise_coefficients_prf_sum)) +
       facet_wrap(names ~ ., ncol = 2) +
       #ylim(c(0, 1)) +
       scale_x_continuous(name = "Experiment") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-sI8n2m/figuredPX5nr.png]]

****** Coefficient Euclidean Distance                         :noexport:
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data_linear_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_small_prf, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                   filter(data_linear_small_prf, names == "Federov with Biased Sample")$coefficient_distance),
                               id = seq(1:nrow(filter(data_linear_small_prf, names == "Federov with Uniform Sample"))))

plot_data_linear_small_prf$experiment = "Linear Model"

#plot_refit_data_linear_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_small_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                         filter(data_linear_small_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                     id = seq(1:nrow(filter(data_linear_small_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_linear_small_prf$experiment = "Linear Model + Re-fit"

plot_data_linear_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_big_prf, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                 filter(data_linear_big_prf, names == "Federov with Biased Sample")$coefficient_distance),
                                       id = seq(1:nrow(filter(data_linear_big_prf, names == "Federov with Uniform Sample"))))

plot_data_linear_big_prf$experiment = "Linear Model"

#plot_refit_data_linear_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_big_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                       filter(data_linear_big_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                             id = seq(1:nrow(filter(data_linear_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_linear_big_prf$experiment = "Linear Model + Re-fit"

plot_data_quadratic_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                      filter(data_quadratic_small_prf, names == "Federov with Biased Sample")$coefficient_distance),
                                            id = seq(1:nrow(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample"))))

plot_data_quadratic_small_prf$experiment = "Quadratic Model"

#plot_refit_data_quadratic_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                            filter(data_quadratic_small_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                  id = seq(1:nrow(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_small_prf$experiment = "Quadratic Model + Re-fit"

plot_data_quadratic_small_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                             filter(data_quadratic_small_prf_normal, names == "Federov with Biased Sample")$coefficient_distance),
                                                   id = seq(1:nrow(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample"))))

plot_data_quadratic_small_prf_normal$experiment = "Quadratic Model + Normal"

#plot_refit_data_quadratic_small_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                                   filter(data_quadratic_small_prf_normal, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                         id = seq(1:nrow(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_small_prf_normal$experiment = "Quadratic Model + Normal + Re-fit"

plot_data_quadratic_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                    filter(data_quadratic_big_prf, names == "Federov with Biased Sample")$coefficient_distance),
                                          id = seq(1:nrow(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample"))))

plot_data_quadratic_big_prf$experiment = "Quadratic Model"

#plot_refit_data_quadratic_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                          filter(data_quadratic_big_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                id = seq(1:nrow(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_big_prf$experiment = "Quadratic Model + Re-fit"

plot_data_quadratic_big_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                           filter(data_quadratic_big_prf_normal, names == "Federov with Biased Sample")$coefficient_distance),
                                                 id = seq(1:nrow(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample"))))

plot_data_quadratic_big_prf_normal$experiment = "Quadratic Model + Normal"

#plot_refit_data_quadratic_big_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                                 filter(data_quadratic_big_prf_normal, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                       id = seq(1:nrow(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_big_prf_normal$experiment = "Quadratic Model + Normal + Re-fit"

plot_data_small_prf <- rbind(plot_data_linear_small_prf,
                             #plot_refit_data_linear_small_prf,
                             plot_data_quadratic_small_prf,
                             #plot_refit_data_quadratic_small_prf,
                             plot_data_quadratic_small_prf_normal
                             #plot_refit_data_quadratic_small_prf_normal
                             )

plot_data_small_prf$experiment <- factor(plot_data_small_prf$experiment, levels = c("Linear Model",
                                                                                    "Quadratic Model",
                                                                                    "Quadratic Model + Normal"))
                                                                                    #"Linear Model + Re-fit",
                                                                                    #"Quadratic Model + Re-fit",
                                                                                    #"Quadratic Model + Normal + Re-fit"))

plot_data_big_prf <- rbind(plot_data_linear_big_prf,
                           #plot_refit_data_linear_big_prf,
                           plot_data_quadratic_big_prf,
                           #plot_refit_data_quadratic_big_prf,
                           plot_data_quadratic_big_prf_normal
                           #plot_refit_data_quadratic_big_prf_normal
                           )

plot_data_big_prf$experiment <- factor(plot_data_big_prf$experiment, levels = c("Linear Model",
                                                                                "Quadratic Model",
                                                                                "Quadratic Model + Normal"))
                                                                                #"Linear Model + Re-fit",
                                                                                #"Quadratic Model + Re-fit",
                                                                                #"Quadratic Model + Normal + Re-fit"))

ggplot(plot_data_small_prf, aes(x = id, y = coefficient_distance_difference)) +
       #facet_wrap(experiment ~ ., scale = "free", ncol = 3) +
       facet_wrap(experiment ~ ., ncol = 3) +
       ylim(c(-100, 100)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Fit Distance Difference (Federov Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dkewY0/figureMtutQk.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :exports results :eval no-export
ggplot(plot_data_big_prf, aes(x = id, y = coefficient_distance_difference)) +
       #facet_wrap(experiment ~ ., scale = "free", ncol = 3) +
       facet_wrap(experiment ~ ., ncol = 3) +
       ylim(c(-100, 100)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Fit Distance Difference (Federov Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dkewY0/figurebUTzaK.png]]

****** Coefficient Vector Distance
#+NAME:fig:coef-refit-results-small-prf
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data_linear_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_small_prf, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                   filter(data_linear_small_prf, names == "Federov with Biased Sample")$coefficient_distance),
                               id = seq(1:nrow(filter(data_linear_small_prf, names == "Federov with Uniform Sample"))))

plot_data_linear_small_prf$experiment = "Linear Model"

plot_refit_data_linear_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_small_prf, names == "Federov with Uniform Sample")$coefficient_refit_distance -
                                                                         filter(data_linear_small_prf, names == "Federov with Biased Sample")$coefficient_refit_distance),
                                     id = seq(1:nrow(filter(data_linear_small_prf, names == "Federov with Uniform Sample"))))

plot_refit_data_linear_small_prf$experiment = "Linear Model + Re-fit"

#plot_data_linear_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_big_prf, names == "Federov with Uniform Sample")$coefficient_distance -
#                                                                 filter(data_linear_big_prf, names == "Federov with Biased Sample")$coefficient_distance),
#                                       id = seq(1:nrow(filter(data_linear_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_data_linear_big_prf$experiment = "Linear Model"
#
#plot_refit_data_linear_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_linear_big_prf, names == "Federov with Uniform Sample")$coefficient_refit_distance -
#                                                                       filter(data_linear_big_prf, names == "Federov with Biased Sample")$coefficient_refit_distance),
#                                             id = seq(1:nrow(filter(data_linear_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_linear_big_prf$experiment = "Linear Model + Re-fit"

#plot_data_quadratic_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample")$coefficient_distance -
#                                                                      filter(data_quadratic_small_prf, names == "Federov with Biased Sample")$coefficient_distance),
#                                            id = seq(1:nrow(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample"))))
#
#plot_data_quadratic_small_prf$experiment = "Quadratic Model"
#
#plot_refit_data_quadratic_small_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample")$coefficient_refit_distance -
#                                                                            filter(data_quadratic_small_prf, names == "Federov with Biased Sample")$coefficient_refit_distance),
#                                                  id = seq(1:nrow(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_small_prf$experiment = "Quadratic Model + Re-fit"
#
plot_data_quadratic_small_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                             filter(data_quadratic_small_prf_normal, names == "Federov with Biased Sample")$coefficient_distance),
                                                   id = seq(1:nrow(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample"))))

plot_data_quadratic_small_prf_normal$experiment = "Quadratic Model + Normal"

plot_refit_data_quadratic_small_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample")$coefficient_refit_distance -
                                                                                   filter(data_quadratic_small_prf_normal, names == "Federov with Biased Sample")$coefficient_refit_distance),
                                                         id = seq(1:nrow(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample"))))

plot_refit_data_quadratic_small_prf_normal$experiment = "Quadratic Model + Normal + Re-fit"

plot_data_quadratic_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample")$coefficient_distance -
                                                                    filter(data_quadratic_big_prf, names == "Federov with Biased Sample")$coefficient_distance),
                                          id = seq(1:nrow(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample"))))

#plot_data_quadratic_big_prf$experiment = "Quadratic Model"
#
#plot_refit_data_quadratic_big_prf <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample")$coefficient_refit_distance -
#                                                                          filter(data_quadratic_big_prf, names == "Federov with Biased Sample")$coefficient_refit_distance),
#                                                id = seq(1:nrow(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_big_prf$experiment = "Quadratic Model + Re-fit"

#plot_data_quadratic_big_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample")$coefficient_distance -
#                                                                           filter(data_quadratic_big_prf_normal, names == "Federov with Biased Sample")$coefficient_distance),
#                                                 id = seq(1:nrow(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample"))))
#
#plot_data_quadratic_big_prf_normal$experiment = "Quadratic Model + Normal"
#
#plot_refit_data_quadratic_big_prf_normal <- data.frame(coefficient_distance_difference = sort(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample")$coefficient_refit_distance -
#                                                                                 filter(data_quadratic_big_prf_normal, names == "Federov with Biased Sample")$coefficient_refit_distance),
#                                                       id = seq(1:nrow(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_big_prf_normal$experiment = "Quadratic Model + Normal + Re-fit"

plot_data_small_prf <- NULL

plot_data_small_prf <- rbind(
  plot_data_linear_small_prf,
  plot_refit_data_linear_small_prf,
  #plot_data_quadratic_small_prf,
  #plot_refit_data_quadratic_small_prf,
  plot_data_quadratic_small_prf_normal,
  plot_refit_data_quadratic_small_prf_normal
)

plot_data_small_prf$experiment <- factor(plot_data_small_prf$experiment, levels = c(
                                                                           "Linear Model",
                                                                           "Linear Model + Re-fit",
                                                                           "Quadratic Model + Normal",
                                                                           "Quadratic Model + Normal + Re-fit"
                                                                           #"Quadratic Model",
                                                                           #"Quadratic Model + Re-fit",
                                                                         ))

#plot_data_big_prf <- NULL
#
#plot_data_big_prf <- rbind(
#  plot_data_linear_big_prf,
#  plot_refit_data_linear_big_prf,
#  #plot_data_quadratic_big_prf,
#  #plot_refit_data_quadratic_big_prf,
#  plot_data_quadratic_big_prf_normal,
#  plot_refit_data_quadratic_big_prf_normal
#)
#
#plot_data_big_prf$experiment <- factor(plot_data_big_prf$experiment, levels = c(
#                                                                       "Linear Model",
#                                                                       "Linear Model + Re-fit",
#                                                                       "Quadratic Model + Normal",
#                                                                       "Quadratic Model + Normal + Re-fit"
#                                                                       #"Quadratic Model",
#                                                                       #"Quadratic Model + Re-fit",
#                                                                     ))

ggplot(plot_data_small_prf, aes(x = id, y = coefficient_distance_difference)) +
       facet_wrap(experiment ~ ., ncol = 2) +
       ylim(c(-100, 100)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Distance Difference (Federov Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 18)
#+END_SRC
#+CAPTION: Coefficient distance difference for linear and quadratic models ($\text{P}_{\text{r}}(>\text{F}) = 0.0001$)
#+LABEL: fig:coef-refit-results-small-prf
#+ATTR_LATEX: :width 0.75\textwidth
#+RESULTS: fig:coef-refit-results-small-prf
[[file:/tmp/babel-PE51oT/figureUX4V5M.png]]

#+NAME: fig:coef-refit-results-big-prf
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :exports none :eval no-export
ggplot(plot_data_big_prf, aes(x = id, y = coefficient_distance_difference)) +
       #facet_wrap(experiment ~ ., scale = "free", ncol = 3) +
       facet_wrap(experiment ~ ., ncol = 2) +
       ylim(c(-100, 100)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Coeff. Distance Difference (Federov Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 18)
#+END_SRC
#+CAPTION: Coefficient distance difference for linear and quadratic models ($\text{P}_{\text{r}}(>\text{F}) = 0.05$)
#+LABEL: fig:coef-refit-results-big-prf
#+ATTR_LATEX: :width 0.75\textwidth
#+RESULTS: fig:coef-refit-results-big-prf
[[file:/tmp/babel-PE51oT/figureIkNS87.png]]
****** Response Vector Distance
#+NAME:fig:refit-results-small-prf
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(dplyr)
library(ggplot2)

plot_data_linear_small_prf <- data.frame(fit_distance_ratio = sort(filter(data_linear_small_prf, names == "Federov with Uniform Sample")$linear_fit_distance -
                                                                   filter(data_linear_small_prf, names == "Federov with Biased Sample")$linear_fit_distance),
                               id = seq(1:nrow(filter(data_linear_small_prf, names == "Federov with Uniform Sample"))))

plot_data_linear_small_prf$experiment = "Linear Model"

plot_refit_data_linear_small_prf <- data.frame(fit_distance_ratio = sort(filter(data_linear_small_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
                                                                         filter(data_linear_small_prf, names == "Federov with Biased Sample")$linear_refit_distance),
                                     id = seq(1:nrow(filter(data_linear_small_prf, names == "Federov with Uniform Sample"))))

plot_refit_data_linear_small_prf$experiment = "Linear Model + Re-fit"

#plot_data_linear_big_prf <- data.frame(fit_distance_ratio = sort(filter(data_linear_big_prf, names == "Federov with Uniform Sample")$linear_fit_distance -
#                                                                 filter(data_linear_big_prf, names == "Federov with Biased Sample")$linear_fit_distance),
#                                       id = seq(1:nrow(filter(data_linear_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_data_linear_big_prf$experiment = "Linear Model"
#
#plot_refit_data_linear_big_prf <- data.frame(fit_distance_ratio = sort(filter(data_linear_big_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                       filter(data_linear_big_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                             id = seq(1:nrow(filter(data_linear_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_linear_big_prf$experiment = "Linear Model + Re-fit"

#plot_data_quadratic_small_prf <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample")$linear_fit_distance -
#                                                                      filter(data_quadratic_small_prf, names == "Federov with Biased Sample")$linear_fit_distance),
#                                            id = seq(1:nrow(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample"))))
#
#plot_data_quadratic_small_prf$experiment = "Quadratic Model"
#
#plot_refit_data_quadratic_small_prf <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                            filter(data_quadratic_small_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                  id = seq(1:nrow(filter(data_quadratic_small_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_small_prf$experiment = "Quadratic Model + Re-fit"
#
plot_data_quadratic_small_prf_normal <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample")$linear_fit_distance -
                                                                             filter(data_quadratic_small_prf_normal, names == "Federov with Biased Sample")$linear_fit_distance),
                                                   id = seq(1:nrow(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample"))))

plot_data_quadratic_small_prf_normal$experiment = "Quadratic Model + Normal"

plot_refit_data_quadratic_small_prf_normal <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample")$linear_refit_distance -
                                                                                   filter(data_quadratic_small_prf_normal, names == "Federov with Biased Sample")$linear_refit_distance),
                                                         id = seq(1:nrow(filter(data_quadratic_small_prf_normal, names == "Federov with Uniform Sample"))))

plot_refit_data_quadratic_small_prf_normal$experiment = "Quadratic Model + Normal + Re-fit"

#plot_data_quadratic_big_prf <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample")$linear_fit_distance -
#                                                                    filter(data_quadratic_big_prf, names == "Federov with Biased Sample")$linear_fit_distance),
#                                          id = seq(1:nrow(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample"))))

#plot_data_quadratic_big_prf$experiment = "Quadratic Model"
#
#plot_refit_data_quadratic_big_prf <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                          filter(data_quadratic_big_prf, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                id = seq(1:nrow(filter(data_quadratic_big_prf, names == "Federov with Uniform Sample"))))
#
#plot_refit_data_quadratic_big_prf$experiment = "Quadratic Model + Re-fit"

#plot_data_quadratic_big_prf_normal <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample")$linear_fit_distance -
#                                                                           filter(data_quadratic_big_prf_normal, names == "Federov with Biased Sample")$linear_fit_distance),
#                                                 id = seq(1:nrow(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample"))))
#
#plot_data_quadratic_big_prf_normal$experiment = "Quadratic Model + Normal"
#
#plot_refit_data_quadratic_big_prf_normal <- data.frame(fit_distance_ratio = sort(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample")$linear_refit_distance -
#                                                                                 filter(data_quadratic_big_prf_normal, names == "Federov with Biased Sample")$linear_refit_distance),
#                                                       id = seq(1:nrow(filter(data_quadratic_big_prf_normal, names == "Federov with Uniform Sample"))))

plot_refit_data_quadratic_big_prf_normal$experiment = "Quadratic Model + Normal + Re-fit"

plot_data_small_prf <- rbind(
  plot_data_linear_small_prf,
  plot_refit_data_linear_small_prf,
  #plot_data_quadratic_small_prf,
  #plot_refit_data_quadratic_small_prf,
  plot_data_quadratic_small_prf_normal,
  plot_refit_data_quadratic_small_prf_normal
)

plot_data_small_prf$experiment <- factor(plot_data_small_prf$experiment, levels = c(
                                                                           "Linear Model",
                                                                           "Linear Model + Re-fit",
                                                                           "Quadratic Model + Normal",
                                                                           "Quadratic Model + Normal + Re-fit"
                                                                           #"Quadratic Model",
                                                                           #"Quadratic Model + Re-fit",
                                                                         ))

#plot_data_big_prf <- rbind(
#  plot_data_linear_big_prf,
#  plot_refit_data_linear_big_prf,
#  #plot_data_quadratic_big_prf,
#  #plot_refit_data_quadratic_big_prf,
#  plot_data_quadratic_big_prf_normal,
#  plot_refit_data_quadratic_big_prf_normal
#)
#
#plot_data_big_prf$experiment <- factor(plot_data_big_prf$experiment, levels = c(
#                                                                       "Linear Model",
#                                                                       "Linear Model + Re-fit",
#                                                                       "Quadratic Model + Normal",
#                                                                       "Quadratic Model + Normal + Re-fit"
#                                                                       #"Quadratic Model",
#                                                                       #"Quadratic Model + Re-fit",
#                                                                     ))

ggplot(plot_data_small_prf, aes(x = id, y = fit_distance_ratio)) +
       facet_wrap(experiment ~ ., ncol = 2) +
       ylim(c(-100, 100)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Fit Distance Difference (Federov Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 18)
#+END_SRC
#+CAPTION: Fit distance difference for linear and quadratic models ($\text{P}_{\text{r}}(>\text{F}) = 0.0001$)
#+LABEL: fig:refit-results-small-prf
#+ATTR_LATEX: :width 0.75\textwidth
#+RESULTS: fig:refit-results-small-prf
[[file:/tmp/babel-PE51oT/figureW3EV3T.png]]

#+NAME: fig:refit-results-big-prf
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :exports none :eval no-export
ggplot(plot_data_big_prf, aes(x = id, y = fit_distance_ratio)) +
       #facet_wrap(experiment ~ ., scale = "free", ncol = 3) +
       facet_wrap(experiment ~ ., ncol = 2) +
       ylim(c(-100, 100)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Fit Distance Difference (Federov Uniform - Biased)") +
       geom_point() +
       geom_hline(yintercept = 0, linetype = 2) +
       theme_bw(base_size = 18)
#+END_SRC
#+CAPTION: Fit distance difference for linear and quadratic models ($\text{P}_{\text{r}}(>\text{F}) = 0.05$)
#+LABEL: fig:refit-results-big-prf
#+ATTR_LATEX: :width 0.75\textwidth
#+RESULTS: fig:refit-results-big-prf
[[file:/tmp/babel-PE51oT/figureEnjmGX.png]]
****** Post-ANOVA + Refit Response Vector Distance            :noexport:
#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data.frame(linear_refit_distance_difference = sort(filter(data, names == "Federov with Uniform Sample")$linear_refit_distance /
                                                                filter(data, names == "Federov with Biased Sample")$linear_refit_distance),
                        id = seq(1:nrow(filter(data, names == "Federov with Uniform Sample"))))

ggplot(plot_data, aes(x = id, y = linear_refit_distance_difference)) +
       #ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiment") +
       scale_y_continuous(name = "Sorted Linear Post-ANOVA-Fit Difference Ratio (Uniform / Biased)") +
       geom_point() +
       geom_hline(yintercept = 1, linetype = 2) +
       theme_bw(base_size = 20) # +
       #theme(axis.ticks.x = element_blank(),
       #      axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figureFZD2Zn.png]]

#+HEADER: :results graphics output :session *R* :exports results :eval no-export
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(ggplot2)

plot_data <- data
plot_data$names <- factor(plot_data$names, levels = c("Federov with Uniform Sample",
                                                      "Federov with Biased Sample",
                                                      "Uniform Sample",
                                                      "Biased Sample"))

ggplot(plot_data, aes(x = id, y = linear_refit_distance)) +
       facet_wrap(names ~ ., ncol = 2) +
       ylim(c(0, NA)) +
       scale_x_continuous(name = "Experiments") +
       #geom_jitter(height = 0, alpha = 0.4) +
       geom_point() +
       theme_bw(base_size = 20) +
       theme(axis.ticks.x = element_blank(),
             axis.text.x = element_blank())
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-dYWgzL/figureMU3lr1.png]]

****** Looking at Sample Designs                              :noexport:

#+HEADER: :results output :session *R* :eval no-export :exports none
#+BEGIN_SRC R
write.csv(federov_design, "dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
write.csv(biased_federov_design, "dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R* :eval no-export :exports none
#+BEGIN_SRC R
federov_design <- read.csv("dopt_sampling_tests/federov_runif_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
biased_federov_design <- read.csv("dopt_sampling_tests/federov_biased_1000_samples_300_repetitions_30_factors_40_exp_coeff_10_sd_2_comp_600_rand.csv", header = TRUE)
#+END_SRC

#+RESULTS:

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)
library(rsm)

ggpairs(decode.data(biased_federov_design)[, c("X1", "X5", "X10", "X15", "X20", "X25", "X30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figureigvcmW.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720 :exports results :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(GGally)

ggpairs(decode.data(federov_design)[, c("X1", "X5", "X10", "X15", "X20", "X25", "X30")]) +
        theme_bw(base_size = 18)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-xq9JVx/figured9w4oC.png]]
