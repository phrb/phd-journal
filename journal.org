# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro H R Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Setup
** Julia
#+NAME: install_julia_deps
#+HEADER: :results output :session *julia*
#+BEGIN_SRC julia
Pkg.add("Plots")
Pkg.add("Lint")
Pkg.add("Gadfly")
Pkg.add("ProfileView")
Pkg.add("CSV")
Pkg.add("StatsBase")
Pkg.add("StatsModels")
Pkg.add("GLM")
Pkg.add("RDatasets")
Pkg.add("IterTools")
Pkg.add("Missings")
Pkg.add("RCall")
Pkg.add("DataFrames")
#+END_SRC

#+RESULTS: install_julia_deps
#+begin_example
INFO: Package Plots is already installed
INFO: Package Lint is already installed
INFO: Package Gadfly is already installed
INFO: Cloning cache of Gtk from https://github.com/JuliaGraphics/Gtk.jl.git
INFO: Cloning cache of GtkReactive from https://github.com/JuliaGizmos/GtkReactive.jl.git
INFO: Cloning cache of IntervalSets from https://github.com/JuliaMath/IntervalSets.jl.git
INFO: Cloning cache of ProfileView from https://github.com/timholy/ProfileView.jl.git
INFO: Cloning cache of Reactive from https://github.com/JuliaGizmos/Reactive.jl.git
INFO: Cloning cache of RoundingIntegers from https://github.com/JuliaMath/RoundingIntegers.jl.git
INFO: Installing Gtk v0.13.1
INFO: Installing GtkReactive v0.4.0
INFO: Installing IntervalSets v0.1.1
INFO: Installing ProfileView v0.3.0
INFO: Installing Reactive v0.6.0
INFO: Installing RoundingIntegers v0.0.3
INFO: Building Cairo
INFO: Building Gtk
INFO: Package database updated
INFO: Package CSV is already installed
INFO: Package StatsBase is already installed
INFO: Package StatsModels is already installed
INFO: Package GLM is already installed
INFO: Package RDatasets is already installed
#+end_example

#+NAME: update_julia_pkg
#+HEADER:  :results output :session *julia*
#+BEGIN_SRC julia
Pkg.update()
#+END_SRC

#+RESULTS: update_julia_pkg
: INFO: Updating METADATA...
: WARNING: Package ASTInterpreter: skipping update (dirty)...
: INFO: Updating Gallium master...
: INFO: Computing changes...
: INFO: No packages to install, update or remove

*** NODAL Development                                          :Code:NODAL:
**** Installing NODAL in Julia Nightly
[[https://github.com/phrb/NODAL.jl][NODAL]] is the autotuning library I am developing in the [[https://julialang.org][Julia]]
language. The idea is to provide tools for the implementation of
parallel and distributed autotuners for various problem domains.
***** Download Julia Nightly
****** [[https://julialang.org/downloads][Download Generic Binary]]
****** Downloading from the CLI
You can run the following to install the latest *Julia* version:
#+BEGIN_SRC bash
cd ~ && mkdir .bin && cd .bin
wget https://julialangnightlies-s3.julialang.org/bin/linux/x64/julia-latest-linux64.tar.gz
tar xvf julia-latest-linux64.tar.gz
mv julia-* julia
rm julia-latest-linux64.tar.gz
#+END_SRC
This will put the *Julia* binary at =~/.bin/julia/bin/julia=.
You can use it like that or add an =alias= to your shell.
***** Installing the unregistered version
This will not be needed after registering NODAL to METADATA.
****** [[https://docs.julialang.org/en/latest/manual/packages/#Installing-Unregistered-Packages-1][Documentation]]
****** Julia Commands
#+BEGIN_SRC julia
Pkg.clone("https://github.com/phrb/NODAL.jl")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
***** Installing from the Julia package manager
****** Julia commands
#+BEGIN_SRC julia
Pkg.add("NODAL")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
**** Setting up a new Release
***** Using Attobot
[[https://github.com/attobot][Attobot]] integrates with *GitHub* to automatically register a new package
or a package version to *Julia*'s =METADATA= package repository.  Attobot
only needs a new *GitHub* release to work.
***** Using *Julia*'s =PkgDev=
Check the [[https://docs.julialang.org/en/latest/manual/packages/#Tagging-and-Publishing-Your-Package-1][documentation]] to learn how to register and publish user
packages to =METADATA=.
**** Development Workflow
The process of fixing an [[https://github.com/phrb/NODAL.jl/issues][issue]] or submitting a new
feature is:
0. Fork [[https://github.com/phrb/NODAL.jl][NODAL on GitHub]]

   You will need a GitHub account for this.

1. Make sure you have the latest version
   #+BEGIN_SRC bash
git checkout master
git fetch
   #+END_SRC

   New branches must be made from the =dev= branch:
   #+BEGIN_SRC bash
git checkout dev
   #+END_SRC
2. Checkout a new branch
   #+BEGIN_SRC bash
git checkout -b fix-or-feature
   #+END_SRC
3. Write code and commit to your new branch

   Make sure you write short and descriptive commit
   messages. Something similar to [[https://udacity.github.io/git-styleguide/][Udacity's guidelines]] is preferred
   but not strictly necessary.

4. Open a [[https://github.com/phrb/NODAL.jl/pulls][pull request]] to the =dev= bran

** R
Installing *R* dependencies:
#+NAME: install_r_deps
#+HEADER: :results output :exports both :session *R*
#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",
                 "plotly", "rPref", "pracma", "FrF2", "AlgDesign",
                 "quantreg"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

#+RESULTS: install_r_deps
#+begin_example
Installing packages into â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™
(as â€˜libâ€™ is unspecified)
trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/ggplot2_2.2.1.tar.gz'
Content type 'application/x-gzip' length 2213308 bytes (2.1 MB)
==================================================
downloaded 2.1 MB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/dplyr_0.7.4.tar.gz'
Content type 'application/x-gzip' length 808054 bytes (789 KB)
==================================================
downloaded 789 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/tidyr_0.8.0.tar.gz'
Content type 'application/x-gzip' length 377417 bytes (368 KB)
==================================================
downloaded 368 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rjson_0.2.18.tar.gz'
Content type 'application/x-gzip' length 99478 bytes (97 KB)
==================================================
downloaded 97 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/GGally_1.3.2.tar.gz'
Content type 'application/x-gzip' length 1031885 bytes (1007 KB)
==================================================
downloaded 1007 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/plotly_4.7.1.tar.gz'
Content type 'application/x-gzip' length 1034951 bytes (1010 KB)
==================================================
downloaded 1010 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rPref_1.2.tar.gz'
Content type 'application/x-gzip' length 99297 bytes (96 KB)
==================================================
downloaded 96 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/pracma_2.1.4.tar.gz'
Content type 'application/x-gzip' length 382113 bytes (373 KB)
==================================================
downloaded 373 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/FrF2_1.7-2.tar.gz'
Content type 'application/x-gzip' length 282582 bytes (275 KB)
==================================================
downloaded 275 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/AlgDesign_1.1-7.3.tar.gz'
Content type 'application/x-gzip' length 514391 bytes (502 KB)
==================================================
downloaded 502 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/quantreg_5.35.tar.gz'
Content type 'application/x-gzip' length 1640297 bytes (1.6 MB)
==================================================
downloaded 1.6 MB

,* installing *source* package â€˜ggplot2â€™ ...
,** package â€˜ggplot2â€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (ggplot2)
ERROR: failed to lock directory â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™ for modifying
Try removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/00LOCK-dplyrâ€™
,* installing *source* package â€˜rjsonâ€™ ...
,** package â€˜rjsonâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c dump.cpp -o dump.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c parser.c -o parser.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c register.c -o register.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rjson.so dump.o parser.o register.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rjson/libs
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (rjson)
,* installing *source* package â€˜pracmaâ€™ ...
,** package â€˜pracmaâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** byte-compile and prepare package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (pracma)
,* installing *source* package â€˜FrF2â€™ ...
,** package â€˜FrF2â€™ successfully unpacked and MD5 sums checked
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (FrF2)
,* installing *source* package â€˜AlgDesignâ€™ ...
,** package â€˜AlgDesignâ€™ successfully unpacked and MD5 sums checked
,** libs
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c FederovOpt.c -o FederovOpt.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c OptBlock.c -o OptBlock.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c Utility.c -o Utility.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o AlgDesign.so FederovOpt.o OptBlock.o Utility.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/AlgDesign/libs
,** R
,** data
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (AlgDesign)
,* installing *source* package â€˜quantregâ€™ ...
,** package â€˜quantregâ€™ successfully unpacked and MD5 sums checked
,** libs
gfortran   -fpic  -g -O2  -c akj.f -o akj.o
gfortran   -fpic  -g -O2  -c boot.f -o boot.o
gfortran   -fpic  -g -O2  -c bound.f -o bound.o
gfortran   -fpic  -g -O2  -c boundc.f -o boundc.o
gfortran   -fpic  -g -O2  -c brute.f -o brute.o
gfortran   -fpic  -g -O2  -c chlfct.f -o chlfct.o
gfortran   -fpic  -g -O2  -c cholesky.f -o cholesky.o
gfortran   -fpic  -g -O2  -c combos.f -o combos.o
gfortran   -fpic  -g -O2  -c crq.f -o crq.o
gfortran   -fpic  -g -O2  -c crqfnb.f -o crqfnb.o
gfortran   -fpic  -g -O2  -c dsel05.f -o dsel05.o
gfortran   -fpic  -g -O2  -c etime.f -o etime.o
gfortran   -fpic  -g -O2  -c extract.f -o extract.o
gfortran   -fpic  -g -O2  -c idmin.f -o idmin.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c init.c -o init.o
gfortran   -fpic  -g -O2  -c iswap.f -o iswap.o
gfortran   -fpic  -g -O2  -c kuantile.f -o kuantile.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c mcmb.c -o mcmb.o
gfortran   -fpic  -g -O2  -c penalty.f -o penalty.o
gfortran   -fpic  -g -O2  -c powell.f -o powell.o
gfortran   -fpic  -g -O2  -c rls.f -o rls.o
gfortran   -fpic  -g -O2  -c rq0.f -o rq0.o
gfortran   -fpic  -g -O2  -c rq1.f -o rq1.o
gfortran   -fpic  -g -O2  -c rqbr.f -o rqbr.o
gfortran   -fpic  -g -O2  -c rqfn.f -o rqfn.o
gfortran   -fpic  -g -O2  -c rqfnb.f -o rqfnb.o
gfortran   -fpic  -g -O2  -c rqfnc.f -o rqfnc.o
gfortran   -fpic  -g -O2  -c rqs.f -o rqs.o
gfortran   -fpic  -g -O2  -c sparskit2.f -o sparskit2.o
gfortran   -fpic  -g -O2  -c srqfn.f -o srqfn.o
gfortran   -fpic  -g -O2  -c srqfnc.f -o srqfnc.o
gfortran   -fpic  -g -O2  -c srtpai.f -o srtpai.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o quantreg.so akj.o boot.o bound.o boundc.o brute.o chlfct.o cholesky.o combos.o crq.o crqfnb.o dsel05.o etime.o extract.o idmin.o init.o iswap.o kuantile.o mcmb.o penalty.o powell.o rls.o rq0.o rq1.o rqbr.o rqfn.o rqfnb.o rqfnc.o rqs.o sparskit2.o srqfn.o srqfnc.o srtpai.o -llapack -lblas -lgfortran -lm -lquadmath -lgfortran -lm -lquadmath -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/quantreg/libs
,** R
,** data
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (quantreg)
,* installing *source* package â€˜tidyrâ€™ ...
,** package â€˜tidyrâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c RcppExports.cpp -o RcppExports.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c fill.cpp -o fill.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c melt.cpp -o melt.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c simplifyPieces.cpp -o simplifyPieces.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o tidyr.so RcppExports.o fill.o melt.o simplifyPieces.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/tidyr/libs
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,*** copying figures
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (tidyr)
,* installing *source* package â€˜GGallyâ€™ ...
,** package â€˜GGallyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (GGally)
,* installing *source* package â€˜rPrefâ€™ ...
,** package â€˜rPrefâ€™ successfully unpacked and MD5 sums checked
,** libs
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c RcppExports.cpp -o RcppExports.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c bnl.cpp -o bnl.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c hasse.cpp -o hasse.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c pref-classes.cpp -o pref-classes.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par-top.cpp -o psel-par-top.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par.cpp -o psel-par.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c scalagon.cpp -o scalagon.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c topk_setting.cpp -o topk_setting.o
g++ -std=gnu++11 -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rPref.so RcppExports.o bnl.o hasse.o pref-classes.o psel-par-top.o psel-par.o scalagon.o topk_setting.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPref/libs
,** R
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜rPrefâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* installing *source* package â€˜plotlyâ€™ ...
,** package â€˜plotlyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜plotlyâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™

The downloaded source packages are in
	â€˜/tmp/RtmpivSTVC/downloaded_packagesâ€™
Warning messages:
1: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜dplyrâ€™ had non-zero exit status
2: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜rPrefâ€™ had non-zero exit status
3: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜plotlyâ€™ had non-zero exit status
#+end_example

** Modifying & Analysing the FPGA Data Set
Cloning and updating the =legup-tuner= repository:

#+NAME: update_legup_tuner
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/legup-tuner.git || (cd legup-tuner && git pull)
#+END_SRC

Export your path to =repository_dir= variable:

#+name: repository_dir
#+begin_src sh :results output :exports both
pwd | tr -d "\n"
#+end_src

** Updating & Cloning Repositories
*** GPU Autotuning Screening Experiment
#+NAME: update_screening_experiment
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/autotuning_screening_experiment.git || (cd autotuning_screening_experiment && git pull)
#+END_SRC
* 2018
** May
*** [2018-05-02 Wed]
**** Summarizing the D-Optimal + ANOVA Strategy for Steven's Experiments
1. Use ~optFederov~ to find 24 experiments for the full model:

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        vector_length + lws_y + 1 / lws_y +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

2. Use ~aov~ to fit the full model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       vector_length + lws_y + 1 / lws_y +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}

4. Identify the most significant factors from the ANOVA summary. In this
   case, they are $vector_length$ and $lws_y$.
5. Use the fitted model to predict the best $time_per_pixel$ value in the
   entire dataset
6. Prune the dataset using the predicted best values for $vector_length$ and $lws_y$
7. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
   than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

8. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}
9. Identify the most significant factors from the ANOVA summary. In this
   case, they are $y_component_number$ and $threads_number$.
10. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
11. Prune the dataset using the predicted best values for $y_component_number$ and
    $threads_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size +
        elements_number + 1 / elements_number
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size +
                       elements_number + 1 / elements_number
\end{equation}
14. Identify the most significant factors from the ANOVA summary. In this
    case, it is $elements_number$
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Prune the dataset using the predicted best values for $elements_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size
\end{equation}
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Compare the predicted $time_per_pixel$ with the global optimum
*** [2018-05-03 Thu]
**** Summarizing Experiments
Make sure you have the data:

#+NAME: update_dopt_aov_experiments
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_anova_experiments.git || (cd dopt_anova_experiments && git pull)
#+END_SRC

#+RESULTS: update_dopt_aov_experiments
: Already up to date.

This [[file:./dopt_anova_experiments/org/report.pdf][file]] contains the report.
*** [2018-05-04 Fri]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)

A = data.frame(x = 1:100,
               y = 1:100,
               z = 1:100)

A$Y = rnorm(n = 100, mean = A$x, sd = 0.4 * A$x)

big_model = lm(Y ~ x + y + z, data = A)
small_model = lm(Y ~ x, data = A)

# A_big_predict = cbind(A, predict(big_model, interval = "confidence"))
# A_small_predict = cbind(A, predict(small_model, interval = "confidence"))
#
# p_small_x <- ggplot(A_small_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_line(aes(x, fit), alpha = 0.8, color = "red1", size = 1)
#
# p_small_y <- ggplot(A_small_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_small_z <- ggplot(A_small_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_x <- ggplot(A_big_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(x, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_y <- ggplot(A_big_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_z <- ggplot(A_big_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)

par(mfrow = c(2, 5))
plot(small_model, which = c(1, 2, 3, 4, 5))
plot(big_model, which = c(1, 2, 3, 4, 5))

# grid.arrange(p_small_x, p_small_y, p_small_z,
#              p_big_x, p_big_y, p_big_z, nrow = 2)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-3203rQF/figure3203Li2.png]]
*** [2018-05-07 Mon]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)
library(AlgDesign)

read_file <- "dopt_anova_experiments/data/search_space.csv"

results <- read.csv(read_file, strip.white = T, header = T)

big_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                 vector_length + lws_y + I(1 / lws_y) +
                                 load_overlap + temporary_size +
                                 elements_number + I(1 / elements_number) +
                                 threads_number + I(1 / threads_number),
                data = results)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length,
                data = results)

bm_predict = data.frame(time_per_pixel = predict(big_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

sm_predict = data.frame(time_per_pixel = predict(small_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_min[1, ]

global_min = results[results$time_per_pixel == min(results$time_per_pixel),
                     c("time_per_pixel", "y_component_number",
                       "vector_length")]

ggplot(results) +
    aes(x = y_component_number, y = time_per_pixel) +
    theme_bw() +
    geom_point(alpha = 0.1) +
    theme(legend.position = "top") +
    geom_point(color= "green", data = bm_min, size = 3, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "red", data = sm_min, size = 4, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "blue", data = global_min, size = 5, alpha = 0.5,
               aes(x = y_component_number, y = time_per_pixel)) +
    theme(axis.text = element_text(size = 12),
          axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-27096ysq/figure27096DcA.png]]
*** [2018-05-09 Wed]
**** Plotting Predicted Values During Experiment
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 720 :height 1280
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(ggplot2)
library(gridExtra)

generate_model_plot <- function(big_model, small_model, results, full_data, metric) {
    bm_predict = data.frame(response = predict(big_model, results),
                            variable = results[metric])

    names(bm_predict)[names(bm_predict) == "response"] <- "time_per_pixel"
    names(bm_predict)[names(bm_predict) == "variable"] <- metric

    sm_predict = data.frame(response = predict(small_model, results),
                            variable = results[metric])

    names(sm_predict)[names(sm_predict) == "response"] <- "time_per_pixel"
    names(sm_predict)[names(sm_predict) == "variable"] <- metric

    bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict$time_per_pixel), ]

    sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict$time_per_pixel), ]

    sm_min = sm_min[1, ]
    bm_min = bm_min[1, ]

    global_min = full_data[full_data$time_per_pixel == min(full_data$time_per_pixel),
                           c("time_per_pixel", metric)]

    p <- ggplot() +
         scale_shape_identity() +
         geom_point(data = full_data, alpha = 0.1,
                    aes(x = full_data[metric], y = time_per_pixel,
                        color = "Search Space")) +
         geom_point(data = bm_min, size = 3, alpha = 1.0,
                    aes(x = bm_min[metric], y = time_per_pixel,
                        color = "Big Model", shape = 7)) +
         geom_point(data = sm_min, size = 3, alpha = 1.0,
                    aes(x = sm_min[metric], y = time_per_pixel,
                        color = "Small Model", shape = 8)) +
         geom_point(data = global_min, size = 3, alpha = 1.0,
                    aes(x = global_min[metric], y = time_per_pixel,
                        color = "Global Minimum", shape = 9)) +
         theme_bw() +
         theme(axis.text = element_text(size = 12),
               axis.title = element_text(size = 14, face = "bold"),
               legend.position = "top") +
         labs(y = "time_per_pixel", x = metric) +
         scale_color_manual(values = c("green", "blue", "black", "red"))

    return(p)
}

complete_data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

budget <- 120

factors = c("elements_number", "y_component_number",
            "vector_length", "temporary_size",
            "load_overlap", "threads_number",
            "lws_y")

used <- 0

data <- complete_data[, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

# Comment/Uncomment to toggle scaling

# scaled_data <- cbind(scale(select_if(data, is.numeric), center = FALSE, scale = TRUE),
#                      select_if(data, Negate(is.numeric)))
# scaled_data <- scaled_data[, names(data)]

# We are able to use the full set in this case
# sampled_data <- scaled_data[sample(nrow(data), 500), ]

# Complete model:
output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      scaled_data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

# Complete model:
regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length + lws_y + I(1 / lws_y) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                  data = federov_design)

p_vectorlength <- generate_model_plot(regression, small_model,
                                      scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

p_lwsy <- generate_model_plot(regression, small_model,
                              scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                   data = random_data)


r_lwsy <- generate_model_plot(big_random, small_random,
                              random_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

r_vectorlength <- generate_model_plot(big_random, small_random,
                                      random_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

used <- used + nrow(federov_design)

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'vector_length' and 'lws_y'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 18) {
    output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                           load_overlap + temporary_size +
                           elements_number + I(1 / elements_number) +
                           threads_number + I(1 / threads_number),
                         scaled_data,
                         nTrials = 18)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

p_ycomponentnumber <- generate_model_plot(regression, small_model,
                                          scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

p_threadsnumber <- generate_model_plot(regression, small_model,
                                       scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    threads_number + I(1 / threads_number),
                   data = random_data)


r_ycomponentnumber <- generate_model_plot(big_random, small_random,
                                          random_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

r_threadsnumber <- generate_model_plot(big_random, small_random,
                                       random_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'y_component_number' and 'threads_number'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 10) {
    output <- optFederov(~ load_overlap + temporary_size +
                            elements_number + I(1 / elements_number),
                          scaled_data,
                          nTrials = 10)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                  data = federov_design)

p_elementsnumber <- generate_model_plot(regression, small_model,
                                        scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                   data = random_data)

r_elementsnumber <- generate_model_plot(big_random, small_random,
                                        random_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

# Checking the ANOVA summary we can identify, at last, one variable
# that seem to have greater impact: 'elements_number'
# Let's fix it to their best predicted value so far,
# then fit a new model without it

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number &
                      complete_data$elements_number == predicted_best$elements_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 6) {
    output <- optFederov(~ load_overlap + temporary_size,
                          scaled_data,
                          nTrials = 6)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size,
                  data = federov_design)

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

best <- complete_data[complete_data$time_per_pixel == min(complete_data$time_per_pixel), ]
best_row <- rownames(best)

predicted_best$slowdown <- predicted_best$time_per_pixel / best$time_per_pixel
predicted_best$method <- rep("DOPTaov", nrow(predicted_best))
predicted_best$point_number <- rep(used, nrow(predicted_best))
predicted_best$vector_recompute <- rep("true", nrow(predicted_best))

predicted_best <- predicted_best[, c("elements_number", "y_component_number",
                                    "vector_length", "temporary_size", "vector_recompute",
                                    "load_overlap", "threads_number", "lws_y",
                                    "time_per_pixel", "point_number", "method",
                                    "slowdown")]

grid.arrange(p_vectorlength + ggtitle("First Step: D-Opt + aov"), p_lwsy + ggtitle(" "),
             r_vectorlength + ggtitle("First Step: Random Selection + lm"), r_lwsy + ggtitle(" "),
             p_ycomponentnumber + ggtitle("Second Step: D-Opt + aov"), p_threadsnumber + ggtitle(" "),
             r_ycomponentnumber + ggtitle("Second Step: Random Selection + lm"), r_threadsnumber + ggtitle(" "),
             p_elementsnumber + ggtitle("Third Step: D-Opt + aov"),
             r_elementsnumber + ggtitle("Third Step: Random Selection + lm"), nrow = 5)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536ZkV.png]]
*** [2018-05-14 Mon]
**** Accuracy of the DOPT+AOV Process in Steven's Case
To verify the "accuracy" of the selected metrics, I adapted the experiment
scripts to check for each removed model variable in the actual =aov= summary.
Those initial choices seem to match in most cases with the variables identified
as most relevant by the =aov= summary, as shown below.

#+HEADER: :results graphics output :session *R* :exports results
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 600 :height 500
#+BEGIN_SRC R
library(ggplot2)

accuracies_file <- "dopt_anova_experiments/data/dopt_accuracies.csv"
results <- read.csv(accuracies_file, strip.white=T, header=T)

names(results) <- c("First", "Second", "Third")
parsed_results = data.frame(names(results), t(results[1, ]))
names(parsed_results) <- c("Steps", "Accuracy")

parsed_results

ggplot(data = parsed_results, aes(x = Steps, y = Accuracy)) +
geom_bar(stat = "identity", width = 0.5) +
#geom_hline(yintercept = 1.0, color = "red", linetype = 2) +
geom_text(aes(label = Accuracy), vjust = 1.6, color = "white", size = 5)+
theme_bw() +
theme(axis.text = element_text(size = 12),
      axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536MMn.png]]

As described previously, at each step a group of variables is removed from the
model based on their "score", that is, the "Pr(>F)" value in the =aov= summary.
I selected at most two variables at each of the three steps, based on preliminary
visual analysis of the =aov= summaries.

To measure how accurate those initial selections were I checked at each step if
the $n$ selected variables were in the $n$ most relevant variables in that
step's =aov= summary. If that was the case I incremented a step-specific
counter. The counters were updated for 1000 iterations and then divided by 1000.
This value represents the accuracy of the static selection in comparison with
the values that would be selected if each individual =aov= summary was analysed.
*** [2018-05-15 Tue]
**** Writing an LM Experiment Using a Big Model
This experiment is a modification of the ``DOPTaov'' experiment that adapts the
``LM'' strategy for fitting linear models to pruned search spaces. Instead of
using small models at each step the experiments starts with large models that
are pruned as meaningful variables are identified in the =aov= summary. The
experiment used the same variables from the ``DOPTaov'' experiement at each
step.

**** Trying to Mitigate Heteroscedasticity
Using some ideas from [[https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/][this website]].

***** For a Uniformly Sampled Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

str(data)
data <- data[sample(1:nrow(data), 100, replace = FALSE), ]

regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                data = data)

summary(regression)
ncvTest(regression)

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = data)

transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$lambda) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = data)

summary(transformed_regression)
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = data)

Residuals:
       Min         1Q     Median         3Q        Max
-4.992e-09 -1.997e-09 -2.596e-10  1.126e-09  2.464e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -5.683e-09  3.382e-09  -1.680 0.096446 .
y_component_number       1.206e-09  5.251e-10   2.298 0.023956 *
I(1/y_component_number)  5.124e-09  3.369e-09   1.521 0.131863
vector_length            2.597e-10  7.338e-11   3.540 0.000643 ***
lws_y                    1.020e-11  2.951e-12   3.458 0.000841 ***
I(1/lws_y)              -2.800e-09  1.270e-09  -2.205 0.030027 *
load_overlaptrue         1.963e-10  8.042e-10   0.244 0.807695
temporary_size           1.977e-10  4.144e-10   0.477 0.634476
elements_number         -1.128e-10  1.048e-10  -1.076 0.284946
I(1/elements_number)     3.968e-09  3.270e-09   1.214 0.228123
threads_number          -2.453e-12  1.856e-12  -1.322 0.189660
I(1/threads_number)      2.043e-08  5.497e-08   0.372 0.711005
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.891e-09 on 88 degrees of freedom
Multiple R-squared:  0.3924,	Adjusted R-squared:  0.3165
F-statistic: 5.167 on 11 and 88 DF,  p-value: 3.271e-06
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 148.428    Df = 1     p = 3.824548e-34

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$lambda) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = data)

Residuals:
    Min      1Q  Median      3Q     Max
-277.65  -81.92  -10.22   79.59  220.80

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -890.45549   99.17068  -8.979 4.53e-14 ***
y_component_number        37.61691   15.39476   2.443   0.0165 *
I(1/y_component_number)  154.21569   98.77625   1.561   0.1221
vector_length             11.59971    2.15156   5.391 5.80e-07 ***
lws_y                      0.50504    0.08652   5.838 8.69e-08 ***
I(1/lws_y)              -258.56389   37.22493  -6.946 6.20e-10 ***
load_overlaptrue         -24.95474   23.57916  -1.058   0.2928
temporary_size             2.76899   12.15003   0.228   0.8203
elements_number           -6.89838    3.07364  -2.244   0.0273 *
I(1/elements_number)     125.77409   95.86892   1.312   0.1930
threads_number            -0.09253    0.05442  -1.700   0.0926 .
I(1/threads_number)     2668.72370 1611.82566   1.656   0.1013
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 114.1 on 88 degrees of freedom
Multiple R-squared:  0.7041,	Adjusted R-squared:  0.6672
F-statistic: 19.04 on 11 and 88 DF,  p-value: < 2.2e-16
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.4543362    Df = 1     p = 0.5002829
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-IVw6xY/figureecbKUS.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-zZp0Xe/figure9dCFD8.png]]

***** For a D-Optimal Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)
str(data)

output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      data = data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

str(federov_design)

# Complete model:
regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    vector_length + lws_y + I(1 / lws_y) +
                                    load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number) +
                                    threads_number + I(1 / threads_number),
                  data = federov_design)

summary(regression)
summary.aov(regression)
ncvTest(regression)

data[predict(regression, data) == min(predict(regression, data)), ]

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = federov_design)

transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$lambda) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = federov_design)

summary(transformed_regression)
summary.aov(transformed_regression)
ncvTest(transformed_regression)

data[predict(transformed_regression, data) == min(predict(transformed_regression, data)), ]
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...
'data.frame':	24 obs. of  9 variables:
 $ elements_number   : int  1 4 2 3 1 4 4 2 1 4 ...
 $ y_component_number: int  1 1 2 3 1 1 1 2 1 1 ...
 $ vector_length     : int  1 16 16 16 1 1 1 1 1 16 ...
 $ temporary_size    : int  2 2 4 4 2 4 4 2 4 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 1 2 1 1 1 1 2 2 ...
 $ threads_number    : int  256 32 32 128 256 32 128 32 32 1024 ...
 $ lws_y             : int  1 1 1 128 32 1 64 32 32 16 ...
 $ time_per_pixel    : num  2.31e-10 7.75e-10 1.70e-09 2.79e-08 7.27e-10 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = federov_design)

Residuals:
       Min         1Q     Median         3Q        Max
-8.775e-09 -4.543e-09 -1.968e-09  2.959e-09  1.856e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)              1.084e-08  1.609e-08   0.674   0.5134
y_component_number      -1.401e-09  2.450e-09  -0.572   0.5778
I(1/y_component_number) -1.300e-08  1.447e-08  -0.898   0.3867
vector_length            5.744e-10  2.591e-10   2.216   0.0467 *
lws_y                    1.121e-11  6.242e-12   1.796   0.0977 .
I(1/lws_y)              -4.183e-09  4.583e-09  -0.913   0.3793
load_overlaptrue        -1.279e-09  3.862e-09  -0.331   0.7462
temporary_size          -2.925e-11  1.926e-09  -0.015   0.9881
elements_number         -4.281e-11  3.725e-10  -0.115   0.9104
I(1/elements_number)     1.238e-08  8.077e-09   1.532   0.1514
threads_number          -3.511e-12  7.437e-12  -0.472   0.6453
I(1/threads_number)     -6.241e-08  2.281e-07  -0.274   0.7890
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 9.262e-09 on 12 degrees of freedom
Multiple R-squared:  0.5646,	Adjusted R-squared:  0.1655
F-statistic: 1.415 on 11 and 12 DF,  p-value: 0.2797
                        Df    Sum Sq   Mean Sq F value Pr(>F)
y_component_number       1 6.980e-17 6.980e-17   0.814 0.3848
I(1/y_component_number)  1 2.000e-18 2.000e-18   0.024 0.8805
vector_length            1 3.875e-16 3.875e-16   4.517 0.0550 .
lws_y                    1 5.023e-16 5.023e-16   5.856 0.0323 *
I(1/lws_y)               1 6.970e-17 6.970e-17   0.812 0.3851
load_overlap             1 6.900e-18 6.900e-18   0.080 0.7818
temporary_size           1 3.200e-18 3.200e-18   0.037 0.8499
elements_number          1 5.620e-17 5.620e-17   0.655 0.4340
I(1/elements_number)     1 2.175e-16 2.175e-16   2.536 0.1372
threads_number           1 1.340e-17 1.340e-17   0.156 0.7001
I(1/threads_number)      1 6.400e-18 6.400e-18   0.075 0.7890
Residuals               12 1.029e-15 8.580e-17
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 14.05733    Df = 1     p = 0.0001773212
      elements_number y_component_number vector_length temporary_size
18584               4                  1             1              4
      vector_recompute load_overlap threads_number lws_y time_per_pixel
18584             true         true           1024     1    3.37552e-10

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$lambda) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = federov_design)

Residuals:
     Min       1Q   Median       3Q      Max
-1.82320 -0.99051 -0.06769  0.85520  1.61215

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -30.303461   2.649147 -11.439 8.23e-08 ***
y_component_number       -0.453056   0.403302  -1.123 0.283253
I(1/y_component_number)  -3.209807   2.383153  -1.347 0.202907
vector_length             0.230701   0.042666   5.407 0.000158 ***
lws_y                     0.004914   0.001028   4.782 0.000447 ***
I(1/lws_y)               -2.779609   0.754470  -3.684 0.003125 **
load_overlaptrue         -0.193140   0.635862  -0.304 0.766525
temporary_size            0.555741   0.317136   1.752 0.105197
elements_number           0.049715   0.061324   0.811 0.433327
I(1/elements_number)      3.458007   1.329789   2.600 0.023208 *
threads_number           -0.002495   0.001224  -2.038 0.064250 .
I(1/threads_number)       6.800059  37.556377   0.181 0.859340
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.525 on 12 degrees of freedom
Multiple R-squared:  0.8911,	Adjusted R-squared:  0.7913
F-statistic:  8.93 on 11 and 12 DF,  p-value: 0.0003389
                        Df Sum Sq Mean Sq F value   Pr(>F)
y_component_number       1   5.93    5.93   2.552  0.13616
I(1/y_component_number)  1   0.06    0.06   0.027  0.87126
vector_length            1  66.84   66.84  28.746  0.00017 ***
lws_y                    1  79.03   79.03  33.992 8.08e-05 ***
I(1/lws_y)               1  30.24   30.24  13.005  0.00360 **
load_overlap             1   0.59    0.59   0.252  0.62477
temporary_size           1   5.50    5.50   2.366  0.14995
elements_number          1   0.39    0.39   0.169  0.68840
I(1/elements_number)     1  17.74   17.74   7.632  0.01720 *
threads_number           1  21.98   21.98   9.452  0.00964 **
I(1/threads_number)      1   0.08    0.08   0.033  0.85934
Residuals               12  27.90    2.33
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.09165178    Df = 1     p = 0.7620877
      elements_number y_component_number vector_length temporary_size
15927               4                  1             1              2
      vector_recompute load_overlap threads_number lws_y time_per_pixel
15927             true         true           1024     1   3.368082e-10
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figurem4Tf5r.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(regression$model$time_per_pixel, breaks = 10)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureYtLgcw.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureGF8dKO.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(transformed_regression$model["bcPower(time_per_pixel, boxcox_transform$lambda)"][[1]])
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-GGHiPj/figureuPx7MI.png]]

*** [2018-05-16 Wed]
**** Power Transforms on Generated Heteroscedastic Data
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(car)
set.seed(1234)
x <- 1:1000
y <- abs(rnorm(n = 1000, mean = x, sd = 0.9 * x))

data <- data.frame(x, y)
plot(lm(y ~ x, data = data), which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5x1ltg/figureDpsnk9.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(lm(y ~ x, data = data))
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 365.3147    Df = 1     p = 1.960365e-81

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
power_transform <- powerTransform(lm(y ~ x,  data = data))#, family = "bcnPower")
coef(power_transform, round = TRUE)
transformed_regression <- lm(bcPower(y, power_transform$roundlam) ~ x, data = data)
                                      #gamma = power_transform$gamma) ~ x, data = data)

plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-5x1ltg/figureKMbSqa.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 41.63823    Df = 1     p = 1.098246e-10

*** [2018-05-17 Thu]
**** Looking for Autotuning Benchmarks
Some candidates:
- Rodinia kernels (http://lava.cs.virginia.edu/Rodinia/download_links.htm)
- SPAPT (https://github.com/brnorris03/Orio/tree/master/testsuite/SPAPT)
- Dongarra's BEAST Project (http://icl.cs.utk.edu/beast/people/index.html)
*** [2018-05-22 Tue]
**** Running SPAPT Benchmarks
- Orio has a not very good documentation
- Compilation of simple examples takes +5min
**** Looking into LLVM and GCC flag autotuning
- CollectiveKnowledge has great flag space descriptions
- If we go back to compiler flag tuning we can tune
  any of the well-stablished HPC benchmarks
*** [2018-05-23 Wed]
**** Orio Setup Scripts
I've forked the Orio repository so I can freely change the code while keeping
version control.

To clone the most recent version, pick a path for the repository by editing the
source block below, using absolute paths.

#+NAME: setup_orio
#+HEADER: :results output
#+HEADER: :var ORIO_PATH="/home/phrb/code/orio"
#+BEGIN_SRC shell
git clone --depth=1 https://github.com/phrb/Orio.git $ORIO_PATH || echo "Orio already installed"
#+END_SRC

#+RESULTS: setup_orio
: Orio already installed

***** Lazy Python Databases with =Dataset=
With this it is possible to create a database from python scripts without much hassle:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
sudo pip install dataset
#+END_SRC

#+RESULTS:
: sudo pip install dataset
: Requirement already satisfied: dataset in /usr/lib/python3.6/site-packages

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import dataset
db = dataset.connect('sqlite:///mydatabase.db')
table = db['user']
table.insert(dict(name='John Doe', age=46, country='China'))
table.insert(dict(name='Jane Doe', age=37, country='France', gender='female'))
table.update(dict(name='John Doe', age=47), ['name'])
#+END_SRC

#+RESULTS:
: Python 3.6.5 (default, May 11 2018, 04:00:52)
: [GCC 8.1.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: python.el: native completion setup loaded

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
print(db.tables)
print(db['user'].columns)
print(len(db['user']))
users = db['user'].all()
print(users)

db.commit()
#+END_SRC

#+RESULTS:
: ['user']
: ['id', 'name', 'age', 'country', 'gender']
: 2
: <dataset.util.ResultIter object at 0x7f7f34e974a8>

Cleaning up:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
rm mydatabase.db
#+END_SRC

#+RESULTS:
: rm mydatabase.db
*** [2018-05-28 Mon]
**** Using AlgDesign & ANOVA from Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
from rpy2.robjects.packages import importr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
#+END_SRC

#+RESULTS:

*** [2018-05-29 Tue]
**** Implementing the D-Optimal Design + AOV Approach in Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import math
from rpy2.robjects.packages import importr
from rpy2.robjects import IntVector, StrVector, Formula, r
from rpy2.robjects.lib.dplyr import dplyr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
car       = importr("car")

def opt_federov(design_formula, data, trials):
    output = algdesign.optFederov(Formula(design_formula),
                                  data,
                                  maxIteration = 1000,
                                  nTrials = trials)
    return output

def transform_design(design, lm_formula, full_model, response):
    transform = car.powerTransform(Formula(lm_formula),
                                   data = design)

    print("Power Transform Step:")
    print(transform)
    transformed_response = car.bcPower(design.rx(response[0]), transform.rx("lambda")[0])
    design               = base.cbind(design, transformed_response)
    transform_lm_formula = "{0}".format(base.names(transformed_response)[0]) + full_model

    return design, transform_lm_formula

def anova(design, formula):
    heteroscedasticity_test = car.ncvTest(stats.lm(Formula(formula), data = design))
    print("Heteroscedasticity Test p-value:")
    print(heteroscedasticity_test.rx("p")[0][0])

    if heteroscedasticity_test.rx("p")[0][0] < 0.05:
        transform_lm_formula = transform_design(design, formula,
                                                full_model, response)
        heteroscedasticity_test = car.ncvTest(stats.lm(Formula(transform_lm_formula),
                                                        data = design))
        print("Heteroscedasticity Test p-value:")
        print(heteroscedasticity_test.rx("p")[0][0])
    else:
        print("No need to power transform")
        transform_lm_formula = lm_formula

    regression = stats.lm(Formula(formula),
                          data = design)

    summary_regression = stats.summary_aov(regression)
    print("Regression Step:")
    print(summary_regression)

    prf_values = {}

    for k, v in zip(base.rownames(summary_regression[0]), summary_regression[0][4]):
        if k.strip() != "Residuals":
            prf_values[k.strip()] = v

    return regression, prf_values

def predict_best(regression, data):
    predicted = stats.predict(regression, data)
    predicted_best = predicted.index(min(predicted))

    p_min = min(predicted)
    i = 0

    for k in range(len(predicted)):
        if math.isclose(predicted[k], p_min, rel_tol = 1e-6):
            i += 1

    print("Identical predictions (tol = 1e-17): {0}".format(i))
    return data.rx(predicted_best, True)

def prune_data(data, fixed_variables):
    print(fixed_variables)
    pruned_data = data
    for k, v in fixed_variables.items():
        pruned_data = pruned_data.rx((pruned_data.rx2(str(k)).ro == str(v)),
                                     True)

    print("Dimensions of Pruned Data: " + str(base.dim(pruned_data)))
    print("Dimensions of Full Data: " + str(base.dim(data)))
    return pruned_data

def get_fixed_variables(predicted_best, ordered_prf_keys, fixed_factors, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    fixed_variables = fixed_factors
    for v in unique_variables:
        fixed_variables[v] = predicted_best.rx2(str(v))[0]

    print("Fixed Variables: " + str(fixed_variables))
    return fixed_variables

def prune_model(factors, inverse_factors, ordered_prf_keys, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    pruned_factors = [f for f in factors if not f in unique_variables]
    pruned_inverse_factors = [f for f in inverse_factors if not f in unique_variables]

    return pruned_factors, pruned_inverse_factors

def dopt_anova_step(response, factors, inverse_factors, data, fixed_factors, budget):
    full_model     = "".join([" ~ ",
                              " + ".join(factors), " + ",
                              " + ".join(["I(1 / {0})".format(f) for f in inverse_factors])])

    design_formula = full_model
    lm_formula     = response[0] + full_model
    trials         = round(2 * (len(factors) + len(inverse_factors) + 1))

    fixed_variables = fixed_factors

    if budget - len(data[0]) < 0:
        print("Full data does not fit on budget")
        if trials < len(data[0]):
            print("Computing D-Optimal Design")
            output = opt_federov(design_formula, data, trials)
            design = output.rx("design")[0]
        else:
            print("Too few data points for a D-Optimal design")
            design = data

        used_experiments = len(design[0])
        regression, prf_values = anova(design, lm_formula)
        ordered_prf_keys       = sorted(prf_values, key = prf_values.get)
        predicted_best         = predict_best(regression, data)
        fixed_variables        = get_fixed_variables(predicted_best, ordered_prf_keys,
                                                     fixed_factors)
        pruned_data            = prune_data(data, fixed_variables)

        pruned_factors, pruned_inverse_factors = prune_model(factors, inverse_factors,
                                                            ordered_prf_keys)
    else:
        print("Full data fits on budget, picking best value")
        used_experiments = len(data[0])
        prf_values = []
        ordered_prf_keys = []
        pruned_data = []
        pruned_factors = []
        pruned_inverse_factors = []
        predicted_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                                  True)

    return {"prf_values": prf_values,
            "ordered_prf_keys": ordered_prf_keys,
            "predicted_best": predicted_best,
            "pruned_data": pruned_data,
            "pruned_factors": pruned_factors,
            "pruned_inverse_factors": pruned_inverse_factors,
            "fixed_factors": fixed_variables,
            "used_experiments": used_experiments}

def dopt_anova():
    data = utils.read_csv("dopt_anova_experiments/data/search_space.csv", header = True)

    initial_factors = ["elements_number", "y_component_number",
                       "vector_length", "temporary_size",
                       "load_overlap", "threads_number", "lws_y"]

    initial_inverse_factors = ["y_component_number", "lws_y",
                               "elements_number", "threads_number"]

    response = ["time_per_pixel"]

    data = data.rx(StrVector(initial_factors + response))
    data_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                        True)

    step_factors = initial_factors
    step_inverse_factors = initial_inverse_factors
    step_space = data

    fixed_factors = {}

    initial_budget = 58
    budget = initial_budget
    used_experiments = 0
    iterations = 3

    for i in range(iterations):
        if step_space == []:
            break

        step_data = dopt_anova_step(response,
                                    step_factors,
                                    step_inverse_factors,
                                    step_space,
                                    fixed_factors,
                                    budget)

        step_space = step_data["pruned_data"]
        step_factors = step_data["pruned_factors"]
        step_inverse_factors = step_data["pruned_inverse_factors"]
        budget -= step_data["used_experiments"]
        used_experiments += step_data["used_experiments"]
        fixed_factors = step_data["fixed_factors"]

        print("Fixed Factors: " + str(fixed_factors))
        print("Slowdown: " + str(step_data["predicted_best"].rx(response[0])[0][0] / data_best.rx(response[0])[0][0]))
        print("Budget: {0}/{1}".format(used_experiments, initial_budget))

dopt_anova()
#+END_SRC

#+RESULTS:
#+begin_example
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.000231109380007393
Power Transform Step:
Estimated transformation parameter
         Y1
-0.08680373

Heteroscedasticity Test p-value:
0.9475422563588601
Regression Step:
                        Df Sum Sq Mean Sq F value   Pr(>F)
elements_number          1  2.290   2.290  11.967 0.004723 **
y_component_number       1  0.529   0.529   2.765 0.122202
vector_length            1  3.766   3.766  19.676 0.000813 ***
temporary_size           1  0.235   0.235   1.225 0.290002
load_overlap             1  0.079   0.079   0.413 0.532456
threads_number           1  0.018   0.018   0.092 0.767416
lws_y                    1  7.271   7.271  37.987 4.85e-05 ***
I(1/y_component_number)  1  0.110   0.110   0.574 0.463168
I(1/lws_y)               1  2.861   2.861  14.949 0.002242 **
I(1/elements_number)     1  0.164   0.164   0.859 0.372333
I(1/threads_number)      1  0.432   0.432   2.259 0.158663
Residuals               12  2.297   0.191
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1}
{'lws_y': 16, 'vector_length': 1}
Dimensions of Pruned Data: [1] 576   8

Dimensions of Full Data: [1] 23120     8

Fixed Factors: {'lws_y': 16, 'vector_length': 1}
Slowdown: 11.64335230377803
Budget: 24/58
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.06519543239335164
No need to power transform
Regression Step:
                        Df    Sum Sq   Mean Sq F value   Pr(>F)
elements_number          1 6.930e-19 6.930e-19   8.120   0.0191 *
y_component_number       1 4.960e-19 4.960e-19   5.814   0.0392 *
temporary_size           1 5.100e-20 5.100e-20   0.599   0.4589
load_overlap             1 5.400e-20 5.400e-20   0.638   0.4451
threads_number           1 5.024e-18 5.024e-18  58.829 3.09e-05 ***
I(1/y_component_number)  1 2.100e-20 2.100e-20   0.251   0.6284
I(1/elements_number)     1 8.200e-20 8.200e-20   0.959   0.3530
I(1/threads_number)      1 5.726e-18 5.726e-18  67.049 1.84e-05 ***
Residuals                9 7.690e-19 8.500e-20
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
{'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Dimensions of Pruned Data: [1] 8 8

Dimensions of Full Data: [1] 576   8

Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.8225092523512674
Budget: 42/58
Full data fits on budget, picking best value
Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.265157794114108
Budget: 50/58
#+end_example
** June
*** [2018-06-04 Mon]
**** Implementing the DOPT-Anova in Orio
I've started implementing our approach in benchmark problems from SPAPT, which
is provided with Orio. I've been understanding Orio's source code implementing
the glue code, with =rpy2=, to be able to use R packages in Python.

I've reached a problem with =optFederov=. One of the benchmark applications
has ~10^{14}~ possible combinations in total. My first approach was trying to
generate a subset of this search space, as I did before, but this did not work.
The function kept finding "singular design" errors, which mean the determinant
of the candidates it tested are negative according to the documentation.

It is not clear how to fix this, so I am now trying to use the optMonteCarlo
function, which tries to generate samples of the search space as it explores
it with the Federov algorithm.
*** [2018-06-11 Mon]
**** Converting R Model Fits into Formulas
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
A <- data.frame(x1 = sample(1:100, 30, replace = T),
                x2 = sample(1:100, 30, replace = T),
                x3 = sample(1:100, 30, replace = T),
                x4 = sample(1:100, 30, replace = T),
                x5 = sample(1:100, 30, replace = T))

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5

regression <- lm(y ~ x1 + x2 + x3 + x4 + x5, data = A)
formula(regression)

new_formula <- as.formula(substituteDirect(formula(regression), as.list(coef(regression))))
new_formula

predict(regression)
predict(regression, type = "terms")
A
#+END_SRC

#+RESULTS:
#+begin_example
y ~ x1 + x2 + x3 + x4 + x5
y ~ 2.3 + 3.1 + 4.5 + 6.8 + 2.31
      1       2       3       4       5       6       7       8       9      10
1480.86  536.12 1174.93  804.95 1487.49 1427.97 1150.58 1014.75  916.79  958.22
     11      12      13      14      15      16      17      18      19      20
 953.27  438.97  710.39  709.89 1234.76 1034.89 1043.24 1622.64 1114.05  739.34
     21      22      23      24      25      26      27      28      29      30
 848.47  928.56 1320.51  744.19 1066.60  946.50 1235.58 1051.55  860.67  706.07
             x1          x2      x3          x4      x5
1    54.4333333 -129.166667  195.15  284.693333   66.99
2   -49.0666667    1.033333 -133.35 -279.706667  -11.55
3   -81.2666667 -104.366667   73.65  264.293333   13.86
4    15.3333333  125.033333 -191.85  -55.306667  -97.02
5   -39.8666667   22.733333  186.15  305.093333    4.62
6    15.3333333  131.233333 -115.35  318.693333   69.30
7    24.5333333    1.033333 -227.85  318.693333   25.41
8    98.1333333 -104.366667  100.65  -14.506667  -73.92
9    47.5333333  143.633333  -29.85 -211.706667  -41.58
10 -106.5666667  140.533333   28.65 -170.906667   57.75
11   29.1333333   44.433333  141.15 -293.306667   23.10
12   24.5333333  -23.766667 -218.85 -259.306667  -92.40
13   91.2333333  -29.966667 -119.85 -313.706667   73.92
14  -42.1666667  -14.466667   73.65 -320.506667    4.62
15   98.1333333   97.133333   55.65   46.693333  -71.61
16  -72.0666667  121.933333  186.15 -306.906667   97.02
17   -0.7666667   16.533333 -169.35  264.293333  -76.23
18   52.1333333  109.533333  163.65  203.093333   85.47
19   -0.7666667  109.533333  -38.85  155.493333 -120.12
20   10.7333333 -119.866667  186.15 -293.306667  -53.13
21   77.4333333   32.033333 -187.35   33.093333 -115.50
22  -72.0666667  -64.066667  136.65  -55.306667  -25.41
23  -30.6666667  156.033333   55.65  121.493333    9.24
24 -101.9666667 -141.566667 -223.35  243.893333  -41.58
25    1.5333333  -67.166667  -97.35  121.493333   99.33
26    1.5333333   38.233333   -7.35 -170.906667   76.23
27   36.0333333 -104.366667  -88.35  311.893333   71.61
28   75.1333333 -147.766667  114.15    5.893333   -4.62
29  -72.0666667  -95.066667  -20.85   39.893333    0.00
30  -83.5666667 -144.666667  172.65 -293.306667   46.20
attr(,"constant")
[1] 1008.76
   x1  x2 x3  x4  x5       y
1  78   8 96  95  86 1480.86
2  33  50 23  12  52  536.12
3  19  16 69  92  63 1174.93
4  61  90 10  45  15  804.95
5  37  57 94  98  59 1487.49
6  61  92 27 100  87 1427.97
7  65  50  2 100  68 1150.58
8  97  16 75  51  25 1014.75
9  75  96 46  22  39  916.79
10  8  95 59  28  82  958.22
11 67  64 84  10  67  953.27
12 65  42  4  15  17  438.97
13 94  40 26   7  89  710.39
14 36  45 69   6  59  709.89
15 97  81 65  60  26 1234.76
16 23  89 94   8  99 1034.89
17 54  55 15  92  24 1043.24
18 77  85 89  83  94 1622.64
19 54  85 44  76   5 1114.05
20 59  11 94  10  34  739.34
21 88  60 11  58   7  848.47
22 23  29 83  45  46  928.56
23 41 100 65  71  61 1320.51
24 10   4  3  89  39  744.19
25 55  28 31  71 100 1066.60
26 55  62 51  28  90  946.50
27 70  16 33  99  88 1235.58
28 87   2 78  54  55 1051.55
29 23  19 48  59  57  860.67
30 18   3 91  10  77  706.07
#+end_example
*** [2018-06-13 Wed]
**** Stepwise Regression in R
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

A <- data.frame(x1 = sample(1:100, 100, replace = T),
                x2 = sample(1:100, 100, replace = T),
                x3 = sample(1:100, 100, replace = T),
                x4 = sample(1:100, 100, replace = T),
                x5 = sample(1:100, 100, replace = T),
                x6 = sample(1:100, 100, replace = T),
                x7 = sample(1:100, 100, replace = T),
                x8 = sample(1:100, 100, replace = T)
)

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5 + 5.2 * (A$x8 * A$x2) + 3.1 * (1 / A$x7)

regression = lm(y ~ ., data = A[1:30, ])
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
A[A$y == min(A$y), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x1           1 2.566e+07 2.566e+07   1.852  0.18796
x2           1 2.007e+09 2.007e+09 144.892 6.88e-11 ***
x3           1 1.796e+05 1.796e+05   0.013  0.91044
x4           1 2.557e+08 2.557e+08  18.458  0.00032 ***
x5           1 4.264e+08 4.264e+08  30.779 1.66e-05 ***
x6           1 1.331e+08 1.331e+08   9.604  0.00544 **
x7           1 1.376e+07 1.376e+07   0.993  0.33033
x8           1 1.639e+09 1.639e+09 118.324 4.35e-10 ***
Residuals   21 2.909e+08 1.385e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
   x2 x8
14 30  1
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression = update(regression, . ~ . - x1 - x3 - x4 - x5 - x6 - x7)
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x2           1 1.953e+09 1.953e+09   127.8 9.58e-12 ***
x8           1 2.427e+09 2.427e+09   158.8 8.03e-13 ***
Residuals   27 4.126e+08 1.528e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
#+end_example
*** [2018-06-14 Thu]
**** Stepwise Regression in R
Creating two datasets, a complete one and a sample of it.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

complete_A <- expand.grid(x1 = seq(1, 10, 1),
                          x2 = seq(1, 10, 1),
                          x3 = seq(1, 10, 1),
                          x4 = seq(1, 10, 1),
                          x5 = seq(1, 10, 1),
                          x6 = seq(1, 10, 1),
                          x7 = seq(1, 10, 1)
)

complete_A$y = 2.3 * complete_A$x1 - 3.1 * complete_A$x2 + 4.5 * complete_A$x3 + 2.5 * (1 / complete_A$x2) - 6.8 * complete_A$x4 + 2.31 * (complete_A$x5 * complete_A$x1) - 4.2 * (complete_A$x2 * complete_A$x4)

A <- complete_A[sample(1:nrow(complete_A), 100, replace = F), ]
#+END_SRC

#+RESULTS:

Now we use the sample from the full set to fit the model. We then use another
sample for the predictions. In a real application this sample would not be
measured, since we want only the parameter values.

We are using the =stepAIC= function, that minimizes the [[https://en.wikipedia.org/wiki/Akaike_information_criterion][Akaike Information
Criterion]], to find the model that best fits the experiment data. We will allow
both removal and addition of model variables, up to the intial model, or "full
model". The initial or "null" model is just a constant.

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
experiment_data = complete_A[sample(1:nrow(complete_A), 2 * nrow(A), replace = F), ]

null = lm(y ~ 1, data = A)
full = lm(y ~ . + I(1 / x2) + I(1 / x3), data = A)

regression = stepAIC(null, scope = list(upper = full), direction = "both", trace = 1)

summary.aov(regression)

formula(full)
formula(regression)
r_names <- attr(terms(regression), "term.labels")
r_names <- r_names[r_names %in% names(complete_A)]
r_names
#+END_SRC

#+RESULTS:
#+begin_example
Start:  AIC=964.42
y ~ 1

          Df Sum of Sq     RSS    AIC
+ x2       1    653372  859229 909.86
+ x4       1    441689 1070912 931.89
+ I(1/x2)  1    383017 1129584 937.22
+ x5       1    129020 1383581 957.50
+ x1       1     48781 1463820 963.14
+ I(1/x3)  1     46868 1465733 963.27
<none>                 1512601 964.42
+ x3       1     10526 1502075 965.72
+ x6       1      9805 1502796 965.77
+ x7       1       309 1512292 966.40

Step:  AIC=909.86
y ~ x2

          Df Sum of Sq     RSS    AIC
+ x4       1    455540  403689 836.32
+ x5       1     86545  772684 901.25
+ I(1/x3)  1     39119  820110 907.20
+ x1       1     30172  829057 908.29
+ x3       1     29944  829285 908.31
<none>                  859229 909.86
+ x7       1      6710  852519 911.08
+ x6       1      5989  853240 911.16
+ I(1/x2)  1      1708  857521 911.66
- x2       1    653372 1512601 964.42

Step:  AIC=836.32
y ~ x2 + x4

          Df Sum of Sq     RSS    AIC
+ x5       1    133553  270136 798.15
+ x1       1    109629  294061 806.64
+ I(1/x3)  1     36532  367158 828.84
+ x3       1     32372  371317 829.96
<none>                  403689 836.32
+ I(1/x2)  1      1695  401994 837.90
+ x6       1      1190  402500 838.03
+ x7       1       134  403555 838.29
- x4       1    455540  859229 909.86
- x2       1    667223 1070912 931.89

Step:  AIC=798.15
y ~ x2 + x4 + x5

          Df Sum of Sq    RSS    AIC
+ x1       1    126352 143785 737.09
+ x3       1      9771 260366 796.47
+ I(1/x3)  1      8609 261527 796.91
<none>                 270136 798.15
+ I(1/x2)  1      5190 264946 798.21
+ x6       1       246 269890 800.06
+ x7       1       148 269988 800.10
- x5       1    133553 403689 836.32
- x4       1    502548 772684 901.25
- x2       1    615682 885819 914.91

Step:  AIC=737.09
y ~ x2 + x4 + x5 + x1

          Df Sum of Sq    RSS    AIC
+ x3       1     15072 128712 728.02
+ I(1/x3)  1      9874 133911 731.98
<none>                 143785 737.09
+ I(1/x2)  1      1196 142589 738.25
+ x6       1        99 143686 739.02
+ x7       1        69 143716 739.04
- x1       1    126352 270136 798.15
- x5       1    150276 294061 806.64
- x2       1    578998 722782 896.57
- x4       1    594448 738233 898.68

Step:  AIC=728.02
y ~ x2 + x4 + x5 + x1 + x3

          Df Sum of Sq    RSS    AIC
<none>                 128712 728.02
+ I(1/x2)  1      1074 127639 729.18
+ x7       1        38 128674 729.99
+ x6       1        20 128692 730.00
+ I(1/x3)  1         0 128712 730.02
- x3       1     15072 143785 737.09
- x5       1    122550 251263 792.91
- x1       1    131653 260366 796.47
- x2       1    592170 720882 898.31
- x4       1    594763 723475 898.67
            Df Sum Sq Mean Sq F value   Pr(>F)
x2           1 653372  653372  477.17  < 2e-16 ***
x4           1 455540  455540  332.69  < 2e-16 ***
x5           1 133553  133553   97.53 3.37e-16 ***
x1           1 126352  126352   92.28 1.26e-15 ***
x3           1  15072   15072   11.01  0.00129 **
Residuals   94 128712    1369
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + I(1/x2) + I(1/x3)
y ~ x2 + x4 + x5 + x1 + x3
[1] "x2" "x4" "x5" "x1" "x3"
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
values = predict(regression, experiment_data)
complete_A[complete_A$y == min(complete_A$y), "y"] / experiment_data[values == min(values), "y"]

values = complete_A[sample(1:nrow(complete_A), nrow(A), replace = F), ]
complete_A[complete_A$y == min(complete_A$y), "y"] / values[values$y == min(values$y), "y"]
#+END_SRC

#+RESULTS:
#+begin_example
  [1] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
  [9] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [17] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [25] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [33] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [41] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [49] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [57] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [65] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [73] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [81] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [89] 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702 1.153702
 [97] 1.153702 1.153702 1.153702 1.153702
  [1] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
  [9] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [17] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [25] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [33] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [41] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [49] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [57] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [65] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [73] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [81] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [89] 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883 1.245883
 [97] 1.245883 1.245883 1.245883 1.245883
#+end_example
*** [2018-06-15 Fri]
**** Working with =optMonteCarlo=
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(MASS)

model_formula = formula(~ . + I(1 / (1 + x2)) + I(1 / (1 + x3)) + I(1 / (1 + x4)))
model_formula

design_data = data.frame(var = c("x1", "x2", "x3", "x4", "x5", "x6", "x7"),
                         low = c(rep(0, 7)),
                         high = c(9, 18, 18, 2, 1, 6, 40),
                         center = c(rep(0, 7)),
                         nLevels = c(10, 19, 19, 3, 2, 7, 41),
                         round = c(rep(0, 7)),
                         factor = c(rep(F, 7)),
                         mix = c(rep(F, 7)))

design_data

optMonteCarlo(model_formula, design_data, nTrials = 25)
#+END_SRC

#+RESULTS:
#+begin_example
~. + I(1/(1 + x2)) + I(1/(1 + x3)) + I(1/(1 + x4))
  var low high center nLevels round factor   mix
1  x1   0    9      0      10     0  FALSE FALSE
2  x2   0   18      0      19     0  FALSE FALSE
3  x3   0   18      0      19     0  FALSE FALSE
4  x4   0    2      0       3     0  FALSE FALSE
5  x5   0    1      0       2     0  FALSE FALSE
6  x6   0    6      0       7     0  FALSE FALSE
7  x7   0   40      0      41     0  FALSE FALSE
$D
[1] 1.248469

$A
[1] 42.83773

$Ge
[1] 0.833

$Dea
[1] 0.818

$design
   x1 x2 x3 x4 x5 x6 x7
1   1  2  6  2  0  6 23
2   1  0  6  1  0  2  5
3   7  0 14  1  0  3 30
4   4 10  0  0  0  5 35
5   2  9 18  2  1  4 32
6   1  5 15  0  0  0 32
7   9 18  0  1  0  5  0
8   8 14 16  0  1  0 12
9   8 17  4  1  0  6  0
10  3 17  4  0  0  4 35
11  3 13 18  1  1  6 11
12  9  5 13  1  0  4  3
13  1  9  7  2  0  0 24
14  2  6  3  1  1  0 23
15  2 18 17  1  1  1 27
16  9  3  4  0  1  5 27
17  4  0 14  2  1  0  9
18  7 12  0  1  1  1 20
19  8 18 12  2  1  2 38
20  2  0 17  1  0  2 31
21  5  0  0  1  1  2 10
22  9  3  6  1  1  4 31
23  9 14 16  2  0  1  3
24  2 14  2  2  1  5  4
25  1  1 15  0  1  3  7
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
set.seed(66182)
model_formula <- formula(~ T1_J + T1_K + T2_I + T2_J + T2_K + ACOPY_x + ACOPY_y + U1_I + U_I + U_K + RT_J + RT_K + SCR + VEC1 + VEC2 + I(1 / (1 + T1_J)) + I(1 / (1 + T1_K)) + I(1 / (1 + T2_I)) + I(1 / (1 + T2_J)) + I(1 / (1 + T2_K)) + I(1 / (1 + U1_I)) + I(1 / (1 + U_I)) + I(1 / (1 + U_K)) + I(1 / (1 + RT_J)) + I(1 / (1 + RT_K)))

design_data <- data.frame(var = c("T1_J", "T1_K", "T2_I", "T2_J", "T2_K", "ACOPY_x",
                                  "ACOPY_y", "U1_I", "U_I", "U_K", "RT_J", "RT_K",
                                  "SCR", "VEC1", "VEC2"),
                          low = rep(0, 15),
                          high = c(6, 6, 6, 6, 6, 1, 1, 29, 29, 29, 2, 2, 1, 1, 1),
                          center = rep(0, 15),
                          nLevels = c(7, 7, 7, 7, 7, 2, 2, 30, 30, 30, 3, 3, 2, 2, 2),
                          round = rep(0, 15),
                          factor = c(rep(F, 15)),
                          mix = c(rep(F, 15)))

design_data

optMonteCarlo(model_formula, data = design_data, nTrials = 37)
#+END_SRC

#+RESULTS:
#+begin_example
       var low high center nLevels round factor   mix
1     T1_J   0    6      0       7     0  FALSE FALSE
2     T1_K   0    6      0       7     0  FALSE FALSE
3     T2_I   0    6      0       7     0  FALSE FALSE
4     T2_J   0    6      0       7     0  FALSE FALSE
5     T2_K   0    6      0       7     0  FALSE FALSE
6  ACOPY_x   0    1      0       2     0  FALSE FALSE
7  ACOPY_y   0    1      0       2     0  FALSE FALSE
8     U1_I   0   29      0      30     0  FALSE FALSE
9      U_I   0   29      0      30     0  FALSE FALSE
10     U_K   0   29      0      30     0  FALSE FALSE
11    RT_J   0    2      0       3     0  FALSE FALSE
12    RT_K   0    2      0       3     0  FALSE FALSE
13     SCR   0    1      0       2     0  FALSE FALSE
14    VEC1   0    1      0       2     0  FALSE FALSE
15    VEC2   0    1      0       2     0  FALSE FALSE
$D
[1] 0.2969417

$A
[1] 73.70555

$Ge
[1] 0.482

$Dea
[1] 0.341

$design
   T1_J T1_K T2_I T2_J T2_K ACOPY_x ACOPY_y U1_I U_I U_K RT_J RT_K SCR VEC1
1     4    0    0    4    1       0       1   28   1   1    2    1   0    0
2     0    5    1    4    6       1       0    3  14  15    1    1   1    1
3     1    4    4    0    0       1       1   16  14   1    0    1   0    0
4     6    2    4    3    5       1       0    7  28  10    0    2   0    0
5     1    2    0    4    3       1       1   11   6   6    1    1   0    0
6     6    4    1    1    1       1       0   27   7  23    1    0   1    0
7     4    5    2    4    1       0       0    4  16  27    0    1   0    1
8     5    2    4    3    1       0       1   13   1   0    1    0   1    1
9     3    6    2    5    0       1       1    0  18  14    2    0   1    1
10    2    2    0    2    6       1       1    0   3  28    2    1   0    1
11    5    1    5    2    0       1       1   12   8   8    1    2   1    1
12    1    0    1    5    4       1       0   28   0  28    1    2   0    1
13    3    2    4    1    0       0       0    1   0   8    1    1   1    1
14    6    1    0    6    2       0       1   29  28  27    0    1   1    0
15    1    0    5    2    5       0       1   13  17  18    0    2   1    1
16    0    2    6    5    5       0       1   25   0   1    0    0   0    0
17    6    4    0    3    6       0       0   29  19  23    1    2   0    1
18    2    2    3    5    3       0       1   19   1  26    0    2   1    0
19    4    6    6    5    6       1       1   12  15  26    1    1   0    1
20    3    6    0    2    2       0       1   23  15  25    0    0   1    1
21    2    1    2    6    2       1       1   24  26   0    1    2   1    0
22    0    4    4    4    3       0       1   27   3  24    2    2   0    0
23    6    6    1    0    4       0       1    0  28   7    2    1   0    0
24    2    0    2    0    6       0       1    6  22  23    2    1   1    0
25    2    1    5    2    5       0       0    0  17  24    1    1   0    1
26    1    3    1    4    5       1       1   29  10   7    0    0   0    1
27    4    5    5    0    1       1       0   17   3   5    0    0   1    1
28    0    2    1    0    5       0       0    6  25  12    1    2   0    1
29    2    6    5    4    6       0       1    0  25  16    0    1   1    0
30    0    5    4    2    0       0       0   17  24  12    0    1   0    0
31    1    1    0    2    0       0       1   16  28  19    1    0   0    0
32    3    6    4    6    3       0       0    3   3   1    1    0   0    0
33    6    2    5    6    0       0       0   12  12   1    2    1   0    0
34    2    4    6    2    5       1       0   28  11  16    2    1   1    1
35    3    4    3    6    3       0       1   10  27   8    2    0   0    1
36    0    2    5    2    4       1       0   10  21  29    2    0   1    0
37    6    3    5    3    1       1       1   26  27   6    1    1   0    1
   VEC2
1     1
2     1
3     0
4     1
5     1
6     1
7     0
8     1
9     1
10    1
11    0
12    0
13    0
14    1
15    1
16    0
17    0
18    1
19    1
20    0
21    0
22    0
23    1
24    0
25    1
26    1
27    1
28    1
29    0
30    1
31    1
32    1
33    0
34    1
35    1
36    0
37    1
#+end_example
*** [2018-06-21 Thu]
**** A Flexible Distributed Optimization Scheme with Asynchronous, Scarse, and Sparse Communications
- Presenter: Frank Iutzeler (http://www.iutzeler.org/)
- Context: Distributed Learning
  - Global Learning Objective
  - Local data
- Proximal Gradient Algorithms
*** [2018-06-28 Thu]
**** Tweaking DLMT
***** Initial Sample for =OptFederov=
- Will take a long time to generate if we choose a large size, because
  constraint checking restricts a lot of the search space
- Will not provide enough candidates for =OptFederov= if the sample size is
  small. This can result in small D-Efficiency for the generated experiments
***** =OptFederov= Function
****** nTrials
- The simplest way to deal with runtime errors is to increase the number of
  experiments generated by =OptFederov=
- Using more trials allows for more failures before ANOVA breaks
- Should we re-generate the design after failures?
****** frml
- Using the inverse of variables in the initial model requires special care to
  remove 2-level factors that may appear due to constraints
***** Problems in Analysis
- When valid experiments in the design fail, what happens with the validity of ANOVA?
- What are the best ways to go around that?
***** Threshold for Removing Variables
- We are using an heuristic to select how many variables to remove at each step
** July
*** [2018-07-03 Tue]
**** Round Table LIG Day
- Blockchain
- HPC & Big Data Convergence
- Machine Learning
- Reproducibility
*** [2018-07-04 Wed]
**** Plotting Sampling Strategies
***** Random Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)

data = data.frame(x1 = sample(0:20, 100, replace = T),
                  x2 = sample(0:20, 100, replace = T))

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figure7g2xWl.png]]

***** LHS Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)

data <- lhs.design(nruns = 100 ,nfactors = 2, digits = 0, type = "maximin",
                   factor.names = list(x1 = c(0, 20), x2 = c(0, 20)))

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figureUvZvfi.png]]
***** D-Optimal Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~., full_factorial, nTrials = 20)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figuretQupRL.png]]

***** D-Optimal Sampling with Interactions
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + .^2, full_factorial, nTrials = 20)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figureCsk30V.png]]
***** D-Optimal Sampling with Quadratic Terms
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = 30)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figure4tuLAm.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
head(model.matrix(~. + I(x1 ^ 2) + I(x2 ^ 2), full_factorial))
#+END_SRC

#+RESULTS:
:   (Intercept)  x1  x2 I(x1^2) I(x2^2)
: 1           1 -19 -19     361     361
: 2           1 -17 -19     289     361
: 3           1 -15 -19     225     361
: 4           1 -13 -19     169     361
: 5           1 -11 -19     121     361
: 6           1  -9 -19      81     361

***** D-Optimal Sampling with Cubic Terms
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~ . + cubic(.), full_factorial, nTrials = 30)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figurel94xYu.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
head(model.matrix(~ . + cubic(.), full_factorial))
#+END_SRC

#+RESULTS:
:   (Intercept)  x1  x2 I(x1^2) I(x2^2) I(x1^3) I(x2^3) x1:x2
: 1           1 -19 -19     361     361   -6859   -6859   361
: 2           1 -17 -19     289     361   -4913   -6859   323
: 3           1 -15 -19     225     361   -3375   -6859   285
: 4           1 -13 -19     169     361   -2197   -6859   247
: 5           1 -11 -19     121     361   -1331   -6859   209
: 6           1  -9 -19      81     361    -729   -6859   171
***** I-Optimal Sampling
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(AlgDesign)

full_factorial <- gen.factorial(c(20, 20))

names(full_factorial) <- c("x1", "x2")

output <- optFederov(~., full_factorial, criterion = "I", nTrials = 60)
data <- output$design

ggplot(data, aes(x = x1, y = x2)) +
    geom_point(shape = 19, alpha = 0.6) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XeXoRm/figure0Y9ZHG.png]]
**** Experiments with the ATAX application
I've been using the smallest input size for the atax application. So far I am
not sure if the larger sizes even work properly, since some of then do not use
the validation code. Some of the also do not have the same parameters.

Experiments with DLMT using all factors and their inverses, when applicable, is
not giving results much different than random search. The model might be more or
less complex. The D-Optimality of the designs is usually low.

I am also not sure of the impact of sample sizes in the two steps where they are
used: the sample for the Fedorov algorithm and the sample for predicting the
best result. These samples are not evaluated, but must be validated using the
constraints provided by the application.

Next steps:
- Run DLMT experiments with a simpler model
- Run DLMT experiments using interactions in the model
- Compare results for other applications and input sizes
- Compare results with other search algorithms
- Address issues in [[Tweaking DLMT][Tweaking DLMT]]
*** [2018-07-05 Thu]
**** Building D-Optimal Designs with Interactions
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(1000),
                   x2 = runif(1000),
                   x3 = runif(1000),
                   x4 = runif(1000),
                   x5 = runif(1000),
                   x6 = runif(1000))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:          x1         x2        x3        x4        x5        x6        y
: 1 0.3145343 0.22122070 0.5791998 0.6329657 0.9590482 0.5580798 6.236130
: 2 0.4204504 0.52465529 0.9820209 0.4381713 0.8702131 0.8412574 7.397551
: 3 0.9683900 0.07251716 0.8357594 0.3334859 0.1510384 0.7234758 5.482052
: 4 0.1489611 0.84208199 0.3208823 0.8697806 0.5734586 0.3794286 6.219130
: 5 0.1562686 0.01527451 0.8520008 0.9050570 0.7896101 0.8236433 6.724943
: 6 0.8644607 0.33800877 0.4537771 0.9145758 0.8455194 0.3449356 7.638418

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
output <- optFederov(~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, data, nTrials = 14)
output

design <- output$design
#+END_SRC

#+RESULTS:
#+begin_example
$D
[1] 0.1539217

$A
[1] 11.50989

$Ge
[1] 0.8

$Dea
[1] 0.779

$design
            x1         x2          x3          x4          x5          x6
5   0.15626861 0.01527451 0.852000780 0.905057031 0.789610110 0.823643283
60  0.14111468 0.06047899 0.005816296 0.122148020 0.241837807 0.204990197
82  0.83102789 0.24063190 0.066638218 0.835985614 0.938030672 0.274673609
167 0.01083552 0.86262930 0.919079563 0.994813522 0.326322796 0.348092876
188 0.99944823 0.48202436 0.184919251 0.034891482 0.945193022 0.029429601
431 0.65156391 0.85404254 0.997202041 0.039830894 0.868841200 0.898136727
479 0.07126425 0.02491834 0.072766728 0.091690738 0.506438140 0.968787553
488 0.90546459 0.08732170 0.985465054 0.616232353 0.107418757 0.003533801
489 0.15103343 0.58518849 0.999827211 0.002674163 0.934751620 0.217067372
566 0.77890381 0.88071788 0.096931037 0.982260734 0.169843512 0.991753875
852 0.03508049 0.98718423 0.066761902 0.912848081 0.375775029 0.103496414
909 0.42360652 0.94109188 0.988913755 0.840630643 0.956933183 0.841327438
915 0.74483130 0.82045704 0.042712603 0.007349035 0.022841742 0.893580211
968 0.99884604 0.02018211 0.894035780 0.503326149 0.001980576 0.860871009
            y
5    6.724943
60   1.487574
82   6.523872
167  7.935617
188  4.651014
431  6.727184
479  3.111935
488  5.092613
489  4.243379
566  7.433653
852  4.648488
909 10.527218
915  3.944372
968  5.940554

$rows
 [1]   5  60  82 167 188 431 479 488 489 566 852 909 915 968
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression <- aov(y ~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, design)
summary(regression)
#+END_SRC

#+RESULTS:
#+begin_example
            Df Sum Sq Mean Sq   F value Pr(>F)
x1           1  1.365   1.365 3.407e+30 <2e-16 ***
x2           1 14.548  14.548 3.630e+31 <2e-16 ***
x3           1 17.450  17.450 4.355e+31 <2e-16 ***
x4           1 19.636  19.636 4.900e+31 <2e-16 ***
x5           1  4.612   4.612 1.151e+31 <2e-16 ***
x6           1  5.692   5.692 1.420e+31 <2e-16 ***
x2:x3:x4     1  3.020   3.020 7.537e+30 <2e-16 ***
Residuals    6  0.000   0.000
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#+end_example
*** [2018-07-09 Mon]
**** Paper Review: Assessing Time Predictability Features of ARM big.LITTLE Multicores
This paper presents qualitative and quantitative assessments of some
characteristics of the ARM big.LITTLE architecture. The objective of the paper
is to measure the applicability of this architecture to critical real-time
embedded systems. The paper is well organized, and it was easy to read and
understand.

The qualitative assessment uses the technical reference manuals of the ARM
Cortex-A53 processor and the ARM Juno R2 board. The quantitative assessment uses
a simple stress benchmark application and measures the cycles per memory access
in different scenarios.

The quantitative assessment is based on three experiments: a scenario without
contention, or isolated, a scenario with contention, and a scenario with
contention and no-ops between memory accesses. The results in Figures 3 and 4
show the median of 1000 measurements for each vector size. The Figures do not
show the standard deviations of the measurements.

It is not discussed whether the median is the best choice to represent the
measurements, or whether the variability of measurements was relevant. Without
knowing the variability of measurements it is hard to know if the differences
observed in each experimental scenario are meaningful. I suggest to include the
standard deviation of each measurement in the data presented. Also, unless there
is a strong reason for using the median, I suggest using the mean of the 1000
measurements. I believe this will provide better support for the conclusions of
the paper.

I also believe that measuring the cycles per access in a more comprehensive
benchmark suite would help support the arguments presented in the paper, as well
as using real applications on critical real-time embedded systems. My
recommendation is that this paper should be submitted again after addressing the
issues with data presentation and analysis, and possibly with more experimental
evaluations using different benchmark applications, potentially using
measurements in real applications.
*** [2018-07-10 Tue]
**** Building Models with Interactions
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:           x1        x2        x3        x4         x5        x6        y
: 1 0.18076715 0.4140549 0.4354110 0.8747773 0.44056661 0.2059766 5.257260
: 2 0.04511744 0.3389018 0.6758916 0.7470131 0.95302942 0.7220186 6.740219
: 3 0.64351052 0.8803408 0.3407263 0.4162235 0.78808724 0.6384063 6.607299
: 4 0.13375844 0.4154362 0.5740708 0.9881421 0.22562579 0.0736479 5.206433
: 5 0.70947263 0.9798590 0.9904583 0.8542580 0.13675687 0.7153690 9.525387
: 6 0.68455712 0.3255436 0.9922359 0.2413945 0.05896366 0.5179531 4.732390

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ x2:x3:x4 + x1 + x2 + x3 + x4 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)         x1        x2        x3        x4         x5        x6
1           1 0.18076715 0.4140549 0.4354110 0.8747773 0.44056661 0.2059766
2           1 0.04511744 0.3389018 0.6758916 0.7470131 0.95302942 0.7220186
3           1 0.64351052 0.8803408 0.3407263 0.4162235 0.78808724 0.6384063
4           1 0.13375844 0.4154362 0.5740708 0.9881421 0.22562579 0.0736479
5           1 0.70947263 0.9798590 0.9904583 0.8542580 0.13675687 0.7153690
6           1 0.68455712 0.3255436 0.9922359 0.2413945 0.05896366 0.5179531
    x2:x3:x4
1 0.15770839
2 0.17111149
3 0.12484840
4 0.23566183
5 0.82906553
6 0.07797429
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(dplyr)
model_matrix <- as.data.frame(model_matrix)
removed_interactions <- names(Filter(function(x)(length(unique(x)) == 1), model_matrix))
removed_interactions
#+END_SRC

#+RESULTS:
: [1] "(Intercept)"
*** [2018-07-13 Fri]
**** Interesting Paper using SPAPT
- [[file:~/Dropbox/papers/autotuning/2017/ogilvie2017minimizing.pdf][Minimizing the Cost of Iterative Compilation with Active Learning]]
**** Building Models with Quadratic Terms
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:           x1        x2        x3         x4        x5        x6        y
: 1 0.03468630 0.5173048 0.1087729 0.08684336 0.6359173 0.9587616 3.802525
: 2 0.98720044 0.4519941 0.9221820 0.28453285 0.9660307 0.4632033 7.232514
: 3 0.31504150 0.9030804 0.4090276 0.21641727 0.5562343 0.5831133 4.864308
: 4 0.48529389 0.7517897 0.3670082 0.37152128 0.9808670 0.4651149 6.086757
: 5 0.07717325 0.1913457 0.3366301 0.82366125 0.7304363 0.9233006 6.068915
: 6 0.29731807 0.5504496 0.2815305 0.21052078 0.8033675 0.7820747 4.992076

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ I(x2 ^ 2) + x1 + x2 + x3 + x4 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)    I(x2^2)         x1        x2        x3         x4        x5
1           1 0.26760424 0.03468630 0.5173048 0.1087729 0.08684336 0.6359173
2           1 0.20429871 0.98720044 0.4519941 0.9221820 0.28453285 0.9660307
3           1 0.81555417 0.31504150 0.9030804 0.4090276 0.21641727 0.5562343
4           1 0.56518779 0.48529389 0.7517897 0.3670082 0.37152128 0.9808670
5           1 0.03661316 0.07717325 0.1913457 0.3366301 0.82366125 0.7304363
6           1 0.30299480 0.29731807 0.5504496 0.2815305 0.21052078 0.8033675
         x6
1 0.9587616
2 0.4632033
3 0.5831133
4 0.4651149
5 0.9233006
6 0.7820747
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- as.data.frame(model_matrix)
removed_interactions <- names(Filter(function(x)(length(unique(x)) == 1), model_matrix))
removed_interactions
#+END_SRC

#+RESULTS:
: [1] "(Intercept)"
*** [2018-07-16 Mon]
**** Temporary Parameters for DLMT
- Using quadratic terms yielded better results: Importance of the model
- Comparing with O2: The same was done in this [[Interesting Paper using SPAPT]]
- Removed binary variables: Caused runtime errors; Removed in other works too
- Less iterations: Trying to minimize executions
**** Plotting Orio Experiments Data (atax, O2, quadratic terms, Xeon E5 2630 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output
#+BEGIN_SRC shell
cd orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/ && ./db2csv.py
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/results.csv")
str(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup_O3))
data_median <- ddply(data, .(technique), summarize, median = median(speedup_O3))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup_O3),
                   err = 2 * sd(speedup_O3) / sqrt(length(speedup_O3)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup_O3))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  18 variables:
 $ id        : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT_K      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ T1_I      : int  1 16 1 16 16 1 16 16 32 1 ...
 $ T1_J      : int  16 16 128 16 256 256 128 256 16 128 ...
 $ T1_K      : int  32 64 512 512 1 512 256 1 64 128 ...
 $ U_K       : int  1 26 28 1 11 1 1 21 26 12 ...
 $ U_J       : int  18 28 19 11 1 22 8 10 26 1 ...
 $ U_I       : int  10 1 1 17 8 11 8 1 1 12 ...
 $ technique : Factor w/ 2 levels "DLMT","RS": 2 2 2 2 2 2 2 2 2 2 ...
 $ U1_I      : int  27 14 20 14 23 21 9 1 19 3 ...
 $ speedup_O3: num  1.68 1.67 1.66 1.71 1.68 ...
 $ T2_K      : int  2048 256 1 1024 256 1024 512 128 1 256 ...
 $ T2_J      : int  2048 256 2048 256 256 1 128 256 128 1024 ...
 $ T2_I      : int  1 1024 1 256 2048 1024 64 512 512 1 ...
 $ points    : int  100 100 100 100 100 100 100 100 100 100 ...
 $ RT_I      : int  1 8 8 8 1 1 1 8 8 1 ...
 $ cost_mean : num  0.109 0.109 0.11 0.107 0.109 ...
 $ RT_J      : int  8 1 1 1 8 8 1 1 1 8 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup_O3), #, y = 0.1 * ..density..),
    binwidth = 0.05) +
    labs(y = "Frequency", x = "Speedup vs O2") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 100)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 30, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 30, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "", breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XqTCQN/figure6NdsNM.png]]
**** Plotting Orio Experiments Data (atax, O2, quadratic terms, Xeon E3 1230 v2)
#+HEADER: :results output
#+BEGIN_SRC shell
git clone https://github.com/phrb/orio_experiments.git || (cd orio_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

#+HEADER: :results output
#+BEGIN_SRC shell
cd orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O2_quad/ && ./db2csv.py
#+END_SRC

#+RESULTS:

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
library(plyr)

data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1230_v2/atax0_O2_quad/results.csv")
str(data)

data_mean <- ddply(data, .(technique), summarize, mean = mean(speedup_O3))
data_median <- ddply(data, .(technique), summarize, median = median(speedup_O3))
data_error <- ddply(data, .(technique), summarize, mean = mean(speedup_O3),
                   err = 2 * sd(speedup_O3) / sqrt(length(speedup_O3)))
data_max <- ddply(data, .(technique), summarize, max = max(speedup_O3))
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  18 variables:
 $ id        : int  1 2 3 4 5 6 7 8 9 10 ...
 $ RT_K      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ T1_I      : int  512 512 1 32 64 128 16 512 32 512 ...
 $ T1_J      : int  16 512 1 1 1 512 1 512 16 128 ...
 $ T1_K      : int  128 1 512 1 1 1 128 512 1 512 ...
 $ U_K       : int  1 1 1 30 1 1 1 12 1 17 ...
 $ U_J       : int  16 21 24 21 22 15 17 1 9 21 ...
 $ U_I       : int  22 15 6 1 10 15 18 19 21 1 ...
 $ technique : Factor w/ 2 levels "DLMT","RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ U1_I      : int  28 30 5 16 14 4 9 2 16 15 ...
 $ speedup_O3: num  1.51 1.53 1.5 1.52 1.52 ...
 $ T2_K      : int  256 1 1 1 1 2048 256 2048 2048 1 ...
 $ T2_J      : int  2048 1024 128 1024 512 1 1 1 2048 2048 ...
 $ T2_I      : int  2048 1 2048 128 1 1 256 2048 2048 2048 ...
 $ points    : int  105 105 162 136 139 114 126 115 123 118 ...
 $ RT_I      : int  8 8 1 8 8 8 8 8 8 8 ...
 $ cost_mean : num  0.0938 0.093 0.0945 0.0935 0.0934 ...
 $ RT_J      : int  1 1 8 1 1 1 1 1 1 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    facet_grid(technique ~ .) +
    geom_histogram(aes(speedup_O3), #, y = 0.01 * ..density..),
    binwidth = 0.05) +
    labs(y = "Frequency", x = "Speedup vs O2") +
    coord_cartesian(xlim = c(1, 2), ylim = c(0, 100)) +
    geom_curve(data = data_max, aes(x = max + 0.1, y = 30, xend = max, yend = max), arrow = arrow(length = unit(0.03, "npc")), curvature = 0.2) +
    geom_text(aes(x = max + 0.12, y = 30, label = "max"), data = data_max) +
    geom_rect(data = data_error, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 100, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), data_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), data_mean, color = "red", linetype = 2) +
    scale_fill_discrete(name = "",breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-AyGCXu/figureKUTrtW.png]]
**** Building Models with Quadratic Terms II
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)

data <- data.frame(x1 = runif(10),
                   x2 = runif(10),
                   x3 = runif(10),
                   x4 = runif(10),
                   x5 = runif(10),
                   x6 = runif(10))

data$y <- 2 * data$x1 + data$x2 + 1.2 * data$x3 + 1.4 * data$x4 + 1.3 * data$x4 + 1.9 * data$x5 + 1.7 * data$x6 + (2.6 * data$x3 * data$x2 * data$x4)

head(data)
#+END_SRC

#+RESULTS:
:          x1         x2        x3        x4         x5          x6        y
: 1 0.6778492 0.92349391 0.1576724 0.9815172 0.77496216 0.282501746 7.442764
: 2 0.7088311 0.51079715 0.2335002 0.5522585 0.22558065 0.005148194 4.308371
: 3 0.5280327 0.03529519 0.5537724 0.2926394 0.67773583 0.236637721 4.250867
: 4 0.4423366 0.93826929 0.4526374 0.7090073 0.84701227 0.157596670 6.940557
: 5 0.2282255 0.37932004 0.5559482 0.7280456 0.04624094 0.923890930 5.526287
: 6 0.7575107 0.69316315 0.1938151 0.7458463 0.29209639 0.149164932 5.523634

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
model_matrix <- model.matrix(~ I(x2 ^ 2) + I(x1 ^ 2) + I(x3 ^ 2) + I(x4 ^ 2) + (x1 + x2 + x3 + x4) ^ 2 + x5 + x6, data)
head(model_matrix)
#+END_SRC

#+RESULTS:
#+begin_example
  (Intercept)     I(x2^2)   I(x1^2)    I(x3^2)   I(x4^2)        x1         x2
1           1 0.852840999 0.4594796 0.02486059 0.9633761 0.6778492 0.92349391
2           1 0.260913733 0.5024415 0.05452236 0.3049895 0.7088311 0.51079715
3           1 0.001245751 0.2788185 0.30666389 0.0856378 0.5280327 0.03529519
4           1 0.880349268 0.1956617 0.20488059 0.5026914 0.4423366 0.93826929
5           1 0.143883695 0.0520869 0.30907835 0.5300504 0.2282255 0.37932004
6           1 0.480475149 0.5738224 0.03756428 0.5562866 0.7575107 0.69316315
         x3        x4         x5          x6      x1:x2     x1:x3     x1:x4
1 0.1576724 0.9815172 0.77496216 0.282501746 0.62598964 0.1068781 0.6653207
2 0.2335002 0.5522585 0.22558065 0.005148194 0.36206889 0.1655122 0.3914580
3 0.5537724 0.2926394 0.67773583 0.236637721 0.01863702 0.2924099 0.1545232
4 0.4526374 0.7090073 0.84701227 0.157596670 0.41503088 0.2002181 0.3136199
5 0.5559482 0.7280456 0.04624094 0.923890930 0.08657052 0.1268816 0.1661586
6 0.1938151 0.7458463 0.29209639 0.149164932 0.52507848 0.1468170 0.5649865
      x2:x3      x2:x4     x3:x4
1 0.1456095 0.90642520 0.1547582
2 0.1192713 0.28209208 0.1289525
3 0.0195455 0.01032876 0.1620556
4 0.4246957 0.66523981 0.3209232
5 0.2108823 0.27616229 0.4047556
6 0.1343455 0.51699314 0.1445562
#+end_example
*** [2018-07-17 Tue]
**** Plotting atax0 O2 Uniform + D-Optimal Sampling (Xeon E5 2630 v2)
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O2_quad/search_space.csv")

data <- data[data$correct_result == "True", ]
str(data)

O2_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_K == 1, ]
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	20999 obs. of  20 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.105 0.129 0.162 0.213 0.106 ...
 $ RT_K                        : int  8 1 1 1 8 1 8 32 8 1 ...
 $ T1_I                        : int  1 32 32 16 32 128 64 256 128 128 ...
 $ T1_J                        : int  128 64 64 64 32 512 32 32 32 32 ...
 $ T1_K                        : int  128 1 64 64 64 64 512 64 256 64 ...
 $ U_K                         : int  1 28 1 5 1 30 6 26 1 1 ...
 $ U_J                         : int  13 1 28 7 10 1 1 5 10 11 ...
 $ U_I                         : int  21 7 23 1 26 20 25 1 5 13 ...
 $ U1_I                        : int  5 23 27 26 5 15 9 2 25 24 ...
 $ T2_K                        : int  512 512 256 1024 2048 512 2048 2048 1024 2048 ...
 $ T2_J                        : int  1024 256 128 512 128 1024 1 512 256 1024 ...
 $ T2_I                        : int  128 1 512 256 1 128 128 2048 128 512 ...
 $ mean_confidence_interval_inf: num  0.105 0.128 0.161 0.211 0.105 ...
 $ cost_std                    : num  0.000172 0.001385 0.001781 0.002961 0.000509 ...
 $ RT_I                        : int  8 1 1 32 8 1 1 1 8 1 ...
 $ cost_mean                   : num  0.105 0.128 0.161 0.212 0.106 ...
 $ RT_J                        : int  1 8 8 1 1 32 1 1 1 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean)) +
    geom_vline(aes(xintercept = cost_mean), O2_baseline, color = "black", linetype = 2) +
    geom_text(aes(x = cost_mean + 0.03, y = 6000, label = "-O2 baseline"), data = O2_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-XqTCQN/figureKW30vP.png]]
*** [2018-07-18 Wed]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e5_2630_v2/atax1_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$ACOPY_x == "False" & data$ACOPY_y == "False" &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1600   26
'data.frame':	1443 obs. of  26 variables:
 $ id                          : int  1 2 4 5 6 7 8 9 11 12 ...
 $ T2_K                        : int  2048 64 2048 256 1 128 64 512 256 1 ...
 $ T2_J                        : int  128 128 2048 256 1 256 1 2048 1 512 ...
 $ T2_I                        : int  512 1 256 512 1024 128 128 256 1 2048 ...
 $ mean_confidence_interval_inf: num  0.183 0.101 0.218 0.113 0.183 ...
 $ RT_K                        : int  1 1 32 8 8 8 1 1 1 1 ...
 $ T1_I                        : int  512 512 64 256 256 32 16 64 128 256 ...
 $ T1_J                        : int  128 32 16 16 64 128 64 128 256 32 ...
 $ T1_K                        : int  512 16 64 16 256 128 16 16 1 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 2 1 2 2 1 1 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 2 1 2 1 1 1 2 2 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 1 1 2 1 1 2 1 2 ...
 $ ACOPY_x                     : Factor w/ 2 levels "False","True": 1 1 1 1 1 2 2 1 1 1 ...
 $ ACOPY_y                     : Factor w/ 2 levels "False","True": 2 1 1 1 2 2 2 2 1 1 ...
 $ U1_I                        : int  4 2 11 2 23 11 10 22 15 2 ...
 $ RT_J                        : int  32 1 1 8 8 1 1 1 1 8 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.183 0.104 0.224 0.117 0.189 ...
 $ cost_std                    : num  0.000204 0.00477 0.008259 0.006777 0.008412 ...
 $ cost_mean                   : num  0.183 0.102 0.221 0.115 0.186 ...
 $ U_J                         : int  8 1 1 1 22 1 1 20 8 4 ...
 $ U_I                         : int  22 18 28 7 1 10 28 1 1 18 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ U_K                         : int  1 28 11 1 16 25 14 24 9 1 ...
 $ RT_I                        : int  1 8 1 1 1 8 8 8 32 8 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean)) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, 0.6), ylim = c(0, 250)) +
    geom_text(aes(x = cost_mean + 0.04, y = 200, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gGwScO/figure35TVyk.png]]
*** [2018-07-19 Thu]
**** Plotting bigckernel O3 Random Uniform Sample (Xeon E5 2630 v2)
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/bicgkernel/xeon_e5_2630_v2/src1_O3_binary/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 &
                    data$T2_I == 1 & data$T2_J == 1 &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$RT_I == 1 & data$RT_J == 1 &
                    data$SCR == "False" & data$OMP == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 3001   21
'data.frame':	2980 obs. of  21 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ runs                        : int  35 35 35 35 35 35 35 35 35 35 ...
 $ mean_confidence_interval_sup: num  0.0847 0.0308 0.086 0.3751 0.2981 ...
 $ mean_confidence_interval_inf: num  0.0819 0.0278 0.0798 0.3741 0.2933 ...
 $ T1_I                        : int  16 1 1 1 128 32 64 256 1 1 ...
 $ T1_J                        : int  128 1 256 1 128 32 128 64 64 128 ...
 $ cost_mean                   : num  0.0833 0.0293 0.0829 0.3746 0.2957 ...
 $ U_J                         : int  1 1 1 4 6 1 1 7 4 24 ...
 $ U_I                         : int  5 16 24 1 1 14 12 1 1 1 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ T2_I                        : int  2048 128 256 2048 2048 256 1 2048 2048 256 ...
 $ cost_std                    : num  0.00432 0.00452 0.00938 0.00145 0.00713 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ T2_J                        : int  1 1 512 128 128 128 1024 2048 1024 2048 ...
 $ U1_I                        : int  5 28 18 16 5 5 30 26 6 2 ...
 $ OMP                         : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 2 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 2 1 2 2 1 1 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 2 2 2 1 1 ...
 $ RT_I                        : int  8 8 1 32 1 32 32 32 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 2 2 1 1 1 1 2 2 1 1 ...
 $ RT_J                        : int  1 1 32 1 32 1 1 1 32 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean)) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 500)) +
    geom_text(aes(x = cost_mean + 0.04, y = 100, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gGwScO/figure2eH8KA.png]]
**** Plotting atax1 O3 Random Uniform Sample (Xeon E3 1505M v6)
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(ggplot2)
data <- read.csv("orio_experiments/testsuite/SPAPT/atax/xeon_e3_1505M_v6/atax1_O3/search_space.csv")

dim(data)
data <- data[data$correct_result == "True", ]
str(data)

O3_baseline <- data[data$T1_I == 1 & data$T1_J == 1 & data$T1_K == 1 &
                    data$T2_I == 1 & data$T2_J == 1 & data$T2_K == 1 &
                    data$ACOPY_x == "False" & data$ACOPY_y == "False" &
                    data$U1_I == 1 & data$U_I == 1 & data$U_J == 1 &
                    data$U_K == 1 & data$RT_I == 1 & data$RT_J == 1 &
                    data$RT_K == 1 & data$SCR == "False" &
                    data$VEC1 == "False" & data$VEC2 == "False", ]
#+END_SRC

#+RESULTS:
#+begin_example
[1] 211  26
'data.frame':	189 obs. of  26 variables:
 $ id                          : int  1 2 3 5 6 7 8 9 11 12 ...
 $ T2_K                        : int  1 512 1 512 128 256 1024 1024 128 1 ...
 $ T2_J                        : int  128 128 2048 2048 2048 1 128 1024 1 1024 ...
 $ T2_I                        : int  64 2048 256 64 1024 128 512 512 512 1024 ...
 $ mean_confidence_interval_inf: num  0.787 0.735 0.254 0.402 0.614 ...
 $ RT_K                        : int  1 8 8 1 1 8 32 1 1 1 ...
 $ T1_I                        : int  16 256 128 16 1 16 128 64 128 1 ...
 $ T1_J                        : int  1 16 32 512 512 128 32 512 1 256 ...
 $ T1_K                        : int  32 128 256 128 16 64 16 16 1 16 ...
 $ technique                   : Factor w/ 1 level "RS": 1 1 1 1 1 1 1 1 1 1 ...
 $ VEC2                        : Factor w/ 2 levels "False","True": 1 1 2 2 2 2 2 1 2 1 ...
 $ VEC1                        : Factor w/ 2 levels "False","True": 1 2 1 2 1 2 1 1 1 1 ...
 $ SCR                         : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 1 2 2 ...
 $ ACOPY_x                     : Factor w/ 2 levels "False","True": 2 1 1 1 1 2 2 1 2 1 ...
 $ ACOPY_y                     : Factor w/ 2 levels "False","True": 1 2 2 1 1 1 1 1 2 1 ...
 $ U1_I                        : int  7 28 9 27 13 29 19 12 7 12 ...
 $ RT_J                        : int  1 8 1 1 32 8 1 8 1 32 ...
 $ runs                        : int  30 30 30 30 30 30 30 30 30 30 ...
 $ mean_confidence_interval_sup: num  0.807 0.744 0.255 0.403 0.628 ...
 $ cost_std                    : num  0.02843 0.0131 0.00178 0.00149 0.02003 ...
 $ cost_mean                   : num  0.797 0.74 0.255 0.403 0.621 ...
 $ U_J                         : int  12 16 1 17 20 1 21 1 7 2 ...
 $ U_I                         : int  1 1 11 15 24 17 1 22 1 1 ...
 $ correct_result              : Factor w/ 2 levels "False","True": 2 2 2 2 2 2 2 2 2 2 ...
 $ U_K                         : int  9 11 30 1 1 28 19 18 12 3 ...
 $ RT_I                        : int  1 1 8 8 1 1 1 1 32 1 ...
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
ggplot(data) +
    geom_histogram(aes(cost_mean)) +
    geom_vline(aes(xintercept = cost_mean), O3_baseline, color = "black", linetype = 2) +
    coord_cartesian(xlim = c(0, max(data$cost_mean)), ylim = c(0, 30)) +
    geom_text(aes(x = cost_mean + 0.1, y = 20, label = "-O3 baseline"), data = O3_baseline) +
    theme_bw()
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-gGwScO/figurea3updp.png]]
**** Paper Review: GASPOW: An Autotunable System for Accelerating General Sliding-Window Streaming Operators on GPU
***** Overall merit
 1. Reject
 2. Weak reject
 * 3. Weak accept
 4. Accept
 5. Strong accept
***** Reviewer expertise
 1. No familiarity
 * 2. Some familiarity
 3. Knowledgeable
 4. Expert
***** Paper short summary
The paper presents a system for offloading stream processing workloads to GPUs.
The system applies the concept of distinct sliding windows that can be run in
parallel in GPUs. It uses online learning and a raindrop algorithm to optimize
the concurrency level and the batch length that should be used in each
application. Experiments with two queries are presented, showing significant
improvements in comparison with execution on CPUs.
***** Comments for PC (hidden from authors) - optional
***** Novelty
 1. Published before or openly commercialized
 * 2. Incremental improvement
 3. New contribution
 4. Surprisingly new contribution
***** Strengths
- Presentation: Clear tables and figures
- Application of the parallel windows inside sliding batches approach on GPUs
- Provides open-source code
***** Weaknesses
- Text could be made clearer
- Evaluation based on only 2 applications and 1 architecture
- Proposed autotuning strategy was not compared with other strategies
***** Well situated regarding previous work
 1. Low
 * 2. Medium
 3. High
***** Detailed comments for author:
****** Disclaimer
Since this was a double blind review, it was not possible to access the source
code used in the experiments or some of the references to previous work.
****** Section 5: Evaluation
It would be interesting to present more explanation on the selection of the
queries for the evaluation, and why only two different queries where selected.
Evaluation on more than a single GPU and CPU would also help support the
conclusions of the paper.
******* Section 5.2: Mechanism Evaluation
The autotuning search space for this problem has two parameters and 150 possible
combinations. This is a very small search space in relation to common autotuning
problems in other domains. The entire search spaces of two scenarios of query 1
are represented in the heatmaps of Figure 11.

As far as I understood there is no diference in result throughput between points
in and out of the Pareto frontier, at least with respect to the number of
replicas. If this is the case, it would be interesting to compare the results of
the active learning approach with much simpler autotuning approaches such as
random uniform sampling or latin hypersquare sampling. If most query search
spaces are like the ones presented in Figure 11, such simple approaches should
have interesting results while consuming few resources.
******* Section 5.3: Autotuning Evaluation
In the last paragraph of this subsection, it is claimed that "[in] almost all
cases the raindrop-based strategy ends up in an optimal configuration on the
Pareto frontier. Only in few cases it picks a point not on the frontier but
still very close to it.".

According to the table, only in 5 out of the 12 scenarios the configuration
found by the raindrop strategy was at distance zero from the Pareto Frontier.
Since the space of configurations is discrete, it may be the case that distances
smaller than 1 could be as close as possible, but if that is the case more
clarification on the meaning of this distance should be provided.
****** Text Clarity
The paper could be made clearer by further text revision. An example, which
appears in the last paragraph of subsubsection "Maximum sustainable input rate"
of subsection 5.1, is the following:

Original:   "The CPU version has a latency lower of some orders of magnitude [..]"
Suggestion: "The CPU version has latency lower by 3 orders of magnitude [...]"

****** Style and Formatting
The following are some style and formatting suggestions:
- Abbreviations such as "a.k.a", used in the first page, might not be
  immediately understood by non-native english speakers, and can easily be
  replaced by clearer expressions. In the first page, for example: "[...]" has
  been applied to sliding-window queries [8] (a.k.a. windowed queries) that
  [...]" could be rewritten as "[...] has been applied to sliding-window, or
  windowed, queries [8] that [...]".
- The intent of other abbreviations such as "e.g." and "i.e.", widely used in
  the text, can be more clearly conveyed by replacing them by "for example" and
  "that is".
