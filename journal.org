#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: ExportableReports(E)
#+TAGS: FAPESP(f)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[showframe,margin=2cm]{geometry}
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{sourcecodepro}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-qtree}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
# #+LATEX_HEADER:   escapeinside={\%*}{*)},
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   language=R,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

* Setup
** Julia
#+NAME: install_julia_deps
#+HEADER: :results output :session *julia*
#+BEGIN_SRC julia
Pkg.add("Plots")
Pkg.add("Lint")
Pkg.add("Gadfly")
Pkg.add("ProfileView")
Pkg.add("CSV")
Pkg.add("StatsBase")
Pkg.add("StatsModels")
Pkg.add("GLM")
Pkg.add("RDatasets")
Pkg.add("IterTools")
Pkg.add("Missings")
Pkg.add("RCall")
Pkg.add("DataFrames")
#+END_SRC

#+RESULTS: install_julia_deps
#+begin_example
INFO: Package Plots is already installed
INFO: Package Lint is already installed
INFO: Package Gadfly is already installed
INFO: Cloning cache of Gtk from https://github.com/JuliaGraphics/Gtk.jl.git
INFO: Cloning cache of GtkReactive from https://github.com/JuliaGizmos/GtkReactive.jl.git
INFO: Cloning cache of IntervalSets from https://github.com/JuliaMath/IntervalSets.jl.git
INFO: Cloning cache of ProfileView from https://github.com/timholy/ProfileView.jl.git
INFO: Cloning cache of Reactive from https://github.com/JuliaGizmos/Reactive.jl.git
INFO: Cloning cache of RoundingIntegers from https://github.com/JuliaMath/RoundingIntegers.jl.git
INFO: Installing Gtk v0.13.1
INFO: Installing GtkReactive v0.4.0
INFO: Installing IntervalSets v0.1.1
INFO: Installing ProfileView v0.3.0
INFO: Installing Reactive v0.6.0
INFO: Installing RoundingIntegers v0.0.3
INFO: Building Cairo
INFO: Building Gtk
INFO: Package database updated
INFO: Package CSV is already installed
INFO: Package StatsBase is already installed
INFO: Package StatsModels is already installed
INFO: Package GLM is already installed
INFO: Package RDatasets is already installed
#+end_example

#+NAME: update_julia_pkg
#+HEADER:  :results output :session *julia*
#+BEGIN_SRC julia
Pkg.update()
#+END_SRC

#+RESULTS: update_julia_pkg
: INFO: Updating METADATA...
: WARNING: Package ASTInterpreter: skipping update (dirty)...
: INFO: Updating Gallium master...
: INFO: Computing changes...
: INFO: No packages to install, update or remove

** R
Installing *R* dependencies:
#+NAME: install_r_deps
#+HEADER: :results output :exports both :session *R*
#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",
                 "plotly", "rPref", "pracma", "FrF2", "AlgDesign",
                 "quantreg"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

#+RESULTS: install_r_deps

** Modifying & Analysing the FPGA Data Set
Cloning and updating the =legup-tuner= repository:

#+NAME: update_legup_tuner
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/legup-tuner.git || (cd legup-tuner && git pull)
#+END_SRC

Export your path to =repository_dir= variable:

#+name: repository_dir
#+begin_src sh :results output :exports both
pwd | tr -d "\n"
#+end_src

** Updating & Cloning Repositories
*** GPU Autotuning Screening Experiment
#+NAME: update_screening_experiment
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/autotuning_screening_experiment.git || (cd autotuning_screening_experiment && git pull)
#+END_SRC
* 2019
** November
*** [2019-11-25 Mon]
**** Gaussian Process Regression on SPAPT kernels      :ExportableReports:
:PROPERTIES:
:EXPORT_FILE_NAME: report.pdf
:END:
***** Results for =bicgkernel=
****** Loading Data                                           :noexport:
#+HEADER: :results output :session *R* :exports none
#+BEGIN_SRC R
library(plyr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(rPref)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

read_experiment <- function(x) {
  data <- read.csv(x, header = TRUE, stringsAsFactors = FALSE)
  data$experiment_id <- str_split(str_split(x, "/")[[1]][6], "_")[[1]][5]
  return(data)
}

current_experiment <- "dlmt_spapt_experiments/data/tests/gpr_expanded_ss"
target_path <- paste(current_experiment, "bicgkernel", sep = "/")


# Loading Data for Histograms and Iterations

data_dir <- current_experiment
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)

    data$experiment_id <- str_split(csv_file, "/")[[1]][6]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

str(data)

full_data <- data

plot_data <- data %>%
             group_by(experiment_id) %>%
             mutate(mean_cost_baseline = mean(cost_baseline)) %>%
             mutate(label_center_x = mean(cost_mean)) %>%
             mutate(label_center_y = mean(best_iteration)) %>%
             ungroup()

complete_plot_data <- plot_data

str(complete_plot_data)

# data_dir <- "dlmt_spapt_experiments/data/results"
# data_dir <- "dlmt_spapt_experiments/data/tests/no_binary_random"
# data_dir <- "dlmt_spapt_experiments/data/results"
data_dir <- "dlmt_spapt_experiments/data/tests/rs_baseline_19Feb"
#target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
target_dirs <- c("bicgkernel")
rs_data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)
    data$experiment_id <- str_split(csv_file, "/")[[1]][6]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(rs_data)) {
            rs_data <- target_data
        } else {
            rs_data <- bind_rows(rs_data, target_data)
        }
    }
}

full_data <- bind_rows(filter(full_data, technique == "GPR"),
                       filter(rs_data, technique == "RS", application == "bicgkernel"))

rs_plot_data <- rs_data %>%
  filter(technique == "RS", application == "bicgkernel") %>%
  group_by(experiment_id) %>%
  mutate(mean_cost_baseline = mean(cost_baseline)) %>%
  mutate(label_center_x = mean(cost_mean)) %>%
  mutate(label_center_y = mean(best_iteration)) %>%
  ungroup()

complete_plot_data <- bind_rows(filter(plot_data, technique == "GPR"),
                                rs_plot_data)

str(complete_plot_data)
#+end_SRC

#+RESULTS:
#+begin_example

'data.frame':	6554 obs. of  32 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  1024 64 512 2048 64 128 2 512 128 4 ...
 $ T2_I                        : int  32 512 64 16 8 2048 64 32 256 256 ...
 $ RT_I                        : int  32 2 4 8 2 4 8 16 1 4 ...
 $ mean_confidence_interval_inf: num  0.527 3.355 0.565 4.691 3.384 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  32 8 512 32 8 4 2 256 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "False" "False" ...
 $ SCR                         : chr  "False" "False" "False" "False" ...
 $ U1_I                        : int  5 10 5 16 20 16 29 19 11 9 ...
 $ RT_J                        : int  4 1 32 4 32 8 8 1 2 1 ...
 $ T1_I                        : int  1 256 32 4 2 512 16 1 4 8 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.11978 0.00729 0.10372 0.00509 0.00406 ...
 $ cost_mean                   : num  0.601 3.359 0.629 4.694 3.386 ...
 $ U_J                         : int  25 8 1 1 1 12 1 7 1 27 ...
 $ U_I                         : int  1 1 7 1 23 1 11 1 19 1 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "True" "False" "True" "False" ...
 $ mean_confidence_interval_sup: num  0.675 3.364 0.693 4.697 3.389 ...
 $ experiment_id               : chr  "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" ...
 $ cost_baseline               : num  4.59 4.59 4.59 4.59 4.59 ...
 $ speedup                     : num  7.629 1.365 7.292 0.977 1.355 ...
 $ max_run_speedup             : num  10.7 10.7 10.7 10.7 10.7 ...
 $ min_run_cost                : num  0.428 0.428 0.428 0.428 0.428 ...
 $ best_iteration              : num  308 308 308 308 308 308 308 308 308 308 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 9 10 ...
 $ points                      : int  323 323 323 323 323 323 323 323 323 323 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	6554 obs. of  35 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  1024 64 512 2048 64 128 2 512 128 4 ...
 $ T2_I                        : int  32 512 64 16 8 2048 64 32 256 256 ...
 $ RT_I                        : int  32 2 4 8 2 4 8 16 1 4 ...
 $ mean_confidence_interval_inf: num  0.527 3.355 0.565 4.691 3.384 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  32 8 512 32 8 4 2 256 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "False" "False" ...
 $ SCR                         : chr  "False" "False" "False" "False" ...
 $ U1_I                        : int  5 10 5 16 20 16 29 19 11 9 ...
 $ RT_J                        : int  4 1 32 4 32 8 8 1 2 1 ...
 $ T1_I                        : int  1 256 32 4 2 512 16 1 4 8 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.11978 0.00729 0.10372 0.00509 0.00406 ...
 $ cost_mean                   : num  0.601 3.359 0.629 4.694 3.386 ...
 $ U_J                         : int  25 8 1 1 1 12 1 7 1 27 ...
 $ U_I                         : int  1 1 7 1 23 1 11 1 19 1 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "True" "False" "True" "False" ...
 $ mean_confidence_interval_sup: num  0.675 3.364 0.693 4.697 3.389 ...
 $ experiment_id               : chr  "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" ...
 $ cost_baseline               : num  4.59 4.59 4.59 4.59 4.59 ...
 $ speedup                     : num  7.629 1.365 7.292 0.977 1.355 ...
 $ max_run_speedup             : num  10.7 10.7 10.7 10.7 10.7 ...
 $ min_run_cost                : num  0.428 0.428 0.428 0.428 0.428 ...
 $ best_iteration              : num  308 308 308 308 308 308 308 308 308 308 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 9 10 ...
 $ points                      : int  323 323 323 323 323 323 323 323 323 323 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...
 $ mean_cost_baseline          : num  4.59 4.59 4.59 4.59 4.59 ...
 $ label_center_x              : num  1.16 1.16 1.16 1.16 1.16 ...
 $ label_center_y              : num  308 308 308 308 308 308 308 308 308 308 ...

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	12521 obs. of  35 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  1024 64 512 2048 64 128 2 512 128 4 ...
 $ T2_I                        : int  32 512 64 16 8 2048 64 32 256 256 ...
 $ RT_I                        : int  32 2 4 8 2 4 8 16 1 4 ...
 $ mean_confidence_interval_inf: num  0.527 3.355 0.565 4.691 3.384 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  32 8 512 32 8 4 2 256 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "False" "False" ...
 $ SCR                         : chr  "False" "False" "False" "False" ...
 $ U1_I                        : int  5 10 5 16 20 16 29 19 11 9 ...
 $ RT_J                        : int  4 1 32 4 32 8 8 1 2 1 ...
 $ T1_I                        : int  1 256 32 4 2 512 16 1 4 8 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.11978 0.00729 0.10372 0.00509 0.00406 ...
 $ cost_mean                   : num  0.601 3.359 0.629 4.694 3.386 ...
 $ U_J                         : int  25 8 1 1 1 12 1 7 1 27 ...
 $ U_I                         : int  1 1 7 1 23 1 11 1 19 1 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "True" "False" "True" "False" ...
 $ mean_confidence_interval_sup: num  0.675 3.364 0.693 4.697 3.389 ...
 $ experiment_id               : chr  "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" "xeon_e5_2630_v3_graoully-13_1581967560" ...
 $ cost_baseline               : num  4.59 4.59 4.59 4.59 4.59 ...
 $ speedup                     : num  7.629 1.365 7.292 0.977 1.355 ...
 $ max_run_speedup             : num  10.7 10.7 10.7 10.7 10.7 ...
 $ min_run_cost                : num  0.428 0.428 0.428 0.428 0.428 ...
 $ best_iteration              : num  308 308 308 308 308 308 308 308 308 308 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 9 10 ...
 $ points                      : int  323 323 323 323 323 323 323 323 323 323 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...
 $ mean_cost_baseline          : num  4.59 4.59 4.59 4.59 4.59 ...
 $ label_center_x              : num  1.16 1.16 1.16 1.16 1.16 ...
 $ label_center_y              : num  308 308 308 308 308 308 308 308 308 308 ...
#+end_example

****** Small Steps and Starting Sample
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figurev67NZ1.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red.
#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figureQi4su4.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figure7vsh8B.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()


ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figure8E0drB.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports results
large_plot_data <- complete_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

alpha <- 0.01

# large_plot_data <- large_plot_data %>%
#   group_by(technique) %>%
#   mutate(mean_runs = mean(min_run_cost)) %>%
#   mutate(sd_runs = sd(min_run_cost)) %>%
#   mutate(ci95_runs = qnorm(.95) * (mean(min_run_cost) / sqrt(length(subset(large_plot_data, technique == technique)$cost_mean)))) %>%
#   ungroup()

large_plot_data <- large_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  mutate(ci95_runs = qt((1 - (alpha / 2)), df = n() - 1) *
           (sd(min_run_cost) /
            sqrt(n()))) %>%
  ungroup()

ggplot(large_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique)) +
  facet_wrap(technique ~ ., scales = "free_x") +
  geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_smooth(method = "lm",
  #             formula = "y ~ 1") +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs,
  #                 ymax = mean_runs + sd_runs),
  #             fill = "red",
  #             alpha = 0.1,
  #             colour = NA) +
  geom_ribbon(aes(ymin = mean_runs - ci95_runs,
                  ymax = mean_runs + ci95_runs),
              fill = "grey70",
              alpha = 0.4,
              colour = NA) +
  geom_point(size = 3) +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  # geom_hline(aes(yintercept = mean_cost_baseline[1],
  #                linetype = "-O3"),
  #            color = "black") +
  theme_bw(base_size = 25) +
  scale_color_brewer(palette = "Set1") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("black")))) +
  theme(legend.position = c(0.8, 0.1),
        legend.direction = "horizontal",
        strip.background = element_rect(fill = "white"),
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figuref1uirL.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
short_plot_data <- complete_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  ungroup()

short_plot_data <- short_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

ggplot(short_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique,
                            group = technique)) +
  facet_wrap(technique ~ .) +
  # geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs, ymax = mean_runs + sd_runs), fill = "grey70", alpha = 0.4, colour = NA) +
  geom_smooth(method = "lm",
              formula = "y ~ 1") +
  geom_point() +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figureHwgMor.pdf]]
***** Results for =atax=
****** Loading Data                                           :noexport:
#+HEADER: :results output :session *R* :exports none
#+BEGIN_SRC R
library(plyr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(rPref)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

read_experiment <- function(x) {
  data <- read.csv(x, header = TRUE, stringsAsFactors = FALSE)
  data$experiment_id <- str_split(str_split(x, "/")[[1]][6], "_")[[1]][5]
  return(data)
}

current_experiment <- "dlmt_spapt_experiments/data/tests/gpr_atax_v0"
target_path <- paste(current_experiment, "atax", sep = "/")

# Loading Data for Histograms and Iterations

data_dir <- current_experiment
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)

    print(str(data))
    data$experiment_id <- str_split(str_split(csv_file, "/")[[1]][6], "_")[[1]][5]
    #data_baseline <- data[data$baseline == "True", "cost_mean"]
    #data$cost_baseline <- rep(data_baseline, nrow(data))
    #data$speedup <- data_baseline / data$cost_mean
    #data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

str(data)

full_data <- data

plot_data <- data %>%
             group_by(experiment_id) %>%
             #mutate(mean_cost_baseline = mean(cost_baseline)) %>%
             mutate(label_center_x = mean(cost_mean)) %>%
             mutate(label_center_y = mean(best_iteration)) %>%
             ungroup()

complete_plot_data <- plot_data

str(complete_plot_data)

# data_dir <- "dlmt_spapt_experiments/data/results"
# data_dir <- "dlmt_spapt_experiments/data/tests/no_binary_random"
# data_dir <- "dlmt_spapt_experiments/data/results"
data_dir <- "dlmt_spapt_experiments/data/tests/random_atax"
#target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
target_dirs <- c("atax")
rs_data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)
    data$experiment_id <- str_split(str_split(csv_file, "/")[[1]][6], "_")[[1]][5]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(rs_data)) {
            rs_data <- target_data
        } else {
            rs_data <- bind_rows(rs_data, target_data)
        }
    }
}

full_data <- bind_rows(filter(full_data, technique == "GPR"),
                       filter(rs_data, technique == "RS", application == "atax"))

rs_plot_data <- rs_data %>%
  filter(technique == "RS", application == "atax") %>%
  group_by(experiment_id) %>%
  mutate(mean_cost_baseline = mean(cost_baseline)) %>%
  mutate(label_center_x = mean(cost_mean)) %>%
  mutate(label_center_y = mean(best_iteration)) %>%
  ungroup()

complete_plot_data <- bind_rows(filter(plot_data, technique == "GPR"),
                                rs_plot_data)

complete_plot_data <- complete_plot_data %>%
  subset(experiment_id != "graoully-7" & experiment_id != "graoully-9")

str(complete_plot_data)
#+end_SRC

#+RESULTS:
#+begin_example

[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-3_1574274689/search_space.csv"
'data.frame':	287 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  1 512 64 1024 1024 32 256 1024 2048 256 ...
 $ T2_J                        : int  1024 32 16 16 64 1024 2048 8 512 128 ...
 $ T2_I                        : int  64 16 1024 1024 512 64 256 16 32 256 ...
 $ mean_confidence_interval_inf: num  3.17 2.34 Inf 3.57 Inf ...
 $ RT_K                        : int  1 4 1 2 8 4 4 8 1 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  64 8 2 1 8 256 16 2 64 4 ...
 $ T1_K                        : int  512 128 16 16 4 8 32 8 2 128 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "False" "True" "False" "False" ...
 $ VEC1                        : chr  "False" "True" "False" "True" ...
 $ SCR                         : chr  "True" "False" "False" "True" ...
 $ ACOPY_x                     : chr  "False" "True" "True" "False" ...
 $ ACOPY_y                     : chr  "True" "True" "True" "True" ...
 $ U1_I                        : int  6 9 30 19 24 26 20 20 13 14 ...
 $ RT_J                        : int  1 8 8 16 4 2 4 1 1 2 ...
 $ T1_I                        : int  1 8 1024 16 2 2 256 1 8 1 ...
 $ runs                        : int  10 10 1 10 1 10 10 10 10 10 ...
 $ RT_I                        : int  32 2 4 1 8 2 8 4 2 4 ...
 $ cost_mean                   : num  3.17 2.35 Inf 3.58 Inf ...
 $ U_J                         : int  1 15 17 1 4 7 11 1 1 6 ...
 $ U_I                         : int  7 1 12 1 1 1 1 30 8 1 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00553 0.01066 Inf 0.00939 Inf ...
 $ correct_result              : chr  "True" "True" "False" "True" ...
 $ U_K                         : int  8 10 1 3 6 20 8 24 10 25 ...
 $ mean_confidence_interval_sup: num  3.17 2.35 Inf 3.58 Inf ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-32_1574199634/search_space.csv"
'data.frame':	295 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  128 2 128 512 1024 1024 2048 1024 32 1024 ...
 $ T2_J                        : int  256 4 512 2048 1 1024 1024 64 64 512 ...
 $ T2_I                        : int  64 128 8 2048 32 1024 128 256 64 16 ...
 $ mean_confidence_interval_inf: num  1.85 7.5 2.22 1.61 2.11 ...
 $ RT_K                        : int  16 1 8 4 4 8 4 16 8 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  4 4 8 2048 2 8 32 32 16 8 ...
 $ T1_K                        : int  64 2 16 64 1 128 16 4 16 32 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "True" "False" ...
 $ VEC1                        : chr  "True" "True" "False" "False" ...
 $ SCR                         : chr  "True" "True" "False" "False" ...
 $ ACOPY_x                     : chr  "False" "False" "True" "False" ...
 $ ACOPY_y                     : chr  "False" "True" "True" "False" ...
 $ U1_I                        : int  25 24 26 29 9 15 8 19 9 7 ...
 $ RT_J                        : int  2 4 2 2 2 8 4 2 8 16 ...
 $ T1_I                        : int  64 4 1 512 32 512 16 4 4 2 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 1 ...
 $ RT_I                        : int  4 16 4 16 4 2 4 4 2 2 ...
 $ cost_mean                   : num  1.86 7.51 2.23 1.61 2.11 ...
 $ U_J                         : int  7 5 28 3 21 1 3 5 1 1 ...
 $ U_I                         : int  1 3 1 7 26 27 1 5 25 27 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.0074 0.01092 0.01453 0.00551 0.0091 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  25 1 15 1 1 19 12 1 8 17 ...
 $ mean_confidence_interval_sup: num  1.86 7.52 2.24 1.62 2.12 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-4_1574199633/search_space.csv"
'data.frame':	288 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  8 1 1024 1 128 4 256 1 1 1024 ...
 $ T2_J                        : int  16 8 128 64 256 8 128 2 512 64 ...
 $ T2_I                        : int  1 64 512 2048 512 2 2 1 256 32 ...
 $ mean_confidence_interval_inf: num  1.83 5.72 6.16 6 1.66 ...
 $ RT_K                        : int  1 2 1 2 4 4 16 16 2 2 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  4 2 2 16 1 4 2 1 4 64 ...
 $ T1_K                        : int  8 64 32 16 128 2 256 16 4 16 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "False" ...
 $ VEC1                        : chr  "False" "True" "False" "True" ...
 $ SCR                         : chr  "True" "False" "True" "False" ...
 $ ACOPY_x                     : chr  "True" "True" "True" "False" ...
 $ ACOPY_y                     : chr  "False" "True" "False" "True" ...
 $ U1_I                        : int  2 25 19 5 27 28 24 27 8 7 ...
 $ RT_J                        : int  4 1 1 8 16 2 4 4 1 32 ...
 $ T1_I                        : int  2048 8 4 1024 8 2 2 256 2 4 ...
 $ runs                        : int  10 10 10 10 10 10 10 1 10 10 ...
 $ RT_I                        : int  4 16 32 1 4 1 2 2 2 2 ...
 $ cost_mean                   : num  1.84 5.73 6.17 6.01 1.66 ...
 $ U_J                         : int  10 1 20 18 1 1 1 14 29 1 ...
 $ U_I                         : int  10 25 1 15 25 6 12 30 12 6 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00722 0.00726 0.00716 0.017 0.00474 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  1 1 23 1 13 24 27 1 1 4 ...
 $ mean_confidence_interval_sup: num  1.84 5.73 6.17 6.02 1.67 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-42_1574274689/search_space.csv"
'data.frame':	292 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  2048 32 32 2048 128 2048 4 16 128 512 ...
 $ T2_J                        : int  4 1024 128 128 1024 32 16 8 2048 16 ...
 $ T2_I                        : int  1024 128 512 1024 16 64 1 2048 128 2048 ...
 $ mean_confidence_interval_inf: num  1.73 1.83 2.54 5.48 1.56 ...
 $ RT_K                        : int  4 1 32 2 2 4 1 2 8 2 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  4 2 64 16 32 8 4 8 2 2 ...
 $ T1_K                        : int  1024 1 1 1024 64 128 2 8 4 512 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "False" ...
 $ VEC1                        : chr  "False" "True" "True" "False" ...
 $ SCR                         : chr  "False" "True" "False" "False" ...
 $ ACOPY_x                     : chr  "False" "False" "True" "True" ...
 $ ACOPY_y                     : chr  "False" "False" "False" "True" ...
 $ U1_I                        : int  14 21 24 17 3 14 26 24 13 6 ...
 $ RT_J                        : int  1 16 2 4 2 4 8 2 2 4 ...
 $ T1_I                        : int  64 32 64 256 8 2 32 16 4 256 ...
 $ runs                        : int  10 10 10 10 10 10 1 10 10 1 ...
 $ RT_I                        : int  4 1 2 8 8 4 4 32 2 4 ...
 $ cost_mean                   : num  1.74 1.83 2.54 5.49 1.57 ...
 $ U_J                         : int  16 1 28 22 6 28 12 1 30 1 ...
 $ U_I                         : int  6 5 22 1 1 3 27 26 1 11 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00812 0.00645 0.0064 0.00919 0.00747 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  1 10 1 13 4 1 1 29 24 23 ...
 $ mean_confidence_interval_sup: num  1.74 1.84 2.55 5.49 1.57 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-5_1574274689/search_space.csv"
'data.frame':	300 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  256 1 1024 128 1024 128 256 16 2048 32 ...
 $ T2_J                        : int  16 1024 1024 32 1 128 1024 2048 1 512 ...
 $ T2_I                        : int  1024 128 512 32 256 512 64 512 128 64 ...
 $ mean_confidence_interval_inf: num  2.39 1.49 1.56 1.83 5.96 ...
 $ RT_K                        : int  1 8 4 4 8 8 2 2 16 4 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  8 64 1 8 32 16 128 1024 2 1 ...
 $ T1_K                        : int  8 32 4 32 2 128 2 8 4 1 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "False" "True" "True" ...
 $ VEC1                        : chr  "True" "False" "True" "False" ...
 $ SCR                         : chr  "False" "True" "True" "True" ...
 $ ACOPY_x                     : chr  "False" "True" "False" "False" ...
 $ ACOPY_y                     : chr  "False" "True" "False" "False" ...
 $ U1_I                        : int  24 9 10 7 16 5 3 4 1 28 ...
 $ RT_J                        : int  4 4 8 1 4 8 4 8 4 2 ...
 $ T1_I                        : int  16 32 512 16 64 16 32 128 16 8 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 1 10 ...
 $ RT_I                        : int  16 8 8 2 2 4 4 1 2 16 ...
 $ cost_mean                   : num  2.39 1.49 1.57 1.83 5.97 ...
 $ U_J                         : int  10 1 29 30 11 7 1 11 6 1 ...
 $ U_I                         : int  1 10 26 1 1 1 18 1 1 9 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00431 0.00552 0.00854 0.00631 0.01127 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  6 3 1 9 7 27 21 28 10 17 ...
 $ mean_confidence_interval_sup: num  2.4 1.5 1.57 1.83 5.98 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-6_1574274690/search_space.csv"
'data.frame':	300 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  64 512 256 128 256 2 256 256 512 128 ...
 $ T2_J                        : int  1024 1024 16 64 512 512 64 1 2 2048 ...
 $ T2_I                        : int  2 1 512 4 64 64 64 32 1024 64 ...
 $ mean_confidence_interval_inf: num  4.73 Inf 5.75 Inf 1.46 ...
 $ RT_K                        : int  4 2 2 4 2 4 8 4 4 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  4 4 16 1 1 512 64 4 1 16 ...
 $ T1_K                        : int  4 4 32 2 128 1 64 64 512 2 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "False" "False" "True" ...
 $ VEC1                        : chr  "True" "True" "False" "False" ...
 $ SCR                         : chr  "False" "False" "False" "True" ...
 $ ACOPY_x                     : chr  "False" "True" "True" "False" ...
 $ ACOPY_y                     : chr  "False" "False" "True" "True" ...
 $ U1_I                        : int  20 10 7 29 6 23 16 12 24 18 ...
 $ RT_J                        : int  2 16 8 16 8 16 4 8 2 8 ...
 $ T1_I                        : int  1 2 128 4 1 64 32 16 2 16 ...
 $ runs                        : int  10 1 10 1 10 1 10 10 10 10 ...
 $ RT_I                        : int  16 4 1 2 8 2 2 8 1 8 ...
 $ cost_mean                   : num  4.73 Inf 5.76 Inf 1.46 ...
 $ U_J                         : int  9 1 1 17 1 1 2 21 17 17 ...
 $ U_I                         : int  1 19 9 1 1 8 9 1 12 18 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00898 Inf 0.01416 Inf 0.00746 ...
 $ correct_result              : chr  "True" "False" "True" "False" ...
 $ U_K                         : int  6 15 2 25 14 30 1 9 1 1 ...
 $ mean_confidence_interval_sup: num  4.74 Inf 5.77 Inf 1.47 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-7_1574274689/search_space.csv"
'data.frame':	19 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  16 2 512 512 512 64 256 32 128 32 ...
 $ T2_J                        : int  8 512 128 1024 512 1024 128 128 4 256 ...
 $ T2_I                        : int  1024 64 256 128 256 1024 512 512 512 32 ...
 $ mean_confidence_interval_inf: num  Inf 3.13 1.7 2.59 1.82 ...
 $ RT_K                        : int  16 4 4 1 2 4 4 1 1 4 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  4 128 32 64 64 64 64 4 4 2 ...
 $ T1_K                        : int  2 1 32 128 8 4 128 2 16 16 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "False" "True" "True" ...
 $ VEC1                        : chr  "True" "True" "False" "True" ...
 $ SCR                         : chr  "False" "True" "True" "True" ...
 $ ACOPY_x                     : chr  "True" "False" "True" "True" ...
 $ ACOPY_y                     : chr  "True" "False" "True" "False" ...
 $ U1_I                        : int  8 13 19 21 6 10 13 8 19 20 ...
 $ RT_J                        : int  4 2 16 2 8 2 2 2 16 2 ...
 $ T1_I                        : int  256 1 16 8 256 32 16 1 4 2 ...
 $ runs                        : int  1 10 10 10 10 10 10 10 1 10 ...
 $ RT_I                        : int  4 8 4 1 8 2 16 32 4 4 ...
 $ U_K                         : int  1 10 1 1 14 1 10 3 25 1 ...
 $ U_J                         : int  26 1 12 26 2 2 19 18 1 9 ...
 $ U_I                         : int  6 7 11 28 1 23 1 1 7 25 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  Inf 0.0134 0.00914 0.00688 0.00848 ...
 $ correct_result              : chr  "False" "True" "True" "True" ...
 $ cost_mean                   : num  Inf 3.14 1.7 2.59 1.82 ...
 $ mean_confidence_interval_sup: num  Inf 3.15 1.71 2.59 1.83 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-8_1574274690/search_space.csv"
'data.frame':	306 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  16 512 256 256 1024 16 1024 1024 1024 128 ...
 $ T2_J                        : int  512 1024 16 64 1024 64 1 64 16 1024 ...
 $ T2_I                        : int  512 16 128 64 1 256 1 1024 512 1024 ...
 $ mean_confidence_interval_inf: num  4.79 1.67 Inf 1.59 2.26 ...
 $ RT_K                        : int  2 4 8 4 4 4 1 8 8 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  256 512 8 32 8 2 1 16 8 16 ...
 $ T1_K                        : int  8 128 2 16 4 4 4 1 64 64 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "False" "False" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "False" "False" ...
 $ SCR                         : chr  "True" "True" "True" "True" ...
 $ ACOPY_x                     : chr  "True" "False" "False" "True" ...
 $ ACOPY_y                     : chr  "True" "True" "True" "True" ...
 $ U1_I                        : int  16 10 13 7 26 28 15 20 10 25 ...
 $ RT_J                        : int  2 1 8 2 4 1 2 1 8 2 ...
 $ T1_I                        : int  4 4 1 8 512 256 2048 128 16 128 ...
 $ runs                        : int  10 10 1 10 10 10 10 10 10 10 ...
 $ RT_I                        : int  1 4 4 8 16 16 8 4 4 8 ...
 $ cost_mean                   : num  4.8 1.67 Inf 1.6 2.27 ...
 $ U_J                         : int  1 1 23 25 4 1 5 29 5 1 ...
 $ U_I                         : int  25 5 1 1 1 26 6 16 1 9 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00746 0.00489 Inf 0.00718 0.0052 ...
 $ correct_result              : chr  "True" "True" "False" "True" ...
 $ U_K                         : int  4 4 8 3 8 14 1 1 11 28 ...
 $ mean_confidence_interval_sup: num  4.8 1.68 Inf 1.6 2.27 ...
NULL
[1] "dlmt_spapt_experiments/data/tests/gpr_atax_v0/atax/xeon_e5_2630_v3_graoully-9_1574274689/search_space.csv"
'data.frame':	21 obs. of  28 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_K                        : int  512 32 128 1024 256 1024 4 1024 32 128 ...
 $ T2_J                        : int  128 512 2048 1024 256 16 512 32 1024 256 ...
 $ T2_I                        : int  16 1024 16 64 512 1 8 128 512 1024 ...
 $ mean_confidence_interval_inf: num  2.49 1.88 6.08 Inf 1.48 ...
 $ RT_K                        : int  2 8 2 8 1 1 4 8 32 4 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  32 1 1024 4 4 8 512 16 1024 64 ...
 $ T1_K                        : int  32 16 4 32 128 1 4 32 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "False" "False" "True" "False" ...
 $ VEC1                        : chr  "True" "False" "True" "False" ...
 $ SCR                         : chr  "False" "True" "True" "True" ...
 $ ACOPY_x                     : chr  "False" "False" "False" "True" ...
 $ ACOPY_y                     : chr  "False" "False" "True" "False" ...
 $ U1_I                        : int  22 24 14 28 18 26 9 4 22 13 ...
 $ RT_J                        : int  16 8 1 8 1 4 4 4 1 2 ...
 $ T1_I                        : int  1 4 4 32 128 2 4 2 8 32 ...
 $ runs                        : int  10 10 10 1 10 10 10 10 1 10 ...
 $ RT_I                        : int  2 2 16 4 8 8 2 2 2 1 ...
 $ cost_mean                   : num  2.5 1.89 6.09 Inf 1.48 ...
 $ U_J                         : int  1 1 16 2 16 1 9 28 29 10 ...
 $ U_I                         : int  22 19 10 1 1 10 1 26 1 30 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00863 0.00847 0.01361 Inf 0.00674 ...
 $ correct_result              : chr  "True" "True" "True" "False" ...
 $ U_K                         : int  18 15 1 2 7 4 6 1 8 1 ...
 $ mean_confidence_interval_sup: num  2.5 1.89 6.09 Inf 1.49 ...
NULL

'data.frame':	1992 obs. of  34 variables:
 $ id                          : int  1 2 4 6 7 8 9 10 11 12 ...
 $ T2_K                        : int  1 512 1024 32 256 1024 2048 256 512 4 ...
 $ T2_J                        : int  1024 32 16 1024 2048 8 512 128 64 1 ...
 $ T2_I                        : int  64 16 1024 64 256 16 32 256 128 4 ...
 $ mean_confidence_interval_inf: num  3.17 2.34 3.57 2.89 1.42 ...
 $ RT_K                        : int  1 4 2 4 4 8 1 1 1 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  64 8 1 256 16 2 64 4 4 2 ...
 $ T1_K                        : int  512 128 16 8 32 8 2 128 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "False" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "True" "True" ...
 $ SCR                         : chr  "True" "False" "True" "False" ...
 $ ACOPY_x                     : chr  "False" "True" "False" "False" ...
 $ ACOPY_y                     : chr  "True" "True" "True" "True" ...
 $ U1_I                        : int  6 9 19 26 20 20 13 14 24 28 ...
 $ RT_J                        : int  1 8 16 2 4 1 1 2 4 16 ...
 $ T1_I                        : int  1 8 16 2 256 1 8 1 8 1 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ RT_I                        : int  32 2 1 2 8 4 2 4 8 4 ...
 $ cost_mean                   : num  3.17 2.35 3.58 2.89 1.42 ...
 $ U_J                         : int  1 15 1 7 11 1 1 6 1 23 ...
 $ U_I                         : int  7 1 1 1 1 30 8 1 19 25 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00553 0.01066 0.00939 0.00837 0.00673 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  8 10 3 20 8 24 10 25 16 1 ...
 $ mean_confidence_interval_sup: num  3.17 2.35 3.58 2.9 1.43 ...
 $ experiment_id               : chr  "graoully-3" "graoully-3" "graoully-3" "graoully-3" ...
 $ min_run_cost                : num  1.25 1.25 1.25 1.25 1.25 ...
 $ best_iteration              : num  228 228 228 228 228 228 228 228 228 228 ...
 $ iteration                   : num  1 2 4 6 7 8 9 10 11 12 ...
 $ points                      : int  266 266 266 266 266 266 266 266 266 266 ...
 $ application                 : chr  "atax" "atax" "atax" "atax" ...

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	1992 obs. of  36 variables:
 $ id                          : int  1 2 4 6 7 8 9 10 11 12 ...
 $ T2_K                        : int  1 512 1024 32 256 1024 2048 256 512 4 ...
 $ T2_J                        : int  1024 32 16 1024 2048 8 512 128 64 1 ...
 $ T2_I                        : int  64 16 1024 64 256 16 32 256 128 4 ...
 $ mean_confidence_interval_inf: num  3.17 2.34 3.57 2.89 1.42 ...
 $ RT_K                        : int  1 4 2 4 4 8 1 1 1 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  64 8 1 256 16 2 64 4 4 2 ...
 $ T1_K                        : int  512 128 16 8 32 8 2 128 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "False" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "True" "True" ...
 $ SCR                         : chr  "True" "False" "True" "False" ...
 $ ACOPY_x                     : chr  "False" "True" "False" "False" ...
 $ ACOPY_y                     : chr  "True" "True" "True" "True" ...
 $ U1_I                        : int  6 9 19 26 20 20 13 14 24 28 ...
 $ RT_J                        : int  1 8 16 2 4 1 1 2 4 16 ...
 $ T1_I                        : int  1 8 16 2 256 1 8 1 8 1 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ RT_I                        : int  32 2 1 2 8 4 2 4 8 4 ...
 $ cost_mean                   : num  3.17 2.35 3.58 2.89 1.42 ...
 $ U_J                         : int  1 15 1 7 11 1 1 6 1 23 ...
 $ U_I                         : int  7 1 1 1 1 30 8 1 19 25 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00553 0.01066 0.00939 0.00837 0.00673 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  8 10 3 20 8 24 10 25 16 1 ...
 $ mean_confidence_interval_sup: num  3.17 2.35 3.58 2.9 1.43 ...
 $ experiment_id               : chr  "graoully-3" "graoully-3" "graoully-3" "graoully-3" ...
 $ min_run_cost                : num  1.25 1.25 1.25 1.25 1.25 ...
 $ best_iteration              : num  228 228 228 228 228 228 228 228 228 228 ...
 $ iteration                   : num  1 2 4 6 7 8 9 10 11 12 ...
 $ points                      : int  266 266 266 266 266 266 266 266 266 266 ...
 $ application                 : chr  "atax" "atax" "atax" "atax" ...
 $ label_center_x              : num  1.67 1.67 1.67 1.67 1.67 ...
 $ label_center_y              : num  228 228 228 228 228 228 228 228 228 228 ...

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	4046 obs. of  40 variables:
 $ id                          : int  1 2 4 6 7 8 9 10 11 12 ...
 $ T2_K                        : int  1 512 1024 32 256 1024 2048 256 512 4 ...
 $ T2_J                        : int  1024 32 16 1024 2048 8 512 128 64 1 ...
 $ T2_I                        : int  64 16 1024 64 256 16 32 256 128 4 ...
 $ mean_confidence_interval_inf: num  3.17 2.34 3.57 2.89 1.42 ...
 $ RT_K                        : int  1 4 2 4 4 8 1 1 1 1 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  64 8 1 256 16 2 64 4 4 2 ...
 $ T1_K                        : int  512 128 16 8 32 8 2 128 4 4 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "False" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "True" "True" "True" ...
 $ SCR                         : chr  "True" "False" "True" "False" ...
 $ ACOPY_x                     : chr  "False" "True" "False" "False" ...
 $ ACOPY_y                     : chr  "True" "True" "True" "True" ...
 $ U1_I                        : int  6 9 19 26 20 20 13 14 24 28 ...
 $ RT_J                        : int  1 8 16 2 4 1 1 2 4 16 ...
 $ T1_I                        : int  1 8 16 2 256 1 8 1 8 1 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ RT_I                        : int  32 2 1 2 8 4 2 4 8 4 ...
 $ cost_mean                   : num  3.17 2.35 3.58 2.89 1.42 ...
 $ U_J                         : int  1 15 1 7 11 1 1 6 1 23 ...
 $ U_I                         : int  7 1 1 1 1 30 8 1 19 25 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ cost_std                    : num  0.00553 0.01066 0.00939 0.00837 0.00673 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ U_K                         : int  8 10 3 20 8 24 10 25 16 1 ...
 $ mean_confidence_interval_sup: num  3.17 2.35 3.58 2.9 1.43 ...
 $ experiment_id               : chr  "graoully-3" "graoully-3" "graoully-3" "graoully-3" ...
 $ min_run_cost                : num  1.25 1.25 1.25 1.25 1.25 ...
 $ best_iteration              : num  228 228 228 228 228 228 228 228 228 228 ...
 $ iteration                   : num  1 2 4 6 7 8 9 10 11 12 ...
 $ points                      : int  266 266 266 266 266 266 266 266 266 266 ...
 $ application                 : chr  "atax" "atax" "atax" "atax" ...
 $ label_center_x              : num  1.67 1.67 1.67 1.67 1.67 ...
 $ label_center_y              : num  228 228 228 228 228 228 228 228 228 228 ...
 $ cost_baseline               : num  NA NA NA NA NA NA NA NA NA NA ...
 $ speedup                     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ max_run_speedup             : num  NA NA NA NA NA NA NA NA NA NA ...
 $ mean_cost_baseline          : num  NA NA NA NA NA NA NA NA NA NA ...
#+end_example

****** Small Steps and Starting Sample
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")
gpr_plot_data$mean_cost_baseline <- mean(subset(complete_plot_data,
                                                technique == "RS")$mean_cost_baseline)

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-xahz2a/figurezV3sGg.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")
gpr_plot_data$mean_cost_baseline <- mean(subset(complete_plot_data,
                                                technique == "RS")$mean_cost_baseline)

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(1.2, 1.5) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red.
#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-X0G35y/figureusnvIV.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  #ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-X0G35y/figureujUE9d.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()


ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(1.2, 1.5) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-X0G35y/figureyY8E7P.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
large_plot_data <- complete_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  ungroup()

large_plot_data$mean_cost_baseline <- subset(large_plot_data, technique == "RS")$mean_cost_baseline[[1]]

large_plot_data <- large_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

ggplot(large_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique)) +
  facet_wrap(technique ~ .) +
  # geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs, ymax = mean_runs + sd_runs), fill = "grey70", alpha = 0.4, colour = NA) +
  geom_smooth(method = "lm",
              formula = "y ~ 1") +
  geom_point() +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  geom_hline(aes(yintercept = mean_cost_baseline[1],
                 linetype = "-O3"),
             color = "black") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set1") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("black")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-X0G35y/figureENgZfa.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
short_plot_data <- complete_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  ungroup()

short_plot_data <- short_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

ggplot(short_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique,
                            group = technique)) +
  facet_wrap(technique ~ .) +
  # geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs, ymax = mean_runs + sd_runs), fill = "grey70", alpha = 0.4, colour = NA) +
  geom_smooth(method = "lm",
              formula = "y ~ 1") +
  geom_point() +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-X0G35y/figureIpol6R.pdf]]
***** Tests with Expected Improvement                          :noexport:
****** Loading Data                                           :noexport:
#+HEADER: :results output :session *R* :exports none
#+BEGIN_SRC R
library(plyr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(rPref)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

read_experiment <- function(x) {
  data <- read.csv(x, header = TRUE, stringsAsFactors = FALSE)
  data$experiment_id <- str_split(str_split(x, "/")[[1]][6], "_")[[1]][5]
  return(data)
}

current_experiment <- "dlmt_spapt_experiments/data/tests/gpr_sobol_ei_parsimonious_v1"
target_path <- paste(current_experiment, "bicgkernel", sep = "/")


# Loading Data for Histograms and Iterations

data_dir <- current_experiment
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)

    data$experiment_id <- str_split(str_split(csv_file, "/")[[1]][6], "_")[[1]][5]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

str(data)

full_data <- data

plot_data <- data %>%
             group_by(experiment_id) %>%
             mutate(mean_cost_baseline = mean(cost_baseline)) %>%
             mutate(label_center_x = mean(cost_mean)) %>%
             mutate(label_center_y = mean(best_iteration)) %>%
             ungroup()

complete_plot_data <- plot_data

str(complete_plot_data)

# data_dir <- "dlmt_spapt_experiments/data/results"
# data_dir <- "dlmt_spapt_experiments/data/tests/no_binary_random"
# data_dir <- "dlmt_spapt_experiments/data/results"
data_dir <- "dlmt_spapt_experiments/data/tests/random_300_graoully"
#target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
target_dirs <- c("bicgkernel")
rs_data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)
    data$experiment_id <- str_split(str_split(csv_file, "/")[[1]][6], "_")[[1]][5]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(rs_data)) {
            rs_data <- target_data
        } else {
            rs_data <- bind_rows(rs_data, target_data)
        }
    }
}

full_data <- bind_rows(filter(full_data, technique == "GPR"),
                       filter(rs_data, technique == "RS", application == "bicgkernel"))

rs_plot_data <- rs_data %>%
  filter(technique == "RS", application == "bicgkernel") %>%
  group_by(experiment_id) %>%
  mutate(mean_cost_baseline = mean(cost_baseline)) %>%
  mutate(label_center_x = mean(cost_mean)) %>%
  mutate(label_center_y = mean(best_iteration)) %>%
  ungroup()

complete_plot_data <- bind_rows(filter(plot_data, technique == "GPR"),
                                rs_plot_data)

str(complete_plot_data)
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:plyr’:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Attaching package: ‘rPref’

The following object is masked from ‘package:dplyr’:

    between

The following objects are masked from ‘package:plyr’:

    empty, true

Attaching package: ‘reshape2’

The following object is masked from ‘package:tidyr’:

    smiths

Registering fonts with R

'data.frame':	3011 obs. of  32 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  512 32 16 32 2048 2048 1024 16 512 64 ...
 $ T2_I                        : int  2048 2048 256 32 4 512 64 16 32 2048 ...
 $ RT_I                        : int  2 4 2 2 8 2 2 4 16 2 ...
 $ mean_confidence_interval_inf: num  5.814 1.478 0.585 1.642 0.543 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  2 1 4 32 256 1024 256 16 16 8 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "False" "True" "True" ...
 $ SCR                         : chr  "True" "True" "False" "True" ...
 $ U1_I                        : int  12 8 30 23 20 2 17 5 13 11 ...
 $ RT_J                        : int  4 16 16 16 8 16 1 16 8 16 ...
 $ T1_I                        : int  32 32 128 16 4 32 16 16 2 1024 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.01111 0.00681 0.0958 0.00667 0.17467 ...
 $ cost_mean                   : num  5.821 1.482 0.645 1.646 0.652 ...
 $ U_J                         : int  21 1 4 14 1 8 1 18 1 1 ...
 $ U_I                         : int  1 6 1 1 16 1 26 1 26 10 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "False" "False" "True" "False" ...
 $ mean_confidence_interval_sup: num  5.828 1.486 0.704 1.65 0.76 ...
 $ experiment_id               : chr  "graoully-11" "graoully-11" "graoully-11" "graoully-11" ...
 $ cost_baseline               : num  4.59 4.59 4.59 4.59 4.59 ...
 $ speedup                     : num  0.789 3.098 7.119 2.789 7.043 ...
 $ max_run_speedup             : num  10.6 10.6 10.6 10.6 10.6 ...
 $ min_run_cost                : num  0.433 0.433 0.433 0.433 0.433 ...
 $ best_iteration              : num  93 93 93 93 93 93 93 93 93 93 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 9 10 ...
 $ points                      : int  306 306 306 306 306 306 306 306 306 306 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	3011 obs. of  35 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  512 32 16 32 2048 2048 1024 16 512 64 ...
 $ T2_I                        : int  2048 2048 256 32 4 512 64 16 32 2048 ...
 $ RT_I                        : int  2 4 2 2 8 2 2 4 16 2 ...
 $ mean_confidence_interval_inf: num  5.814 1.478 0.585 1.642 0.543 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  2 1 4 32 256 1024 256 16 16 8 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "False" "True" "True" ...
 $ SCR                         : chr  "True" "True" "False" "True" ...
 $ U1_I                        : int  12 8 30 23 20 2 17 5 13 11 ...
 $ RT_J                        : int  4 16 16 16 8 16 1 16 8 16 ...
 $ T1_I                        : int  32 32 128 16 4 32 16 16 2 1024 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.01111 0.00681 0.0958 0.00667 0.17467 ...
 $ cost_mean                   : num  5.821 1.482 0.645 1.646 0.652 ...
 $ U_J                         : int  21 1 4 14 1 8 1 18 1 1 ...
 $ U_I                         : int  1 6 1 1 16 1 26 1 26 10 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "False" "False" "True" "False" ...
 $ mean_confidence_interval_sup: num  5.828 1.486 0.704 1.65 0.76 ...
 $ experiment_id               : chr  "graoully-11" "graoully-11" "graoully-11" "graoully-11" ...
 $ cost_baseline               : num  4.59 4.59 4.59 4.59 4.59 ...
 $ speedup                     : num  0.789 3.098 7.119 2.789 7.043 ...
 $ max_run_speedup             : num  10.6 10.6 10.6 10.6 10.6 ...
 $ min_run_cost                : num  0.433 0.433 0.433 0.433 0.433 ...
 $ best_iteration              : num  93 93 93 93 93 93 93 93 93 93 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 9 10 ...
 $ points                      : int  306 306 306 306 306 306 306 306 306 306 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...
 $ mean_cost_baseline          : num  4.59 4.59 4.59 4.59 4.59 ...
 $ label_center_x              : num  0.726 0.726 0.726 0.726 0.726 ...
 $ label_center_y              : num  93 93 93 93 93 93 93 93 93 93 ...

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	5989 obs. of  35 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 9 10 ...
 $ T2_J                        : int  512 32 16 32 2048 2048 1024 16 512 64 ...
 $ T2_I                        : int  2048 2048 256 32 4 512 64 16 32 2048 ...
 $ RT_I                        : int  2 4 2 2 8 2 2 4 16 2 ...
 $ mean_confidence_interval_inf: num  5.814 1.478 0.585 1.642 0.543 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  2 1 4 32 256 1024 256 16 16 8 ...
 $ technique                   : chr  "GPR" "GPR" "GPR" "GPR" ...
 $ VEC2                        : chr  "True" "True" "False" "True" ...
 $ VEC1                        : chr  "False" "False" "True" "True" ...
 $ SCR                         : chr  "True" "True" "False" "True" ...
 $ U1_I                        : int  12 8 30 23 20 2 17 5 13 11 ...
 $ RT_J                        : int  4 16 16 16 8 16 1 16 8 16 ...
 $ T1_I                        : int  32 32 128 16 4 32 16 16 2 1024 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.01111 0.00681 0.0958 0.00667 0.17467 ...
 $ cost_mean                   : num  5.821 1.482 0.645 1.646 0.652 ...
 $ U_J                         : int  21 1 4 14 1 8 1 18 1 1 ...
 $ U_I                         : int  1 6 1 1 16 1 26 1 26 10 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "False" "False" "True" "False" ...
 $ mean_confidence_interval_sup: num  5.828 1.486 0.704 1.65 0.76 ...
 $ experiment_id               : chr  "graoully-11" "graoully-11" "graoully-11" "graoully-11" ...
 $ cost_baseline               : num  4.59 4.59 4.59 4.59 4.59 ...
 $ speedup                     : num  0.789 3.098 7.119 2.789 7.043 ...
 $ max_run_speedup             : num  10.6 10.6 10.6 10.6 10.6 ...
 $ min_run_cost                : num  0.433 0.433 0.433 0.433 0.433 ...
 $ best_iteration              : num  93 93 93 93 93 93 93 93 93 93 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 9 10 ...
 $ points                      : int  306 306 306 306 306 306 306 306 306 306 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...
 $ mean_cost_baseline          : num  4.59 4.59 4.59 4.59 4.59 ...
 $ label_center_x              : num  0.726 0.726 0.726 0.726 0.726 ...
 $ label_center_y              : num  93 93 93 93 93 93 93 93 93 93 ...
#+end_example

****** Small Steps and Starting Sample
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along the iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-o88n4X/figurefmMmRp.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-o88n4X/figureMMKE1d.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along the iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-o88n4X/figureuypoYn.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()


ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-o88n4X/figurekLvqpW.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
ggplot(complete_plot_data, aes(x = min_run_cost,
                               y = best_iteration,
                               color = technique)) +
  geom_point() +
  stat_ellipse(type = "t", linetype = 13) +
  ylab("Iteration where Best was Found") +
  xlab("Best Cost in Seconds") +
  geom_vline(aes(xintercept = mean_cost_baseline[1], linetype = "-O3"), color = "black") +
  #geom_smooth(formula = "y ~ 1",
  #            method = "lm") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Dark2") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("black")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-o88n4X/figureSUGLME.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
ggplot(complete_plot_data, aes(x = min_run_cost,
                               y = best_iteration,
                               color = technique)) +
  geom_point() +
  stat_ellipse(type = "t", linetype = 13) +
  ylab("Iteration where Best was Found") +
  xlab("Best Cost in Seconds") +
  #geom_smooth(formula = "y ~ 1",
  #            method = "lm") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-o88n4X/figuregqlnvn.pdf]]
** December
*** [2019-12-10 Tue]
**** Graphs for Gaussian Process Regression
***** Sampling from Multivariate Normal Distributions
****** 2-Dimensional Example
Sampling from a 2-dimension normal distribution, with no correlation between dimensions,
that is, identity covariance matrix:

|----+-----+-----|
|    |  v1 |  v2 |
|----+-----+-----|
| v1 | 1.0 | 0.0 |
| v2 | 0.0 | 1.0 |
|----+-----+-----|

#+begin_SRC R :results output :session *R*
library(latex2exp)
library(MASS)

n <- 2
sigma <- data.frame(d1 = c(1, 0), d2 = c(0, 1))
means <- c(0, 0)

mv_sample <- mvrnorm(n = 300, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

#+end_SRC

#+RESULTS:
:
: Attaching package: ‘MASS’
:
: The following object is masked from ‘package:dplyr’:
:
:     select

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 28)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figureuAZRdX.png]]

To get strong correlations, the covariance matrix could be:

|----+-----+-----|
|    |  v1 |  v2 |
|----+-----+-----|
| v1 | 1.0 | 0.8 |
| v2 | 0.8 | 1.0 |
|----+-----+-----|

#+begin_SRC R :results output :session *R*
library(MASS)

n <- 2
sigma <- data.frame(d1 = c(1, 0.8), d2 = c(0.8, 1))
means <- c(0, 0)

mv_sample <- mvrnorm(n = 600, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("V", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 26)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figureReB2Vm.png]]

****** 10-Dimensional Example: Reinterpreting Samples
#+begin_SRC R :results output :session *R*
library(latex2exp)
library(MASS)

n <- 10
sigma <- data.frame(diag(10))
names(sigma) <- paste("d", seq(1, n), sep = "")

means <- rep(0, n)

mv_sample <- mvrnorm(n = 300, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("d", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 22)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figurebdO8hQ.png]]

#+begin_SRC R :results output :session *R*
library(latex2exp)
library(MASS)

n <- 10
sigma <- data.frame(diag(10))
names(sigma) <- paste("d", seq(1, n), sep = "")

means <- rep(0, n)

mv_sample <- mvrnorm(n = 1000, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("d", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)

plot_data <- mv_sample
sampled_function <- sample_n(plot_data, 1)

plot_data <- plot_data %>%
  gather("x", "f_x") %>%
  mutate(x = ordered(x, levels = names(mv_sample)))

sampled_function <- sampled_function %>%
  gather("x", "f_x") %>%
  mutate(x = ordered(x, levels = names(mv_sample)))

ggplot(plot_data, aes(x = x, y = f_x)) +
  geom_jitter(color = "gray48", size = 3, width = 0.25, alpha = 0.2) +
  geom_point(data = sampled_function,
             aes(color = "Sample of Multivariate Normal"),
             size = 4) +
  geom_line(data = sampled_function,
            color = "red",
            size = 1,
            alpha = 0.3) +
  ylab(TeX("Sampled Values")) +
  xlab(TeX("Dimensions")) +
  scale_fill_manual("", values = "gray48") +
  scale_color_brewer(palette = "Set1") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.24, 0.06))

#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figurehtbQ7z.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)

n <- 10

plot_data <- mv_sample
names(plot_data) <- seq(1, n)

sampled_function <- sample_n(plot_data, 1)

plot_data <- plot_data %>%
  gather("x", "f_x") %>%
  mutate(x = as.numeric(x))

sampled_function <- sampled_function %>%
  gather("x", "f_x") %>%
  mutate(x = as.numeric(x))

ggplot(plot_data, aes(x = x, y = f_x)) +
  geom_jitter(color = "gray48", size = 3, width = 0.25, alpha = 0.2) +
  geom_point(data = sampled_function,
             aes(color = "Sampled Function"),
             size = 4) +
  geom_line(data = sampled_function,
            color = "red",
            size = 1,
            alpha = 0.3) +
  ylab(TeX("Samples of $(d_1,\\ldots,d_{10})$ interpreted as $f(x \\in \\lbrack 1,10 \\rbrack)$")) +
  xlab(TeX("$(d_1,\\ldots,d_{10})$ interpreted as discrete $x \\in \\lbrack 1,10 \\rbrack$")) +
  scale_x_discrete(limits = seq(1, 10)) +
  scale_fill_manual("", values = "gray48") +
  scale_color_brewer(palette = "Set1") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.2, 0.06))

#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figurey5cDt0.png]]

***** Sampling Functions
#+begin_SRC R :results output :session *R*
d <- 2
n <- 30

target_X <- expand.grid(x1 = seq(-1, 1, length = n), x2 = seq(-1, 1, length = n))
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(target_X) +
  theme_bw(base_size = 26)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figureB084Wp.png]]

#+begin_SRC R :results output :session *R*
library(Rcpp)
library(dplyr)
library(MASS)

src <-
"#include <Rcpp.h>
#include <math.h>

using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix rcpp_squared_exponential_kernel(NumericMatrix x,
                                              NumericMatrix y,
                                              float amplitude,
                                              NumericVector lengthscale){
  NumericMatrix output(x.nrow(), y.nrow());
  float distance;

  for(int i = 0; i < x.nrow(); i++) {
    for(int j = 0; j < y.nrow(); j++) {
      distance = 0;
      for(int k = 0; k < x.ncol(); k++) {
        distance += (((x(i, k) - y(j, k)) *
                      (x(i, k) - y(j, k))) /
                      (lengthscale(k) * lengthscale(k)));
      }

      output(i, j) = amplitude *
                     amplitude *
                     exp(-0.5 * distance);
    }
  }
  return(output);
}"

sourceCpp(code = src)

random_design <- function(factors, size) {
  data.frame(replicate(factors, runif(size)))
}

simulate_gp <- function(covariance_matrix) {
  return(unname(mvrnorm(n = 1,
                        rep(0.0, nrow(covariance_matrix)),
                        covariance_matrix)))
}

sqexp_covariance_matrix <- function(data, amplitude, lengthscale) {
  return(rcpp_squared_exponential_kernel(as.matrix(data),
                                         as.matrix(data),
                                         amplitude,
                                         lengthscale))
}

significance_probability <- 0.10
amplitude <- 1.0
lengthscale <- rep(0.2, n)

cov_matrix <- sqexp_covariance_matrix(target_X, amplitude, lengthscale)
plot_data <- target_X
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(DiceKriging)
library(dplyr)
library(RColorBrewer)
library(lattice)

se_kernel <- function(x, x_p, l = 0.2) {
  return(exp(-(
    ((x - x_p) ^ 2) /
    ((2 * l) ^ 2)
  )))
}

d <- 2
n <- 32

point_grid <- data.frame(x1 = seq(-1, 1, length = n), x2 = rep(0, n))
evaluated_grid <- data.frame(point_grid)

evaluated_grid$y <- se_kernel(point_grid$x1, point_grid$x2)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

ggplot(evaluated_grid, aes(x = x1, y = y)) +
  geom_point() +
  theme_bw(base_size = 18)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figurebGMFGA.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(lattice)
library(RColorBrewer)

plot_data$y <- simulate_gp(cov_matrix)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

wireframe(y ~ x1 * x2,
          data = plot_data,
          xlab = "x1",
          ylab = "x2",
          zlab = list("k(x,x')",
                      rot = "90"),
          #zlim = range(seq(0.0, 1.0, by = 0.5)),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          #shade = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE),
          screen = list(z = 20, x = -60, y = 0),
          par.settings = list(axis.line = list(col = "transparent")))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-6kiapU/figureW4Ug8i.png]]

***** Regression, Sampling, EI
#+begin_SRC R :results output :session *R*
library(ggplot2)
library(dplyr)
library(DiceKriging)
library(DiceOptim)

gpr_data <- data.frame(x = c(0.1, 0.25, 0.3, 0.67, 0.8),
                       y = c(0.1, 0.1, 0.6, -0.02, 0.1))

# gpr_data <- data.frame(x = c(0.45, 0.5, 0.55),
#                        y = c(-0.1, 0.08, 0.2))

x_allowed <- data.frame(x = seq(0, 1, length = 300))

# reg <- km(formula = ~ I(x^2), design = dplyr::select(gpr_data, -y),
#           control = list(pop.size = 40,
#                          BFGSburnin = 4),
#           response = gpr_data$y)

reg <- km(design = dplyr::select(gpr_data, -y),
          control = list(pop.size = 400,
                         BFGSburnin = 400),
          response = gpr_data$y)

pred <- predict(reg, x_allowed, "UK")

x_allowed$y <- pred$mean
x_allowed$ymin <- pred$mean - (2 * pred$sd)
x_allowed$ymax <- pred$mean + (2 * pred$sd)
x_allowed$ei <- apply(dplyr::select(x_allowed, x), 1, EI, reg)
x_allowed$sampled_y <- simulate(reg, cond = TRUE, newdata = dplyr::select(x_allowed, x))[1, ]
x_allowed$uncond_y <- simulate(reg, cond = FALSE, newdata = dplyr::select(x_allowed, x))[1, ]
#+end_SRC

#+RESULTS:
#+begin_example


optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  matern5_2
  - nugget : NO
  - parameters lower bounds :  1e-10
  - parameters upper bounds :  1.4
  - best initial criterion value(s) :  0.5438007

N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -0.5438  |proj g|=   1.6681e-60
Derivative >= 0, backtracking line search impossible.final  value -0.543801
stopped after 0 iterations
#+end_example

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
ggplot(data = gpr_data, aes(x = x, y = y)) +
  geom_ribbon(data = x_allowed, aes(x = x, ymin = ymin, ymax = ymax, fill = "CI of the Mean"), alpha = 0.3) +
  geom_line(data = x_allowed, size = 1, aes(color = "Predicted Mean")) +
  #geom_line(data = x_allowed, size = 1, aes(x = x, y = sampled_y, color = "Conditioned Sample")) +
  #geom_line(data = x_allowed, size = 1, aes(x = x, y = uncond_y, color = "Prior Sample")) +
  geom_line(data = x_allowed, size = 1, aes(x = x, y = ei, color = "Expected Improvement")) +
  geom_point(stroke = 2, shape = 3, size = 3, aes(color = "Observed")) +
  geom_point(data = subset(x_allowed, ei == max(ei)), size = 4, stroke = 2, shape = 3, aes(x = x, y = ei, color = "Maximum EI")) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_manual("", values = "gray48") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.2, 0.2))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-VOewyd/figure4dUwKd.png]]

**** Plotting Covariance Kernels
***** Constant
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(DiceKriging)
library(dplyr)
library(lattice)

constant_kernel <- function(x) {
  return(1.0)
}

d <- 2
n <- 16

point_grid <- expand.grid(x1 = seq(0, 1, length = n), x2 = seq(0, 1, length = n))
evaluated_grid <- data.frame(point_grid)

evaluated_grid$y <- apply(point_grid, 1, constant_kernel)

str(evaluated_grid)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

wireframe(y ~ x1 * x2,
          data = evaluated_grid,
          xlab = "x1",
          ylab = "x2",
          zlab = list("k(x,x')",
                      rot = "90"),
          zlim = range(seq(0.0, 2.0, by = 0.5)),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          #shade = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE),
          screen = list(z = 140, x = -60, y = 0),
          par.settings = list(axis.line = list(col = "transparent")))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-jn9WIm/figure7Zu2B4.png]]
***** Squared Exponential
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(DiceKriging)
library(dplyr)
library(lattice)

se_kernel <- function(x, x_p, l = 0.2) {
  return(exp(-(
    ((x - x_p) ^ 2) /
    ((2 * l) ^ 2)
  )))
}

d <- 2
n <- 32

point_grid <- data.frame(x1 = seq(-1, 1, length = n), x2 = rep(0, n))
evaluated_grid <- data.frame(point_grid)

evaluated_grid$y <- se_kernel(point_grid$x1, point_grid$x2)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

ggplot(evaluated_grid, aes(x = x1, y = y)) +
  geom_point() +
  theme_bw(base_size = 18)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-jn9WIm/figurehyJIns.png]]
*** [2019-12-19 Thu]
**** Estimating GPR Parameters using RS Data
****** Loading Data                                           :noexport:
#+HEADER: :results output :session *R* :exports none
#+BEGIN_SRC R
library(plyr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(rPref)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

# data_dir <- "dlmt_spapt_experiments/data/results"
# data_dir <- "dlmt_spapt_experiments/data/tests/no_binary_random"
# data_dir <- "dlmt_spapt_experiments/data/results"
data_dir <- "dlmt_spapt_experiments/data/tests/random_300_graoully_debnew"
#target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
target_dirs <- c("bicgkernel")
rs_data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)
    data$experiment_id <- str_split(csv_file, "/")[[1]][6]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(rs_data)) {
            rs_data <- target_data
        } else {
            rs_data <- bind_rows(rs_data, target_data)
        }
    }
}

full_data <- filter(rs_data, technique == "RS", application == "bicgkernel")

rs_plot_data <- rs_data %>%
  filter(technique == "RS", application == "bicgkernel") %>%
  group_by(experiment_id) %>%
  mutate(mean_cost_baseline = mean(cost_baseline)) %>%
  mutate(label_center_x = mean(cost_mean)) %>%
  mutate(label_center_y = mean(best_iteration)) %>%
  ungroup()

complete_plot_data <- rs_plot_data

str(complete_plot_data)
#+end_SRC

#+RESULTS:
#+begin_example

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	11927 obs. of  35 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 10 11 ...
 $ T2_J                        : int  512 128 1 4 1 1024 2048 16 128 1024 ...
 $ T2_I                        : int  1 1024 16 2048 2048 32 512 16 256 32 ...
 $ RT_I                        : int  4 1 2 1 1 1 4 32 1 1 ...
 $ mean_confidence_interval_inf: num  3.844 0.439 0.574 0.432 5.218 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  32 2 32 4 4 64 2048 1 64 512 ...
 $ technique                   : chr  "RS" "RS" "RS" "RS" ...
 $ VEC2                        : chr  "True" "False" "True" "False" ...
 $ VEC1                        : chr  "False" "False" "True" "True" ...
 $ SCR                         : chr  "False" "False" "False" "True" ...
 $ U1_I                        : int  16 26 11 18 24 17 20 5 10 27 ...
 $ RT_J                        : int  16 2 4 32 8 4 2 4 4 4 ...
 $ T1_I                        : int  128 8 4 16 2048 1 1 1 8 16 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.11057 0.15496 0.09848 0.11809 0.00476 ...
 $ cost_mean                   : num  3.912 0.535 0.635 0.505 5.221 ...
 $ U_J                         : int  21 1 24 1 15 1 2 17 29 11 ...
 $ U_I                         : int  1 8 1 12 1 1 1 1 1 1 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "False" "True" "True" "True" ...
 $ mean_confidence_interval_sup: num  3.981 0.631 0.696 0.578 5.224 ...
 $ experiment_id               : chr  "xeon_e5_2630_v3_graoully-10_1573846466" "xeon_e5_2630_v3_graoully-10_1573846466" "xeon_e5_2630_v3_graoully-10_1573846466" "xeon_e5_2630_v3_graoully-10_1573846466" ...
 $ cost_baseline               : num  4.58 4.58 4.58 4.58 4.58 ...
 $ speedup                     : num  1.172 8.57 7.215 9.075 0.878 ...
 $ max_run_speedup             : num  10 10 10 10 10 ...
 $ min_run_cost                : num  0.458 0.458 0.458 0.458 0.458 ...
 $ best_iteration              : num  146 146 146 146 146 146 146 146 146 146 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 10 11 ...
 $ points                      : int  297 297 297 297 297 297 297 297 297 297 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...
 $ mean_cost_baseline          : num  4.58 4.58 4.58 4.58 4.58 ...
 $ label_center_x              : num  2.1 2.1 2.1 2.1 2.1 ...
 $ label_center_y              : num  146 146 146 146 146 146 146 146 146 146 ...
#+end_example
****** Analyzing Data
#+begin_SRC R :results output :session *R*
results_data <- select(complete_plot_data,
                       OMP, SCR, VEC1, VEC2,
                       RT_I, RT_J, U1_I, T1_I,
                       T1_J, T2_I, T2_J, U_I, U_J,
                       cost_mean)
#+end_SRC

#+RESULTS:

#+begin_SRC R :results output :session *R*
summary(aov(cost_mean ~ .*., results_data))
#+end_SRC

#+RESULTS:
#+begin_example
               Df Sum Sq Mean Sq   F value   Pr(>F)
OMP             1  19192   19192 38112.189  < 2e-16 ***
SCR             1   3272    3272  6498.543  < 2e-16 ***
VEC1            1      0       0     0.001 0.970549
VEC2            1      1       1     1.624 0.202602
RT_I            1    169     169   336.253  < 2e-16 ***
RT_J            1     65      65   129.166  < 2e-16 ***
U1_I            1      0       0     0.283 0.594502
T1_I            1      5       5     9.617 0.001933 **
T1_J            1     36      36    71.053  < 2e-16 ***
T2_I            1      2       2     3.997 0.045608 *
T2_J            1     12      12    24.159 8.99e-07 ***
U_I             1     64      64   127.359  < 2e-16 ***
U_J             1     59      59   117.487  < 2e-16 ***
OMP:SCR         1   2844    2844  5647.467  < 2e-16 ***
OMP:VEC1        1      0       0     0.259 0.611151
OMP:VEC2        1      1       1     2.758 0.096771 .
OMP:RT_I        1    150     150   298.553  < 2e-16 ***
OMP:RT_J        1     62      62   123.269  < 2e-16 ***
OMP:U1_I        1      0       0     0.092 0.761103
OMP:T1_I        1      5       5     9.749 0.001799 **
OMP:T1_J        1     35      35    69.874  < 2e-16 ***
OMP:T2_I        1      3       3     5.981 0.014475 *
OMP:T2_J        1     10      10    20.631 5.62e-06 ***
OMP:U_I         1     45      45    88.417  < 2e-16 ***
OMP:U_J         1     43      43    84.456  < 2e-16 ***
SCR:VEC1        1      1       1     1.040 0.307732
SCR:VEC2        1      2       2     3.950 0.046888 *
SCR:RT_I        1     10      10    19.513 1.01e-05 ***
SCR:RT_J        1      1       1     1.041 0.307683
SCR:U1_I        1      0       0     0.809 0.368458
SCR:T1_I        1      0       0     0.944 0.331208
SCR:T1_J        1     14      14    26.834 2.25e-07 ***
SCR:T2_I        1      0       0     0.061 0.804390
SCR:T2_J        1     11      11    22.349 2.30e-06 ***
SCR:U_I         1      2       2     4.720 0.029841 *
SCR:U_J         1      1       1     1.997 0.157646
VEC1:VEC2       1      0       0     0.068 0.793657
VEC1:RT_I       1      0       0     0.526 0.468237
VEC1:RT_J       1      0       0     0.267 0.605334
VEC1:U1_I       1      0       0     0.306 0.580000
VEC1:T1_I       1      1       1     1.181 0.277249
VEC1:T1_J       1      0       0     0.371 0.542290
VEC1:T2_I       1      0       0     0.001 0.976130
VEC1:T2_J       1      0       0     0.935 0.333683
VEC1:U_I        1      0       0     0.332 0.564436
VEC1:U_J        1      0       0     0.240 0.624535
VEC2:RT_I       1      2       2     4.620 0.031620 *
VEC2:RT_J       1      0       0     0.109 0.740790
VEC2:U1_I       1      0       0     0.742 0.389009
VEC2:T1_I       1      2       2     3.216 0.072926 .
VEC2:T1_J       1      0       0     0.470 0.493163
VEC2:T2_I       1      0       0     0.158 0.690606
VEC2:T2_J       1      0       0     0.429 0.512448
VEC2:U_I        1      1       1     2.582 0.108095
VEC2:U_J        1      0       0     0.576 0.447972
RT_I:RT_J       1     43      43    85.147  < 2e-16 ***
RT_I:U1_I       1      1       1     1.256 0.262507
RT_I:T1_I       1      5       5     9.666 0.001881 **
RT_I:T1_J       1      4       4     7.960 0.004791 **
RT_I:T2_I       1      3       3     5.370 0.020508 *
RT_I:T2_J       1      4       4     7.676 0.005605 **
RT_I:U_I        1      3       3     5.409 0.020053 *
RT_I:U_J        1      1       1     2.243 0.134269
RT_J:U1_I       1      0       0     0.031 0.859404
RT_J:T1_I       1      6       6    11.039 0.000895 ***
RT_J:T1_J       1      0       0     0.282 0.595687
RT_J:T2_I       1      0       0     0.433 0.510771
RT_J:T2_J       1      1       1     1.434 0.231172
RT_J:U_I        1      0       0     0.217 0.641335
RT_J:U_J        1      0       0     0.585 0.444387
U1_I:T1_I       1      1       1     1.514 0.218576
U1_I:T1_J       1      1       1     2.152 0.142439
U1_I:T2_I       1      0       0     0.087 0.768016
U1_I:T2_J       1      4       4     7.919 0.004899 **
U1_I:U_I        1      0       0     0.635 0.425402
U1_I:U_J        1      0       0     0.107 0.743765
T1_I:T1_J       1      1       1     1.064 0.302230
T1_I:T2_I       1      4       4     8.085 0.004471 **
T1_I:T2_J       1      0       0     0.116 0.733167
T1_I:U_I        1      0       0     0.084 0.772076
T1_I:U_J        1      1       1     2.303 0.129124
T1_J:T2_I       1      0       0     0.390 0.532562
T1_J:T2_J       1      7       7    13.810 0.000203 ***
T1_J:U_I        1      6       6    12.445 0.000421 ***
T1_J:U_J        1      5       5    10.613 0.001126 **
T2_I:T2_J       1      0       0     0.073 0.786504
T2_I:U_I        1      0       0     0.233 0.629516
T2_I:U_J        1      1       1     2.282 0.130881
T2_J:U_I        1      3       3     4.965 0.025886 *
T2_J:U_J        1      0       0     0.329 0.566421
Residuals   11836   5960       1
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example

#+begin_SRC R :results output :session *R*
nobin_results_data <- results_data %>%
                      filter(OMP == "True", SCR == "True",
                             VEC1 == "True", VEC2 == "True") %>%
                      select(-OMP, -VEC1,
                             -VEC2, -SCR)

summary(aov(cost_mean ~ .*., nobin_results_data))
#+end_SRC

#+RESULTS:
#+begin_example

             Df Sum Sq Mean Sq F value   Pr(>F)
RT_I          1  0.002 0.00151   0.263  0.60846
RT_J          1  0.128 0.12832  22.306 2.84e-06 ***
U1_I          1  0.006 0.00619   1.076  0.30006
T1_I          1  0.109 0.10865  18.887 1.60e-05 ***
T1_J          1  0.013 0.01290   2.242  0.13482
T2_I          1  0.026 0.02609   4.535  0.03357 *
T2_J          1  0.006 0.00608   1.057  0.30423
U_I           1  0.057 0.05742   9.982  0.00165 **
U_J           1  0.055 0.05450   9.474  0.00217 **
RT_I:RT_J     1  0.059 0.05861  10.187  0.00148 **
RT_I:U1_I     1  0.043 0.04340   7.544  0.00618 **
RT_I:T1_I     1  0.002 0.00154   0.267  0.60549
RT_I:T1_J     1  0.000 0.00016   0.028  0.86628
RT_I:T2_I     1  0.031 0.03055   5.311  0.02150 *
RT_I:T2_J     1  0.001 0.00096   0.167  0.68337
RT_I:U_I      1  0.002 0.00151   0.262  0.60909
RT_I:U_J      1  0.002 0.00162   0.281  0.59596
RT_J:U1_I     1  0.004 0.00402   0.698  0.40370
RT_J:T1_I     1  0.000 0.00044   0.076  0.78225
RT_J:T1_J     1  0.014 0.01435   2.495  0.11469
RT_J:T2_I     1  0.004 0.00413   0.718  0.39700
RT_J:T2_J     1  0.001 0.00137   0.238  0.62601
RT_J:U_I      1  0.000 0.00014   0.025  0.87394
RT_J:U_J      1  0.015 0.01523   2.647  0.10421
U1_I:T1_I     1  0.000 0.00048   0.083  0.77362
U1_I:T1_J     1  0.001 0.00129   0.224  0.63606
U1_I:T2_I     1  0.016 0.01631   2.835  0.09271 .
U1_I:T2_J     1  0.005 0.00464   0.807  0.36947
U1_I:U_I      1  0.001 0.00057   0.099  0.75268
U1_I:U_J      1  0.001 0.00094   0.164  0.68540
T1_I:T1_J     1  0.003 0.00301   0.523  0.47001
T1_I:T2_I     1  0.000 0.00012   0.021  0.88580
T1_I:T2_J     1  0.000 0.00019   0.032  0.85730
T1_I:U_I      1  0.000 0.00009   0.015  0.90240
T1_I:U_J      1  0.003 0.00323   0.562  0.45385
T1_J:T2_I     1  0.019 0.01927   3.349  0.06769 .
T1_J:T2_J     1  0.017 0.01686   2.930  0.08739 .
T1_J:U_I      1  0.005 0.00468   0.814  0.36729
T1_J:U_J      1  0.000 0.00033   0.058  0.80961
T2_I:T2_J     1  0.016 0.01560   2.713  0.10004
T2_I:U_I      1  0.001 0.00127   0.220  0.63927
T2_I:U_J      1  0.001 0.00131   0.227  0.63385
T2_J:U_I      1  0.006 0.00608   1.056  0.30450
T2_J:U_J      1  0.002 0.00159   0.276  0.59942
Residuals   664  3.820 0.00575
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example
****** Estimating GPR Kernel Parameters
#+begin_SRC R :results output :session *R*
results_data <- select(complete_plot_data,
                       OMP, SCR, VEC1, VEC2,
                       RT_I, RT_J, U1_I, T1_I,
                       T1_J, T2_I, T2_J, U_I, U_J,
                       cost_mean)
#+end_SRC

#+RESULTS:

#+begin_SRC R :results output :session *R*
library(DiceKriging)
library(dplyr)
library(rsm)

sampled_results_data <- sample_n(results_data, 5000)

sampled_results_data <- sampled_results_data %>% mutate_all(funs(str_replace(., "True", "1"))) %>%
  mutate_all(funs(str_replace(., "False", "0"))) %>%
  mutate_all(funs(as.integer(.)))

coded_sampled_design <- coded.data(select(sampled_results_data, -cost_mean),
                                   formulas = list(T1_Ie = T1_Ie ~ (T1_I - 5.5) / 5.5,
                                                   T1_Je = T1_Je ~ (T1_J - 5.5) / 5.5,
                                                   U_Je = U_Je ~ (U_J - 14.5) / 14.5,
                                                   U_Ie = U_Ie ~ (U_I - 14.5) / 14.5,
                                                   T2_Ie = T2_Ie ~ (T2_I - 5.5) / 5.5,
                                                   T2_Je = T2_Je ~ (T2_J - 5.5) / 5.5,
                                                   U1_Ie = U1_Ie ~ (U1_I - 14.5) / 14.5,
                                                   OMPe = OMPe ~ (OMP - 0.5) / 0.5,
                                                   SCRe = SCRe ~ (SCR - 0.5) / 0.5,
                                                   VEC1e = VEC1e ~ (VEC1 - 0.5) / 0.5,
                                                   VEC2e = VEC2e ~ (VEC2 - 0.5) / 0.5,
                                                   RT_Ie = RT_Ie ~ (RT_I - 2.5) / 2.5,
                                                   RT_Je = RT_Je ~ (RT_J - 2.5) / 2.5))

reg <- km(design = coded_sampled_design,
          response = sampled_results_data$cost_mean)
#+end_SRC

#+RESULTS:
#+begin_example


optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  matern5_2
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10
  - parameters upper bounds :  4 4 4 4 24.8 24.8 4 744.3636 744.3636 744.3636 744.3636 4 4
  - best initial criterion value(s) :  -7562.03

N = 13, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=         7562  |proj g|=        24.25
At iterate     1  f =       7413.9  |proj g|=        23.848
At iterate     2  f =       7392.2  |proj g|=        23.778
At iterate     3  f =       7325.1  |proj g|=        9.7341
At iterate     4  f =       7296.3  |proj g|=        9.6407
At iterate     5  f =       6766.4  |proj g|=        6.9176
At iterate     6  f =       6709.6  |proj g|=        7.5269
At iterate     7  f =       6694.2  |proj g|=        7.5742
At iterate     8  f =       6663.7  |proj g|=        7.5463
At iterate     9  f =       6620.9  |proj g|=        6.3228
At iterate    10  f =       6571.9  |proj g|=         5.853
At iterate    11  f =       6533.7  |proj g|=        5.0285
At iterate    12  f =       6464.6  |proj g|=        5.2125
At iterate    13  f =       6342.8  |proj g|=         7.443
At iterate    14  f =       6324.5  |proj g|=        7.3488
At iterate    15  f =       6323.4  |proj g|=        7.3507
At iterate    16  f =       6321.3  |proj g|=        7.4102
At iterate    17  f =       6318.6  |proj g|=        7.4496
At iterate    18  f =       6316.4  |proj g|=        7.3897
At iterate    19  f =       6313.5  |proj g|=        7.5038
At iterate    20  f =       6310.5  |proj g|=        7.7871
At iterate    21  f =       6310.2  |proj g|=        7.7868
At iterate    22  f =       6309.9  |proj g|=        7.7872
At iterate    23  f =       6308.1  |proj g|=        7.7314
At iterate    24  f =       6303.1  |proj g|=        6.9893
At iterate    25  f =         6301  |proj g|=        6.9916
At iterate    26  f =       6300.5  |proj g|=        6.9778
At iterate    27  f =       6297.4  |proj g|=        6.9241
At iterate    28  f =       6290.7  |proj g|=        6.7774
At iterate    29  f =       6283.6  |proj g|=        6.5873
At iterate    30  f =       6282.4  |proj g|=        6.4988
At iterate    31  f =       6282.2  |proj g|=        6.4529
At iterate    32  f =       6281.9  |proj g|=        6.4503
At iterate    33  f =       6280.7  |proj g|=         6.434
At iterate    34  f =       6279.2  |proj g|=        6.4062
At iterate    35  f =       6276.6  |proj g|=         6.345
At iterate    36  f =       6275.7  |proj g|=        6.3308
At iterate    37  f =       6275.3  |proj g|=        6.3282
At iterate    38  f =       6274.8  |proj g|=        6.3181
At iterate    39  f =       6274.2  |proj g|=        6.2935
At iterate    40  f =       6273.6  |proj g|=        6.2499
At iterate    41  f =       6273.1  |proj g|=        6.1728
At iterate    42  f =       6272.8  |proj g|=        6.0966
At iterate    43  f =       6272.4  |proj g|=        6.0309
At iterate    44  f =       6272.2  |proj g|=        6.0113
At iterate    45  f =       6271.9  |proj g|=        5.9816
At iterate    46  f =       6271.4  |proj g|=        5.9122
At iterate    47  f =         6270  |proj g|=        6.0153
At iterate    48  f =       6266.5  |proj g|=        6.8669
At iterate    49  f =       6265.1  |proj g|=        4.2789
At iterate    50  f =       6261.3  |proj g|=        5.3392
At iterate    51  f =       6258.6  |proj g|=        5.1225
At iterate    52  f =       6255.9  |proj g|=        3.7405
At iterate    53  f =       6255.4  |proj g|=        2.8486
At iterate    54  f =       6255.2  |proj g|=        3.8057
At iterate    55  f =       6255.1  |proj g|=        3.8055
At iterate    56  f =       6254.8  |proj g|=        3.5039
At iterate    57  f =       6254.6  |proj g|=        3.8092
At iterate    58  f =       6254.3  |proj g|=        3.7397
At iterate    59  f =         6254  |proj g|=        3.4176
At iterate    60  f =         6254  |proj g|=        2.7449
At iterate    61  f =         6254  |proj g|=        2.1714
At iterate    62  f =         6254  |proj g|=        2.1588
At iterate    63  f =         6254  |proj g|=        3.7402
At iterate    64  f =       6253.9  |proj g|=        3.7424
At iterate    65  f =       6253.8  |proj g|=        3.7462
At iterate    66  f =       6253.6  |proj g|=        3.4908
At iterate    67  f =       6253.4  |proj g|=        3.7551
At iterate    68  f =       6253.3  |proj g|=        3.7525
At iterate    69  f =       6253.2  |proj g|=        2.7038
At iterate    70  f =       6253.2  |proj g|=        3.1047
At iterate    71  f =       6253.1  |proj g|=        2.0107
At iterate    72  f =       6253.1  |proj g|=        1.9014
At iterate    73  f =       6253.1  |proj g|=         3.809
At iterate    74  f =       6253.1  |proj g|=        2.9691
At iterate    75  f =       6253.1  |proj g|=        2.3959
At iterate    76  f =       6253.1  |proj g|=        2.4247
At iterate    77  f =       6253.1  |proj g|=        1.2962
At iterate    78  f =       6253.1  |proj g|=        1.5339
At iterate    79  f =         6253  |proj g|=        3.8108
At iterate    80  f =         6253  |proj g|=        3.8109
At iterate    81  f =         6253  |proj g|=        3.8109
At iterate    82  f =         6253  |proj g|=       0.96266
At iterate    83  f =       6252.9  |proj g|=       0.96558
At iterate    84  f =       6252.7  |proj g|=        3.7463
At iterate    85  f =       6252.6  |proj g|=        3.7469
At iterate    86  f =       6252.6  |proj g|=        1.5952
At iterate    87  f =       6252.5  |proj g|=        2.0364
At iterate    88  f =       6252.5  |proj g|=        3.7467
At iterate    89  f =       6252.5  |proj g|=        2.2482
At iterate    90  f =       6252.5  |proj g|=        2.2013
At iterate    91  f =       6252.5  |proj g|=        2.5787
At iterate    92  f =       6252.4  |proj g|=        3.8048
At iterate    93  f =       6252.4  |proj g|=        3.5006
At iterate    94  f =       6252.4  |proj g|=        2.4506
At iterate    95  f =       6252.4  |proj g|=        2.5164
At iterate    96  f =       6252.4  |proj g|=        3.8054
At iterate    97  f =       6252.3  |proj g|=        3.8053
At iterate    98  f =       6252.2  |proj g|=        3.8082
At iterate    99  f =       6251.9  |proj g|=        3.8117
At iterate   100  f =       6251.8  |proj g|=        3.8177
At iterate   101  f =       6251.6  |proj g|=        3.8191
final  value 6251.598219
stopped after 101 iterations
#+end_example

#+begin_SRC R :results output :session *R*
reg
#+end_SRC

#+RESULTS:
#+begin_example

Call:
km(design = coded_sampled_design, response = sampled_results_data$cost_mean)

Trend  coeff.:
               Estimate
 (Intercept)     1.4689

Covar. type  : matern5_2
Covar. coeff.:
               Estimate
 theta(OMPe)     0.0000
 theta(SCRe)     1.3433
theta(VEC1e)     4.0000
theta(VEC2e)     4.0000
theta(RT_Ie)     4.0576
theta(RT_Je)     7.0118
theta(U1_Ie)     0.2681
theta(T1_Ie)   648.8870
theta(T1_Je)   290.8718
theta(T2_Ie)   174.3350
theta(T2_Je)   744.2121
 theta(U_Ie)     0.4684
 theta(U_Je)     0.1809

Variance estimate: 1.510738
#+end_example
* 2020
** February
*** [2020-02-05 Wed]
**** Review for COMCOM                                       :PaperReview:
The  paper  presents  the  \varepsilon-sticky  algorithm, an  extension  of  the  \varepsilon-greedy
algorithm for the  Multi-Armed Bandit problem to the selection  of Access Points
by user stations, and evaluates the  performance of the proposed approach in two
scenarios, varying in the distribution  of user stations.  The proposed approach
presents significant improvements in user station satisfaction, in relation to a
Strongest  Signal Access  Point  selection algorithm.   The paper  significantly
extends  the authors'  previous  work,  providing a  brief  introduction to  the
Multi-Armed Bandit  problem and  its different types,  and a  significantly more
extensive evaluation and validation of  the proposed approach.  The experimental
methodology is  solid, and  the proposed  approach convincingly  performs better
than a Strongest Signal selection algorithm, in all scenarios studied. The study
of the best  values of \varepsilon was  also interesting, since choosing a  good value for
the parameter has a significant impact on the observed throughput.
**** Applications of Program Autotuning: Call for Proposals for Funding for Undergraduates, Masters, and PhD Students
We are glad to  announce a call for masters and PhD  scholarship proposals for a
research project, done in the context  of a collaboration between the University
of  São  Paulo   and  the  Hewlett  Packard  Enterprise   company,  starting  on
February 2020.

The work to be performed by candidates  will require 10 hours per week, and will
involve the application of techniques for statistical optimization and design of
experiments to a problem presented by the candidate. We are looking for students
interested in optimizing their applications, or applications used in their work,
targeting different metrics, such as performance, memory or network usage.

Ideal  problems  for this  project  include  problems  where the  search  spaces
involved  are  too large  to  be  explored  exhaustively,  and have  no  trivial
configuration. Examples of problems include  the selection of hyperparameters of
Machine Learning algorithms to  increase prediction accuracy, selecting compiler
flags  to improve  performance, and  configuring user-developed  applications to
improve user-defined metrics.

We would like students who have  already stated their research problems and have
had some progress towards measuring  and evaluating their objectives, but highly
motivated students who are just starting are also welcome to apply.

Up to  5 accepted  candidates will receive  funding, comparable  to scholarships
from  CAPES,   payed  by  IME/USP   for  eight  months,  between   February  and
September 2020. Students currently without funding are preferred.

We invite  interested students to  submit a CV  and a cover  letter, summarizing
their problem and  their interest in applying optimization techniques  to it, by
February  20th 2020.  We assume  your advisor  is aware  and consents  with your
participation in this project. Accepted candidates  will be invited to a two-day
workshop at IME/USP, where techniques and libraries for statistical optimization
and design  of experiments will  be presented, and  candidates will be  asked to
give short presentations about their work. Final acceptance notification will be
sent by February 29th.

Project Coordinator: Prof. Dr. Alfredo Goldman
Project Manager: Pedro Bruel
*** [2020-02-06 Thu]
**** Autotuning for GCC
***** Some References
- https://en.wikipedia.org/wiki/MILEPOST_GCC
- https://github.com/ctuning
- https://github.com/ctuning/ck/wiki
***** Ideas
****** Benchmark
- Ofast
  - unsafe optim
  - error rate
****** Search Space
- Focus on *flags* at first:
  - Explore *numerical, categorical parameters* later
  - LTO
- Which flags should be chosen?
- Baselines:
  - O0, ..., O3, (Ofast, ...?)
  - Random sample of flag configurations
****** Metrics / Search Objectives
- Performance, memory, binary size, ...
- Focus on *one metric* first
  - Pareto frontier of the others
- We could try something like a normalized, weighted sum of metrics
****** Experiments and Analysis Plan
- Plan a screening experiment: Identify Main Effects
- Leverage expert knowledge plus screening to plan next experiments
- Low-discrepancy sampling, linear model, ANOVA
- Gaussian Process Regression?
***** Plan
1. Define a benchmark
   - Firefox
   - SPEC
   - BLAS Lapack
2. Define metrics
3. Define the search space
   1. Flags
   2. Others
4. Start experiments
   1. Random sample
   2. Screening
   3. Linear models + ANOVA
   4. Linear models + ANOVA + Optimal Design
   5. Gaussian Process Regression
*** [2020-02-13 Thu]
**** Experimental Design for a 2D Problem
***** Defining a Problem
Consider the following function of two factors $(x_1, x_2)$:

\[
f(x_1, x_2) + \varepsilon = 2.3 + (1.1x_1) + (1.6x_{1}^{2}) + (2.2x_2) + (3.2x_{2}^{2}) + \varepsilon
\]

That is, $f(x_1, x_2)$ has linear and quadratic dependence on $(x_1, x_2)$.  Here, \varepsilon
is a normally distributed added error with mean zero and variance one.

In R, we can write $f$ as:

#+begin_SRC R :results output :session *R*
f <- function(x1, x2) {
  return(2.3 + (1.1 * x1) + (1.6 * x1 * x1) +
         (2.2 * x2) + (3.2 * x2 * x2) + (2.7 * x1 * x2) + rnorm(length(x1)))
}

# Measuring 5 values of f(x1, x2)
f(rnorm(5), rnorm(5))
#+end_SRC

#+RESULTS:
:
: [1]  1.022636 11.213378  5.611775  5.246327  5.553409

The inputs of $f$ are defined to  be real numbers in the interval $[-1.0, 1.0]$.
Since we cannot generate  the set of *all possible inputs* of $f$,  we have to set
for a *sample of limited size*. Say that we cover the $(x_1, x_2)$ space with a grid
of points in $[-1.0, 1.0]$, spaced by $0.05$ in each axis. In R, we can write:

#+begin_SRC R :results output :session *R*
resolution <- 0.05

sample_grid <- expand.grid(x1 = seq(-1.0, 1.0, by = resolution),
                           x2 = seq(-1.0, 1.0, by = resolution))

sample_grid$f <- f(sample_grid$x1, sample_grid$x2)

str(sample_grid)
#+end_SRC

#+RESULTS:
#+begin_example

'data.frame':	1681 obs. of  3 variables:
 $ x1: num  -1 -0.95 -0.9 -0.85 -0.8 -0.75 -0.7 -0.65 -0.6 -0.55 ...
 $ x2: num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
 $ f : num  7.32 7.01 7.26 7.09 5.93 ...
 - attr(*, "out.attrs")=List of 2
  ..$ dim     : Named int  41 41
  .. ..- attr(*, "names")= chr  "x1" "x2"
  ..$ dimnames:List of 2
  .. ..$ x1: chr  "x1=-1.00" "x1=-0.95" "x1=-0.90" "x1=-0.85" ...
  .. ..$ x2: chr  "x2=-1.00" "x2=-0.95" "x2=-0.90" "x2=-0.85" ...
#+end_example

Plotting this low-resolution view the space, we get:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(lattice)
library(RColorBrewer)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

wireframe(f ~ x1 * x2,
          data = sample_grid,
          xlab = "x1",
          ylab = "x2",
          zlab = list("f(x1,x2)",
                      rot = "90"),
          zlim = range(seq(min(sample_grid$f) - 2.0, max(sample_grid$f) + 2.0, by = 0.5)),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          #shade = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE),
          screen = list(z = 20, x = -60, y = 0),
          par.settings = list(axis.line = list(col = "transparent")))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-qRrjQN/figureElZ1TS.png]]
***** Design of Experiments
Suppose we want to minimize $f$, but we do not have access to the computation of
$f(x_1, x_2)$, and  suppose that it is really expensive  to obtain each evaluation
for the inputs, so expensive in fact,  that we can only afford 20 evaluations of
$f$.  In this scenario, how to best explore the search space?

#+begin_SRC R :results output :session *R*
library(dplyr)
library(AlgDesign)

budget <- 200
sample_rs <- sample_n(sample_grid, budget)

sample_rs$method <- "Random Design"
exploration <- sample_rs

sample_linear_model <- optFederov(~ x1 + x2 + x1:x2,
                                  data = sample_grid,
                                  nTrials = budget)$design
sample_linear_model$method <- "Linear Model Design"
exploration <- bind_rows(exploration,
                         sample_linear_model)

sample_quadratic_model <- optFederov(~ (x1 + x2) + (x1:x2) + I(x1 ^ 2) + I(x2 ^ 2),
                                     data = sample_grid,
                                     nTrials = budget)$design

sample_quadratic_model$method <- "Quadratic Model Design"
exploration <- bind_rows(exploration,
                         sample_quadratic_model)

str(exploration)
#+end_SRC

#+RESULTS:
:
: 'data.frame':	600 obs. of  4 variables:
:  $ x1    : num  1 -0.45 0.35 -0.7 -0.5 1 0.6 -0.25 -0.65 0.2 ...
:  $ x2    : num  0.4 0.65 0.4 -0.15 0.1 -0.15 -0.15 -1 0.8 0.1 ...
:  $ f     : num  8.29 5.12 6.08 1.46 1.86 ...
:  $ method: chr  "Random Design" "Random Design" "Random Design" "Random Design" ...

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 400
library(ggplot2)

ggplot(data = exploration, aes(x = x1, y = x2)) +
  facet_wrap(method ~ ., ncol = 3) +
  geom_point(size = 3) +
  theme_bw(base_size = 22)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-qRrjQN/figuretwUzsW.png]]

***** Fitting and Predicting Values
We now remember the definition of $f$:

\[
f(x_1, x_2) = 2.3 + (1.1x_1) + (1.6x_{1}^{2}) + (2.2x_2) + (3.2x_{2}^{2}) + \varepsilon
\]

If  we try  to  obtain a  regression  function for  $f$,  testing the  quadratic
hypotheses using the different designs, we would get:

#+begin_SRC R :results output :session *R*
rs_lm <- lm(f ~ x1 * x2 * I(x1 ^ 2) * I(x2 ^ 2), data = filter(exploration, method == "Random Design"))
lin_lm <- lm(f ~ x1 * x2 * I(x1 ^ 2) * I(x2 ^ 2), data = filter(exploration, method == "Linear Model Design"))
quad_lm <- lm(f ~ x1 * x2 * I(x1 ^ 2) * I(x2 ^ 2), data = filter(exploration, method == "Quadratic Model Design"))

coef(rs_lm)
coef(lin_lm)
coef(quad_lm)
#+end_SRC

#+RESULTS:
#+begin_example

          (Intercept)                    x1                    x2
           2.27465946            0.53902138            1.79331229
              I(x1^2)               I(x2^2)                 x1:x2
           1.83183371            3.21003163            2.89395296
           x1:I(x1^2)            x2:I(x1^2)            x1:I(x2^2)
           0.55369765            0.65345353            1.82588396
           x2:I(x2^2)       I(x1^2):I(x2^2)         x1:x2:I(x1^2)
           0.44999902           -0.16324027           -0.94369167
        x1:x2:I(x2^2)    x1:I(x1^2):I(x2^2)    x2:I(x1^2):I(x2^2)
           0.08833311           -2.20304930           -1.10485608
x1:x2:I(x1^2):I(x2^2)
           1.20883837

          (Intercept)                    x1                    x2
            2.3489419            -0.4718219             0.6435793
              I(x1^2)               I(x2^2)                 x1:x2
            1.4927455             3.0955042             0.1057755
           x1:I(x1^2)            x2:I(x1^2)            x1:I(x2^2)
            1.9184213             2.1306245             2.0737388
           x2:I(x2^2)       I(x1^2):I(x2^2)         x1:x2:I(x1^2)
            1.9934353             0.3227596             2.2008337
        x1:x2:I(x2^2)    x1:I(x1^2):I(x2^2)    x2:I(x1^2):I(x2^2)
            2.9995394            -2.6608638            -2.7244153
x1:x2:I(x1^2):I(x2^2)
           -2.5155163

          (Intercept)                    x1                    x2
            2.0576184             2.0389072             1.2520964
              I(x1^2)               I(x2^2)                 x1:x2
            1.8942690             3.7012931             1.3285130
           x1:I(x1^2)            x2:I(x1^2)            x1:I(x2^2)
           -1.0086035             1.6426495            -0.4026176
           x2:I(x2^2)       I(x1^2):I(x2^2)         x1:x2:I(x1^2)
            1.0391508            -0.4528896             0.6970911
        x1:x2:I(x2^2)    x1:I(x1^2):I(x2^2)    x2:I(x1^2):I(x2^2)
            1.5442947             0.2870124            -1.9088828
x1:x2:I(x1^2):I(x2^2)
           -0.6770010
#+end_example

#+begin_SRC R :results output :session *R*
rs_lm <- aov(f ~ x1 * x2 + I(x1 ^ 2) + I(x2 ^ 2), data = filter(exploration, method == "Random Design"))
quad_lm <- aov(f ~ x1 * x2 + I(x1 ^ 2) + I(x2 ^ 2), data = filter(exploration, method == "Quadratic Model Design"))
summary.aov(rs_lm)
#+end_SRC

#+RESULTS:
#+begin_example

             Df Sum Sq Mean Sq F value   Pr(>F)
x1            1   67.3    67.3   66.44 4.37e-14 ***
x2            1  324.3   324.3  320.20  < 2e-16 ***
I(x1^2)       1   39.8    39.8   39.28 2.32e-09 ***
I(x2^2)       1  195.4   195.4  192.94  < 2e-16 ***
x1:x2         1  166.6   166.6  164.49  < 2e-16 ***
Residuals   194  196.5     1.0
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example

#+begin_SRC R :results output :session *R*
prediction_rs <- predict(rs_lm, newdata = select(sample_grid, -f))
prediction_error <- data.frame(error = sample_grid[prediction_rs == min(prediction_rs), ]$f - min(prediction_rs))
prediction_error$method <- "Random Design"

prediction_lin <- predict(lin_lm, newdata = select(sample_grid, -f))
prediction_error_lin <- data.frame(error = sample_grid[prediction_lin == min(prediction_lin), ]$f - min(prediction_lin))
prediction_error_lin$method <- "Linear Model Design"

prediction_error <- bind_rows(prediction_error,
                              prediction_error_lin)

prediction_quad <- predict(quad_lm, newdata = select(sample_grid, -f))
prediction_error_quad <- data.frame(error = sample_grid[prediction_quad == min(prediction_quad), ]$f - min(prediction_quad))
prediction_error_quad$method <- "Quadratic Model Design"

prediction_error <- bind_rows(prediction_error,
                              prediction_error_quad)

prediction_error
#+end_SRC

#+RESULTS:
:
:        error                 method
: 1 -1.2170481          Random Design
: 2  0.6531471    Linear Model Design
: 3  0.3871706 Quadratic Model Design
**** Underlying Hypotheses of Autotuning Methods       :ExportableReports:
***** Introduction                                             :noexport:
Given  a program  with $X  \in \mathcal{X}$  configurable parameters,  we want  to
choose the best parameter values according  to a performance metric given by the
function  $f(X)$.   Autotuning methods  attempt  find  the $X_{*}$  that  minimizes
$f(\cdot)$.   Despite  their different  approaches,  autotuning  methods share  some
common hypotheses:

- There is no knowledge about the global optimal configuration
- There could be some problem-specific knowledge to exploit
- Measuring the effects of a choice of parameter values is possible but costly

Each  autotuning method  has  assumptions that  justify  its implementation  and
usage. Some of  these hypotheses are explicit,  such as the ones  that come from
the  linear model.   Others are  implicit,  such as  the ones  that support  the
implementation and the justification of optimization heuristics.
***** Overview of Autotuning Methods
#+begin_export latex
\begin{sidewaysfigure}[h]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      for tree={%
        anchor = center,
        align = center,
        l sep+=2em
      },
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
        draw,
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$?},
          draw,
          color = NavyBlue
          [{Search\\Heuristics},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{\textbf{Random}\\\textbf{Sampling}}, draw]
            [{Reachable\\Optima},
              draw,
              color = BurntOrange
              [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
                draw,
                color = BurntOrange
                [{Strong\\$corr(f(X),d(X,X_{*}))$?},
                  draw,
                  color = NavyBlue
                  [{More\\Global},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{Introduce a \textit{population}\\$(X_1,\dots,X_n)$ of $X$},
                      draw,
                      color = BurntOrange
                      [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]
                      [, phantom]]
                    [, phantom]]
                  [{More\\Local},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{High local\\optima density?},
                      draw,
                      color = NavyBlue
                      [{Steepest\\Descent},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                        [{\textbf{Greedy}\\\textbf{Search}}, draw]
                        [{Estimate $f^{\prime}(X)$},
                          draw,
                          color = BurntOrange,
                          [{\textbf{Gradient}\\\textbf{Descent}}, draw]
                          [, phantom]]]
                      [{Allows\\exploration},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                        [{Accept\\worst $f(X)$},
                          draw,
                          color = BurntOrange
                          [{\textbf{Simulated}\\\textbf{Annealing}}, draw]
                          [, phantom]]
                        [{Avoid\\recent $X$},
                          draw,
                          color = BurntOrange
                          [, phantom]
                          [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]]
                [,phantom]]
              [,phantom]]]
          [{Statistical\\Learning},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{Parametric\\Learning},
              draw,
              color = BurntOrange
              [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                draw,
                color = BurntOrange
                [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                [, phantom]]
              [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                draw,
                color = BurntOrange
                [, phantom]
                [{Check for\\model adequacy?},
                  draw,
                  alias = adequacy,
                  color = NavyBlue
                  [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                    draw,
                    alias = interactions,
                    color = NavyBlue,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{$\forall x_i \in X: x_i \in \{-1, 1\}$},
                      draw,
                      color = BurntOrange,
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                      [{\textbf{Screening}\\\textbf{Designs}},
                        draw
                        [, phantom]
                        [{Select $\hat{X}_{*}$, reduce\\dimension of $\mathcal{X}$},
                          edge = {-stealth, ForestGreen, semithick},
                          draw,
                          alias = estimate,
                          color = ForestGreen]]
                      [, phantom]]
                    [{\textbf{Optimal}\\\textbf{Design}},
                      draw,
                      alias = optimal,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [{\textbf{Space-filling}\\\textbf{Designs}},
                    draw,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{Model selection},
                      edge = {-stealth, ForestGreen, semithick},
                      draw,
                      alias = selection,
                      color = ForestGreen]]]]]
            [{Nonparametric\\Learning},
              draw,
              color = BurntOrange
              [{Splitting\\rules on X},
                draw,
                color = BurntOrange
                [, phantom]
                [{\textbf{Decision}\\\textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}$ and its \textit{uncertainty}},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize\\$uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize\\$\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]]
              [{\textbf{Gaussian}\\\textbf{Process Regression}},
                alias = gaussian,
                draw]
              [{\textbf{Neural}\\\textbf{Networks}}, draw]]]]]
      \draw [-stealth, semithick, ForestGreen](selection) to[bend left=22] (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](estimate.east) to[bend right=30] (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
      \draw [-stealth, semithick, ForestGreen](optimal) to (estimate);
    \end{forest}
  }
  \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
    denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
    denote specific method choices, and \textbf{bold} boxes denote specific methods.}
\end{sidewaysfigure}
#+end_export

***** Previous Attempts                                        :noexport:
#+begin_export latex
\forestset{linebreaks/.style={for tree={align = center}}}
\begin{sidewaysfigure}
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      linebreaks
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\ $Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$}
        [{Does not construct\\estimate $Y = \hat{f}(\cdot, \theta{}(X))$}
          [{Reachable\\optima}
            [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$}
              [{Strong\\$corr(f(X),d(X,X_{*}))$}
                [{Low local\\optima density}
                  [{\textbf{Greedy}\\\textbf{Search}}, draw]
                  [{Estimate $f^{\prime}(X)$}
                    [{\textbf{Gradient}\\\textbf{Descent}}, draw]]]
                [{Introduce a ``population''\\$\mathbf{X} = (X_1,\dots,X_n)$}
                  [{Combination, mutation,\\within $\mathbf{X}$}
                    [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]]
                  [{\textbf{Ant}\\\textbf{Colony}}, draw]]]
              [{Weaker\\$corr(f(X),d(X,X_{*}))$}
                [{Accept\\worst $f(X)$}
                  [{\textbf{Simulated}\\\textbf{Annealing}}, draw]]
                [{Avoid\\recent $X$}
                  [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]
          [{\textbf{Random}\\\textbf{Sampling}}, draw]]
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$}
          [{Parametric\\Learning}
            [{$\hat{f}(X) \approx f_1(X_1) + \dots + f_k(X_k)$}
              [{\textbf{Independent}\\\textbf{Bandit}}, draw]]
            [{$\hat{f}(X) = \mathcal{B}(logit(\mathcal{M}(X)\theta(X) + \varepsilon))$}
              [{\textbf{Logistic}\\\textbf{Regression}}, draw]]
            [{$\hat{f}(X) = \mathcal{M}(X)\theta(X) + \varepsilon$}
              [{\textbf{Linear}\\\textbf{Regression}}, draw]
              [{Measure\\properties of $X$}
                [{Independance\\of effects}
                  [{\textbf{Screening}}, draw]]
                [{Homoscedasticity of $\varepsilon$}
                  [{\textbf{Optimal}\\\textbf{Design}}, draw]]]]]
          [{Nonparametric\\Learning}
            [{Splitting\\rules on $X$}
              [{\textbf{Decision}\\\textbf{Trees}}, draw]]
            [{$\hat{f} = \mathcal{GP}(X; \mathcal{K})$}
              [{\textbf{Gaussian}\\\textbf{Process Regression}}, draw]]
            [{\textbf{Neural}\\\textbf{Networks}}, draw]
            [{\textbf{Multi-armed}\\\textbf{Bandit (?)}}, draw]]]]
    \end{forest}
  }
  \caption{Some hypothesis of some autotuning methods}
\end{sidewaysfigure}

#+end_export

#+begin_export latex
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\begin{table}[ht]
  \center
  \begin{tabular}{@{}p{0.3\textwidth}p{0.5\textwidth}@{}}
    \toprule
    Method &  Hypotheses \\ \midrule
    Metaheuristics & \tabitem There are similarities between natural fenomena and the target problem \\
    & \tabitem Gradual changes in configurations produce gradual changes in performance \\
    & \tabitem The optimal configuration is ``reachable'', by small changes, from non-optimal configurations  \\
    \addlinespace \\
    Machine Learning & \tabitem As more samples are obtained, decreases in ``out-of-sample error'' imply decreases ``in-sample error'' \\
    & \tabitem \textbf{TODO} What are the classes of models? \\
    \addlinespace \\
    Design of Experiments & \tabitem There is ``exploitable search space structure''\\
    & \tabitem Linear model: Response $\bm{Y}$ is an ``unobservable function'' of parameters $\bm{X}$: \\
    & \hspace{0.15\textwidth} $f(\bm{X}) = \bm{Y} = \bm{X\beta} + \bm{\varepsilon}$ \\
    & \tabitem Optimal Design: Variance of estimator $\hat{\bm{\beta}}$ is proportional to $\bm{X}$: \\
    & \hspace{0.15\textwidth} $\bm{\hat{\beta}} = \left(\bm{X}^{\intercal}\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{Y}$ \\
    \addlinespace \\
    Gaussian Process Regression & \tabitem Response $\bm{Y}$ is a sample from a multidimensional Gaussian distribution, with mean $m(\bf{X})$ and variance $k(\bm{X}, \bm{X}^{\intercal})$: \\
    & \hspace{0.1\textwidth} $\bm{Y} = f(\bm{X}) \sim \mathcal{N}(m(\bm{X}), k(\bm{X}, \bm{X}^{\intercal}))$ \\
    & \tabitem Predictions $\bm{Y_{*}}$ can be made conditioning distribution to observed data\\ \bottomrule
  \end{tabular}%
\end{table}
#+end_export

#+begin_export latex
\resizebox{!}{\textheight}{%
  \begin{tikzpicture}[rotate = -90]
    \begin{scope}
      \tikzset{every tree node/.style = {align = center}}
      \tikzset{level 1+/.style={level distance = 40pt}}
      \Tree [.\node(n0){Minimize $f: X \mapsto \mathbb{R}$ \\ $f(X) = f^{*}(X) + \varepsilon = m$};
        [.{Does not construct \\ estimate $\hat{f}(X; \theta)$}
          [.{Reachability of \\ optima}
            [.{\textbf{Greedy} \\ \textbf{Search}} ]
            [.{$d(x_i, x_j) \to 0$ $\implies$ \\ $d(f(x_i), f(x_j)) \to 0$}
              [.{Abundance of \\ local optima}
                [.{\textbf{Simulated} \\ \textbf{Annealing}} ]]
              [.{Closeness of a \\ ``population'' of $X$}
                [.{\textbf{Genetic} \\ \textbf{Algorithms}} ]]]]
          [.{\textbf{Random} \\ \textbf{Sampling}} ] ]
        [.\node(r1){Constructs surrogate \\ estimate $\hat{f}(X; \theta)$};
          [.{Explicit, variable \\ models of $\theta$}
            [.{$\hat{f} = M(X)\theta + \varepsilon$}
              [.{Independance \\ of effects}
                [.{\textbf{Screening}} ] ]
              [.{Homoscedasticity}
                [.{\textbf{Optimal} \\ \textbf{Design}} ] ] ] ]
          [.{Implicit, fixed \\ models of $\theta$}
            [.{\textbf{Neural Networks}} ] ]
          [.{Samples \\ functions}
            [.{$\hat{f} = \mathcal{GP}(X; \theta, \mathcal{K})$}
              [.{\textbf{Gaussian Process} \\ \textbf{Regression}} ] ] ] ] ]
    \end{scope}
    % \begin{scope}[thick]
    %   \draw [color = orange] (n0) to [bend left = 2] (r1);
    %   \draw [color = green] (n0) to [bend right = 2] (r1);
    % \end{scope}
  \end{tikzpicture}
}
#+end_export

*** [2020-02-18 Tue]
**** Functions in R
#+begin_SRC R :results output :session *R*
my_function <- function(x, y, z) {
  x + y
  paste(z, collapse = "", sep = "")
}

my_function(2, 3, c("a", "v", "c"))
#+end_SRC

#+RESULTS:
:
: [1] "avc"
**** PARCO Review                                            :PaperReview:
- Title: A Comparative Study of Parallel and Serial Implementations of Content-Based Filtering
***** Review
****** Summary
The paper  presents a shared  memory and a  message passing implementation  of a
recommender system, and  evaluates the performance of  these two implementations
on a data set produced for the study.
****** Writing
- There is no need to use the he/she formula, use the neutral singular "they".
****** Experimental Validation
The results  shown in Figures  17 to 19 compare  the performance of  the serial,
shared memory and message passing implementations, but experimental settings are
not clearly stated.  Without the  information of how many experiment repetitions
were performed, and what was the standard deviation of the samples collected, it
is  harder  to support  the  conclusions  of  the  paper, especially  since  the
variations on performance  that is shown in the Figures  show large amplitude. A
major  revision  of  this  paper  would involve  running  more  experiments  and
analyzing the mean and standard deviation of the samples.
****** Recommendation
I recommend this  paper to be rejected,  since its contributions seem  to be the
performance evaluation of  well known and studied  parallelization libraries, in
the  context of  an existing  recommender  system.  The  data sets  used in  the
experiments are also limited  in scope, and it would be  useful for the analyses
presented  here to  use  data sets  from  real scenarios.   The  paper could  be
resubmitted to this paper after a statistically sound evaluation and analysis of
the  performance of  more parallel  recommender systems  in more  representative
scenarios.
*** [2020-02-27 Thu]
**** Reproducible Science for SBC
Write a small  manifesto for reproducible science, in  portuguese, for brazilian
CS conferences.

- https://github.com/ReScience
- https://www.nature.com/articles/s41562-016-0021
- https://www.semanticscholar.org/paper/A-manifesto-for-reproducible-science-Munaf%C3%B2-Nosek/a68ce412e92d87ca0116519651bbc484d98c76ae
- https://rescience.github.io/faq/
- https://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2012.pdf

***** Manifesto pela Reprodutibilidade da Ciência da Computação no Brasil
Conclusões produzidas a partir de  dados obtidos experimentalmente não podem ser
consideradas  validadas  até  que   seja  possível  reproduzi-las  em  condições
experimentais independentes. Esse princípio  orienta todo o progresso científico
baseado em  metodologias experimentais.  A  pesquisa experimental em  Ciência da
Computação está em posição singular para reforçar e promover a reprodutibilidade
científica,  pois experimentos  computacionais em  determinados casos  podem ser
acompanhados,  registrados, e  repetidos  com precisão  e controle  praticamente
impossíveis em  áreas como a  biologia e a química.  As organizações em  prol da
ciência brasileira têm portanto grandes  justificativas para promover e reforçar
a reprodutibilidade científica.

Os  [[https://www.acm.org/publications/policies/artifact-review-badging][esforços da  ACM]] são  um bom  exemplo dos  esforços iniciais  que podem  ser
realizados em prol da reprodutibilidade.   Diversas conferências e periódicos da
ACM adotam  um sistema de  insígnias para  marcar trabalhos cujos  esforços para
reprodutibilidade  de  seus  experimentos são  significativos.   A  nomenclatura
utilizada pela ACM é derivada  do [[https://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2012.pdf][Vocabulário Internacional de Metrologia]] (VIM),
e distingue entre resultados e conclusões que podem ser reproduzidos:

- Pela mesma equipe, nas mesmas condições experimentais (Repetibilidade)
- Por uma equipe diferente, nas mesmas condições experimentais (Replicabilidade)
- Por   uma   equipe   diferente,    em   condições   experimentais   diferentes
  (Reprodutibilidade)

O código e os dados que dão  suporte às conclusões de um estudo científico devem
ser  submetidos junto  ao documento  que  será publicado.   Esses /artefatos/  são
avaliados pelos  revisores e insígnias são  conferidas de acordo com  o nível de
reprodutibilidade   alcançado.    Outras    organizações   também   promovem   a
reprodutibilidade, como a  [[https://rescience.github.io/faq/][ReScience]], que recentemente promoveu o  [[https://rescience.github.io/ten-years/][Desafio de 10
Anos  da Reprodutibilidade]],  onde  pesquisadores foram  incentivados a  submeter
artigos com  a reprodução de  seus próprios resultados de  no mínimo 10  anos de
idade.

Ações  como as  tomadas pela  ACM  podem ter  um  grande impacto  no reforço  da
credibilidade  do método  científico e  no  avanço da  descoberta científica  no
Brasil.
** March
*** [2020-03-11 Wed]
**** Review for ICS2020
***** Summary
The  paper presents  an  interesting study  of the  performance  of a  redundant
operation detection tool,  CIDetector.  The tool is used to  identify regions in
the  machine code  generated by  a compiler  that contain  redundant operations.
These regions  are then modified to  remove redundancies, and the  percentage of
redundancy  reduction and  the resulting  speedups are  reported.  The  paper is
clearly structured and easy to follow.

The  paper  evaluates   the  detection  tool  on  14   programs,  composing  the
CIBenchmark, which  is also introduced by  the paper. CIDetector is  tested on 3
GCC  versions,  1 ICC,  and  1  LLVM versions,  by  measuring  the reduction  on
redundant operations,  caused by  changes in regions  detected to  produce these
kinds  of redundancies.   In  some scenarios,  eliminating redundant  operations
produces execution time speedups.

Strangely, the paper does not provide access  to any of its source code or data,
but mentions that the code will  be open-sourced provided the paper is accepted.
On top of being  a strange practice, this means I was  not able to independently
verify or validate any of the data or the code presented in this paper.

The paper does not discuss whether the  results reported are a mean of a certain
number of  executions, and no standard  deviation or confidence interval  of the
mean  is  presented.   It  is  strongly  suggested  that  these  statistics  and
discussions are added to the paper, in order to strengthen its conclusions.

Overall,  the  paper  presents  valuable insights  and  careful  evaluation  and
discussion of the results. I believe this  to be a borderline paper, which could
be accepted provided  the statistical analysis methodology  is clearly presented
and discussed. I also believe it would be interesting to compare the performance
of the code generated by different compilers.
*** [2020-03-13 Fri]
**** First results from Emanuel's work
***** Cloning the Git Repository                               :noexport:
#+begin_SRC shell :results output :session *Shell*
git clone git@github.com:phrb/matrix-multiply-test.git || (cd matrix-multiply-test && git pull)
#+end_SRC

#+RESULTS:
: git clone git@github.com:phrb/matrix-multiply-test.git || (cd matrix-multiply-test && git p<test.git || (cd matrix-multiply-test && git pu                                               <test.git || (cd matrix-multiply-test && git pull)<test.git || [33m([39m[32mc[32md[39m matrix-multiply-test && [32mg[32mi[32mt[39m pull[33m)[39m[?2004l
: fatal: destination path 'matrix-multiply-test' already exists and is not an empty directory.
: Already up to date.
***** Boxplots of Selected Flags
#+begin_SRC R :results graphics output :session *R* :file "/tmp/heap_vec_nolib_30.pdf" :width 20 :height 7 :eval no-export
library(ggplot2)

df <- read.csv("matrix-multiply-test/results/results.csv", header = TRUE)

ggplot(df, aes(x = as.factor(id), y = execution_time)) +
  geom_jitter(alpha = 0.6, size = 4, height = 0, width = 0.2) +
  ylab("Execution Time (s)") +
  theme_bw(base_size = 38) +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(size = 25))
#+end_SRC

#+RESULTS:
[[file:/tmp/heap_vec_nolib_30.pdf]]
***** LLVM Command to Spit Flags
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
llvm-as < /dev/null | opt -O2 -disable-output -debug-pass=Arguments
#+end_SRC

#+RESULTS:
: [32ml[32ml[32mv[32mm[32m-[32ma[32ms[39m < /dev/null | [32mo[32mp[32mt[39m -O2 -disable-output -debug-pass=Arguments[?2004l
: Pass Arguments:  -tti -tbaa -scoped-noalias -assumption-cache-tracker -targetlibinfo -verify -ee-instrument -simplifycfg -domtree -sroa -early-cse -lower-expect
: Pass Arguments:  -targetlibinfo -tti -tbaa -scoped-noalias -assumption-cache-tracker -profile-summary-info -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor -globalopt -domtree -mem2reg -deadargelim -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -simplifycfg -basiccg -globals-aa -prune-eh -inline -functionattrs -domtree -sroa -basicaa -aa -memoryssa -early-cse-memssa -speculative-execution -basicaa -aa -lazy-value-info -jump-threading -correlated-propagation -simplifycfg -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -libcalls-shrinkwrap -loops -branch-prob -block-freq -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -pgo-memop-opt -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -tailcallelim -simplifycfg -reassociate -domtree -loops -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -loop-rotate -licm -loop-unswitch -simplifycfg -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -loop-simplify -lcssa-verification -lcssa -scalar-evolution -indvars -loop-idiom -loop-deletion -loop-unroll -mldst-motion -phi-values -basicaa -aa -memdep -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -gvn -phi-values -basicaa -aa -memdep -memcpyopt -sccp -demanded-bits -bdce -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -lazy-value-info -jump-threading -correlated-propagation -basicaa -aa -phi-values -memdep -dse -loops -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -licm -postdomtree -adce -simplifycfg -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -barrier -elim-avail-extern -basiccg -rpo-functionattrs -globalopt -globaldce -basiccg -globals-aa -float2int -domtree -loops -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -loop-rotate -loop-accesses -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -loop-distribute -branch-prob -block-freq -scalar-evolution -basicaa -aa -loop-accesses -demanded-bits -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -loop-vectorize -loop-simplify -scalar-evolution -aa -loop-accesses -lazy-branch-prob -lazy-block-freq -loop-load-elim -basicaa -aa -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -simplifycfg -domtree -loops -scalar-evolution -basicaa -aa -demanded-bits -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -slp-vectorizer -opt-remark-emitter -instcombine -loop-simplify -lcssa-verification -lcssa -scalar-evolution -loop-unroll -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -loop-simplify -lcssa-verification -lcssa -scalar-evolution -licm -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -transform-warning -alignment-from-assumptions -strip-dead-prototypes -globaldce -constmerge -domtree -loops -branch-prob -block-freq -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -block-freq -loop-sink -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instsimplify -div-rem-pairs -simplifycfg -verify
: Pass Arguments:  -domtree
: Pass Arguments:  -targetlibinfo -domtree -loops -branch-prob -block-freq
: Pass Arguments:  -targetlibinfo -domtree -loops -branch-prob -block-freq
*** [2020-03-16 Mon]
**** Tests
#+begin_SRC R :results output :session *R* :eval no-export :exports results
print("test")
1 + 2
a <- 3
a
#+end_SRC

#+RESULTS:
: [1] "test"
:
: [1] 3
:
: [1] 3

#+begin_SRC julia :eval no-export :exports results
println("test")
1 + 3
#+end_SRC

#+RESULTS:
:RESULTS:
: test
: 4
:END:
