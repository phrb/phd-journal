# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro H R Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Setup
** Julia
#+NAME: install_julia_deps
#+HEADER: :results output :session *julia*
#+BEGIN_SRC julia
Pkg.add("Plots")
Pkg.add("Lint")
Pkg.add("Gadfly")
Pkg.add("ProfileView")
Pkg.add("CSV")
Pkg.add("StatsBase")
Pkg.add("StatsModels")
Pkg.add("GLM")
Pkg.add("RDatasets")
Pkg.add("IterTools")
Pkg.add("Missings")
Pkg.add("RCall")
Pkg.add("DataFrames")
#+END_SRC

#+RESULTS: install_julia_deps
#+begin_example
INFO: Package Plots is already installed
INFO: Package Lint is already installed
INFO: Package Gadfly is already installed
INFO: Cloning cache of Gtk from https://github.com/JuliaGraphics/Gtk.jl.git
INFO: Cloning cache of GtkReactive from https://github.com/JuliaGizmos/GtkReactive.jl.git
INFO: Cloning cache of IntervalSets from https://github.com/JuliaMath/IntervalSets.jl.git
INFO: Cloning cache of ProfileView from https://github.com/timholy/ProfileView.jl.git
INFO: Cloning cache of Reactive from https://github.com/JuliaGizmos/Reactive.jl.git
INFO: Cloning cache of RoundingIntegers from https://github.com/JuliaMath/RoundingIntegers.jl.git
INFO: Installing Gtk v0.13.1
INFO: Installing GtkReactive v0.4.0
INFO: Installing IntervalSets v0.1.1
INFO: Installing ProfileView v0.3.0
INFO: Installing Reactive v0.6.0
INFO: Installing RoundingIntegers v0.0.3
INFO: Building Cairo
INFO: Building Gtk
INFO: Package database updated
INFO: Package CSV is already installed
INFO: Package StatsBase is already installed
INFO: Package StatsModels is already installed
INFO: Package GLM is already installed
INFO: Package RDatasets is already installed
#+end_example

#+NAME: update_julia_pkg
#+HEADER:  :results output :session *julia*
#+BEGIN_SRC julia
Pkg.update()
#+END_SRC

#+RESULTS: update_julia_pkg
: INFO: Updating METADATA...
: WARNING: Package ASTInterpreter: skipping update (dirty)...
: INFO: Updating Gallium master...
: INFO: Computing changes...
: INFO: No packages to install, update or remove

*** NODAL Development                                          :Code:NODAL:
**** Installing NODAL in Julia Nightly
[[https://github.com/phrb/NODAL.jl][NODAL]] is the autotuning library I am developing in the [[https://julialang.org][Julia]]
language. The idea is to provide tools for the implementation of
parallel and distributed autotuners for various problem domains.
***** Download Julia Nightly
****** [[https://julialang.org/downloads][Download Generic Binary]]
****** Downloading from the CLI
You can run the following to install the latest *Julia* version:
#+BEGIN_SRC bash
cd ~ && mkdir .bin && cd .bin
wget https://julialangnightlies-s3.julialang.org/bin/linux/x64/julia-latest-linux64.tar.gz
tar xvf julia-latest-linux64.tar.gz
mv julia-* julia
rm julia-latest-linux64.tar.gz
#+END_SRC
This will put the *Julia* binary at =~/.bin/julia/bin/julia=.
You can use it like that or add an =alias= to your shell.
***** Installing the unregistered version
This will not be needed after registering NODAL to METADATA.
****** [[https://docs.julialang.org/en/latest/manual/packages/#Installing-Unregistered-Packages-1][Documentation]]
****** Julia Commands
#+BEGIN_SRC julia
Pkg.clone("https://github.com/phrb/NODAL.jl")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
***** Installing from the Julia package manager
****** Julia commands
#+BEGIN_SRC julia
Pkg.add("NODAL")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
**** Setting up a new Release
***** Using Attobot
[[https://github.com/attobot][Attobot]] integrates with *GitHub* to automatically register a new package
or a package version to *Julia*'s =METADATA= package repository.  Attobot
only needs a new *GitHub* release to work.
***** Using *Julia*'s =PkgDev=
Check the [[https://docs.julialang.org/en/latest/manual/packages/#Tagging-and-Publishing-Your-Package-1][documentation]] to learn how to register and publish user
packages to =METADATA=.
**** Development Workflow
The process of fixing an [[https://github.com/phrb/NODAL.jl/issues][issue]] or submitting a new
feature is:
0. Fork [[https://github.com/phrb/NODAL.jl][NODAL on GitHub]]

   You will need a GitHub account for this.

1. Make sure you have the latest version
   #+BEGIN_SRC bash
git checkout master
git fetch
   #+END_SRC

   New branches must be made from the =dev= branch:
   #+BEGIN_SRC bash
git checkout dev
   #+END_SRC
2. Checkout a new branch
   #+BEGIN_SRC bash
git checkout -b fix-or-feature
   #+END_SRC
3. Write code and commit to your new branch

   Make sure you write short and descriptive commit
   messages. Something similar to [[https://udacity.github.io/git-styleguide/][Udacity's guidelines]] is preferred
   but not strictly necessary.

4. Open a [[https://github.com/phrb/NODAL.jl/pulls][pull request]] to the =dev= bran

** R
Installing *R* dependencies:
#+NAME: install_r_deps
#+HEADER: :results output :exports both :session *R*
#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",
                 "plotly", "rPref", "pracma", "FrF2", "AlgDesign",
                 "quantreg"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

#+RESULTS: install_r_deps
#+begin_example
Installing packages into â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™
(as â€˜libâ€™ is unspecified)
trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/ggplot2_2.2.1.tar.gz'
Content type 'application/x-gzip' length 2213308 bytes (2.1 MB)
==================================================
downloaded 2.1 MB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/dplyr_0.7.4.tar.gz'
Content type 'application/x-gzip' length 808054 bytes (789 KB)
==================================================
downloaded 789 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/tidyr_0.8.0.tar.gz'
Content type 'application/x-gzip' length 377417 bytes (368 KB)
==================================================
downloaded 368 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rjson_0.2.18.tar.gz'
Content type 'application/x-gzip' length 99478 bytes (97 KB)
==================================================
downloaded 97 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/GGally_1.3.2.tar.gz'
Content type 'application/x-gzip' length 1031885 bytes (1007 KB)
==================================================
downloaded 1007 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/plotly_4.7.1.tar.gz'
Content type 'application/x-gzip' length 1034951 bytes (1010 KB)
==================================================
downloaded 1010 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/rPref_1.2.tar.gz'
Content type 'application/x-gzip' length 99297 bytes (96 KB)
==================================================
downloaded 96 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/pracma_2.1.4.tar.gz'
Content type 'application/x-gzip' length 382113 bytes (373 KB)
==================================================
downloaded 373 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/FrF2_1.7-2.tar.gz'
Content type 'application/x-gzip' length 282582 bytes (275 KB)
==================================================
downloaded 275 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/AlgDesign_1.1-7.3.tar.gz'
Content type 'application/x-gzip' length 514391 bytes (502 KB)
==================================================
downloaded 502 KB

trying URL 'https://mirror.ibcp.fr/pub/CRAN/src/contrib/quantreg_5.35.tar.gz'
Content type 'application/x-gzip' length 1640297 bytes (1.6 MB)
==================================================
downloaded 1.6 MB

,* installing *source* package â€˜ggplot2â€™ ...
,** package â€˜ggplot2â€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (ggplot2)
ERROR: failed to lock directory â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4â€™ for modifying
Try removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/00LOCK-dplyrâ€™
,* installing *source* package â€˜rjsonâ€™ ...
,** package â€˜rjsonâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c dump.cpp -o dump.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c parser.c -o parser.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c register.c -o register.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rjson.so dump.o parser.o register.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rjson/libs
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (rjson)
,* installing *source* package â€˜pracmaâ€™ ...
,** package â€˜pracmaâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** byte-compile and prepare package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (pracma)
,* installing *source* package â€˜FrF2â€™ ...
,** package â€˜FrF2â€™ successfully unpacked and MD5 sums checked
,** R
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** testing if installed package can be loaded
,* DONE (FrF2)
,* installing *source* package â€˜AlgDesignâ€™ ...
,** package â€˜AlgDesignâ€™ successfully unpacked and MD5 sums checked
,** libs
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c FederovOpt.c -o FederovOpt.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c OptBlock.c -o OptBlock.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c Utility.c -o Utility.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o AlgDesign.so FederovOpt.o OptBlock.o Utility.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/AlgDesign/libs
,** R
,** data
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (AlgDesign)
,* installing *source* package â€˜quantregâ€™ ...
,** package â€˜quantregâ€™ successfully unpacked and MD5 sums checked
,** libs
gfortran   -fpic  -g -O2  -c akj.f -o akj.o
gfortran   -fpic  -g -O2  -c boot.f -o boot.o
gfortran   -fpic  -g -O2  -c bound.f -o bound.o
gfortran   -fpic  -g -O2  -c boundc.f -o boundc.o
gfortran   -fpic  -g -O2  -c brute.f -o brute.o
gfortran   -fpic  -g -O2  -c chlfct.f -o chlfct.o
gfortran   -fpic  -g -O2  -c cholesky.f -o cholesky.o
gfortran   -fpic  -g -O2  -c combos.f -o combos.o
gfortran   -fpic  -g -O2  -c crq.f -o crq.o
gfortran   -fpic  -g -O2  -c crqfnb.f -o crqfnb.o
gfortran   -fpic  -g -O2  -c dsel05.f -o dsel05.o
gfortran   -fpic  -g -O2  -c etime.f -o etime.o
gfortran   -fpic  -g -O2  -c extract.f -o extract.o
gfortran   -fpic  -g -O2  -c idmin.f -o idmin.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c init.c -o init.o
gfortran   -fpic  -g -O2  -c iswap.f -o iswap.o
gfortran   -fpic  -g -O2  -c kuantile.f -o kuantile.o
gcc -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c mcmb.c -o mcmb.o
gfortran   -fpic  -g -O2  -c penalty.f -o penalty.o
gfortran   -fpic  -g -O2  -c powell.f -o powell.o
gfortran   -fpic  -g -O2  -c rls.f -o rls.o
gfortran   -fpic  -g -O2  -c rq0.f -o rq0.o
gfortran   -fpic  -g -O2  -c rq1.f -o rq1.o
gfortran   -fpic  -g -O2  -c rqbr.f -o rqbr.o
gfortran   -fpic  -g -O2  -c rqfn.f -o rqfn.o
gfortran   -fpic  -g -O2  -c rqfnb.f -o rqfnb.o
gfortran   -fpic  -g -O2  -c rqfnc.f -o rqfnc.o
gfortran   -fpic  -g -O2  -c rqs.f -o rqs.o
gfortran   -fpic  -g -O2  -c sparskit2.f -o sparskit2.o
gfortran   -fpic  -g -O2  -c srqfn.f -o srqfn.o
gfortran   -fpic  -g -O2  -c srqfnc.f -o srqfnc.o
gfortran   -fpic  -g -O2  -c srtpai.f -o srtpai.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o quantreg.so akj.o boot.o bound.o boundc.o brute.o chlfct.o cholesky.o combos.o crq.o crqfnb.o dsel05.o etime.o extract.o idmin.o init.o iswap.o kuantile.o mcmb.o penalty.o powell.o rls.o rq0.o rq1.o rqbr.o rqfn.o rqfnb.o rqfnc.o rqs.o sparskit2.o srqfn.o srqfnc.o srtpai.o -llapack -lblas -lgfortran -lm -lquadmath -lgfortran -lm -lquadmath -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/quantreg/libs
,** R
,** data
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (quantreg)
,* installing *source* package â€˜tidyrâ€™ ...
,** package â€˜tidyrâ€™ successfully unpacked and MD5 sums checked
,** libs
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c RcppExports.cpp -o RcppExports.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c fill.cpp -o fill.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c melt.cpp -o melt.o
g++  -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt  -c simplifyPieces.cpp -o simplifyPieces.o
g++ -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o tidyr.so RcppExports.o fill.o melt.o simplifyPieces.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/tidyr/libs
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,*** copying figures
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (tidyr)
,* installing *source* package â€˜GGallyâ€™ ...
,** package â€˜GGallyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** inst
,** preparing package for lazy loading
,** help
,*** installing help indices
,** building package indices
,** installing vignettes
,** testing if installed package can be loaded
,* DONE (GGally)
,* installing *source* package â€˜rPrefâ€™ ...
,** package â€˜rPrefâ€™ successfully unpacked and MD5 sums checked
,** libs
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c RcppExports.cpp -o RcppExports.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c bnl.cpp -o bnl.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c hasse.cpp -o hasse.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c pref-classes.cpp -o pref-classes.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par-top.cpp -o psel-par-top.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c psel-par.cpp -o psel-par.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c scalagon.cpp -o scalagon.o
g++ -std=gnu++11 -I/usr/include/R/ -DNDEBUG  -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include" -I"/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/RcppParallel/include" -D_FORTIFY_SOURCE=2   -fpic  -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt -c topk_setting.cpp -o topk_setting.o
g++ -std=gnu++11 -shared -L/usr/lib64/R/lib -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o rPref.so RcppExports.o bnl.o hasse.o pref-classes.o psel-par-top.o psel-par.o scalagon.o topk_setting.o -L/usr/lib64/R/lib -lR
installing to /home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPref/libs
,** R
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜rPrefâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/rPrefâ€™
,* installing *source* package â€˜plotlyâ€™ ...
,** package â€˜plotlyâ€™ successfully unpacked and MD5 sums checked
,** R
,** data
,*** moving datasets to lazyload DB
,** demo
,** inst
,** preparing package for lazy loading
Warning: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.data.frame.tbl_dfâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_data_frame.grouped_dfâ€™, â€˜as_data_frame.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€ [... truncated]
Error in library.dynam(lib, package, package.lib) :
  shared object â€˜dplyr.soâ€™ not found
ERROR: lazy loading failed for package â€˜plotlyâ€™
,* removing â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™
,* restoring previous â€˜/home/phrb/R/x86_64-pc-linux-gnu-library/3.4/plotlyâ€™

The downloaded source packages are in
	â€˜/tmp/RtmpivSTVC/downloaded_packagesâ€™
Warning messages:
1: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜dplyrâ€™ had non-zero exit status
2: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜rPrefâ€™ had non-zero exit status
3: In install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",  :
  installation of package â€˜plotlyâ€™ had non-zero exit status
#+end_example

** Modifying & Analysing the FPGA Data Set
Cloning and updating the =legup-tuner= repository:

#+NAME: update_legup_tuner
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/legup-tuner.git || (cd legup-tuner && git pull)
#+END_SRC

Export your path to =repository_dir= variable:

#+name: repository_dir
#+begin_src sh :results output :exports both
pwd | tr -d "\n"
#+end_src

** Updating & Cloning Repositories
*** GPU Autotuning Screening Experiment
#+NAME: update_screening_experiment
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/autotuning_screening_experiment.git || (cd autotuning_screening_experiment && git pull)
#+END_SRC
* Bibliography
* 2018
** May
*** [2018-05-02 Wed]
**** Summarizing the D-Optimal + ANOVA Strategy for Steven's Experiments
1. Use ~optFederov~ to find 24 experiments for the full model:

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        vector_length + lws_y + 1 / lws_y +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

2. Use ~aov~ to fit the full model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       vector_length + lws_y + 1 / lws_y +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}

4. Identify the most significant factors from the ANOVA summary. In this
   case, they are $vector_length$ and $lws_y$.
5. Use the fitted model to predict the best $time_per_pixel$ value in the
   entire dataset
6. Prune the dataset using the predicted best values for $vector_length$ and $lws_y$
7. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
   than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ y_component_number + 1 / y_component_number +
        load_overlap + temporary_size +
        elements_number + 1 / elements_number +
        threads_number + 1 / threads_number
\end{equation}

8. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ y_component_number + 1 / y_component_number +
                       load_overlap + temporary_size +
                       elements_number + 1 / elements_number +
                       threads_number + 1 / threads_number
\end{equation}
9. Identify the most significant factors from the ANOVA summary. In this
   case, they are $y_component_number$ and $threads_number$.
10. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
11. Prune the dataset using the predicted best values for $y_component_number$ and
    $threads_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size +
        elements_number + 1 / elements_number
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size +
                       elements_number + 1 / elements_number
\end{equation}
14. Identify the most significant factors from the ANOVA summary. In this
    case, it is $elements_number$
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Prune the dataset using the predicted best values for $elements_number$
12. Use ~optFederov~ to find 24 experiments for the pruned model. If there are less
    than or exactly 24 candidates, use the full candidate set.

\begin{equation}
    Y ~ load_overlap + temporary_size
\end{equation}

13. Use ~aov~ to fit the pruned model, spending the 24 evaluations:

\begin{equation}
      time_per_pixel ~ load_overlap + temporary_size
\end{equation}
15. Use the fitted model to predict the best $time_per_pixel$ value in the
    entire dataset
16. Compare the predicted $time_per_pixel$ with the global optimum
*** [2018-05-03 Thu]
**** Summarizing Experiments
Make sure you have the data:

#+NAME: update_dopt_aov_experiments
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/dopt_anova_experiments.git || (cd dopt_anova_experiments && git pull)
#+END_SRC

#+RESULTS: update_dopt_aov_experiments
: Already up to date.

This [[file:./dopt_anova_experiments/org/report.pdf][file]] contains the report.
*** [2018-05-04 Fri]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)

A = data.frame(x = 1:100,
               y = 1:100,
               z = 1:100)

A$Y = rnorm(n = 100, mean = A$x, sd = 0.4 * A$x)

big_model = lm(Y ~ x + y + z, data = A)
small_model = lm(Y ~ x, data = A)

# A_big_predict = cbind(A, predict(big_model, interval = "confidence"))
# A_small_predict = cbind(A, predict(small_model, interval = "confidence"))
#
# p_small_x <- ggplot(A_small_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_line(aes(x, fit), alpha = 0.8, color = "red1", size = 1)
#
# p_small_y <- ggplot(A_small_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_small_z <- ggplot(A_small_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_x <- ggplot(A_big_predict, aes(x, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(x, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_y <- ggplot(A_big_predict, aes(y, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(y, fit), alpha = 0.2, color = "red1", size = 4)
#
# p_big_z <- ggplot(A_big_predict, aes(z, Y)) +
#                     geom_point(color = "black", alpha = 1) +
#                     geom_point(aes(z, fit), alpha = 0.2, color = "red1", size = 4)

par(mfrow = c(2, 5))
plot(small_model, which = c(1, 2, 3, 4, 5))
plot(big_model, which = c(1, 2, 3, 4, 5))

# grid.arrange(p_small_x, p_small_y, p_small_z,
#              p_big_x, p_big_y, p_big_z, nrow = 2)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-3203rQF/figure3203Li2.png]]
*** [2018-05-07 Mon]
**** Looking at Model Fit with Heteroscedasticity
#+HEADER: :results output graphics :session *R* :exports both
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
library(ggplot2)
library(dplyr)
library(gridExtra)
library(AlgDesign)

read_file <- "dopt_anova_experiments/data/search_space.csv"

results <- read.csv(read_file, strip.white = T, header = T)

big_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                 vector_length + lws_y + I(1 / lws_y) +
                                 load_overlap + temporary_size +
                                 elements_number + I(1 / elements_number) +
                                 threads_number + I(1 / threads_number),
                data = results)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length,
                data = results)

bm_predict = data.frame(time_per_pixel = predict(big_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

sm_predict = data.frame(time_per_pixel = predict(small_model, results),
                        y_component_number = results$y_component_number,
                        vector_length = results$vector_length)

bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict),
                    c("time_per_pixel", "y_component_number",
                      "vector_length")]

sm_min = sm_min[1, ]

global_min = results[results$time_per_pixel == min(results$time_per_pixel),
                     c("time_per_pixel", "y_component_number",
                       "vector_length")]

ggplot(results) +
    aes(x = y_component_number, y = time_per_pixel) +
    theme_bw() +
    geom_point(alpha = 0.1) +
    theme(legend.position = "top") +
    geom_point(color= "green", data = bm_min, size = 3, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "red", data = sm_min, size = 4, alpha = 0.6,
               aes(x = y_component_number, y = time_per_pixel)) +
    geom_point(color = "blue", data = global_min, size = 5, alpha = 0.5,
               aes(x = y_component_number, y = time_per_pixel)) +
    theme(axis.text = element_text(size = 12),
          axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-27096ysq/figure27096DcA.png]]
*** [2018-05-09 Wed]
**** Plotting Predicted Values During Experiment
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 720 :height 1280
#+BEGIN_SRC R
library(AlgDesign)
library(dplyr)
library(ggplot2)
library(gridExtra)

generate_model_plot <- function(big_model, small_model, results, full_data, metric) {
    bm_predict = data.frame(response = predict(big_model, results),
                            variable = results[metric])

    names(bm_predict)[names(bm_predict) == "response"] <- "time_per_pixel"
    names(bm_predict)[names(bm_predict) == "variable"] <- metric

    sm_predict = data.frame(response = predict(small_model, results),
                            variable = results[metric])

    names(sm_predict)[names(sm_predict) == "response"] <- "time_per_pixel"
    names(sm_predict)[names(sm_predict) == "variable"] <- metric

    bm_min = bm_predict[bm_predict$time_per_pixel == min(bm_predict$time_per_pixel), ]

    sm_min = sm_predict[sm_predict$time_per_pixel == min(sm_predict$time_per_pixel), ]

    sm_min = sm_min[1, ]
    bm_min = bm_min[1, ]

    global_min = full_data[full_data$time_per_pixel == min(full_data$time_per_pixel),
                           c("time_per_pixel", metric)]

    p <- ggplot() +
         scale_shape_identity() +
         geom_point(data = full_data, alpha = 0.1,
                    aes(x = full_data[metric], y = time_per_pixel,
                        color = "Search Space")) +
         geom_point(data = bm_min, size = 3, alpha = 1.0,
                    aes(x = bm_min[metric], y = time_per_pixel,
                        color = "Big Model", shape = 7)) +
         geom_point(data = sm_min, size = 3, alpha = 1.0,
                    aes(x = sm_min[metric], y = time_per_pixel,
                        color = "Small Model", shape = 8)) +
         geom_point(data = global_min, size = 3, alpha = 1.0,
                    aes(x = global_min[metric], y = time_per_pixel,
                        color = "Global Minimum", shape = 9)) +
         theme_bw() +
         theme(axis.text = element_text(size = 12),
               axis.title = element_text(size = 14, face = "bold"),
               legend.position = "top") +
         labs(y = "time_per_pixel", x = metric) +
         scale_color_manual(values = c("green", "blue", "black", "red"))

    return(p)
}

complete_data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

budget <- 120

factors = c("elements_number", "y_component_number",
            "vector_length", "temporary_size",
            "load_overlap", "threads_number",
            "lws_y")

used <- 0

data <- complete_data[, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

# Comment/Uncomment to toggle scaling

# scaled_data <- cbind(scale(select_if(data, is.numeric), center = FALSE, scale = TRUE),
#                      select_if(data, Negate(is.numeric)))
# scaled_data <- scaled_data[, names(data)]

# We are able to use the full set in this case
# sampled_data <- scaled_data[sample(nrow(data), 500), ]

# Complete model:
output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      scaled_data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

# Complete model:
regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   vector_length + lws_y + I(1 / lws_y) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                  data = federov_design)

p_vectorlength <- generate_model_plot(regression, small_model,
                                      scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

p_lwsy <- generate_model_plot(regression, small_model,
                              scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ vector_length + lws_y + I(1 / lws_y),
                   data = random_data)


r_lwsy <- generate_model_plot(big_random, small_random,
                              random_data, complete_data[ , c(factors, "time_per_pixel")],
                              "lws_y")

r_vectorlength <- generate_model_plot(big_random, small_random,
                                      random_data, complete_data[ , c(factors, "time_per_pixel")],
                                      "vector_length")

used <- used + nrow(federov_design)

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'vector_length' and 'lws_y'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 18) {
    output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                           load_overlap + temporary_size +
                           elements_number + I(1 / elements_number) +
                           threads_number + I(1 / threads_number),
                         scaled_data,
                         nTrials = 18)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   load_overlap + temporary_size +
                                   elements_number + I(1 / elements_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                   threads_number + I(1 / threads_number),
                  data = federov_design)

p_ycomponentnumber <- generate_model_plot(regression, small_model,
                                          scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

p_threadsnumber <- generate_model_plot(regression, small_model,
                                       scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    threads_number + I(1 / threads_number),
                   data = random_data)


r_ycomponentnumber <- generate_model_plot(big_random, small_random,
                                          random_data, complete_data[ , c(factors, "time_per_pixel")],
                                          "y_component_number")

r_threadsnumber <- generate_model_plot(big_random, small_random,
                                       random_data, complete_data[ , c(factors, "time_per_pixel")],
                                       "threads_number")

# Checking the ANOVA summary we can identify at least two variables
# that seem to have greater impact: 'y_component_number' and 'threads_number'.
# Let's fix those variables to their best predicted value so far,
# then fit a new model without them

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 10) {
    output <- optFederov(~ load_overlap + temporary_size +
                            elements_number + I(1 / elements_number),
                          scaled_data,
                          nTrials = 10)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number),
                  data = federov_design)

small_model <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                  data = federov_design)

p_elementsnumber <- generate_model_plot(regression, small_model,
                                        scaled_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

random_data <- complete_data[sample(nrow(complete_data), nrow(federov_design)), c(factors, "time_per_pixel")]

big_random <- lm(time_per_pixel ~ load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number),
                 data = random_data)

small_random <- lm(time_per_pixel ~ elements_number + I(1 / elements_number),
                   data = random_data)

r_elementsnumber <- generate_model_plot(big_random, small_random,
                                        random_data, complete_data[ , c(factors, "time_per_pixel")],
                                        "elements_number")

# Checking the ANOVA summary we can identify, at last, one variable
# that seem to have greater impact: 'elements_number'
# Let's fix it to their best predicted value so far,
# then fit a new model without it

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

data <- complete_data[complete_data$vector_length == predicted_best$vector_length &
                      complete_data$lws_y == predicted_best$lws_y &
                      complete_data$y_component_number == predicted_best$y_component_number &
                      complete_data$threads_number == predicted_best$threads_number &
                      complete_data$elements_number == predicted_best$elements_number, c(factors, "time_per_pixel")]
scaled_data <- data[, factors]

if (nrow(scaled_data) > 6) {
    output <- optFederov(~ load_overlap + temporary_size,
                          scaled_data,
                          nTrials = 6)

    federov_design <- data[output$rows, ]
} else {
    federov_design <- data
}

used_rows <- rownames(federov_design)[!(rownames(federov_design) %in% experiments)]
used <- used + nrow(federov_design[used_rows, ])
experiments <- c(experiments, output$rows[!(output$rows %in% experiments)])

regression <- aov(time_per_pixel ~ load_overlap + temporary_size,
                  data = federov_design)

predicted_best <- data[predict(regression, data) == min(predict(regression, data)), ]

best <- complete_data[complete_data$time_per_pixel == min(complete_data$time_per_pixel), ]
best_row <- rownames(best)

predicted_best$slowdown <- predicted_best$time_per_pixel / best$time_per_pixel
predicted_best$method <- rep("DOPTaov", nrow(predicted_best))
predicted_best$point_number <- rep(used, nrow(predicted_best))
predicted_best$vector_recompute <- rep("true", nrow(predicted_best))

predicted_best <- predicted_best[, c("elements_number", "y_component_number",
                                    "vector_length", "temporary_size", "vector_recompute",
                                    "load_overlap", "threads_number", "lws_y",
                                    "time_per_pixel", "point_number", "method",
                                    "slowdown")]

grid.arrange(p_vectorlength + ggtitle("First Step: D-Opt + aov"), p_lwsy + ggtitle(" "),
             r_vectorlength + ggtitle("First Step: Random Selection + lm"), r_lwsy + ggtitle(" "),
             p_ycomponentnumber + ggtitle("Second Step: D-Opt + aov"), p_threadsnumber + ggtitle(" "),
             r_ycomponentnumber + ggtitle("Second Step: Random Selection + lm"), r_threadsnumber + ggtitle(" "),
             p_elementsnumber + ggtitle("Third Step: D-Opt + aov"),
             r_elementsnumber + ggtitle("Third Step: Random Selection + lm"), nrow = 5)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536ZkV.png]]
*** [2018-05-14 Mon]
**** Accuracy of the DOPT+AOV Process in Steven's Case
To verify the "accuracy" of the selected metrics, I adapted the experiment
scripts to check for each removed model variable in the actual =aov= summary.
Those initial choices seem to match in most cases with the variables identified
as most relevant by the =aov= summary, as shown below.

#+HEADER: :results graphics output :session *R* :exports results
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 600 :height 500
#+BEGIN_SRC R
library(ggplot2)

accuracies_file <- "dopt_anova_experiments/data/dopt_accuracies.csv"
results <- read.csv(accuracies_file, strip.white=T, header=T)

names(results) <- c("First", "Second", "Third")
parsed_results = data.frame(names(results), t(results[1, ]))
names(parsed_results) <- c("Steps", "Accuracy")

parsed_results

ggplot(data = parsed_results, aes(x = Steps, y = Accuracy)) +
geom_bar(stat = "identity", width = 0.5) +
#geom_hline(yintercept = 1.0, color = "red", linetype = 2) +
geom_text(aes(label = Accuracy), vjust = 1.6, color = "white", size = 5)+
theme_bw() +
theme(axis.text = element_text(size = 12),
      axis.title = element_text(size = 14, face = "bold"))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-6536hXY/figure6536MMn.png]]

As described previously, at each step a group of variables is removed from the
model based on their "score", that is, the "Pr(>F)" value in the =aov= summary.
I selected at most two variables at each of the three steps, based on preliminary
visual analysis of the =aov= summaries.

To measure how accurate those initial selections were I checked at each step if
the $n$ selected variables were in the $n$ most relevant variables in that
step's =aov= summary. If that was the case I incremented a step-specific
counter. The counters were updated for 1000 iterations and then divided by 1000.
This value represents the accuracy of the static selection in comparison with
the values that would be selected if each individual =aov= summary was analysed.
*** [2018-05-15 Tue]
**** Writing an LM Experiment Using a Big Model
This experiment is a modification of the ``DOPTaov'' experiment that adapts the
``LM'' strategy for fitting linear models to pruned search spaces. Instead of
using small models at each step the experiments starts with large models that
are pruned as meaningful variables are identified in the =aov= summary. The
experiment used the same variables from the ``DOPTaov'' experiement at each
step.

**** Trying to Mitigate Heteroscedasticity
Using some ideas from [[https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/][this website]].

***** For a Uniformly Sampled Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)

str(data)
data <- data[sample(1:nrow(data), 100, replace = FALSE), ]

regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                  vector_length + lws_y + I(1 / lws_y) +
                                  load_overlap + temporary_size +
                                  elements_number + I(1 / elements_number) +
                                  threads_number + I(1 / threads_number),
                data = data)

summary(regression)
ncvTest(regression)

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = data)

coef(boxcox_transform, round=TRUE)
transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$roundlam) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = data)

summary(transformed_regression)
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = data)

Residuals:
       Min         1Q     Median         3Q        Max
-6.687e-09 -1.046e-09 -1.469e-10  5.563e-10  8.666e-09

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -1.019e-09  1.829e-09  -0.557 0.578791
y_component_number       4.245e-10  3.114e-10   1.363 0.176303
I(1/y_component_number) -1.516e-09  1.774e-09  -0.854 0.395276
vector_length            1.481e-10  3.826e-11   3.869 0.000209 ***
lws_y                    1.257e-11  1.292e-12   9.733 1.27e-15 ***
I(1/lws_y)              -1.589e-09  6.900e-10  -2.303 0.023611 *
load_overlaptrue         5.742e-10  4.373e-10   1.313 0.192630
temporary_size          -2.444e-11  2.154e-10  -0.113 0.909941
elements_number         -1.293e-10  6.037e-11  -2.141 0.035025 *
I(1/elements_number)     3.721e-09  2.355e-09   1.580 0.117678
threads_number           3.000e-13  9.281e-13   0.323 0.747316
I(1/threads_number)      5.964e-08  2.745e-08   2.172 0.032511 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.02e-09 on 88 degrees of freedom
Multiple R-squared:  0.6835,	Adjusted R-squared:  0.6439
F-statistic: 17.27 on 11 and 88 DF,  p-value: < 2.2e-16
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 147.0183    Df = 1     p = 7.775729e-34
   Y1
-0.33

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$roundlam) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = data)

Residuals:
     Min       1Q   Median       3Q      Max
-1360.65  -458.00   -81.85   427.87  1346.29

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -2919.5704   583.3570  -5.005 2.84e-06 ***
y_component_number       -124.1127    99.3191  -1.250  0.21475
I(1/y_component_number)  -605.0674   565.7834  -1.069  0.28780
vector_length              97.8093    12.2025   8.016 4.31e-12 ***
lws_y                       3.3485     0.4119   8.129 2.53e-12 ***
I(1/lws_y)              -1041.1237   220.0439  -4.731 8.44e-06 ***
load_overlaptrue          -84.8944   139.4673  -0.609  0.54429
temporary_size            -31.2706    68.7013  -0.455  0.65011
elements_number           -16.0872    19.2519  -0.836  0.40563
I(1/elements_number)      702.7164   751.0018   0.936  0.35199
threads_number             -0.9935     0.2960  -3.357  0.00117 **
I(1/threads_number)     19520.4029  8754.4028   2.230  0.02831 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 644.1 on 88 degrees of freedom
Multiple R-squared:  0.708,	Adjusted R-squared:  0.6715
F-statistic:  19.4 on 11 and 88 DF,  p-value: < 2.2e-16
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.4176077    Df = 1     p = 0.5181332
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-1306ie-/figure1306yLi.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-1306ie-/figure1306Zq0.png]]

***** For a D-Optimal Design
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(AlgDesign)
library(car)
data = read.csv("dopt_anova_experiments/data/search_space.csv", header = TRUE)
str(data)

output <- optFederov(~ y_component_number + I(1 / y_component_number) +
                        vector_length + lws_y + I(1 / lws_y) +
                        load_overlap + temporary_size +
                        elements_number + I(1 / elements_number) +
                        threads_number + I(1 / threads_number),
                      data = data,
                      nTrials = 24)

federov_design <- data[output$rows, ]
experiments <- output$rows

str(federov_design)

# Complete model:
regression <- lm(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                                    vector_length + lws_y + I(1 / lws_y) +
                                    load_overlap + temporary_size +
                                    elements_number + I(1 / elements_number) +
                                    threads_number + I(1 / threads_number),
                  data = federov_design)

summary(regression)
summary.aov(regression)
ncvTest(regression)

data[predict(regression, data) == min(predict(regression, data)), ]

boxcox_transform <- powerTransform(time_per_pixel ~ y_component_number + I(1 / y_component_number) +
                            vector_length + lws_y + I(1 / lws_y) +
                            load_overlap + temporary_size +
                            elements_number + I(1 / elements_number) +
                            threads_number + I(1 / threads_number),
                        data = federov_design)

coef(boxcox_transform, round=TRUE)
transformed_regression <- lm(bcPower(time_per_pixel, boxcox_transform$roundlam) ~ y_component_number + I(1 / y_component_number) +
                                vector_length + lws_y + I(1 / lws_y) +
                                load_overlap + temporary_size +
                                elements_number + I(1 / elements_number) +
                                threads_number + I(1 / threads_number),
                             data = federov_design)

summary(transformed_regression)
summary.aov(transformed_regression)
ncvTest(transformed_regression)

data[predict(transformed_regression, data) == min(predict(transformed_regression, data)), ]
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: carData
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...
'data.frame':	24 obs. of  9 variables:
 $ elements_number   : int  1 4 2 3 1 4 4 2 1 4 ...
 $ y_component_number: int  1 1 2 3 1 1 1 2 1 1 ...
 $ vector_length     : int  1 16 16 16 1 1 1 1 1 16 ...
 $ temporary_size    : int  2 2 4 4 2 4 4 2 4 4 ...
 $ vector_recompute  : Factor w/ 1 level "true": 1 1 1 1 1 1 1 1 1 1 ...
 $ load_overlap      : Factor w/ 2 levels "false","true": 2 1 1 2 1 1 1 1 2 2 ...
 $ threads_number    : int  256 32 32 128 256 32 128 32 32 1024 ...
 $ lws_y             : int  1 1 1 128 32 1 64 32 32 16 ...
 $ time_per_pixel    : num  2.31e-10 7.75e-10 1.70e-09 2.79e-08 7.27e-10 ...

Call:
lm(formula = time_per_pixel ~ y_component_number + I(1/y_component_number) +
    vector_length + lws_y + I(1/lws_y) + load_overlap + temporary_size +
    elements_number + I(1/elements_number) + threads_number +
    I(1/threads_number), data = federov_design)

Residuals:
       Min         1Q     Median         3Q        Max
-8.775e-09 -4.543e-09 -1.968e-09  2.959e-09  1.856e-08

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)              1.084e-08  1.609e-08   0.674   0.5134
y_component_number      -1.401e-09  2.450e-09  -0.572   0.5778
I(1/y_component_number) -1.300e-08  1.447e-08  -0.898   0.3867
vector_length            5.744e-10  2.591e-10   2.216   0.0467 *
lws_y                    1.121e-11  6.242e-12   1.796   0.0977 .
I(1/lws_y)              -4.183e-09  4.583e-09  -0.913   0.3793
load_overlaptrue        -1.279e-09  3.862e-09  -0.331   0.7462
temporary_size          -2.925e-11  1.926e-09  -0.015   0.9881
elements_number         -4.281e-11  3.725e-10  -0.115   0.9104
I(1/elements_number)     1.238e-08  8.077e-09   1.532   0.1514
threads_number          -3.511e-12  7.437e-12  -0.472   0.6453
I(1/threads_number)     -6.241e-08  2.281e-07  -0.274   0.7890
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 9.262e-09 on 12 degrees of freedom
Multiple R-squared:  0.5646,	Adjusted R-squared:  0.1655
F-statistic: 1.415 on 11 and 12 DF,  p-value: 0.2797
                        Df    Sum Sq   Mean Sq F value Pr(>F)
y_component_number       1 6.980e-17 6.980e-17   0.814 0.3848
I(1/y_component_number)  1 2.000e-18 2.000e-18   0.024 0.8805
vector_length            1 3.875e-16 3.875e-16   4.517 0.0550 .
lws_y                    1 5.023e-16 5.023e-16   5.856 0.0323 *
I(1/lws_y)               1 6.970e-17 6.970e-17   0.812 0.3851
load_overlap             1 6.900e-18 6.900e-18   0.080 0.7818
temporary_size           1 3.200e-18 3.200e-18   0.037 0.8499
elements_number          1 5.620e-17 5.620e-17   0.655 0.4340
I(1/elements_number)     1 2.175e-16 2.175e-16   2.536 0.1372
threads_number           1 1.340e-17 1.340e-17   0.156 0.7001
I(1/threads_number)      1 6.400e-18 6.400e-18   0.075 0.7890
Residuals               12 1.029e-15 8.580e-17
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 14.05733    Df = 1     p = 0.0001773212
      elements_number y_component_number vector_length temporary_size
18584               4                  1             1              4
      vector_recompute load_overlap threads_number lws_y time_per_pixel
18584             true         true           1024     1    3.37552e-10
Y1
 0

Call:
lm(formula = bcPower(time_per_pixel, boxcox_transform$roundlam) ~
    y_component_number + I(1/y_component_number) + vector_length +
        lws_y + I(1/lws_y) + load_overlap + temporary_size +
        elements_number + I(1/elements_number) + threads_number +
        I(1/threads_number), data = federov_design)

Residuals:
     Min       1Q   Median       3Q      Max
-0.89985 -0.44415 -0.00551  0.42496  0.77873

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)             -2.050e+01  1.298e+00 -15.790 2.16e-09 ***
y_component_number      -2.236e-01  1.976e-01  -1.131 0.279986
I(1/y_component_number) -1.623e+00  1.168e+00  -1.389 0.189939
vector_length            1.129e-01  2.091e-02   5.401 0.000160 ***
lws_y                    2.404e-03  5.036e-04   4.775 0.000453 ***
I(1/lws_y)              -1.355e+00  3.697e-01  -3.664 0.003240 **
load_overlaptrue        -7.992e-02  3.116e-01  -0.257 0.801908
temporary_size           2.625e-01  1.554e-01   1.689 0.117043
elements_number          2.314e-02  3.005e-02   0.770 0.456153
I(1/elements_number)     1.718e+00  6.516e-01   2.636 0.021735 *
threads_number          -1.205e-03  6.000e-04  -2.009 0.067569 .
I(1/threads_number)      2.967e+00  1.840e+01   0.161 0.874607
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7472 on 12 degrees of freedom
Multiple R-squared:  0.8907,	Adjusted R-squared:  0.7906
F-statistic: 8.894 on 11 and 12 DF,  p-value: 0.0003456
                        Df Sum Sq Mean Sq F value   Pr(>F)
y_component_number       1  1.389   1.389   2.487 0.140751
I(1/y_component_number)  1  0.021   0.021   0.037 0.850581
vector_length            1 15.974  15.974  28.609 0.000174 ***
lws_y                    1 19.156  19.156  34.308 7.75e-05 ***
I(1/lws_y)               1  7.195   7.195  12.886 0.003715 **
load_overlap             1  0.110   0.110   0.198 0.664444
temporary_size           1  1.205   1.205   2.157 0.167606
elements_number          1  0.122   0.122   0.218 0.648767
I(1/elements_number)     1  4.369   4.369   7.824 0.016127 *
threads_number           1  5.071   5.071   9.083 0.010786 *
I(1/threads_number)      1  0.015   0.015   0.026 0.874607
Residuals               12  6.700   0.558
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Non-constant Variance Score Test
Variance formula: ~ fitted.values
Chisquare = 0.1956185    Df = 1     p = 0.658281
      elements_number y_component_number vector_length temporary_size
15927               4                  1             1              2
      vector_recompute load_overlap threads_number lws_y time_per_pixel
15927             true         true           1024     1   3.368082e-10
#+end_example

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-169411LY/figure169411rg.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(regression$model$time_per_pixel, breaks = 10)
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-15923bT/figure1592duD.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 600
#+BEGIN_SRC R
plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-169411LY/figure16941OUC.png]]

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
hist(transformed_regression$model["bcPower(time_per_pixel, boxcox_transform$roundlam)"][[1]])
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-169411LY/figure16941beI.png]]

*** [2018-05-16 Wed]
**** Power Transforms on Generated Heteroscedastic Data
#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
library(car)
set.seed(1234)
x <- 1:1000
y <- abs(rnorm(n = 1000, mean = x, sd = 0.9 * x))

data <- data.frame(x, y)
plot(lm(y ~ x, data = data), which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-2906328c/figure29063Dnr.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(lm(y ~ x, data = data))
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 365.3147    Df = 1     p = 1.960365e-81

#+HEADER: :results graphics output :session *R*
#+HEADER: :file (org-babel-temp-file "figure" ".png")
#+HEADER: :width 800 :height 720
#+BEGIN_SRC R
power_transform <- powerTransform(lm(y ~ x,  data = data))#, family = "bcnPower")
coef(power_transform, round = TRUE)
transformed_regression <- lm(bcPower(y, power_transform$roundlam) ~ x, data = data)
                                      #gamma = power_transform$gamma) ~ x, data = data)

plot(transformed_regression, which = c(1))
#+END_SRC

#+RESULTS:
[[file:/tmp/babel-2906328c/figure29063Qxx.png]]

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
ncvTest(transformed_regression)
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test
: Variance formula: ~ fitted.values
: Chisquare = 41.63823    Df = 1     p = 1.098246e-10

*** [2018-05-17 Thu]
**** Looking for Autotuning Benchmarks
Some candidates:
- Rodinia kernels (http://lava.cs.virginia.edu/Rodinia/download_links.htm)
- SPAPT (https://github.com/brnorris03/Orio/tree/master/testsuite/SPAPT)
- Dongarra's BEAST Project (http://icl.cs.utk.edu/beast/people/index.html)
*** [2018-05-22 Tue]
**** Running SPAPT Benchmarks
- Orio has a not very good documentation
- Compilation of simple examples takes +5min
**** Looking into LLVM and GCC flag autotuning
- CollectiveKnowledge has great flag space descriptions
- If we go back to compiler flag tuning we can tune
  any of the well-stablished HPC benchmarks
*** [2018-05-23 Wed]
**** Orio Setup Scripts
I've forked the Orio repository so I can freely change the code while keeping
version control.

To clone the most recent version, pick a path for the repository by editing the
source block below, using absolute paths.

#+NAME: setup_orio
#+HEADER: :results output
#+HEADER: :var ORIO_PATH="/home/phrb/code/orio"
#+BEGIN_SRC shell
git clone --depth=1 https://github.com/phrb/Orio.git $ORIO_PATH || echo "Orio already installed"
#+END_SRC

#+RESULTS: setup_orio
: Orio already installed

***** Lazy Python Databases with =Dataset=
With this it is possible to create a database from python scripts without much hassle:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
sudo pip install dataset
#+END_SRC

#+RESULTS:
: sudo pip install dataset
: Requirement already satisfied: dataset in /usr/lib/python3.6/site-packages

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import dataset
db = dataset.connect('sqlite:///mydatabase.db')
table = db['user']
table.insert(dict(name='John Doe', age=46, country='China'))
table.insert(dict(name='Jane Doe', age=37, country='France', gender='female'))
table.update(dict(name='John Doe', age=47), ['name'])
#+END_SRC

#+RESULTS:
: Python 3.6.5 (default, May 11 2018, 04:00:52)
: [GCC 8.1.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: python.el: native completion setup loaded

#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
print(db.tables)
print(db['user'].columns)
print(len(db['user']))
users = db['user'].all()
print(users)

db.commit()
#+END_SRC

#+RESULTS:
: ['user']
: ['id', 'name', 'age', 'country', 'gender']
: 2
: <dataset.util.ResultIter object at 0x7f7f34e974a8>

Cleaning up:

#+HEADER: :results output :session *Shell*
#+BEGIN_SRC shell
rm mydatabase.db
#+END_SRC

#+RESULTS:
: rm mydatabase.db
*** [2018-05-28 Mon]
**** Using AlgDesign & ANOVA from Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
from rpy2.robjects.packages import importr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
#+END_SRC

#+RESULTS:

*** [2018-05-29 Tue]
**** Implementing the D-Optimal Design + AOV Approach in Python
#+HEADER: :results output :session *Python*
#+BEGIN_SRC python
import math
from rpy2.robjects.packages import importr
from rpy2.robjects import IntVector, StrVector, Formula, r
from rpy2.robjects.lib.dplyr import dplyr

base      = importr("base")
utils     = importr("utils")
stats     = importr("stats")
algdesign = importr("AlgDesign")
car       = importr("car")

def opt_federov(design_formula, data, trials):
    output = algdesign.optFederov(Formula(design_formula),
                                  data,
                                  maxIteration = 1000,
                                  nTrials = trials)
    return output

def transform_design(design, lm_formula, full_model, response):
    transform = car.powerTransform(Formula(lm_formula),
                                   data = design)

    print("Power Transform Step:")
    print(transform)
    transformed_response = car.bcPower(design.rx(response[0]), transform.rx("lambda")[0])
    design               = base.cbind(design, transformed_response)
    transform_lm_formula = "{0}".format(base.names(transformed_response)[0]) + full_model

    return design, transform_lm_formula

def anova(design, formula):
    heteroscedasticity_test = car.ncvTest(stats.lm(Formula(formula), data = design))
    print("Heteroscedasticity Test p-value:")
    print(heteroscedasticity_test.rx("p")[0][0])

    if heteroscedasticity_test.rx("p")[0][0] < 0.05:
        transform_lm_formula = transform_design(design, formula,
                                                full_model, response)
        heteroscedasticity_test = car.ncvTest(stats.lm(Formula(transform_lm_formula),
                                                        data = design))
        print("Heteroscedasticity Test p-value:")
        print(heteroscedasticity_test.rx("p")[0][0])
    else:
        print("No need to power transform")
        transform_lm_formula = lm_formula

    regression = stats.lm(Formula(formula),
                          data = design)

    summary_regression = stats.summary_aov(regression)
    print("Regression Step:")
    print(summary_regression)

    prf_values = {}

    for k, v in zip(base.rownames(summary_regression[0]), summary_regression[0][4]):
        if k.strip() != "Residuals":
            prf_values[k.strip()] = v

    return regression, prf_values

def predict_best(regression, data):
    predicted = stats.predict(regression, data)
    predicted_best = predicted.index(min(predicted))

    p_min = min(predicted)
    i = 0

    for k in range(len(predicted)):
        if math.isclose(predicted[k], p_min, rel_tol = 1e-6):
            i += 1

    print("Identical predictions (tol = 1e-17): {0}".format(i))
    return data.rx(predicted_best, True)

def prune_data(data, fixed_variables):
    print(fixed_variables)
    pruned_data = data
    for k, v in fixed_variables.items():
        pruned_data = pruned_data.rx((pruned_data.rx2(str(k)).ro == str(v)),
                                     True)

    print("Dimensions of Pruned Data: " + str(base.dim(pruned_data)))
    print("Dimensions of Full Data: " + str(base.dim(data)))
    return pruned_data

def get_fixed_variables(predicted_best, ordered_prf_keys, fixed_factors, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    fixed_variables = fixed_factors
    for v in unique_variables:
        fixed_variables[v] = predicted_best.rx2(str(v))[0]

    print("Fixed Variables: " + str(fixed_variables))
    return fixed_variables

def prune_model(factors, inverse_factors, ordered_prf_keys, threshold = 2):
    variables = ordered_prf_keys
    variables = [v.strip("I)(/1 ") for v in variables]

    unique_variables = []

    for v in variables:
        if v not in unique_variables:
            unique_variables.append(v)
        if len(unique_variables) >= threshold:
            break

    pruned_factors = [f for f in factors if not f in unique_variables]
    pruned_inverse_factors = [f for f in inverse_factors if not f in unique_variables]

    return pruned_factors, pruned_inverse_factors

def dopt_anova_step(response, factors, inverse_factors, data, fixed_factors, budget):
    full_model     = "".join([" ~ ",
                              " + ".join(factors), " + ",
                              " + ".join(["I(1 / {0})".format(f) for f in inverse_factors])])

    design_formula = full_model
    lm_formula     = response[0] + full_model
    trials         = round(2 * (len(factors) + len(inverse_factors) + 1))

    fixed_variables = fixed_factors

    if budget - len(data[0]) < 0:
        print("Full data does not fit on budget")
        if trials < len(data[0]):
            print("Computing D-Optimal Design")
            output = opt_federov(design_formula, data, trials)
            design = output.rx("design")[0]
        else:
            print("Too few data points for a D-Optimal design")
            design = data

        used_experiments = len(design[0])
        regression, prf_values = anova(design, lm_formula)
        ordered_prf_keys       = sorted(prf_values, key = prf_values.get)
        predicted_best         = predict_best(regression, data)
        fixed_variables        = get_fixed_variables(predicted_best, ordered_prf_keys,
                                                     fixed_factors)
        pruned_data            = prune_data(data, fixed_variables)

        pruned_factors, pruned_inverse_factors = prune_model(factors, inverse_factors,
                                                            ordered_prf_keys)
    else:
        print("Full data fits on budget, picking best value")
        used_experiments = len(data[0])
        prf_values = []
        ordered_prf_keys = []
        pruned_data = []
        pruned_factors = []
        pruned_inverse_factors = []
        predicted_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                                  True)

    return {"prf_values": prf_values,
            "ordered_prf_keys": ordered_prf_keys,
            "predicted_best": predicted_best,
            "pruned_data": pruned_data,
            "pruned_factors": pruned_factors,
            "pruned_inverse_factors": pruned_inverse_factors,
            "fixed_factors": fixed_variables,
            "used_experiments": used_experiments}

def dopt_anova():
    data = utils.read_csv("dopt_anova_experiments/data/search_space.csv", header = True)

    initial_factors = ["elements_number", "y_component_number",
                       "vector_length", "temporary_size",
                       "load_overlap", "threads_number", "lws_y"]

    initial_inverse_factors = ["y_component_number", "lws_y",
                               "elements_number", "threads_number"]

    response = ["time_per_pixel"]

    data = data.rx(StrVector(initial_factors + response))
    data_best = data.rx((data.rx2(response[0]).ro == min(data.rx(response[0])[0])),
                        True)

    step_factors = initial_factors
    step_inverse_factors = initial_inverse_factors
    step_space = data

    fixed_factors = {}

    initial_budget = 58
    budget = initial_budget
    used_experiments = 0
    iterations = 3

    for i in range(iterations):
        if step_space == []:
            break

        step_data = dopt_anova_step(response,
                                    step_factors,
                                    step_inverse_factors,
                                    step_space,
                                    fixed_factors,
                                    budget)

        step_space = step_data["pruned_data"]
        step_factors = step_data["pruned_factors"]
        step_inverse_factors = step_data["pruned_inverse_factors"]
        budget -= step_data["used_experiments"]
        used_experiments += step_data["used_experiments"]
        fixed_factors = step_data["fixed_factors"]

        print("Fixed Factors: " + str(fixed_factors))
        print("Slowdown: " + str(step_data["predicted_best"].rx(response[0])[0][0] / data_best.rx(response[0])[0][0]))
        print("Budget: {0}/{1}".format(used_experiments, initial_budget))

dopt_anova()
#+END_SRC

#+RESULTS:
#+begin_example
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.000231109380007393
Power Transform Step:
Estimated transformation parameter
         Y1
-0.08680373

Heteroscedasticity Test p-value:
0.9475422563588601
Regression Step:
                        Df Sum Sq Mean Sq F value   Pr(>F)
elements_number          1  2.290   2.290  11.967 0.004723 **
y_component_number       1  0.529   0.529   2.765 0.122202
vector_length            1  3.766   3.766  19.676 0.000813 ***
temporary_size           1  0.235   0.235   1.225 0.290002
load_overlap             1  0.079   0.079   0.413 0.532456
threads_number           1  0.018   0.018   0.092 0.767416
lws_y                    1  7.271   7.271  37.987 4.85e-05 ***
I(1/y_component_number)  1  0.110   0.110   0.574 0.463168
I(1/lws_y)               1  2.861   2.861  14.949 0.002242 **
I(1/elements_number)     1  0.164   0.164   0.859 0.372333
I(1/threads_number)      1  0.432   0.432   2.259 0.158663
Residuals               12  2.297   0.191
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1}
{'lws_y': 16, 'vector_length': 1}
Dimensions of Pruned Data: [1] 576   8

Dimensions of Full Data: [1] 23120     8

Fixed Factors: {'lws_y': 16, 'vector_length': 1}
Slowdown: 11.64335230377803
Budget: 24/58
Full data does not fit on budget
Computing D-Optimal Design
Heteroscedasticity Test p-value:
0.06519543239335164
No need to power transform
Regression Step:
                        Df    Sum Sq   Mean Sq F value   Pr(>F)
elements_number          1 6.930e-19 6.930e-19   8.120   0.0191 *
y_component_number       1 4.960e-19 4.960e-19   5.814   0.0392 *
temporary_size           1 5.100e-20 5.100e-20   0.599   0.4589
load_overlap             1 5.400e-20 5.400e-20   0.638   0.4451
threads_number           1 5.024e-18 5.024e-18  58.829 3.09e-05 ***
I(1/y_component_number)  1 2.100e-20 2.100e-20   0.251   0.6284
I(1/elements_number)     1 8.200e-20 8.200e-20   0.959   0.3530
I(1/threads_number)      1 5.726e-18 5.726e-18  67.049 1.84e-05 ***
Residuals                9 7.690e-19 8.500e-20
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Identical predictions (tol = 1e-17): 1
Fixed Variables: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
{'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Dimensions of Pruned Data: [1] 8 8

Dimensions of Full Data: [1] 576   8

Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.8225092523512674
Budget: 42/58
Full data fits on budget, picking best value
Fixed Factors: {'lws_y': 16, 'vector_length': 1, 'threads_number': 512, 'elements_number': 8}
Slowdown: 1.265157794114108
Budget: 50/58
#+end_example
*** [2018-06-04 Mon]
**** Implementing the DOPT-Anova in Orio
I've started implementing our approach in benchmark problems from SPAPT, which
is provided with Orio. I've been understanding Orio's source code implementing
the glue code, with =rpy2=, to be able to use R packages in Python.

I've reached a problem with =optFederov=. One of the benchmark applications
has ~10^{14}~ possible combinations in total. My first approach was trying to
generate a subset of this search space, as I did before, but this did not work.
The function kept finding "singular design" errors, which mean the determinant
of the candidates it tested are negative according to the documentation.

It is not clear how to fix this, so I am now trying to use the optMonteCarlo
function, which tries to generate samples of the search space as it explores
it with the Federov algorithm.
*** [2018-06-11 Mon]
**** Converting R Model Fits into Formulas
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
A <- data.frame(x1 = sample(1:100, 30, replace = T),
                x2 = sample(1:100, 30, replace = T),
                x3 = sample(1:100, 30, replace = T),
                x4 = sample(1:100, 30, replace = T),
                x5 = sample(1:100, 30, replace = T))

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5

regression <- lm(y ~ x1 + x2 + x3 + x4 + x5, data = A)
formula(regression)

new_formula <- as.formula(substituteDirect(formula(regression), as.list(coef(regression))))
new_formula

predict(regression)
predict(regression, type = "terms")
A
#+END_SRC

#+RESULTS:
#+begin_example
y ~ x1 + x2 + x3 + x4 + x5
y ~ 2.3 + 3.1 + 4.5 + 6.8 + 2.31
      1       2       3       4       5       6       7       8       9      10
1480.86  536.12 1174.93  804.95 1487.49 1427.97 1150.58 1014.75  916.79  958.22
     11      12      13      14      15      16      17      18      19      20
 953.27  438.97  710.39  709.89 1234.76 1034.89 1043.24 1622.64 1114.05  739.34
     21      22      23      24      25      26      27      28      29      30
 848.47  928.56 1320.51  744.19 1066.60  946.50 1235.58 1051.55  860.67  706.07
             x1          x2      x3          x4      x5
1    54.4333333 -129.166667  195.15  284.693333   66.99
2   -49.0666667    1.033333 -133.35 -279.706667  -11.55
3   -81.2666667 -104.366667   73.65  264.293333   13.86
4    15.3333333  125.033333 -191.85  -55.306667  -97.02
5   -39.8666667   22.733333  186.15  305.093333    4.62
6    15.3333333  131.233333 -115.35  318.693333   69.30
7    24.5333333    1.033333 -227.85  318.693333   25.41
8    98.1333333 -104.366667  100.65  -14.506667  -73.92
9    47.5333333  143.633333  -29.85 -211.706667  -41.58
10 -106.5666667  140.533333   28.65 -170.906667   57.75
11   29.1333333   44.433333  141.15 -293.306667   23.10
12   24.5333333  -23.766667 -218.85 -259.306667  -92.40
13   91.2333333  -29.966667 -119.85 -313.706667   73.92
14  -42.1666667  -14.466667   73.65 -320.506667    4.62
15   98.1333333   97.133333   55.65   46.693333  -71.61
16  -72.0666667  121.933333  186.15 -306.906667   97.02
17   -0.7666667   16.533333 -169.35  264.293333  -76.23
18   52.1333333  109.533333  163.65  203.093333   85.47
19   -0.7666667  109.533333  -38.85  155.493333 -120.12
20   10.7333333 -119.866667  186.15 -293.306667  -53.13
21   77.4333333   32.033333 -187.35   33.093333 -115.50
22  -72.0666667  -64.066667  136.65  -55.306667  -25.41
23  -30.6666667  156.033333   55.65  121.493333    9.24
24 -101.9666667 -141.566667 -223.35  243.893333  -41.58
25    1.5333333  -67.166667  -97.35  121.493333   99.33
26    1.5333333   38.233333   -7.35 -170.906667   76.23
27   36.0333333 -104.366667  -88.35  311.893333   71.61
28   75.1333333 -147.766667  114.15    5.893333   -4.62
29  -72.0666667  -95.066667  -20.85   39.893333    0.00
30  -83.5666667 -144.666667  172.65 -293.306667   46.20
attr(,"constant")
[1] 1008.76
   x1  x2 x3  x4  x5       y
1  78   8 96  95  86 1480.86
2  33  50 23  12  52  536.12
3  19  16 69  92  63 1174.93
4  61  90 10  45  15  804.95
5  37  57 94  98  59 1487.49
6  61  92 27 100  87 1427.97
7  65  50  2 100  68 1150.58
8  97  16 75  51  25 1014.75
9  75  96 46  22  39  916.79
10  8  95 59  28  82  958.22
11 67  64 84  10  67  953.27
12 65  42  4  15  17  438.97
13 94  40 26   7  89  710.39
14 36  45 69   6  59  709.89
15 97  81 65  60  26 1234.76
16 23  89 94   8  99 1034.89
17 54  55 15  92  24 1043.24
18 77  85 89  83  94 1622.64
19 54  85 44  76   5 1114.05
20 59  11 94  10  34  739.34
21 88  60 11  58   7  848.47
22 23  29 83  45  46  928.56
23 41 100 65  71  61 1320.51
24 10   4  3  89  39  744.19
25 55  28 31  71 100 1066.60
26 55  62 51  28  90  946.50
27 70  16 33  99  88 1235.58
28 87   2 78  54  55 1051.55
29 23  19 48  59  57  860.67
30 18   3 91  10  77  706.07
#+end_example
*** [2018-06-13 Wed]
**** Stepwise Regression in R
#+HEADER: :results output :session *R*
#+BEGIN_SRC R
library(MASS)

A <- data.frame(x1 = sample(1:100, 100, replace = T),
                x2 = sample(1:100, 100, replace = T),
                x3 = sample(1:100, 100, replace = T),
                x4 = sample(1:100, 100, replace = T),
                x5 = sample(1:100, 100, replace = T),
                x6 = sample(1:100, 100, replace = T),
                x7 = sample(1:100, 100, replace = T),
                x8 = sample(1:100, 100, replace = T)
)

A$y = 2.3 * A$x1 + 3.1 * A$x2 + 4.5 * A$x3 + 6.8 * A$x4 + 2.31 * A$x5 + 5.2 * (A$x8 * A$x2) + 3.1 * (1 / A$x7)

regression = lm(y ~ ., data = A[1:30, ])
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
A[A$y == min(A$y), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x1           1 2.566e+07 2.566e+07   1.852  0.18796
x2           1 2.007e+09 2.007e+09 144.892 6.88e-11 ***
x3           1 1.796e+05 1.796e+05   0.013  0.91044
x4           1 2.557e+08 2.557e+08  18.458  0.00032 ***
x5           1 4.264e+08 4.264e+08  30.779 1.66e-05 ***
x6           1 1.331e+08 1.331e+08   9.604  0.00544 **
x7           1 1.376e+07 1.376e+07   0.993  0.33033
x8           1 1.639e+09 1.639e+09 118.324 4.35e-10 ***
Residuals   21 2.909e+08 1.385e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
   x2 x8
14 30  1
#+end_example

#+HEADER: :results output :session *R*
#+BEGIN_SRC R
regression = update(regression, . ~ . - x1 - x3 - x4 - x5 - x6 - x7)
summary.aov(regression)

values = predict(regression, data = A)
A[values == min(values), c("x2", "x8")]
#+END_SRC

#+RESULTS:
#+begin_example
            Df    Sum Sq   Mean Sq F value   Pr(>F)
x2           1 1.953e+09 1.953e+09   127.8 9.58e-12 ***
x8           1 2.427e+09 2.427e+09   158.8 8.03e-13 ***
Residuals   27 4.126e+08 1.528e+07
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
   x2 x8
14 30  1
44 48 68
74 93 59
#+end_example
