# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro H R Bruel
#+LANGUAGE:    en
#+TAGS: LIG(L) HOME(H) Europe(E) Blog(B) noexport(n) Stats(S)
#+TAGS: Epistemology(E) Vulgarization(V) Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* 2017
** November 
*** [2017-11-28 Tue]
**** Install and Configure Emacs
- Attempted to use vim-orgmode but gave up: not enough features
- Switch to Emacs
**** Start an OrgMode Journal
- Use template from webinars
- Populate with some tasks
**** Advance on Reconfig Presentation
- Should add presentation notes for Alfredo
*** [2017-11-29 Wed]
- Complete Reconfig Presentation
- Read Fisher's Design of Experiments
**** Meeting with Alfredo
- Present our work for Brice and his research center
- Finish slides for Reconfig
*** [2017-11-30 Thu]
- Fix OFII form error and post it
- Start working on NODAL poster
**** Amélie's Presentation
- She is the author of Johanne's NIPS paper
- Two bandit cases: bandit and semi-bandit
- It is possible to achieve Nash Equilibria with both, given certain
  conditions
- I sent her an e-mail, asking for her C++ implementation of her work
- She already sent her code
- Print and read Amélie's paper
** December
*** Reading Fisher's Design & Montgomery's Design                :Book:DOE:
**** Quotes for key concepts
- "Every experiment may be said to exist only in order to give the
    facts a chance of disproving the null hypothesis"
- "The null hypothesis must be exact, that is free from vagueness and
    ambiguity"
**** A null hipothesis for autotuning techniques
***** Definitions
- A base autotuning technique t_b
- A new autotuning technique t_n
- A set of autotuning problems P
- A metric M
***** An exact null hipothesis H 
- We can state H as: The improvement of M produced by t_n is equal to t_b for all
  problems p \in P, that is, t_n performance is equal to t_b for P.
***** Problems 
- What is the chance of disproving H? In other words, to be considered better for P,
  for how many problems p \in P must t_n perform better than t_b?
- The set P must be very well chosen for this experiment to make sense. 
*** Studying "Learning with Bandit Feedback in Potential Games"      :Code:
**** Reading "Learning with Bandit Feedback in Potential Games" 
The [[file:~/Dropbox/papers/bandit-problem/cohen2017learning.pdf][pdf file]] is in my paper library.
***** Managing Autotuning Techniques as an N-Player Game
- Understanding applicability will require studying the implementation
  in C++ shared by Amélie.
- The players are distributed processes
- The actions are changing, keeping, restarting or reconfiguring
  search techniques
- The payoff is finding better configurations
  - Related to the Area Under the Curve Credit Assignment
  - The 'full bandit' case is very similar to MAB AUC
- Gaming strategies could consist of policies to select
  techniques based on the number of processes, past results,
  and maybe characteristics of the search space
- In this context, what would be equivalent to the *Nash Equilibrium*?
  - No process "wants" to change its policy for selecting techniques
  - No process "wants" to change its current technique
**** Studying the code from "Learning with Bandit Feedback in Potential Games"
The [[file:~/code/bandit-johanne/][source code]] is located in my code library.  
***** General Questions & Considerations
It seems the game has only 2 players, but the paper considers N-player
games. From the paper, it seems that the N-player implementation would
work without much change.

Payoffs seem to be pre-computed for each strategy but this does not,
at first, imply that needing to compute the payoffs would change
anything.

To adapt this code to the selection of search algorithms by Julia
processes we would need a way to implement the strategies.
***** Questions about specific points in code
****** =main.cpp=
Questions and execution flow related to the [[file:~/code/bandit-johanne/code/main.cpp][main file]].

******* Questions

- Why weren't random payoffs used?
- How are strategies represented in the =evol= array?

******* Execution Flow

1. Declare payoff and strategy arrays
2. Initialize pre-computed payoffs
3. Initialize seeds array
4. Instantiate a new =Game=
5. Call =Game->Play=
6. Save output to file
   
****** =game.h= & =game.cpp=
Questions and execution flow related to the [[file:~/code/bandit-johanne/code/game.h][header]] and the [[file:~/code/bandit-johanne/code/game.cpp][implementation]].

******* Questions

Re-read [[file:~/Dropbox/papers/bandit-problem/cohen2017learning.pdf][the paper]] to understand:

- What is a potential game?
- What does the =potential_function= do?
- What are the constraints on payoff arrays?
- What are =gamma= & =utility=?
- What is =res= used for inside =Game->play=? And inside =Game=?

******* Execution Flow

1. Instantiated by =main.cpp=
2. =void Game->play= is called by =main.cpp=
3. Open output file
4. Loop for the number of steps:
   1. Registers player strategies in =evol= for step =i= such that:
      #+BEGIN_SRC C
evol[i][(int)floor(P1->proba_strat[0]*100)][(int)floor(P2->proba_strat[0]*100)]++;
      #+END_SRC
   
      Where =P1= and =P2= are =Player= objects and the =proba_strat= arrays store
      the current strategy of each player.
   2. Calls =play_one_turn= (see below)
5. Save output to file

******** Execution Flow of =play_one_turn=

1. Called by =Game->play=
2. Initializes =epsilon=, =gamma= and =utility=
3. Set strategies for each player with =P->setStrat()= and
   =P->draw_proba()=
4. Update =utility= arrays with =P1->utility(P2->getStrat())=
   and =P2->utility(P1->getStrat())=
5. Update =y_strat= arrays with =utility= and =gamma=
6. Calls =P->update_proba(epsilon)= for each player

****** =player.h= & =player.cpp=
Questions and execution flow related to the
[[file:~/code/bandit-johanne/code/player.h][header]] and the
[[file:~/code/bandit-johanne/code/player.cpp][implementation]].

******* Questions

- What are the arrays =proba_strat= & =y_strat=?

*** NODAL Development                                          :Code:NODAL:
**** Installing NODAL in Julia Nightly
[[https://github.com/phrb/NODAL.jl][NODAL]] is the autotuning library I am developing in the [[https://julialang.org][Julia]]
language. The idea is to provide tools for the implementation of
parallel and distributed autotuners for various problem domains.
***** Download Julia Nightly
****** [[https://julialang.org/downloads][Download Generic Binary]] 
****** Downloading from the CLI
You can run the following to install the latest *Julia* version:
#+BEGIN_SRC bash
cd ~ && mkdir .bin && cd .bin
wget https://julialangnightlies-s3.julialang.org/bin/linux/x64/julia-latest-linux64.tar.gz
tar xvf julia-latest-linux64.tar.gz
mv julia-* julia
rm julia-latest-linux64.tar.gz
#+END_SRC
This will put the *Julia* binary at =~/.bin/julia/bin/julia=.
You can use it like that or add an =alias= to your shell.
***** Installing the unregistered version
This will not be needed after registering NODAL to METADATA.
****** [[https://docs.julialang.org/en/latest/manual/packages/#Installing-Unregistered-Packages-1][Documentation]]
****** Julia Commands
#+BEGIN_SRC julia
Pkg.clone("https://github.com/phrb/NODAL.jl")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
***** Installing from the Julia package manager
****** Julia commands
#+BEGIN_SRC julia
Pkg.add("NODAL")
Pkg.build("NODAL")
Pkg.test("NODAL")
#+END_SRC
**** Setting up a new Release
***** Using Attobot
[[https://github.com/attobot][Attobot]] integrates with *GitHub* to automatically register a new package
or a package version to *Julia*'s =METADATA= package repository.  Attobot
only needs a new *GitHub* release to work.
***** Using *Julia*'s =PkgDev=
Check the [[https://docs.julialang.org/en/latest/manual/packages/#Tagging-and-Publishing-Your-Package-1][documentation]] to learn how to register and publish user
packages to =METADATA=.
**** Development Workflow
The process of fixing an [[https://github.com/phrb/NODAL.jl/issues][issue]] or submitting a new
feature is:
0. Fork [[https://github.com/phrb/NODAL.jl][NODAL on GitHub]]
   
   You will need a GitHub account for this.

1. Make sure you have the latest version
   #+BEGIN_SRC bash
git checkout master
git fetch
   #+END_SRC

   New branches must be made from the =dev= branch:
   #+BEGIN_SRC bash
git checkout dev
   #+END_SRC
2. Checkout a new branch
   #+BEGIN_SRC bash
git checkout -b fix-or-feature
   #+END_SRC
3. Write code and commit to your new branch
   
   Make sure you write short and descriptive commit
   messages. Something similar to [[https://udacity.github.io/git-styleguide/][Udacity's guidelines]] is preferred
   but not strictly necessary.

4. Open a [[https://github.com/phrb/NODAL.jl/pulls][pull request]] to the =dev= bran
*** Creating a Data Frame for FPGA Autotuning Samples              :R:Code:
**** Installing R Dependencies
The next code block install all =R= dependencies.
We are not using =ggplot2= to create the =csv= files,
but it will be used later for plotting.

We are installing =rjson= because part of the data files were generated
by OpenTuner in the =JSON= format.  The other packages are from Arnaud's
[[https://github.com/alegrand/SMPE#learning-r][guidelines]] for the SMPE course.

#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

**** Generating =csv= Files with the Data
The following script is hosted at the [[https://github.com/phrb/legup-tuner/blob/master/post_place_and_route/py/results/r_scripts/generate_csv_files.r][LegUp autotuner repository]].

To run it, you first need to clone the repository to get the data:

#+BEGIN_SRC sh
git clone https://github.com/phrb/legup-tuner.git
#+END_SRC

Then, replace the contents of the variable =repository= with the path
into which you cloned the repository.
The following script is hosted at [[https://raw.githubusercontent.com/phrb/legup-tuner/master/post_place_and_route/py/results/r_scripts/generate_csv_files.r][GitHub]]:

#+BEGIN_SRC R
library(dplyr)
library(tidyr)
library(jsonlite)

runs         <- 10
tuning_time  <- 5400

repository   <- "~/code/legup-tuner"
results      <- "post_place_and_route/py/results"

output_dir   <- paste(repository, results, "r_scripts/data", sep = "/")

experiments  <- c("default_stratixV_perf", "default_stratixV_perflat",
                  "default_stratixV_area", "default_stratixV_balanced")

applications <- c("dfadd", "dfdiv", "dfmul", "sha", "motion", "adpcm",
                  "dfsin", "aes", "blowfish", "gsm", "mips")

txt_measurements <- c("log_details.txt", "best_cycles_log.txt",
                      "best_fmax_log.txt", "best_lu_log.txt",
                      "best_pins_log.txt", "best_regs_log.txt",
                      "best_block_log.txt", "best_ram_log.txt",
                      "best_dps_log.txt")

json_configurations <- "best_log.json"

headers <- c("WNS", "Cycles", "FMax", "LUs", "Pins", "Regs", "Blocks", "RAM",
             "DPS")

#
# This function merges columns and fills missing rows with 'NA'.
# This happens in this dataset because of unsynchronized logging,
# where the last configuration was saved twice in one of the log
# files. Will require to later clean the lines with 'NA's.
#
# Function from:
#
#    https://stackoverflow.com/questions/7962267/cbind-a-df-with-an-empty-df-cbind-fill
#
#
cbind.fill <- function(...){
    nm <- list(...)
    nm <- lapply(nm, as.matrix)
    n <- max(sapply(nm, nrow))
    do.call(cbind, lapply(nm, function (x)
        rbind(x, matrix(, n-nrow(x), ncol(x)))))
}

dir.create(output_dir)

for (experiment in experiments) {
    dir.create(paste(output_dir, strsplit(experiment, "_")[[1]][3], sep = "/"))

    for (application in applications) {
        data <- data.frame()

        for (iteration in 1:runs) {
            target_file <- (paste(repository, results, experiment,
                                  paste(application, tuning_time, iteration,
                                        sep = "_"), json_configurations,
                                  sep = "/"))

            if (file.exists(target_file)) {
                configuration <- fromJSON(target_file)
            }

            columns <- data.frame()

            for (measurement in txt_measurements) {
                target_file <- paste(repository, results, experiment,
                                     paste(application, tuning_time, iteration,
                                           sep = "_"), measurement,
                                     sep = "/")

                if (file.exists(target_file)) {
                    new_column <- read.table(target_file, header = FALSE)[2]

                    if (ncol(columns) == 0) {
                        columns <- new_column
                    } else {
                        columns = cbind.fill(columns, new_column)
                    }
                }
            }

            if (ncol(columns) != 0) {
                colnames(columns) <- headers

                columns = cbind.fill(configuration, columns)

                if (nrow(data) == 0) {
                    data <- columns
                } else {
                    data = bind_rows(as.data.frame(data),
                                     as.data.frame(columns))
                }
            }
        }

        data <- data[complete.cases(data), ]

        write.csv(data, file = paste(paste(output_dir, strsplit(experiment,
                                                                "_")[[1]][3],
                                           application, sep = "/"), ".csv",
                                     sep = ""))
    }
}
#+END_SRC

#+RESULTS:

*** Analysing FPGA Autotuning Samples                              :R:Code:
The generated =csv= files live in the [[https://github.com/phrb/legup-tuner/tree/master/post_place_and_route/py/results/r_scripts/data][repository]], and are organized by
*experiment* and CHStone *application*. The *experiments* are the autotuning
runs targeting different optimization objectives. The *applications*
are the different programs that were autotuned.

It makes sense to combine data from different *experiments*, for a
same *application*, because the search space is the same and the
individual hardware metrics refer to the same FPGA circuit.

The =WNS= column is not directly comparable between different
*experiments*, even for the same *application*, because it represents a
different computation over the absolute metric values for each
*application*.  To compare =WNS= columns it would be necessary to
recompute =WNS= with different weights using the other hardware metrics.

Combining data from different *applications* would be more complicated.
The search spaces are not the same, but the *target FPGA* is the same.
We could try to understand some property of the hardware by looking
at the variability of the hardware metrics.

**** Multivariate Analysis
Implementing some ideas from [[https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/src/multivariateanalysis.html][The Little Book of R for Multivariate
Analysis]].

***** Summary Statistics
First, we will combine all *experiments* with the *dfdiv* application. In
this case, the =WNS= columns are not directly comparable so I am not
removing the rows where ~WNS == Inf~. This indicates a penalty that
happened when there was some problem during HLS, synthesis, or
testing. The following script, hosted at [[https://raw.githubusercontent.com/phrb/legup-tuner/master/post_place_and_route/py/results/r_scripts/mean_sd.r][GitHub]], will print the *mean*
and *standard deviation* for the FPGA *hardware metrics* and *WNS*:

#+BEGIN_SRC R :results output
library(dplyr)

csv_dir <- c("~/code/legup-tuner/",
             "post_place_and_route/py/results/r_scripts/",
             "data")

experiments <- c("balanced", "area", "perf", "perflat")

applications <- c("dfadd", "dfdiv", "dfmul", "sha", "motion", "adpcm",
                  "dfsin", "aes", "blowfish", "gsm", "mips")

data       <- data.frame()
clean_data <- data.frame()

application <- applications[2]

for (experiment in experiments) {
    new_data <- read.csv(paste(paste(csv_dir, collapse = ""),
                               experiment, paste(application,
                                                 ".csv",
                                                 sep = ""),
                               sep = "/"),
                         header = TRUE, sep = ",")

    new_data       <- as.data.frame(new_data)
    new_clean_data <- new_data[is.finite(new_data$WNS),]

    if (ncol(data) == 0) {
        data <- new_data
    } else {
        data <- rbind(data, new_data)
    }

    if (ncol(clean_data) == 0) {
        clean_data <- new_clean_data
    } else {
        clean_data <- rbind(clean_data, new_clean_data)
    }
}

names <- c("WNS","RAM")

idx <- match(names, names(data))

print("Data with 'WNS == Inf' rows:")

print("Mean:")
sapply(data[idx[1]:idx[2]], mean)

print("Standard Deviation:")
sapply(data[idx[1]:idx[2]], sd)

print("Data without 'WNS == Inf' rows:")

print("Mean:")
sapply(clean_data[idx[1]:idx[2]], mean)

print("Standard Deviation:")
sapply(clean_data[idx[1]:idx[2]], sd)
#+END_SRC

#+RESULTS:
#+begin_example
[1] "Data with 'WNS == Inf' rows:"
[1] "Mean:"
         WNS       Cycles         FMax          LUs         Pins         Regs 
         Inf  587.1083650   24.1486882    1.0000000    3.2813688 4153.5665399 
      Blocks          RAM 
   0.8992395    0.8992395 
[1] "Standard Deviation:"
         WNS       Cycles         FMax          LUs         Pins         Regs 
         NaN  227.1528406   13.1032536    0.0000000    0.4500950 1123.2949061 
      Blocks          RAM 
   0.3012978    0.3012978 
[1] "Data without 'WNS == Inf' rows:"
[1] "Mean:"
         WNS       Cycles         FMax          LUs         Pins         Regs 
   1.0651531  564.3596215   23.6492429    1.0000000    3.2492114 4032.5268139 
      Blocks          RAM 
   0.9116719    0.9116719 
[1] "Standard Deviation:"
         WNS       Cycles         FMax          LUs         Pins         Regs 
   0.3079517  207.3545319   12.8765244    0.0000000    0.4332403 1049.1423668 
      Blocks          RAM 
   0.2842201    0.2842201 
#+end_example

***** Attempts at Computing Correlations
Following [[https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/src/multivariateanalysis.html#calculating-correlations-for-multivariate-data][this guide]], I attempted to compute and plot the largest
correlations between *parameters* and *hardware metrics*.  The largest
correlations might not be really signficant, because the relationship
between variables might not be linear, as illustrated [[http://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/][here]]. Therefore,
the following script plots the 30 largest correlations so that we can
look at the relationships between variables. The script is hosted at
[[https://raw.githubusercontent.com/phrb/legup-tuner/master/post_place_and_route/py/results/r_scripts/correlations.r][GitHub]].

#+BEGIN_SRC R :file correlations.eps :results output graphics
library(dplyr)

setEPS()
postscript("correlations.eps", width = 16, height = 11)

#
# Function adapted from:
#
#   https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/src/multivariateanalysis.html#calculating-correlations-for-multivariate-data
#
sorted_correlations <- function(data, datapoints) {
    cormatrix <- cor(data)

    diag(cormatrix) <- 0
    cormatrix[lower.tri(cormatrix)] <- 0

    fm <- as.data.frame(as.table(cormatrix))

    names(fm) <- c("First.Variable", "Second.Variable","Correlation")

    hardware_metrics <- c("WNS", "Cycles", "FMax", "LUs", "Pins", "Regs",
                          "Blocks", "RAM", "DPS")

    fm <- dplyr::filter(fm, grepl(paste(hardware_metrics, collapse = "|"),
                                  fm$Second.Variable))

    head(fm[order(abs(fm$Correlation), decreasing = T), ], n = datapoints)
}

csv_dir <- c("~/code/legup-tuner/",
             "post_place_and_route/py/results/r_scripts/",
             "data")

experiments <- c("balanced", "area", "perf", "perflat")

applications <- c("dfadd", "dfdiv", "dfmul", "sha", "motion", "adpcm",
                  "dfsin", "aes", "blowfish", "gsm", "mips")

data       <- data.frame()
clean_data <- data.frame()

application <- applications[2]

for (experiment in experiments) {
    new_data <- read.csv(paste(paste(csv_dir, collapse = ""),
                               experiment, paste(application,
                                                 ".csv",
                                                 sep = ""),
                               sep = "/"),
                         header = TRUE, sep = ",")

    new_data <- as.data.frame(new_data)
    new_data <- new_data[is.finite(new_data$WNS),]

    if (ncol(data) == 0) {
        data <- new_data
    } else {
        data <- rbind(data, new_data)
    }
}

names <- c("WNS","RAM")
idx   <- match(names, names(data))
data  <- sapply(data, as.numeric)

correlation <- sorted_correlations(data, 120)

print("120 Largest Correlations:")
#print(correlation)

print("Scatter Plots of the 10 Largest Correlations")
short_correlation <- correlation[1:30, ]

old.par <- par(mfrow = c(5, 6))

for (i in 1:nrow(short_correlation)) {
    first  <- as.character(short_correlation[i, 'First.Variable'])
    second <- as.character(short_correlation[i, 'Second.Variable'])

    plot(data[, first], data[, second], xlab = first, ylab = second)
}

par(old.par)
dev.off()
#+END_SRC

#+RESULTS:
[[file:correlations.eps]]

*** [2017-12-01 Fri]
- Finish the Reconfig poster
- Converge to final Reconfig presentation
*** [2017-12-04 Mon]
- Set up build environment for latest Julia version
- Apply Sai's corrections to presentation
**** Fix NODAL's status in Julia pkg Manager
- Fixed warnings and deprecations
- Tagged new version 'v0.3.5'
- Released new version
- Made new pull request to METADATA
- Waiting for merge
*** [2017-12-05 Tue]
- Started using new us-layout keyboard.
- Study the Bandit's code and paper
**** Start NODAL's 'dev' branch
- Pushed 'dev' branch to GitHub repository.
- Added development workflow to Journal
*** [2017-12-06 Wed]
- Describe Hedge's code exec flow ([[Studying "Learning with Bandit Feedback in Potential Games"][Main Section]])
- Elaborate concrete bandit implementation plan for autotuning ([[Studying "Learning with Bandit Feedback in Potential Games"][Main Section]])
- Resume study of Fisher's and Montgomer's books ([[Reading Fisher's Design & Montgomery's Design][Main Section]])
**** Discussion with Brice and Arnaud
Meeting notes kindly taken by Arnaud.
***** Questions left open in Pedro's journal
- Bandits: This would clearly be useful for auto-tuning as we may not
  know in advance what would work better on a given instance.
  - Bandit algorithms could be used to select which algorithms work better.
  - They could also be used to select which areas to explore.
  A possibly interesting question is "How does parallel bandit work?".

- Game Theory: branch of mathematics for
  1. modeling/studying situations where agents compete with each others.
  2. improving situation where agents compete with each others
  3. design fully distributed algorithms/protocols
  It's not clear yet how game theory would help in our auto-tuning
  context but we can keep this in mind.

- Design of Experiments: Master 2 lecture on
[[https://github.com/alegrand/SMPE][SMPE]]. Description and
  rooms are
[[http://mescal.imag.fr/membres/arnaud.legrand/teaching/2017/M2R_SMPE.php][here]].

- Quick discussions about Julia and how it compares with other
  languages (compiled on the fly, FFI, ...).
***** Autotuning context:
  - So far, with FPGAs, what was distinctive was:
    1. Many many compilation parameters
    2. Several parameters to optimize
    3. Extremely long experiments
    4. Parallel exploration
    Can we keep all these research challenges open or should we focus
    on one or two of them ?
  - Brice thinks there is:
    - The structure of the code is also important for the compiler and
      it may be worth looking at code transformation. Changing the
      code may have a lot of impact on performance and this
      variability may contain a lot of useful information. BOAST
      (meta-programming rather than code transformation) can help to
      investigate this.

      Actually, the fact some gain can be obtained by tweaking the
      compiler option may mean we're far from the peak performance,
      which may be caused by an ineffective code structure. *This could
      be explored with BOAST and* *we should probably check this before
      going into compiler flag exploration*.
  - Discussions about compiler passes (LLVM) and the importance of
    unroll/jam.
  - Questions:
    - Do we have FPGAs to play with ?
      - Some of the recent Intel Skylake CPUs have FPGAs. They were
        released in Spring but Google and Amazon bought them all so we
        have to wait for new ones to be produced.
    - Do we have the right compilers/toolchain ? Is it functional ?
      - Maybe. :) Intel is supposed to provide us with an OpenCL to
        FPGA compiler with their Skylake chips.
      - Before, Pedro used an open-source High Level Synthesis tool.
    - Can we explore application structures for FPGAs with BOAST ?
      - Sure, why not ?

  - Arnaud mentions http://www.exanest.eu/ and Fabien Chaix he knows
    quite well.
  - Pedro's work was published at ReConfig (Cancun) and Alfredo's
    currently presenting it.
    - https://github.com/phrb/slides-reconfig-2017-autotuning
    -
https://github.com/phrb/slides-reconfig-2017-autotuning/raw/master/src/presentation.pdf
      - Slide 9: We have a quick discussion on why there is a single
        configuration file controlling all the knobs/parameters of the
        blue workflow compared to being able to obtain intermediary
        information and possibly perform selections at each step
        before moving to the next one.
      - Slide 9: This phase is fast. It's the compiling from the
        Verilog format to the hardware synthesis that is really
        slow. They use Quartus. Quartus also has parameters but this
        parameter space was not explored yet in this work.
        - LegUp is supposed to have a cost model that allows to guide
          the optimization without going all the way down to the
          hardware synthesis to evaluate configurations but it was not
          functional.
      - Slide 11: summarizes the whole workflow. 6+ compilation passes
      - Brice wonders if peak performance models exists for such
        app/systems. It would be worth knowing how far we are from the
        optimal to decide whether further optimizations are needed.
      - Quick discussion about multi-criteria optimization and Pareto
        front.
***** Stuff to do
- Create a big data frame with all the samples you obtained during the
  exploration of the autotuner. We'll try to explore (metric
  variability, parameter space exploration, criteria Pareto structure)
  it together.
- The same could be done for GPU experiments.
*** [2017-12-07 Thu]
- Creating data frames for FPGA data ([[Creating a Data Frame for FPGA Autotuning Samples][Main Section]])
**** Arnaud's Lecture on Linear Regression
The slides and code are hosted at
[[https://github.com/alegrand/SMPE][GitHub]].
*** [2017-12-08 Fri]
- Finish the data frames for FPGA data ([[Creating a Data Frame for FPGA Autotuning Samples][Main Section]])
- Initial FPGA Data Analysis ([[Analysing FPGA Autotuning Samples][Main Section]])
- Ask for help to complete ADUM Registration
