#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Pedro's Journal
#+AUTHOR:      Pedro Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: ExportableReports(E)
#+TAGS: FAPESP(f)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[margin=2cm]{geometry}
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{sourcecodepro}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-qtree}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
# #+LATEX_HEADER:   escapeinside={\%*}{*)},
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   language=R,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

* Setup
** Julia
#+NAME: install_julia_deps
#+HEADER: :results output :session *julia*
#+BEGIN_SRC julia
Pkg.add("Plots")
Pkg.add("Lint")
Pkg.add("Gadfly")
Pkg.add("ProfileView")
Pkg.add("CSV")
Pkg.add("StatsBase")
Pkg.add("StatsModels")
Pkg.add("GLM")
Pkg.add("RDatasets")
Pkg.add("IterTools")
Pkg.add("Missings")
Pkg.add("RCall")
Pkg.add("DataFrames")
#+END_SRC

#+RESULTS: install_julia_deps
#+begin_example
INFO: Package Plots is already installed
INFO: Package Lint is already installed
INFO: Package Gadfly is already installed
INFO: Cloning cache of Gtk from https://github.com/JuliaGraphics/Gtk.jl.git
INFO: Cloning cache of GtkReactive from https://github.com/JuliaGizmos/GtkReactive.jl.git
INFO: Cloning cache of IntervalSets from https://github.com/JuliaMath/IntervalSets.jl.git
INFO: Cloning cache of ProfileView from https://github.com/timholy/ProfileView.jl.git
INFO: Cloning cache of Reactive from https://github.com/JuliaGizmos/Reactive.jl.git
INFO: Cloning cache of RoundingIntegers from https://github.com/JuliaMath/RoundingIntegers.jl.git
INFO: Installing Gtk v0.13.1
INFO: Installing GtkReactive v0.4.0
INFO: Installing IntervalSets v0.1.1
INFO: Installing ProfileView v0.3.0
INFO: Installing Reactive v0.6.0
INFO: Installing RoundingIntegers v0.0.3
INFO: Building Cairo
INFO: Building Gtk
INFO: Package database updated
INFO: Package CSV is already installed
INFO: Package StatsBase is already installed
INFO: Package StatsModels is already installed
INFO: Package GLM is already installed
INFO: Package RDatasets is already installed
#+end_example

#+NAME: update_julia_pkg
#+HEADER:  :results output :session *julia*
#+BEGIN_SRC julia
Pkg.update()
#+END_SRC

#+RESULTS: update_julia_pkg
: INFO: Updating METADATA...
: WARNING: Package ASTInterpreter: skipping update (dirty)...
: INFO: Updating Gallium master...
: INFO: Computing changes...
: INFO: No packages to install, update or remove

** R
Installing *R* dependencies:
#+NAME: install_r_deps
#+HEADER: :results output :exports both :session *R*
#+BEGIN_SRC R
install.packages(c("ggplot2", "dplyr", "tidyr", "rjson", "GGally",
                 "plotly", "rPref", "pracma", "FrF2", "AlgDesign",
                 "quantreg"),
                 repos = "https://mirror.ibcp.fr/pub/CRAN/")
#+END_SRC

#+RESULTS: install_r_deps

** Modifying & Analysing the FPGA Data Set
Cloning and updating the =legup-tuner= repository:

#+NAME: update_legup_tuner
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/legup-tuner.git || (cd legup-tuner && git pull)
#+END_SRC

Export your path to =repository_dir= variable:

#+name: repository_dir
#+begin_src sh :results output :exports both
pwd | tr -d "\n"
#+end_src

** Updating & Cloning Repositories
*** GPU Autotuning Screening Experiment
#+NAME: update_screening_experiment
#+BEGIN_SRC sh :results output
git clone https://github.com/phrb/autotuning_screening_experiment.git || (cd autotuning_screening_experiment && git pull)
#+END_SRC
* 2019
** November
*** [2019-11-25 Mon]
**** Gaussian Process Regression on SPAPT kernels      :ExportableReports:
:PROPERTIES:
:EXPORT_FILE_NAME: gpr-spapt-kernels.pdf
:END:
***** Loading Data Functions                                   :noexport:
#+HEADER: :results output :session *R* :exports none
#+BEGIN_SRC R
library(dplyr)
library(stringr)
library(ggplot2)
library(rPref)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

read_gpr <- function(current_path, application) {
  read_experiment <- function(x) {
    data <- read.csv(x, header = TRUE, stringsAsFactors = FALSE)
    data$experiment_id <- str_split(str_split(x, "/")[[1]][6], "_")[[1]][5]
    return(data)
  }

  current_experiment <- current_path
  target_path <- paste(current_experiment, application, sep = "/")

  data_dir <- target_path
  target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
  data <- NULL

  read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)

    data$experiment_id <- str_split(csv_file, "/")[[1]][6]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
  }

  for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
      csv_files <- paste0(target_path, csv_files)

      info <- file.info(csv_files)
      non_empty <- rownames(info[info$size != 0, ])
      csv_files <- csv_files[csv_files %in% non_empty]
      target_data <- lapply(csv_files, read.csv.iterations.cost)
      target_data <- bind_rows(target_data)
      target_data$application <- rep(target_dir, nrow(target_data))

      if (is.null(data)) {
        data <- target_data
      } else {
        data <- bind_rows(data, target_data)
      }
    }
  }


  full_data <- data

  plot_data <- data %>%
    group_by(experiment_id) %>%
    mutate(mean_cost_baseline = mean(cost_baseline)) %>%
    mutate(label_center_x = mean(cost_mean)) %>%
    mutate(label_center_y = mean(best_iteration)) %>%
    ungroup()

  complete_plot_data <- plot_data

  return(list(complete_plot_data, full_data))
}

read_rs <- function(current_path, application, complete_plot_data, full_data){
  data_dir <- current_path
  target_dirs <- c(application)
  rs_data <- NULL

  read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)
    data$experiment_id <- str_split(csv_file, "/")[[1]][6]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
  }

  for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
      csv_files <- paste0(target_path, csv_files)

      info <- file.info(csv_files)
      non_empty <- rownames(info[info$size != 0, ])
      csv_files <- csv_files[csv_files %in% non_empty]
      target_data <- lapply(csv_files, read.csv.iterations.cost)
      target_data <- bind_rows(target_data)
                                        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
      target_data$application <- rep(target_dir, nrow(target_data))

      if (is.null(rs_data)) {
        rs_data <- target_data
      } else {
        rs_data <- bind_rows(rs_data, target_data)
      }
    }
  }

  full_data <- bind_rows(filter(full_data, technique == "GPR"),
                         filter(rs_data, technique == "RS"))

  rs_plot_data <- rs_data %>%
    filter(technique == "RS") %>%
    group_by(experiment_id) %>%
    mutate(mean_cost_baseline = mean(cost_baseline)) %>%
    mutate(label_center_x = mean(cost_mean)) %>%
    mutate(label_center_y = mean(best_iteration)) %>%
    ungroup()

  complete_plot_data <- bind_rows(filter(complete_plot_data, technique == "GPR"),
                                  rs_plot_data)

  return(list(complete_plot_data, full_data))
}
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Attaching package: ‘rPref’

The following object is masked from ‘package:dplyr’:

    between

Registering fonts with R
#+end_example
***** Results for =bicgkernel=
****** Loading Data                                           :noexport:
#+begin_SRC R :results output :session *R* :exports none
gpr_data <- read_gpr("dlmt_spapt_experiments/data/tests/gpr_expanded_ss", "bicgkernel")
rs_data <- read_rs("dlmt_spapt_experiments/data/tests/rs_baseline", "bicgkernel", gpr_data[[1]], gpr_data[[2]])

complete_plot_data <- rs_data[[1]]
full_data <- rs_data[[2]]
#+end_SRC

#+RESULTS:

****** Figures
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

gpr_plot_data <- gpr_plot_data %>%
  mutate(first_step = as.factor(step == 1))

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point(aes(color = first_step), alpha = .5, show.legend = FALSE) +
  geom_step(data = pareto_data) +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  scale_y_log10() +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set1") +
  scale_y_log10() +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-gmALUx/figuretyK4z6.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports none :eval no-export
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red.
#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-oICNIQ/figure7ZwcnW.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-gmALUx/figureuw3ezq.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports none :eval no-export
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()


ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figure8E0drB.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports results
library(dplyr)

large_plot_data <- complete_plot_data %>%
  group_by(technique, experiment_id) %>%
  filter(cost_mean == min_run_cost) %>%
  ungroup()

alpha <- 0.05

# large_plot_data <- large_plot_data %>%
#   group_by(technique) %>%
#   mutate(mean_runs = mean(min_run_cost)) %>%
#   mutate(sd_runs = sd(min_run_cost)) %>%
#   mutate(ci95_runs = qnorm(.95) * (mean(min_run_cost) / sqrt(length(subset(large_plot_data, technique == technique)$cost_mean)))) %>%
#   ungroup()

large_plot_data <- large_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  mutate(ci95_runs = qt((1 - (alpha / 2)), df = n() - 1) *
           (sd(min_run_cost) /
            sqrt(n()))) %>%
  ungroup()

ggplot(large_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique)) +
  facet_wrap(technique ~ ., scales = "free_x") +
  geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_smooth(method = "lm",
  #             formula = "y ~ 1") +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs,
  #                 ymax = mean_runs + sd_runs),
  #             fill = "red",
  #             alpha = 0.1,
  #             colour = NA) +
  geom_ribbon(aes(ymin = mean_runs - ci95_runs,
                  ymax = mean_runs + ci95_runs),
              fill = "grey70",
              alpha = 0.4,
              colour = NA) +
  geom_point(size = 3) +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  # geom_hline(aes(yintercept = mean_cost_baseline[1],
  #                linetype = "-O3"),
  #            color = "black") +
  theme_bw(base_size = 25) +
  scale_color_brewer(palette = "Set1") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("black")))) +
  theme(legend.position = c(0.8, 0.1),
        legend.direction = "horizontal",
        strip.background = element_rect(fill = "white"),
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureW2AQ15.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports none :eval no-export
short_plot_data <- complete_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  ungroup()

short_plot_data <- short_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

ggplot(short_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique,
                            group = technique)) +
  facet_wrap(technique ~ .) +
  # geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs, ymax = mean_runs + sd_runs), fill = "grey70", alpha = 0.4, colour = NA) +
  geom_smooth(method = "lm",
              formula = "y ~ 1") +
  geom_point() +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-gmALUx/figureKj92Ux.pdf]]
***** Results for =atax=
****** Loading Data                                           :noexport:
#+begin_SRC R :results output :session *R* :exports results
complete_plot_data <- NULL
full_data <- NULL

gpr_data <- read_gpr("dlmt_spapt_experiments/data/tests/gpr_expanded_ss", "atax")
rs_data <- read_rs("dlmt_spapt_experiments/data/tests/rs_baseline", "atax", gpr_data[[1]], gpr_data[[2]])

complete_plot_data <- rs_data[[1]]
full_data <- rs_data[[2]]
#+end_SRC

#+RESULTS:

****** Figures
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

gpr_plot_data <- gpr_plot_data %>%
  mutate(first_step = as.factor(step == 1))

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point(aes(color = first_step), alpha = .5, show.legend = FALSE) +
  geom_step(data = pareto_data) +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set1") +
  scale_y_log10() +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-dNt8Ki/figure5r0tu0.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10  :exports none :eval no-export
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.0, 2.5) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red.
#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureINt2cB.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureXqbH8R.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports none :eval no-export
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()


ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figure8E0drB.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports results
library(dplyr)

large_plot_data <- complete_plot_data %>%
  group_by(technique, experiment_id) %>%
  filter(cost_mean == min_run_cost) %>%
  ungroup()

alpha <- 0.05

# large_plot_data <- large_plot_data %>%
#   group_by(technique) %>%
#   mutate(mean_runs = mean(min_run_cost)) %>%
#   mutate(sd_runs = sd(min_run_cost)) %>%
#   mutate(ci95_runs = qnorm(.95) * (mean(min_run_cost) / sqrt(length(subset(large_plot_data, technique == technique)$cost_mean)))) %>%
#   ungroup()

large_plot_data <- large_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  mutate(ci95_runs = qt((1 - (alpha / 2)), df = n() - 1) *
           (sd(min_run_cost) /
            sqrt(n()))) %>%
  ungroup()

ggplot(large_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique)) +
  facet_wrap(technique ~ ., scales = "free_x") +
  geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_smooth(method = "lm",
  #             formula = "y ~ 1") +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs,
  #                 ymax = mean_runs + sd_runs),
  #             fill = "red",
  #             alpha = 0.1,
  #             colour = NA) +
  geom_ribbon(aes(ymin = mean_runs - ci95_runs,
                  ymax = mean_runs + ci95_runs),
              fill = "grey70",
              alpha = 0.4,
              colour = NA) +
  geom_point(size = 3) +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  # geom_hline(aes(yintercept = mean_cost_baseline[1],
  #                linetype = "-O3"),
  #            color = "black") +
  theme_bw(base_size = 25) +
  scale_color_brewer(palette = "Set1") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("black")))) +
  theme(legend.position = c(0.8, 0.1),
        legend.direction = "horizontal",
        strip.background = element_rect(fill = "white"),
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureJCSGZ2.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports none :eval no-export
short_plot_data <- complete_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  ungroup()

short_plot_data <- short_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

ggplot(short_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique,
                            group = technique)) +
  facet_wrap(technique ~ .) +
  # geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs, ymax = mean_runs + sd_runs), fill = "grey70", alpha = 0.4, colour = NA) +
  geom_smooth(method = "lm",
              formula = "y ~ 1") +
  geom_point() +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-oICNIQ/figure7QGS5X.pdf]]
***** Results for =dgemv=
****** Loading Data                                           :noexport:
#+begin_SRC R :results output :session *R* :exports results
complete_plot_data <- NULL
full_data <- NULL

#gpr_data <- read_gpr("dlmt_spapt_experiments/data/tests/gpr_expanded_ss", "dgemv")
gpr_data <- read_gpr("dlmt_spapt_experiments/data/tests/gpr_trend_sd", "dgemv")
rs_data <- read_rs("dlmt_spapt_experiments/data/tests/rs_baseline", "dgemv", gpr_data[[1]], gpr_data[[2]])

complete_plot_data <- rs_data[[1]]
full_data <- rs_data[[2]]
#+end_SRC

#+RESULTS:

****** Figures
#+begin_SRC R :results output :session *R* :eval no-export :exports results
(2*2000^3)/307.2e9
#+end_SRC

#+RESULTS:
: [1] 0.05208333

#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(ggplot2)
library(dplyr)
library(sensitivity)
library(DiceKriging)

gpr_plot_data <- subset(complete_plot_data, technique == "GPR") %>%
  mutate(first_step = as.factor(step == 1)) %>%
  filter(first_step == TRUE) %>%
  filter(SCR == "True") %>%
  select(-SCR) %>%
  ungroup()

samples <- gpr_plot_data %>%
  select(-id, -technique, -mean_confidence_interval_inf,
         -mean_confidence_interval_sup, -cost_std,
         -baseline, -runs, -step,
         -correct_result, -experiment_id, -cost_baseline, -cost_baseline,
         -speedup,-max_run_speedup, -min_run_cost, -best_iteration,
         -iteration, -points, -application, -mean_cost_baseline,
         -label_center_x, -label_center_y, -first_step) %>%
  mutate_if(is.character, as.factor) %>%
  mutate_if(is.factor, as.numeric)


#km_design <- sample_n(samples, 200)
km_design <- samples

model <- km(design = select(km_design, -cost_mean),
            response = km_design$cost_mean)

x <- sobolGP(model = model,
             type = "UK",
             X1 = samples %>%
               head(n = floor(nrow(.) / 2)) %>%
               select(-cost_mean),
             X2 = samples %>%
               tail(n = floor(nrow(.) / 2)) %>%
               select(-cost_mean),
             nboot = 100)
#+end_SRC

#+RESULTS:
#+begin_example


optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  matern5_2
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10
  - parameters upper bounds :  4094 4094 62 62 2 62 4094 2 4094 4094 62 2 4094 4094 4094 4094 2 2 2 4094 2 4094 2 4094 2 30 30 4094 2 30 30 30 30 30 30 30 30 30 62 62 4094 4094 30 4094 30 62 62 30
  - best initial criterion value(s) :  -712.9143

N = 48, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       712.91  |proj g|=      0.15272
At iterate     1  f =       712.88  |proj g|=       0.15294
ys=-1.679e-05  -gs= 3.601e-02, BFGS update SKIPPED
At iterate     2  f =       711.11  |proj g|=       0.16401
At iterate     3  f =       708.01  |proj g|=       0.18559
ys=-1.555e-01  -gs= 3.027e+00, BFGS update SKIPPED
At iterate     4  f =       703.39  |proj g|=       0.22168
ys=-4.679e-01  -gs= 4.374e+00, BFGS update SKIPPED
At iterate     5  f =       698.95  |proj g|=       0.25502
ys=-3.892e-01  -gs= 4.231e+00, BFGS update SKIPPED
At iterate     6  f =       690.43  |proj g|=        0.2156
At iterate     7  f =       688.99  |proj g|=        0.1577
At iterate     8  f =          688  |proj g|=      0.010538
At iterate     9  f =       687.94  |proj g|=      0.013195
At iterate    10  f =       687.66  |proj g|=       0.04427
At iterate    11  f =       687.49  |proj g|=      0.023682
At iterate    12  f =       687.24  |proj g|=      0.010114
At iterate    13  f =       687.22  |proj g|=      0.010098
At iterate    14  f =       687.17  |proj g|=      0.010069
At iterate    15  f =       686.88  |proj g|=       0.01359
At iterate    16  f =       686.58  |proj g|=     0.0094514
At iterate    17  f =       686.45  |proj g|=       0.02374
At iterate    18  f =       686.28  |proj g|=       0.03131
At iterate    19  f =       686.14  |proj g|=      0.015667
At iterate    20  f =       685.98  |proj g|=     0.0087571
At iterate    21  f =       685.91  |proj g|=     0.0086694
At iterate    22  f =       685.87  |proj g|=     0.0086355
At iterate    23  f =       685.25  |proj g|=      0.028855
At iterate    24  f =        684.9  |proj g|=      0.015802
At iterate    25  f =       684.16  |proj g|=      0.006761
At iterate    26  f =       682.51  |proj g|=      0.012195
At iterate    27  f =       682.18  |proj g|=     0.0074427
At iterate    28  f =       682.16  |proj g|=     0.0048397
At iterate    29  f =       681.51  |proj g|=      0.046568
At iterate    30  f =       681.47  |proj g|=       0.01972
At iterate    31  f =       681.46  |proj g|=     0.0042831
At iterate    32  f =       681.45  |proj g|=     0.0042837
At iterate    33  f =       681.45  |proj g|=     0.0042772
At iterate    34  f =       681.44  |proj g|=      0.016674
At iterate    35  f =       680.36  |proj g|=       0.03849
At iterate    36  f =       677.18  |proj g|=     0.0067063
At iterate    37  f =          677  |proj g|=     0.0030387
At iterate    38  f =       676.96  |proj g|=     0.0038301
At iterate    39  f =       676.81  |proj g|=     0.0046546
At iterate    40  f =       676.64  |proj g|=      0.063529
At iterate    41  f =       676.54  |proj g|=     0.0050253
At iterate    42  f =       676.39  |proj g|=      0.016501
At iterate    43  f =       675.15  |proj g|=        0.1002
At iterate    44  f =       674.62  |proj g|=      0.063694
At iterate    45  f =       674.47  |proj g|=      0.013433
At iterate    46  f =       674.38  |proj g|=      0.016088
At iterate    47  f =       674.23  |proj g|=      0.045203
At iterate    48  f =       673.83  |proj g|=      0.081005
At iterate    49  f =       672.99  |proj g|=       0.10431
At iterate    50  f =       672.61  |proj g|=      0.053047
At iterate    51  f =       672.01  |proj g|=      0.022967
At iterate    52  f =       671.77  |proj g|=      0.042637
At iterate    53  f =       671.06  |proj g|=      0.071004
At iterate    54  f =       670.98  |proj g|=      0.021088
At iterate    55  f =       670.97  |proj g|=    0.00061831
At iterate    56  f =       670.97  |proj g|=    0.00083355
At iterate    57  f =       670.97  |proj g|=     0.0025396
At iterate    58  f =       670.97  |proj g|=     0.0051888
At iterate    59  f =       670.96  |proj g|=     0.0094556
At iterate    60  f =       670.96  |proj g|=      0.015551
At iterate    61  f =       670.95  |proj g|=      0.023173
At iterate    62  f =       670.93  |proj g|=      0.028516
At iterate    63  f =       670.89  |proj g|=      0.021213
At iterate    64  f =       670.87  |proj g|=      0.014479
At iterate    65  f =       670.83  |proj g|=     0.0023803
At iterate    66  f =       670.77  |proj g|=      0.018208
At iterate    67  f =       670.66  |proj g|=      0.032607
At iterate    68  f =       670.64  |proj g|=     0.0050425
At iterate    69  f =        670.4  |proj g|=     0.0040009
At iterate    70  f =       670.17  |proj g|=     0.0085998
At iterate    71  f =       669.94  |proj g|=     0.0020373
At iterate    72  f =       669.93  |proj g|=     0.0073922
At iterate    73  f =        669.9  |proj g|=      0.021443
At iterate    74  f =       669.81  |proj g|=      0.048122
At iterate    75  f =       669.61  |proj g|=      0.079202
At iterate    76  f =       669.41  |proj g|=      0.080633
At iterate    77  f =       669.22  |proj g|=      0.028117
At iterate    78  f =       669.18  |proj g|=     0.0045062
At iterate    79  f =       669.18  |proj g|=     0.0010684
At iterate    80  f =       669.18  |proj g|=     0.0019219
At iterate    81  f =       669.18  |proj g|=     0.0021834
At iterate    82  f =       669.17  |proj g|=    0.00082061
At iterate    83  f =       669.16  |proj g|=     0.0052274
At iterate    84  f =       669.14  |proj g|=     0.0096021
At iterate    85  f =        669.1  |proj g|=     0.0028422
At iterate    86  f =       669.06  |proj g|=      0.033474
At iterate    87  f =       668.99  |proj g|=      0.026382
At iterate    88  f =       668.66  |proj g|=      0.010887
At iterate    89  f =       668.62  |proj g|=     0.0090599
At iterate    90  f =       668.57  |proj g|=     0.0061299
At iterate    91  f =       668.26  |proj g|=      0.014394
At iterate    92  f =       667.89  |proj g|=      0.028062
At iterate    93  f =       667.26  |proj g|=      0.063625
At iterate    94  f =       666.72  |proj g|=      0.027098
At iterate    95  f =       666.13  |proj g|=      0.011637
At iterate    96  f =       665.98  |proj g|=     0.0075273
At iterate    97  f =       665.86  |proj g|=      0.011719
At iterate    98  f =       665.54  |proj g|=      0.023306
At iterate    99  f =       665.46  |proj g|=         0.026
At iterate   100  f =       665.33  |proj g|=      0.012704
At iterate   101  f =       664.82  |proj g|=       0.01516
final  value 664.821889
stopped after 101 iterations
#+end_example

#+begin_SRC R :results graphics output :session *R* :file "/tmp/figure21213.pdf") :width 13 :height 10 :eval no-export
library(ggplot2)
df <- data.frame(mean = x$S$mean[1, ],
                 var = x$S$var[1, ],
                 varPG = x$S$varPG[1, ],
                 ci = sqrt(x$S$var[1, ]) * 1.96,
                 ci_inf = x$S$ci[1, ],
                 ci_sup = x$S$ci[2, ]
                 )

df <- df %>%
  mutate(id = row_number())

ggplot(df) +
  geom_point(aes(x = id, y = mean), size = 4) +
  geom_errorbar(aes(x = id, ymin = mean - ci, ymax = mean + ci)) +
  #geom_errorbar(aes(x = id, ymin = mean - varPG, ymax = mean + varPG)) +
  theme_bw(base_size = 28)
#+end_SRC

#+RESULTS:
[[file:/tmp/figure21213.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id, technique) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

gpr_plot_data <- gpr_plot_data %>%
  mutate(first_step = as.factor(step == 1)) %>%
  mutate(scr_on = as.factor(SCR == "True"))

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  #geom_rect(aes(xmin = 1.0, xmax = dim(filter(gpr_plot_data, step == 1))[1], ymin = 1., ymax = 5.), alpha = 0.5) +
  geom_point(aes(color = scr_on), alpha = .5, show.legend = FALSE) +
  geom_step(data = pareto_data) +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set1") +
  #scale_y_log10() +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank()) +
  facet_wrap(~ experiment_id)
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-XCC5io/figurecMFz1v.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id, technique) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

gpr_plot_data <- gpr_plot_data %>%
  mutate(first_step = as.factor(step == 1)) %>%
  mutate(scr_on = as.factor(SCR == "True"))

gpr_plot_data <- gpr_plot_data %>%
    filter(first_step == TRUE) %>%
    select(-scr_on, -first_step, -label_center_y, -label_center_x,
           -mean_cost_baseline, -application, -points, -iteration,
           -best_iteration, -min_run_cost, -max_run_speedup, -speedup,
           -cost_baseline, -experiment_id, -correct_result, -step,
           -cost_mean, -mean_confidence_interval_sup, -baseline, -runs,
           -mean_confidence_interval_inf, -cost_std, -technique, -id) %>%
    mutate(across(where(is.character), as.factor)) %>%
    mutate(across(where(is.factor), as.numeric)) %>%
    gather("Parameter", "Value")

ggplot(gpr_plot_data) +
    geom_histogram(aes(x = Value)) +
    ylab("Execution Time") +
    xlab("Iteration") +
    theme_bw(base_size = 18) +
    facet_wrap(~ Parameter, scale = "free_x")
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-XCC5io/figure4SaaD4.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports none :eval no-export
gpr_plot_data <- subset(complete_plot_data, technique == "GPR")

pareto_data <- gpr_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(gpr_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_rect(aes(xmin = 1.0, xmax = 150., ymin = 1., ymax = 5.), alpha = 0.2) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.0, 2.5) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  scale_color_brewer(palette = "Set3") +
  scale_y_log10() +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red.
#+CAPTION: A closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-XCC5io/figurePmbIYM.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports results
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()

ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_rect(aes(xmin = 1.0, xmax = 150., ymin = 1., ymax = 5.), alpha = 0.2) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0, NA) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_y_log10() +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-XCC5io/figurenXziDX.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 10 :exports none :eval no-export
rs_plot_data <- subset(complete_plot_data, technique == "RS")

pareto_data <- rs_plot_data %>%
  group_by(experiment_id) %>%
  psel(low(iteration) * low(cost_mean)) %>%
  ungroup()


ggplot(rs_plot_data, aes(y = cost_mean, x = iteration)) +
  facet_wrap(experiment_id ~ .) +
  geom_point() +
  geom_step(data = pareto_data, color = "red") +
  ylim(0.35, 0.6) +
  ylab("Execution Time") +
  xlab("Iteration") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set3") +
  geom_hline(aes(yintercept = mean_cost_baseline[1], linetype = "-O3"), color = "blue") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("blue")))) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Execution time of points measured by RS along iterations,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-6kiapU/figure8E0drB.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports results
library(dplyr)

large_plot_data <- complete_plot_data %>%
  group_by(technique, experiment_id) %>%
  filter(cost_mean == min_run_cost) %>%
  ungroup()

alpha <- 0.05

# large_plot_data <- large_plot_data %>%
#   group_by(technique) %>%
#   mutate(mean_runs = mean(min_run_cost)) %>%
#   mutate(sd_runs = sd(min_run_cost)) %>%
#   mutate(ci95_runs = qnorm(.95) * (mean(min_run_cost) / sqrt(length(subset(large_plot_data, technique == technique)$cost_mean)))) %>%
#   ungroup()

large_plot_data <- large_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  mutate(ci95_runs = qt((1 - (alpha / 2)), df = n() - 1) *
           (sd(min_run_cost) /
            sqrt(n()))) %>%
  ungroup()

ggplot(large_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique)) +
  facet_wrap(technique ~ ., scales = "free_x") +
  geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_smooth(method = "lm",
  #             formula = "y ~ 1") +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs,
  #                 ymax = mean_runs + sd_runs),
  #             fill = "red",
  #             alpha = 0.1,
  #             colour = NA) +
  geom_ribbon(aes(ymin = mean_runs - ci95_runs,
                  ymax = mean_runs + ci95_runs),
              fill = "grey70",
              alpha = 0.4,
              colour = NA) +
  geom_point(size = 3) +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  # geom_hline(aes(yintercept = mean_cost_baseline[1],
  #                linetype = "-O3"),
  #            color = "black") +
  theme_bw(base_size = 25) +
  scale_color_brewer(palette = "Set1") +
  scale_linetype_manual(name = "limit", values = c(2),
                        guide = guide_legend(override.aes = list(color = c("black")))) +
  theme(legend.position = c(0.8, 0.1),
        legend.direction = "horizontal",
        strip.background = element_rect(fill = "white"),
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-XCC5io/figureh6PD8d.pdf]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".pdf") :width 13 :height 7 :exports none :eval no-export
short_plot_data <- complete_plot_data %>%
  group_by(technique) %>%
  mutate(mean_runs = mean(min_run_cost)) %>%
  mutate(sd_runs = sd(min_run_cost)) %>%
  ungroup()

short_plot_data <- short_plot_data %>%
  group_by(technique, experiment_id) %>%
  subset(cost_mean == min_run_cost) %>%
  ungroup()

ggplot(short_plot_data, aes(y = min_run_cost,
                            x = best_iteration,
                            color = technique,
                            group = technique)) +
  facet_wrap(technique ~ .) +
  # geom_hline(aes(yintercept = mean_runs), linetype = 3) +
  # geom_ribbon(aes(ymin = mean_runs - sd_runs, ymax = mean_runs + sd_runs), fill = "grey70", alpha = 0.4, colour = NA) +
  geom_smooth(method = "lm",
              formula = "y ~ 1") +
  geom_point() +
  xlab("Iteration where Best was Found") +
  ylab("Best Cost in Seconds") +
  theme_bw(base_size = 18) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
#+end_SRC

#+CAPTION: Results with a starting sample of size 15, steps add the 13 best predictions,
#+CAPTION: a closer look at the data from the previous figure
#+LABEL: fig:a
#+ATTR_LATEX: :width 0.65\textwidth :placement [H]
#+RESULTS:
[[file:/tmp/babel-XCC5io/figureOdNMsD.pdf]]
** December
*** [2019-12-10 Tue]
**** Graphs for Gaussian Process Regression
***** Sampling from Multivariate Normal Distributions
****** 2-Dimensional Example
Sampling from a 2-dimension normal distribution, with no correlation between dimensions,
that is, identity covariance matrix:

|----+-----+-----|
|    |  v1 |  v2 |
|----+-----+-----|
| v1 | 1.0 | 0.0 |
| v2 | 0.0 | 1.0 |
|----+-----+-----|

#+begin_SRC R :results output :session *R*
library(latex2exp)
library(MASS)

n <- 2
sigma <- data.frame(d1 = c(1, 0), d2 = c(0, 1))
means <- c(0, 0)

mv_sample <- mvrnorm(n = 300, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘MASS’

The following object is masked from ‘package:patchwork’:

    area

The following object is masked from ‘package:dplyr’:

    select
#+end_example

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 28)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureUeKgAu.png]]

To get strong correlations, the covariance matrix could be:

|----+-----+-----|
|    |  v1 |  v2 |
|----+-----+-----|
| v1 | 1.0 | 0.8 |
| v2 | 0.8 | 1.0 |
|----+-----+-----|

#+begin_SRC R :results output :session *R*
library(MASS)

n <- 2
sigma <- data.frame(d1 = c(1, 0.8), d2 = c(0.8, 1))
means <- c(0, 0)

mv_sample <- mvrnorm(n = 600, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("V", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 26)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureiaJjWN.png]]

****** 10-Dimensional Example: Reinterpreting Samples
#+begin_SRC R :results output :session *R*
library(latex2exp)
library(MASS)

n <- 10
sigma <- data.frame(diag(10))
names(sigma) <- paste("d", seq(1, n), sep = "")

means <- rep(0, n)

mv_sample <- mvrnorm(n = 300, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("d", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 22)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figurec4TROr.png]]

#+begin_SRC R :results output :session *R*
library(latex2exp)
library(MASS)

n <- 10
sigma <- data.frame(diag(10))
names(sigma) <- paste("d", seq(1, n), sep = "")

means <- rep(0, n)

mv_sample <- mvrnorm(n = 1000, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("d", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)

plot_data <- mv_sample
sampled_function <- sample_n(plot_data, 1)

plot_data <- plot_data %>%
  gather("x", "f_x") %>%
  mutate(x = ordered(x, levels = names(mv_sample)))

sampled_function <- sampled_function %>%
  gather("x", "f_x") %>%
  mutate(x = ordered(x, levels = names(mv_sample)))

ggplot(plot_data, aes(x = x, y = f_x)) +
  geom_jitter(color = "gray48", size = 3, width = 0.25, alpha = 0.2) +
  geom_point(data = sampled_function,
             aes(color = "Sample of Multivariate Normal"),
             size = 4) +
  geom_line(data = sampled_function,
            color = "red",
            size = 1,
            alpha = 0.3) +
  ylab(TeX("Sampled Values")) +
  xlab(TeX("Dimensions")) +
  scale_fill_manual("", values = "gray48") +
  scale_color_brewer(palette = "Set1") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.24, 0.06))

#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureNntbaA.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)

n <- 10

plot_data <- mv_sample
names(plot_data) <- seq(1, n)

sampled_function <- sample_n(plot_data, 1)

plot_data <- plot_data %>%
  gather("x", "f_x") %>%
  mutate(x = as.numeric(x))

sampled_function <- sampled_function %>%
  gather("x", "f_x") %>%
  mutate(x = as.numeric(x))

ggplot(plot_data, aes(x = x, y = f_x)) +
  geom_jitter(color = "gray48", size = 3, width = 0.25, alpha = 0.2) +
  geom_point(data = sampled_function,
             aes(color = "Sampled Function"),
             size = 4) +
  geom_line(data = sampled_function,
            color = "red",
            size = 1,
            alpha = 0.3) +
  ylab(TeX("Samples of $(d_1,\\ldots,d_{10})$ interpreted as $f(x \\in \\lbrack 1,10 \\rbrack)$")) +
  xlab(TeX("$(d_1,\\ldots,d_{10})$ interpreted as discrete $x \\in \\lbrack 1,10 \\rbrack$")) +
  scale_x_discrete(limits = seq(1, 10)) +
  scale_fill_manual("", values = "gray48") +
  scale_color_brewer(palette = "Set1") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.2, 0.06))

#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figure128cP4.png]]

***** Sampling Functions
#+begin_SRC R :results output :session *R*
d <- 2
n <- 30

target_X <- expand.grid(x1 = seq(-1, 1, length = n), x2 = seq(-1, 1, length = n))
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(ggplot2)
library(GGally)

ggpairs(target_X) +
  theme_bw(base_size = 26)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figure1PC6N4.png]]

#+begin_SRC R :results output :session *R*
library(Rcpp)
library(dplyr)
library(MASS)

src <-
"#include <Rcpp.h>
#include <math.h>

using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix rcpp_squared_exponential_kernel(NumericMatrix x,
                                              NumericMatrix y,
                                              float amplitude,
                                              NumericVector lengthscale){
  NumericMatrix output(x.nrow(), y.nrow());
  float distance;

  for(int i = 0; i < x.nrow(); i++) {
    for(int j = 0; j < y.nrow(); j++) {
      distance = 0;
      for(int k = 0; k < x.ncol(); k++) {
        distance += (((x(i, k) - y(j, k)) *
                      (x(i, k) - y(j, k))) /
                      (lengthscale(k) * lengthscale(k)));
      }

      output(i, j) = amplitude *
                     amplitude *
                     exp(-0.5 * distance);
    }
  }
  return(output);
}"

sourceCpp(code = src)

random_design <- function(factors, size) {
  data.frame(replicate(factors, runif(size)))
}

simulate_gp <- function(covariance_matrix) {
  return(unname(mvrnorm(n = 1,
                        rep(0.0, nrow(covariance_matrix)),
                        covariance_matrix)))
}

sqexp_covariance_matrix <- function(data, amplitude, lengthscale) {
  return(rcpp_squared_exponential_kernel(as.matrix(data),
                                         as.matrix(data),
                                         amplitude,
                                         lengthscale))
}

significance_probability <- 0.10
amplitude <- 1.0
lengthscale <- rep(0.2, n)

cov_matrix <- sqexp_covariance_matrix(target_X, amplitude, lengthscale)
plot_data <- target_X
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(DiceKriging)
library(dplyr)
library(RColorBrewer)
library(lattice)

se_kernel <- function(x, x_p, l = 0.2) {
  return(exp(-(
    ((x - x_p) ^ 2) /
    ((2 * l) ^ 2)
  )))
}

d <- 2
n <- 32

point_grid <- data.frame(x1 = seq(-1, 1, length = n), x2 = rep(0, n))
evaluated_grid <- data.frame(point_grid)

evaluated_grid$y <- se_kernel(point_grid$x1, point_grid$x2)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

ggplot(evaluated_grid, aes(x = x1, y = y)) +
  geom_point() +
  theme_bw(base_size = 18)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureGmKWqY.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(lattice)
library(RColorBrewer)

plot_data$y <- simulate_gp(cov_matrix)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

wireframe(y ~ x1 * x2,
          data = plot_data,
          xlab = "x1",
          ylab = "x2",
          zlab = list("k(x,x')",
                      rot = "90"),
          #zlim = range(seq(0.0, 1.0, by = 0.5)),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          #shade = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE),
          screen = list(z = 20, x = -60, y = 0),
          par.settings = list(axis.line = list(col = "transparent")))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureoOKTS3.png]]

***** Regression, Sampling, EI
#+begin_SRC R :results output :session *R*
library(ggplot2)
library(dplyr)
library(DiceKriging)
library(DiceOptim)

gpr_data <- data.frame(x = c(0.1, 0.25, 0.3, 0.67, 0.8),
                       y = c(0.1, 0.1, 0.6, -0.02, 0.1))

# gpr_data <- data.frame(x = c(0.45, 0.5, 0.55),
#                        y = c(-0.1, 0.08, 0.2))

x_allowed <- data.frame(x = seq(0, 1, length = 300))

# reg <- km(formula = ~ I(x^2), design = dplyr::select(gpr_data, -y),
#           control = list(pop.size = 40,
#                          BFGSburnin = 4),
#           response = gpr_data$y)

reg <- km(design = dplyr::select(gpr_data, -y),
          control = list(pop.size = 400,
                         BFGSburnin = 400),
          response = gpr_data$y)

pred <- predict(reg, x_allowed, "UK")

x_allowed$y <- pred$mean
x_allowed$ymin <- pred$mean - (2 * pred$sd)
x_allowed$ymax <- pred$mean + (2 * pred$sd)
x_allowed$ei <- apply(dplyr::select(x_allowed, x), 1, EI, reg)
x_allowed$sampled_y <- simulate(reg, cond = TRUE, newdata = dplyr::select(x_allowed, x))[1, ]
x_allowed$uncond_y <- simulate(reg, cond = FALSE, newdata = dplyr::select(x_allowed, x))[1, ]
#+end_SRC

#+RESULTS:
#+begin_example


optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  matern5_2
  - nugget : NO
  - parameters lower bounds :  1e-10
  - parameters upper bounds :  1.4
  - best initial criterion value(s) :  0.5438007

N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -0.5438  |proj g|=   1.6681e-60
Derivative >= 0, backtracking line search impossible.final  value -0.543801
stopped after 0 iterations
#+end_example

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
ggplot(data = gpr_data, aes(x = x, y = y)) +
  geom_ribbon(data = x_allowed, aes(x = x, ymin = ymin, ymax = ymax, fill = "CI of the Mean"), alpha = 0.3) +
  geom_line(data = x_allowed, size = 1, aes(color = "Predicted Mean")) +
  #geom_line(data = x_allowed, size = 1, aes(x = x, y = sampled_y, color = "Conditioned Sample")) +
  #geom_line(data = x_allowed, size = 1, aes(x = x, y = uncond_y, color = "Prior Sample")) +
  geom_line(data = x_allowed, size = 1, aes(x = x, y = ei, color = "Expected Improvement")) +
  geom_point(stroke = 2, shape = 3, size = 3, aes(color = "Observed")) +
  geom_point(data = subset(x_allowed, ei == max(ei)), size = 4, stroke = 2, shape = 3, aes(x = x, y = ei, color = "Maximum EI")) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_manual("", values = "gray48") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.2, 0.2))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-VOewyd/figure4dUwKd.png]]

**** Plotting Covariance Kernels
***** Constant
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(DiceKriging)
library(dplyr)
library(lattice)

constant_kernel <- function(x) {
  return(1.0)
}

d <- 2
n <- 16

point_grid <- expand.grid(x1 = seq(0, 1, length = n), x2 = seq(0, 1, length = n))
evaluated_grid <- data.frame(point_grid)

evaluated_grid$y <- apply(point_grid, 1, constant_kernel)

str(evaluated_grid)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

wireframe(y ~ x1 * x2,
          data = evaluated_grid,
          xlab = "x1",
          ylab = "x2",
          zlab = list("k(x,x')",
                      rot = "90"),
          zlim = range(seq(0.0, 2.0, by = 0.5)),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          #shade = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE),
          screen = list(z = 140, x = -60, y = 0),
          par.settings = list(axis.line = list(col = "transparent")))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-jn9WIm/figure7Zu2B4.png]]
***** Squared Exponential
#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(DiceKriging)
library(dplyr)
library(lattice)

se_kernel <- function(x, x_p, l = 0.2) {
  return(exp(-(
    ((x - x_p) ^ 2) /
    ((2 * l) ^ 2)
  )))
}

d <- 2
n <- 32

point_grid <- data.frame(x1 = seq(-1, 1, length = n), x2 = rep(0, n))
evaluated_grid <- data.frame(point_grid)

evaluated_grid$y <- se_kernel(point_grid$x1, point_grid$x2)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

ggplot(evaluated_grid, aes(x = x1, y = y)) +
  geom_point() +
  theme_bw(base_size = 18)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-jn9WIm/figurehyJIns.png]]
*** [2019-12-19 Thu]
**** Estimating GPR Parameters using RS Data
****** Loading Data                                           :noexport:
#+HEADER: :results output :session *R* :exports none
#+BEGIN_SRC R
library(plyr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(rPref)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

# data_dir <- "dlmt_spapt_experiments/data/results"
# data_dir <- "dlmt_spapt_experiments/data/tests/no_binary_random"
# data_dir <- "dlmt_spapt_experiments/data/results"
data_dir <- "dlmt_spapt_experiments/data/tests/random_300_graoully_debnew"
#target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
target_dirs <- c("bicgkernel")
rs_data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE, stringsAsFactor = FALSE)
    data$experiment_id <- str_split(csv_file, "/")[[1]][6]
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$cost_mean == min(data$cost_mean), ])), nrow(data))
    data$iteration <- as.numeric(rownames(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        # target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(rs_data)) {
            rs_data <- target_data
        } else {
            rs_data <- bind_rows(rs_data, target_data)
        }
    }
}

full_data <- filter(rs_data, technique == "RS", application == "bicgkernel")

rs_plot_data <- rs_data %>%
  filter(technique == "RS", application == "bicgkernel") %>%
  group_by(experiment_id) %>%
  mutate(mean_cost_baseline = mean(cost_baseline)) %>%
  mutate(label_center_x = mean(cost_mean)) %>%
  mutate(label_center_y = mean(best_iteration)) %>%
  ungroup()

complete_plot_data <- rs_plot_data

str(complete_plot_data)
#+end_SRC

#+RESULTS:
#+begin_example

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	11927 obs. of  35 variables:
 $ id                          : int  1 2 3 4 5 6 7 8 10 11 ...
 $ T2_J                        : int  512 128 1 4 1 1024 2048 16 128 1024 ...
 $ T2_I                        : int  1 1024 16 2048 2048 32 512 16 256 32 ...
 $ RT_I                        : int  4 1 2 1 1 1 4 32 1 1 ...
 $ mean_confidence_interval_inf: num  3.844 0.439 0.574 0.432 5.218 ...
 $ baseline                    : chr  "False" "False" "False" "False" ...
 $ T1_J                        : int  32 2 32 4 4 64 2048 1 64 512 ...
 $ technique                   : chr  "RS" "RS" "RS" "RS" ...
 $ VEC2                        : chr  "True" "False" "True" "False" ...
 $ VEC1                        : chr  "False" "False" "True" "True" ...
 $ SCR                         : chr  "False" "False" "False" "True" ...
 $ U1_I                        : int  16 26 11 18 24 17 20 5 10 27 ...
 $ RT_J                        : int  16 2 4 32 8 4 2 4 4 4 ...
 $ T1_I                        : int  128 8 4 16 2048 1 1 1 8 16 ...
 $ runs                        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ cost_std                    : num  0.11057 0.15496 0.09848 0.11809 0.00476 ...
 $ cost_mean                   : num  3.912 0.535 0.635 0.505 5.221 ...
 $ U_J                         : int  21 1 24 1 15 1 2 17 29 11 ...
 $ U_I                         : int  1 8 1 12 1 1 1 1 1 1 ...
 $ step                        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ correct_result              : chr  "True" "True" "True" "True" ...
 $ OMP                         : chr  "False" "True" "True" "True" ...
 $ mean_confidence_interval_sup: num  3.981 0.631 0.696 0.578 5.224 ...
 $ experiment_id               : chr  "xeon_e5_2630_v3_graoully-10_1573846466" "xeon_e5_2630_v3_graoully-10_1573846466" "xeon_e5_2630_v3_graoully-10_1573846466" "xeon_e5_2630_v3_graoully-10_1573846466" ...
 $ cost_baseline               : num  4.58 4.58 4.58 4.58 4.58 ...
 $ speedup                     : num  1.172 8.57 7.215 9.075 0.878 ...
 $ max_run_speedup             : num  10 10 10 10 10 ...
 $ min_run_cost                : num  0.458 0.458 0.458 0.458 0.458 ...
 $ best_iteration              : num  146 146 146 146 146 146 146 146 146 146 ...
 $ iteration                   : num  1 2 3 4 5 6 7 8 10 11 ...
 $ points                      : int  297 297 297 297 297 297 297 297 297 297 ...
 $ application                 : chr  "bicgkernel" "bicgkernel" "bicgkernel" "bicgkernel" ...
 $ mean_cost_baseline          : num  4.58 4.58 4.58 4.58 4.58 ...
 $ label_center_x              : num  2.1 2.1 2.1 2.1 2.1 ...
 $ label_center_y              : num  146 146 146 146 146 146 146 146 146 146 ...
#+end_example
****** Analyzing Data
#+begin_SRC R :results output :session *R*
results_data <- select(complete_plot_data,
                       OMP, SCR, VEC1, VEC2,
                       RT_I, RT_J, U1_I, T1_I,
                       T1_J, T2_I, T2_J, U_I, U_J,
                       cost_mean)
#+end_SRC

#+RESULTS:

#+begin_SRC R :results output :session *R*
summary(aov(cost_mean ~ .*., results_data))
#+end_SRC

#+RESULTS:
#+begin_example
               Df Sum Sq Mean Sq   F value   Pr(>F)
OMP             1  19192   19192 38112.189  < 2e-16 ***
SCR             1   3272    3272  6498.543  < 2e-16 ***
VEC1            1      0       0     0.001 0.970549
VEC2            1      1       1     1.624 0.202602
RT_I            1    169     169   336.253  < 2e-16 ***
RT_J            1     65      65   129.166  < 2e-16 ***
U1_I            1      0       0     0.283 0.594502
T1_I            1      5       5     9.617 0.001933 **
T1_J            1     36      36    71.053  < 2e-16 ***
T2_I            1      2       2     3.997 0.045608 *
T2_J            1     12      12    24.159 8.99e-07 ***
U_I             1     64      64   127.359  < 2e-16 ***
U_J             1     59      59   117.487  < 2e-16 ***
OMP:SCR         1   2844    2844  5647.467  < 2e-16 ***
OMP:VEC1        1      0       0     0.259 0.611151
OMP:VEC2        1      1       1     2.758 0.096771 .
OMP:RT_I        1    150     150   298.553  < 2e-16 ***
OMP:RT_J        1     62      62   123.269  < 2e-16 ***
OMP:U1_I        1      0       0     0.092 0.761103
OMP:T1_I        1      5       5     9.749 0.001799 **
OMP:T1_J        1     35      35    69.874  < 2e-16 ***
OMP:T2_I        1      3       3     5.981 0.014475 *
OMP:T2_J        1     10      10    20.631 5.62e-06 ***
OMP:U_I         1     45      45    88.417  < 2e-16 ***
OMP:U_J         1     43      43    84.456  < 2e-16 ***
SCR:VEC1        1      1       1     1.040 0.307732
SCR:VEC2        1      2       2     3.950 0.046888 *
SCR:RT_I        1     10      10    19.513 1.01e-05 ***
SCR:RT_J        1      1       1     1.041 0.307683
SCR:U1_I        1      0       0     0.809 0.368458
SCR:T1_I        1      0       0     0.944 0.331208
SCR:T1_J        1     14      14    26.834 2.25e-07 ***
SCR:T2_I        1      0       0     0.061 0.804390
SCR:T2_J        1     11      11    22.349 2.30e-06 ***
SCR:U_I         1      2       2     4.720 0.029841 *
SCR:U_J         1      1       1     1.997 0.157646
VEC1:VEC2       1      0       0     0.068 0.793657
VEC1:RT_I       1      0       0     0.526 0.468237
VEC1:RT_J       1      0       0     0.267 0.605334
VEC1:U1_I       1      0       0     0.306 0.580000
VEC1:T1_I       1      1       1     1.181 0.277249
VEC1:T1_J       1      0       0     0.371 0.542290
VEC1:T2_I       1      0       0     0.001 0.976130
VEC1:T2_J       1      0       0     0.935 0.333683
VEC1:U_I        1      0       0     0.332 0.564436
VEC1:U_J        1      0       0     0.240 0.624535
VEC2:RT_I       1      2       2     4.620 0.031620 *
VEC2:RT_J       1      0       0     0.109 0.740790
VEC2:U1_I       1      0       0     0.742 0.389009
VEC2:T1_I       1      2       2     3.216 0.072926 .
VEC2:T1_J       1      0       0     0.470 0.493163
VEC2:T2_I       1      0       0     0.158 0.690606
VEC2:T2_J       1      0       0     0.429 0.512448
VEC2:U_I        1      1       1     2.582 0.108095
VEC2:U_J        1      0       0     0.576 0.447972
RT_I:RT_J       1     43      43    85.147  < 2e-16 ***
RT_I:U1_I       1      1       1     1.256 0.262507
RT_I:T1_I       1      5       5     9.666 0.001881 **
RT_I:T1_J       1      4       4     7.960 0.004791 **
RT_I:T2_I       1      3       3     5.370 0.020508 *
RT_I:T2_J       1      4       4     7.676 0.005605 **
RT_I:U_I        1      3       3     5.409 0.020053 *
RT_I:U_J        1      1       1     2.243 0.134269
RT_J:U1_I       1      0       0     0.031 0.859404
RT_J:T1_I       1      6       6    11.039 0.000895 ***
RT_J:T1_J       1      0       0     0.282 0.595687
RT_J:T2_I       1      0       0     0.433 0.510771
RT_J:T2_J       1      1       1     1.434 0.231172
RT_J:U_I        1      0       0     0.217 0.641335
RT_J:U_J        1      0       0     0.585 0.444387
U1_I:T1_I       1      1       1     1.514 0.218576
U1_I:T1_J       1      1       1     2.152 0.142439
U1_I:T2_I       1      0       0     0.087 0.768016
U1_I:T2_J       1      4       4     7.919 0.004899 **
U1_I:U_I        1      0       0     0.635 0.425402
U1_I:U_J        1      0       0     0.107 0.743765
T1_I:T1_J       1      1       1     1.064 0.302230
T1_I:T2_I       1      4       4     8.085 0.004471 **
T1_I:T2_J       1      0       0     0.116 0.733167
T1_I:U_I        1      0       0     0.084 0.772076
T1_I:U_J        1      1       1     2.303 0.129124
T1_J:T2_I       1      0       0     0.390 0.532562
T1_J:T2_J       1      7       7    13.810 0.000203 ***
T1_J:U_I        1      6       6    12.445 0.000421 ***
T1_J:U_J        1      5       5    10.613 0.001126 **
T2_I:T2_J       1      0       0     0.073 0.786504
T2_I:U_I        1      0       0     0.233 0.629516
T2_I:U_J        1      1       1     2.282 0.130881
T2_J:U_I        1      3       3     4.965 0.025886 *
T2_J:U_J        1      0       0     0.329 0.566421
Residuals   11836   5960       1
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example

#+begin_SRC R :results output :session *R*
nobin_results_data <- results_data %>%
                      filter(OMP == "True", SCR == "True",
                             VEC1 == "True", VEC2 == "True") %>%
                      select(-OMP, -VEC1,
                             -VEC2, -SCR)

summary(aov(cost_mean ~ .*., nobin_results_data))
#+end_SRC

#+RESULTS:
#+begin_example

             Df Sum Sq Mean Sq F value   Pr(>F)
RT_I          1  0.002 0.00151   0.263  0.60846
RT_J          1  0.128 0.12832  22.306 2.84e-06 ***
U1_I          1  0.006 0.00619   1.076  0.30006
T1_I          1  0.109 0.10865  18.887 1.60e-05 ***
T1_J          1  0.013 0.01290   2.242  0.13482
T2_I          1  0.026 0.02609   4.535  0.03357 *
T2_J          1  0.006 0.00608   1.057  0.30423
U_I           1  0.057 0.05742   9.982  0.00165 **
U_J           1  0.055 0.05450   9.474  0.00217 **
RT_I:RT_J     1  0.059 0.05861  10.187  0.00148 **
RT_I:U1_I     1  0.043 0.04340   7.544  0.00618 **
RT_I:T1_I     1  0.002 0.00154   0.267  0.60549
RT_I:T1_J     1  0.000 0.00016   0.028  0.86628
RT_I:T2_I     1  0.031 0.03055   5.311  0.02150 *
RT_I:T2_J     1  0.001 0.00096   0.167  0.68337
RT_I:U_I      1  0.002 0.00151   0.262  0.60909
RT_I:U_J      1  0.002 0.00162   0.281  0.59596
RT_J:U1_I     1  0.004 0.00402   0.698  0.40370
RT_J:T1_I     1  0.000 0.00044   0.076  0.78225
RT_J:T1_J     1  0.014 0.01435   2.495  0.11469
RT_J:T2_I     1  0.004 0.00413   0.718  0.39700
RT_J:T2_J     1  0.001 0.00137   0.238  0.62601
RT_J:U_I      1  0.000 0.00014   0.025  0.87394
RT_J:U_J      1  0.015 0.01523   2.647  0.10421
U1_I:T1_I     1  0.000 0.00048   0.083  0.77362
U1_I:T1_J     1  0.001 0.00129   0.224  0.63606
U1_I:T2_I     1  0.016 0.01631   2.835  0.09271 .
U1_I:T2_J     1  0.005 0.00464   0.807  0.36947
U1_I:U_I      1  0.001 0.00057   0.099  0.75268
U1_I:U_J      1  0.001 0.00094   0.164  0.68540
T1_I:T1_J     1  0.003 0.00301   0.523  0.47001
T1_I:T2_I     1  0.000 0.00012   0.021  0.88580
T1_I:T2_J     1  0.000 0.00019   0.032  0.85730
T1_I:U_I      1  0.000 0.00009   0.015  0.90240
T1_I:U_J      1  0.003 0.00323   0.562  0.45385
T1_J:T2_I     1  0.019 0.01927   3.349  0.06769 .
T1_J:T2_J     1  0.017 0.01686   2.930  0.08739 .
T1_J:U_I      1  0.005 0.00468   0.814  0.36729
T1_J:U_J      1  0.000 0.00033   0.058  0.80961
T2_I:T2_J     1  0.016 0.01560   2.713  0.10004
T2_I:U_I      1  0.001 0.00127   0.220  0.63927
T2_I:U_J      1  0.001 0.00131   0.227  0.63385
T2_J:U_I      1  0.006 0.00608   1.056  0.30450
T2_J:U_J      1  0.002 0.00159   0.276  0.59942
Residuals   664  3.820 0.00575
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example
****** Estimating GPR Kernel Parameters
#+begin_SRC R :results output :session *R*
results_data <- select(complete_plot_data,
                       OMP, SCR, VEC1, VEC2,
                       RT_I, RT_J, U1_I, T1_I,
                       T1_J, T2_I, T2_J, U_I, U_J,
                       cost_mean)
#+end_SRC

#+RESULTS:

#+begin_SRC R :results output :session *R*
library(DiceKriging)
library(dplyr)
library(rsm)

sampled_results_data <- sample_n(results_data, 5000)

sampled_results_data <- sampled_results_data %>% mutate_all(funs(str_replace(., "True", "1"))) %>%
  mutate_all(funs(str_replace(., "False", "0"))) %>%
  mutate_all(funs(as.integer(.)))

coded_sampled_design <- coded.data(select(sampled_results_data, -cost_mean),
                                   formulas = list(T1_Ie = T1_Ie ~ (T1_I - 5.5) / 5.5,
                                                   T1_Je = T1_Je ~ (T1_J - 5.5) / 5.5,
                                                   U_Je = U_Je ~ (U_J - 14.5) / 14.5,
                                                   U_Ie = U_Ie ~ (U_I - 14.5) / 14.5,
                                                   T2_Ie = T2_Ie ~ (T2_I - 5.5) / 5.5,
                                                   T2_Je = T2_Je ~ (T2_J - 5.5) / 5.5,
                                                   U1_Ie = U1_Ie ~ (U1_I - 14.5) / 14.5,
                                                   OMPe = OMPe ~ (OMP - 0.5) / 0.5,
                                                   SCRe = SCRe ~ (SCR - 0.5) / 0.5,
                                                   VEC1e = VEC1e ~ (VEC1 - 0.5) / 0.5,
                                                   VEC2e = VEC2e ~ (VEC2 - 0.5) / 0.5,
                                                   RT_Ie = RT_Ie ~ (RT_I - 2.5) / 2.5,
                                                   RT_Je = RT_Je ~ (RT_J - 2.5) / 2.5))

reg <- km(design = coded_sampled_design,
          response = sampled_results_data$cost_mean)
#+end_SRC

#+RESULTS:
#+begin_example


optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  matern5_2
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10
  - parameters upper bounds :  4 4 4 4 24.8 24.8 4 744.3636 744.3636 744.3636 744.3636 4 4
  - best initial criterion value(s) :  -7562.03

N = 13, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=         7562  |proj g|=        24.25
At iterate     1  f =       7413.9  |proj g|=        23.848
At iterate     2  f =       7392.2  |proj g|=        23.778
At iterate     3  f =       7325.1  |proj g|=        9.7341
At iterate     4  f =       7296.3  |proj g|=        9.6407
At iterate     5  f =       6766.4  |proj g|=        6.9176
At iterate     6  f =       6709.6  |proj g|=        7.5269
At iterate     7  f =       6694.2  |proj g|=        7.5742
At iterate     8  f =       6663.7  |proj g|=        7.5463
At iterate     9  f =       6620.9  |proj g|=        6.3228
At iterate    10  f =       6571.9  |proj g|=         5.853
At iterate    11  f =       6533.7  |proj g|=        5.0285
At iterate    12  f =       6464.6  |proj g|=        5.2125
At iterate    13  f =       6342.8  |proj g|=         7.443
At iterate    14  f =       6324.5  |proj g|=        7.3488
At iterate    15  f =       6323.4  |proj g|=        7.3507
At iterate    16  f =       6321.3  |proj g|=        7.4102
At iterate    17  f =       6318.6  |proj g|=        7.4496
At iterate    18  f =       6316.4  |proj g|=        7.3897
At iterate    19  f =       6313.5  |proj g|=        7.5038
At iterate    20  f =       6310.5  |proj g|=        7.7871
At iterate    21  f =       6310.2  |proj g|=        7.7868
At iterate    22  f =       6309.9  |proj g|=        7.7872
At iterate    23  f =       6308.1  |proj g|=        7.7314
At iterate    24  f =       6303.1  |proj g|=        6.9893
At iterate    25  f =         6301  |proj g|=        6.9916
At iterate    26  f =       6300.5  |proj g|=        6.9778
At iterate    27  f =       6297.4  |proj g|=        6.9241
At iterate    28  f =       6290.7  |proj g|=        6.7774
At iterate    29  f =       6283.6  |proj g|=        6.5873
At iterate    30  f =       6282.4  |proj g|=        6.4988
At iterate    31  f =       6282.2  |proj g|=        6.4529
At iterate    32  f =       6281.9  |proj g|=        6.4503
At iterate    33  f =       6280.7  |proj g|=         6.434
At iterate    34  f =       6279.2  |proj g|=        6.4062
At iterate    35  f =       6276.6  |proj g|=         6.345
At iterate    36  f =       6275.7  |proj g|=        6.3308
At iterate    37  f =       6275.3  |proj g|=        6.3282
At iterate    38  f =       6274.8  |proj g|=        6.3181
At iterate    39  f =       6274.2  |proj g|=        6.2935
At iterate    40  f =       6273.6  |proj g|=        6.2499
At iterate    41  f =       6273.1  |proj g|=        6.1728
At iterate    42  f =       6272.8  |proj g|=        6.0966
At iterate    43  f =       6272.4  |proj g|=        6.0309
At iterate    44  f =       6272.2  |proj g|=        6.0113
At iterate    45  f =       6271.9  |proj g|=        5.9816
At iterate    46  f =       6271.4  |proj g|=        5.9122
At iterate    47  f =         6270  |proj g|=        6.0153
At iterate    48  f =       6266.5  |proj g|=        6.8669
At iterate    49  f =       6265.1  |proj g|=        4.2789
At iterate    50  f =       6261.3  |proj g|=        5.3392
At iterate    51  f =       6258.6  |proj g|=        5.1225
At iterate    52  f =       6255.9  |proj g|=        3.7405
At iterate    53  f =       6255.4  |proj g|=        2.8486
At iterate    54  f =       6255.2  |proj g|=        3.8057
At iterate    55  f =       6255.1  |proj g|=        3.8055
At iterate    56  f =       6254.8  |proj g|=        3.5039
At iterate    57  f =       6254.6  |proj g|=        3.8092
At iterate    58  f =       6254.3  |proj g|=        3.7397
At iterate    59  f =         6254  |proj g|=        3.4176
At iterate    60  f =         6254  |proj g|=        2.7449
At iterate    61  f =         6254  |proj g|=        2.1714
At iterate    62  f =         6254  |proj g|=        2.1588
At iterate    63  f =         6254  |proj g|=        3.7402
At iterate    64  f =       6253.9  |proj g|=        3.7424
At iterate    65  f =       6253.8  |proj g|=        3.7462
At iterate    66  f =       6253.6  |proj g|=        3.4908
At iterate    67  f =       6253.4  |proj g|=        3.7551
At iterate    68  f =       6253.3  |proj g|=        3.7525
At iterate    69  f =       6253.2  |proj g|=        2.7038
At iterate    70  f =       6253.2  |proj g|=        3.1047
At iterate    71  f =       6253.1  |proj g|=        2.0107
At iterate    72  f =       6253.1  |proj g|=        1.9014
At iterate    73  f =       6253.1  |proj g|=         3.809
At iterate    74  f =       6253.1  |proj g|=        2.9691
At iterate    75  f =       6253.1  |proj g|=        2.3959
At iterate    76  f =       6253.1  |proj g|=        2.4247
At iterate    77  f =       6253.1  |proj g|=        1.2962
At iterate    78  f =       6253.1  |proj g|=        1.5339
At iterate    79  f =         6253  |proj g|=        3.8108
At iterate    80  f =         6253  |proj g|=        3.8109
At iterate    81  f =         6253  |proj g|=        3.8109
At iterate    82  f =         6253  |proj g|=       0.96266
At iterate    83  f =       6252.9  |proj g|=       0.96558
At iterate    84  f =       6252.7  |proj g|=        3.7463
At iterate    85  f =       6252.6  |proj g|=        3.7469
At iterate    86  f =       6252.6  |proj g|=        1.5952
At iterate    87  f =       6252.5  |proj g|=        2.0364
At iterate    88  f =       6252.5  |proj g|=        3.7467
At iterate    89  f =       6252.5  |proj g|=        2.2482
At iterate    90  f =       6252.5  |proj g|=        2.2013
At iterate    91  f =       6252.5  |proj g|=        2.5787
At iterate    92  f =       6252.4  |proj g|=        3.8048
At iterate    93  f =       6252.4  |proj g|=        3.5006
At iterate    94  f =       6252.4  |proj g|=        2.4506
At iterate    95  f =       6252.4  |proj g|=        2.5164
At iterate    96  f =       6252.4  |proj g|=        3.8054
At iterate    97  f =       6252.3  |proj g|=        3.8053
At iterate    98  f =       6252.2  |proj g|=        3.8082
At iterate    99  f =       6251.9  |proj g|=        3.8117
At iterate   100  f =       6251.8  |proj g|=        3.8177
At iterate   101  f =       6251.6  |proj g|=        3.8191
final  value 6251.598219
stopped after 101 iterations
#+end_example

#+begin_SRC R :results output :session *R*
reg
#+end_SRC

#+RESULTS:
#+begin_example

Call:
km(design = coded_sampled_design, response = sampled_results_data$cost_mean)

Trend  coeff.:
               Estimate
 (Intercept)     1.4689

Covar. type  : matern5_2
Covar. coeff.:
               Estimate
 theta(OMPe)     0.0000
 theta(SCRe)     1.3433
theta(VEC1e)     4.0000
theta(VEC2e)     4.0000
theta(RT_Ie)     4.0576
theta(RT_Je)     7.0118
theta(U1_Ie)     0.2681
theta(T1_Ie)   648.8870
theta(T1_Je)   290.8718
theta(T2_Ie)   174.3350
theta(T2_Je)   744.2121
 theta(U_Ie)     0.4684
 theta(U_Je)     0.1809

Variance estimate: 1.510738
#+end_example

* 2020
** February
*** [2020-02-05 Wed]
**** Review for COMCOM                                       :PaperReview:
The  paper  presents  the  \varepsilon-sticky  algorithm, an  extension  of  the  \varepsilon-greedy
algorithm for the  Multi-Armed Bandit problem to the selection  of Access Points
by user stations, and evaluates the  performance of the proposed approach in two
scenarios, varying in the distribution  of user stations.  The proposed approach
presents significant improvements in user station satisfaction, in relation to a
Strongest  Signal Access  Point  selection algorithm.   The paper  significantly
extends  the authors'  previous  work,  providing a  brief  introduction to  the
Multi-Armed Bandit  problem and  its different types,  and a  significantly more
extensive evaluation and validation of  the proposed approach.  The experimental
methodology is  solid, and  the proposed  approach convincingly  performs better
than a Strongest Signal selection algorithm, in all scenarios studied. The study
of the best  values of \varepsilon was  also interesting, since choosing a  good value for
the parameter has a significant impact on the observed throughput.
**** Applications of Program Autotuning: Call for Proposals for Funding for Undergraduates, Masters, and PhD Students
We are glad to  announce a call for masters and PhD  scholarship proposals for a
research project, done in the context  of a collaboration between the University
of  São  Paulo   and  the  Hewlett  Packard  Enterprise   company,  starting  on
February 2020.

The work to be performed by candidates  will require 10 hours per week, and will
involve the application of techniques for statistical optimization and design of
experiments to a problem presented by the candidate. We are looking for students
interested in optimizing their applications, or applications used in their work,
targeting different metrics, such as performance, memory or network usage.

Ideal  problems  for this  project  include  problems  where the  search  spaces
involved  are  too large  to  be  explored  exhaustively,  and have  no  trivial
configuration. Examples of problems include  the selection of hyperparameters of
Machine Learning algorithms to  increase prediction accuracy, selecting compiler
flags  to improve  performance, and  configuring user-developed  applications to
improve user-defined metrics.

We would like students who have  already stated their research problems and have
had some progress towards measuring  and evaluating their objectives, but highly
motivated students who are just starting are also welcome to apply.

Up to  5 accepted  candidates will receive  funding, comparable  to scholarships
from  CAPES,   payed  by  IME/USP   for  eight  months,  between   February  and
September 2020. Students currently without funding are preferred.

We invite  interested students to  submit a CV  and a cover  letter, summarizing
their problem and  their interest in applying optimization techniques  to it, by
February  20th 2020.  We assume  your advisor  is aware  and consents  with your
participation in this project. Accepted candidates  will be invited to a two-day
workshop at IME/USP, where techniques and libraries for statistical optimization
and design  of experiments will  be presented, and  candidates will be  asked to
give short presentations about their work. Final acceptance notification will be
sent by February 29th.

Project Coordinator: Prof. Dr. Alfredo Goldman
Project Manager: Pedro Bruel
*** [2020-02-06 Thu]
**** Autotuning for GCC
***** Some References
- https://en.wikipedia.org/wiki/MILEPOST_GCC
- https://github.com/ctuning
- https://github.com/ctuning/ck/wiki
***** Ideas
****** Benchmark
- Ofast
  - unsafe optim
  - error rate
****** Search Space
- Focus on *flags* at first:
  - Explore *numerical, categorical parameters* later
  - LTO
- Which flags should be chosen?
- Baselines:
  - O0, ..., O3, (Ofast, ...?)
  - Random sample of flag configurations
****** Metrics / Search Objectives
- Performance, memory, binary size, ...
- Focus on *one metric* first
  - Pareto frontier of the others
- We could try something like a normalized, weighted sum of metrics
****** Experiments and Analysis Plan
- Plan a screening experiment: Identify Main Effects
- Leverage expert knowledge plus screening to plan next experiments
- Low-discrepancy sampling, linear model, ANOVA
- Gaussian Process Regression?
***** Plan
1. Define a benchmark
   - Firefox
   - SPEC
   - BLAS Lapack
2. Define metrics
3. Define the search space
   1. Flags
   2. Others
4. Start experiments
   1. Random sample
   2. Screening
   3. Linear models + ANOVA
   4. Linear models + ANOVA + Optimal Design
   5. Gaussian Process Regression
*** [2020-02-13 Thu]
**** Experimental Design for a 2D Problem
***** Defining a Problem
Consider the following function of two factors $(x_1, x_2)$:

\[
f(x_1, x_2) + \varepsilon = 2.3 + (1.1x_1) + (1.6x_{1}^{2}) + (2.2x_2) + (3.2x_{2}^{2}) + \varepsilon
\]

That is, $f(x_1, x_2)$ has linear and quadratic dependence on $(x_1, x_2)$.  Here, \varepsilon
is a normally distributed added error with mean zero and variance one.

In R, we can write $f$ as:

#+begin_SRC R :results output :session *R*
f <- function(x1, x2) {
  return(2.3 + (1.1 * x1) + (1.6 * x1 * x1) +
         (2.2 * x2) + (3.2 * x2 * x2) + (2.7 * x1 * x2) + rnorm(length(x1)))
}

# Measuring 5 values of f(x1, x2)
f(rnorm(5), rnorm(5))
#+end_SRC

#+RESULTS:
:
: [1]  1.022636 11.213378  5.611775  5.246327  5.553409

The inputs of $f$ are defined to  be real numbers in the interval $[-1.0, 1.0]$.
Since we cannot generate  the set of *all possible inputs* of $f$,  we have to set
for a *sample of limited size*. Say that we cover the $(x_1, x_2)$ space with a grid
of points in $[-1.0, 1.0]$, spaced by $0.05$ in each axis. In R, we can write:

#+begin_SRC R :results output :session *R*
resolution <- 0.05

sample_grid <- expand.grid(x1 = seq(-1.0, 1.0, by = resolution),
                           x2 = seq(-1.0, 1.0, by = resolution))

sample_grid$f <- f(sample_grid$x1, sample_grid$x2)

str(sample_grid)
#+end_SRC

#+RESULTS:
#+begin_example

'data.frame':	1681 obs. of  3 variables:
 $ x1: num  -1 -0.95 -0.9 -0.85 -0.8 -0.75 -0.7 -0.65 -0.6 -0.55 ...
 $ x2: num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
 $ f : num  7.32 7.01 7.26 7.09 5.93 ...
 - attr(*, "out.attrs")=List of 2
  ..$ dim     : Named int  41 41
  .. ..- attr(*, "names")= chr  "x1" "x2"
  ..$ dimnames:List of 2
  .. ..$ x1: chr  "x1=-1.00" "x1=-0.95" "x1=-0.90" "x1=-0.85" ...
  .. ..$ x2: chr  "x2=-1.00" "x2=-0.95" "x2=-0.90" "x2=-0.85" ...
#+end_example

Plotting this low-resolution view the space, we get:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720
library(lattice)
library(RColorBrewer)

colors = colorRampPalette(brewer.pal(11, "RdYlBu"))(69)

wireframe(f ~ x1 * x2,
          data = sample_grid,
          xlab = "x1",
          ylab = "x2",
          zlab = list("f(x1,x2)",
                      rot = "90"),
          zlim = range(seq(min(sample_grid$f) - 2.0, max(sample_grid$f) + 2.0, by = 0.5)),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          #shade = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE),
          screen = list(z = 20, x = -60, y = 0),
          par.settings = list(axis.line = list(col = "transparent")))
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-qRrjQN/figureElZ1TS.png]]
***** Design of Experiments
Suppose we want to minimize $f$, but we do not have access to the computation of
$f(x_1, x_2)$, and  suppose that it is really expensive  to obtain each evaluation
for the inputs, so expensive in fact,  that we can only afford 20 evaluations of
$f$.  In this scenario, how to best explore the search space?

#+begin_SRC R :results output :session *R*
library(dplyr)
library(AlgDesign)

budget <- 200
sample_rs <- sample_n(sample_grid, budget)

sample_rs$method <- "Random Design"
exploration <- sample_rs

sample_linear_model <- optFederov(~ x1 + x2 + x1:x2,
                                  data = sample_grid,
                                  nTrials = budget)$design
sample_linear_model$method <- "Linear Model Design"
exploration <- bind_rows(exploration,
                         sample_linear_model)

sample_quadratic_model <- optFederov(~ (x1 + x2) + (x1:x2) + I(x1 ^ 2) + I(x2 ^ 2),
                                     data = sample_grid,
                                     nTrials = budget)$design

sample_quadratic_model$method <- "Quadratic Model Design"
exploration <- bind_rows(exploration,
                         sample_quadratic_model)

str(exploration)
#+end_SRC

#+RESULTS:
:
: 'data.frame':	600 obs. of  4 variables:
:  $ x1    : num  1 -0.45 0.35 -0.7 -0.5 1 0.6 -0.25 -0.65 0.2 ...
:  $ x2    : num  0.4 0.65 0.4 -0.15 0.1 -0.15 -0.15 -1 0.8 0.1 ...
:  $ f     : num  8.29 5.12 6.08 1.46 1.86 ...
:  $ method: chr  "Random Design" "Random Design" "Random Design" "Random Design" ...

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 400
library(ggplot2)

ggplot(data = exploration, aes(x = x1, y = x2)) +
  facet_wrap(method ~ ., ncol = 3) +
  geom_point(size = 3) +
  theme_bw(base_size = 22)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-qRrjQN/figuretwUzsW.png]]

***** Fitting and Predicting Values
We now remember the definition of $f$:

\[
f(x_1, x_2) = 2.3 + (1.1x_1) + (1.6x_{1}^{2}) + (2.2x_2) + (3.2x_{2}^{2}) + \varepsilon
\]

If  we try  to  obtain a  regression  function for  $f$,  testing the  quadratic
hypotheses using the different designs, we would get:

#+begin_SRC R :results output :session *R*
rs_lm <- lm(f ~ x1 * x2 * I(x1 ^ 2) * I(x2 ^ 2), data = filter(exploration, method == "Random Design"))
lin_lm <- lm(f ~ x1 * x2 * I(x1 ^ 2) * I(x2 ^ 2), data = filter(exploration, method == "Linear Model Design"))
quad_lm <- lm(f ~ x1 * x2 * I(x1 ^ 2) * I(x2 ^ 2), data = filter(exploration, method == "Quadratic Model Design"))

coef(rs_lm)
coef(lin_lm)
coef(quad_lm)
#+end_SRC

#+RESULTS:
#+begin_example

          (Intercept)                    x1                    x2
           2.27465946            0.53902138            1.79331229
              I(x1^2)               I(x2^2)                 x1:x2
           1.83183371            3.21003163            2.89395296
           x1:I(x1^2)            x2:I(x1^2)            x1:I(x2^2)
           0.55369765            0.65345353            1.82588396
           x2:I(x2^2)       I(x1^2):I(x2^2)         x1:x2:I(x1^2)
           0.44999902           -0.16324027           -0.94369167
        x1:x2:I(x2^2)    x1:I(x1^2):I(x2^2)    x2:I(x1^2):I(x2^2)
           0.08833311           -2.20304930           -1.10485608
x1:x2:I(x1^2):I(x2^2)
           1.20883837

          (Intercept)                    x1                    x2
            2.3489419            -0.4718219             0.6435793
              I(x1^2)               I(x2^2)                 x1:x2
            1.4927455             3.0955042             0.1057755
           x1:I(x1^2)            x2:I(x1^2)            x1:I(x2^2)
            1.9184213             2.1306245             2.0737388
           x2:I(x2^2)       I(x1^2):I(x2^2)         x1:x2:I(x1^2)
            1.9934353             0.3227596             2.2008337
        x1:x2:I(x2^2)    x1:I(x1^2):I(x2^2)    x2:I(x1^2):I(x2^2)
            2.9995394            -2.6608638            -2.7244153
x1:x2:I(x1^2):I(x2^2)
           -2.5155163

          (Intercept)                    x1                    x2
            2.0576184             2.0389072             1.2520964
              I(x1^2)               I(x2^2)                 x1:x2
            1.8942690             3.7012931             1.3285130
           x1:I(x1^2)            x2:I(x1^2)            x1:I(x2^2)
           -1.0086035             1.6426495            -0.4026176
           x2:I(x2^2)       I(x1^2):I(x2^2)         x1:x2:I(x1^2)
            1.0391508            -0.4528896             0.6970911
        x1:x2:I(x2^2)    x1:I(x1^2):I(x2^2)    x2:I(x1^2):I(x2^2)
            1.5442947             0.2870124            -1.9088828
x1:x2:I(x1^2):I(x2^2)
           -0.6770010
#+end_example

#+begin_SRC R :results output :session *R*
rs_lm <- aov(f ~ x1 * x2 + I(x1 ^ 2) + I(x2 ^ 2), data = filter(exploration, method == "Random Design"))
quad_lm <- aov(f ~ x1 * x2 + I(x1 ^ 2) + I(x2 ^ 2), data = filter(exploration, method == "Quadratic Model Design"))
summary.aov(rs_lm)
#+end_SRC

#+RESULTS:
#+begin_example

             Df Sum Sq Mean Sq F value   Pr(>F)
x1            1   67.3    67.3   66.44 4.37e-14 ***
x2            1  324.3   324.3  320.20  < 2e-16 ***
I(x1^2)       1   39.8    39.8   39.28 2.32e-09 ***
I(x2^2)       1  195.4   195.4  192.94  < 2e-16 ***
x1:x2         1  166.6   166.6  164.49  < 2e-16 ***
Residuals   194  196.5     1.0
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example

#+begin_SRC R :results output :session *R*
prediction_rs <- predict(rs_lm, newdata = select(sample_grid, -f))
prediction_error <- data.frame(error = sample_grid[prediction_rs == min(prediction_rs), ]$f - min(prediction_rs))
prediction_error$method <- "Random Design"

prediction_lin <- predict(lin_lm, newdata = select(sample_grid, -f))
prediction_error_lin <- data.frame(error = sample_grid[prediction_lin == min(prediction_lin), ]$f - min(prediction_lin))
prediction_error_lin$method <- "Linear Model Design"

prediction_error <- bind_rows(prediction_error,
                              prediction_error_lin)

prediction_quad <- predict(quad_lm, newdata = select(sample_grid, -f))
prediction_error_quad <- data.frame(error = sample_grid[prediction_quad == min(prediction_quad), ]$f - min(prediction_quad))
prediction_error_quad$method <- "Quadratic Model Design"

prediction_error <- bind_rows(prediction_error,
                              prediction_error_quad)

prediction_error
#+end_SRC

#+RESULTS:
:
:        error                 method
: 1 -1.2170481          Random Design
: 2  0.6531471    Linear Model Design
: 3  0.3871706 Quadratic Model Design
**** Underlying Hypotheses of Autotuning Methods       :ExportableReports:
***** Introduction                                             :noexport:
Given  a program  with $X  \in \mathcal{X}$  configurable parameters,  we want  to
choose the best parameter values according  to a performance metric given by the
function  $f(X)$.   Autotuning methods  attempt  find  the $X_{*}$  that  minimizes
$f(\cdot)$.   Despite  their different  approaches,  autotuning  methods share  some
common hypotheses:

- There is no knowledge about the global optimal configuration
- There could be some problem-specific knowledge to exploit
- Measuring the effects of a choice of parameter values is possible but costly

Each  autotuning method  has  assumptions that  justify  its implementation  and
usage. Some of  these hypotheses are explicit,  such as the ones  that come from
the  linear model.   Others are  implicit,  such as  the ones  that support  the
implementation and the justification of optimization heuristics.
***** Overview of Autotuning Methods
#+begin_export latex
\begin{sidewaysfigure}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      for tree={%
        anchor = north,
        align = center,
        l sep+=1em
      },
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
        draw,
        [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
          draw,
          color = NavyBlue
          [{Search Heuristics},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{\textbf{Random} \textbf{Sampling}}, draw]
            [{Reachable Optima},
              draw,
              color = BurntOrange
              [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
                draw,
                color = BurntOrange
                [{Strong $corr(f(X),d(X,X_{*}))$?},
                  draw,
                  color = NavyBlue
                  [{More Global},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                    [, phantom]]
                  [{More Local},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{High local optima density?},
                      draw,
                      color = NavyBlue
                      [{Exploit Steepest Descent},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                        [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                        [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                      [{Allows\\exploration},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                        [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                        [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
                [,phantom]]
              [,phantom]]]
          [{Statistical Learning},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{Parametric Learning},
              draw,
              color = BurntOrange
              [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                draw,
                color = BurntOrange
                [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                [, phantom]]
              [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                draw,
                color = BurntOrange
                [, phantom]
                [{Check for model adequacy?},
                  draw,
                  alias = adequacy,
                  color = NavyBlue
                  [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                    draw,
                    alias = interactions,
                    color = NavyBlue,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                      draw
                      [, phantom]
                      [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                        edge = {-stealth, ForestGreen, semithick},
                        draw,
                        alias = estimate,
                        color = ForestGreen]]
                    [{\textbf{Optimal} \textbf{Design}},
                      draw,
                      alias = optimal,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [{\textbf{Space-filling} \textbf{Designs}},
                    draw,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{Model selection},
                      edge = {-stealth, ForestGreen, semithick},
                      draw,
                      alias = selection,
                      color = ForestGreen]]]]]
            [{Nonparametric Learning},
              draw,
              color = BurntOrange
              [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
              [{\textbf{Gaussian} \textbf{Process Regression}},
                alias = gaussian,
                draw]
              [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
      \draw [-stealth, semithick, ForestGreen](selection) to[bend left=22] (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](estimate.east) to[bend right=30] (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
      \draw [-stealth, semithick, ForestGreen](optimal) to (estimate);
    \end{forest}
  }
  \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
    denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
    denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
    denote specific method choices, and \textbf{bold} boxes denote specific methods.}
\end{sidewaysfigure}
#+end_export

***** Previous Attempts                                        :noexport:
#+begin_export latex
\forestset{linebreaks/.style={for tree={align = center}}}
\begin{sidewaysfigure}
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      linebreaks
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\ $Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$}
        [{Does not construct\\estimate $Y = \hat{f}(\cdot, \theta{}(X))$}
          [{Reachable\\optima}
            [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$}
              [{Strong\\$corr(f(X),d(X,X_{*}))$}
                [{Low local\\optima density}
                  [{\textbf{Greedy}\\\textbf{Search}}, draw]
                  [{Estimate $f^{\prime}(X)$}
                    [{\textbf{Gradient}\\\textbf{Descent}}, draw]]]
                [{Introduce a ``population''\\$\mathbf{X} = (X_1,\dots,X_n)$}
                  [{Combination, mutation,\\within $\mathbf{X}$}
                    [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]]
                  [{\textbf{Ant}\\\textbf{Colony}}, draw]]]
              [{Weaker\\$corr(f(X),d(X,X_{*}))$}
                [{Accept\\worst $f(X)$}
                  [{\textbf{Simulated}\\\textbf{Annealing}}, draw]]
                [{Avoid\\recent $X$}
                  [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]
          [{\textbf{Random}\\\textbf{Sampling}}, draw]]
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$}
          [{Parametric\\Learning}
            [{$\hat{f}(X) \approx f_1(X_1) + \dots + f_k(X_k)$}
              [{\textbf{Independent}\\\textbf{Bandit}}, draw]]
            [{$\hat{f}(X) = \mathcal{B}(logit(\mathcal{M}(X)\theta(X) + \varepsilon))$}
              [{\textbf{Logistic}\\\textbf{Regression}}, draw]]
            [{$\hat{f}(X) = \mathcal{M}(X)\theta(X) + \varepsilon$}
              [{\textbf{Linear}\\\textbf{Regression}}, draw]
              [{Measure\\properties of $X$}
                [{Independance\\of effects}
                  [{\textbf{Screening}}, draw]]
                [{Homoscedasticity of $\varepsilon$}
                  [{\textbf{Optimal}\\\textbf{Design}}, draw]]]]]
          [{Nonparametric\\Learning}
            [{Splitting\\rules on $X$}
              [{\textbf{Decision}\\\textbf{Trees}}, draw]]
            [{$\hat{f} = \mathcal{GP}(X; \mathcal{K})$}
              [{\textbf{Gaussian}\\\textbf{Process Regression}}, draw]]
            [{\textbf{Neural}\\\textbf{Networks}}, draw]
            [{\textbf{Multi-armed}\\\textbf{Bandit (?)}}, draw]]]]
    \end{forest}
  }
  \caption{Some hypothesis of some autotuning methods}
\end{sidewaysfigure}

#+end_export

#+begin_export latex
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\begin{table}[ht]
  \center
  \begin{tabular}{@{}p{0.3\textwidth}p{0.5\textwidth}@{}}
    \toprule
    Method &  Hypotheses \\ \midrule
    Metaheuristics & \tabitem There are similarities between natural fenomena and the target problem \\
    & \tabitem Gradual changes in configurations produce gradual changes in performance \\
    & \tabitem The optimal configuration is ``reachable'', by small changes, from non-optimal configurations  \\
    \addlinespace \\
    Machine Learning & \tabitem As more samples are obtained, decreases in ``out-of-sample error'' imply decreases ``in-sample error'' \\
    & \tabitem \textbf{TODO} What are the classes of models? \\
    \addlinespace \\
    Design of Experiments & \tabitem There is ``exploitable search space structure''\\
    & \tabitem Linear model: Response $\bm{Y}$ is an ``unobservable function'' of parameters $\bm{X}$: \\
    & \hspace{0.15\textwidth} $f(\bm{X}) = \bm{Y} = \bm{X\beta} + \bm{\varepsilon}$ \\
    & \tabitem Optimal Design: Variance of estimator $\hat{\bm{\beta}}$ is proportional to $\bm{X}$: \\
    & \hspace{0.15\textwidth} $\bm{\hat{\beta}} = \left(\bm{X}^{\intercal}\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{Y}$ \\
    \addlinespace \\
    Gaussian Process Regression & \tabitem Response $\bm{Y}$ is a sample from a multidimensional Gaussian distribution, with mean $m(\bf{X})$ and variance $k(\bm{X}, \bm{X}^{\intercal})$: \\
    & \hspace{0.1\textwidth} $\bm{Y} = f(\bm{X}) \sim \mathcal{N}(m(\bm{X}), k(\bm{X}, \bm{X}^{\intercal}))$ \\
    & \tabitem Predictions $\bm{Y_{*}}$ can be made conditioning distribution to observed data\\ \bottomrule
  \end{tabular}%
\end{table}
#+end_export

#+begin_export latex
\resizebox{!}{\textheight}{%
  \begin{tikzpicture}[rotate = -90]
    \begin{scope}
      \tikzset{every tree node/.style = {align = center}}
      \tikzset{level 1+/.style={level distance = 40pt}}
      \Tree [.\node(n0){Minimize $f: X \mapsto \mathbb{R}$ \\ $f(X) = f^{*}(X) + \varepsilon = m$};
        [.{Does not construct \\ estimate $\hat{f}(X; \theta)$}
          [.{Reachability of \\ optima}
            [.{\textbf{Greedy} \\ \textbf{Search}} ]
            [.{$d(x_i, x_j) \to 0$ $\implies$ \\ $d(f(x_i), f(x_j)) \to 0$}
              [.{Abundance of \\ local optima}
                [.{\textbf{Simulated} \\ \textbf{Annealing}} ]]
              [.{Closeness of a \\ ``population'' of $X$}
                [.{\textbf{Genetic} \\ \textbf{Algorithms}} ]]]]
          [.{\textbf{Random} \\ \textbf{Sampling}} ] ]
        [.\node(r1){Constructs surrogate \\ estimate $\hat{f}(X; \theta)$};
          [.{Explicit, variable \\ models of $\theta$}
            [.{$\hat{f} = M(X)\theta + \varepsilon$}
              [.{Independance \\ of effects}
                [.{\textbf{Screening}} ] ]
              [.{Homoscedasticity}
                [.{\textbf{Optimal} \\ \textbf{Design}} ] ] ] ]
          [.{Implicit, fixed \\ models of $\theta$}
            [.{\textbf{Neural Networks}} ] ]
          [.{Samples \\ functions}
            [.{$\hat{f} = \mathcal{GP}(X; \theta, \mathcal{K})$}
              [.{\textbf{Gaussian Process} \\ \textbf{Regression}} ] ] ] ] ]
    \end{scope}
    % \begin{scope}[thick]
    %   \draw [color = orange] (n0) to [bend left = 2] (r1);
    %   \draw [color = green] (n0) to [bend right = 2] (r1);
    % \end{scope}
  \end{tikzpicture}
}
#+end_export

*** [2020-02-18 Tue]
**** Functions in R
#+begin_SRC R :results output :session *R*
my_function <- function(x, y, z) {
  x + y
  paste(z, collapse = "", sep = "")
}

my_function(2, 3, c("a", "v", "c"))
#+end_SRC

#+RESULTS:
:
: [1] "avc"
**** PARCO Review                                            :PaperReview:
- Title: A Comparative Study of Parallel and Serial Implementations of Content-Based Filtering
***** Review
****** Summary
The paper  presents a shared  memory and a  message passing implementation  of a
recommender system, and  evaluates the performance of  these two implementations
on a data set produced for the study.
****** Writing
- There is no need to use the he/she formula, use the neutral singular "they".
****** Experimental Validation
The results  shown in Figures  17 to 19 compare  the performance of  the serial,
shared memory and message passing implementations, but experimental settings are
not clearly stated.  Without the  information of how many experiment repetitions
were performed, and what was the standard deviation of the samples collected, it
is  harder  to support  the  conclusions  of  the  paper, especially  since  the
variations on performance  that is shown in the Figures  show large amplitude. A
major  revision  of  this  paper  would involve  running  more  experiments  and
analyzing the mean and standard deviation of the samples.
****** Recommendation
I recommend this  paper to be rejected,  since its contributions seem  to be the
performance evaluation of  well known and studied  parallelization libraries, in
the  context of  an existing  recommender  system.  The  data sets  used in  the
experiments are also limited  in scope, and it would be  useful for the analyses
presented  here to  use  data sets  from  real scenarios.   The  paper could  be
resubmitted to this paper after a statistically sound evaluation and analysis of
the  performance of  more parallel  recommender systems  in more  representative
scenarios.
*** [2020-02-27 Thu]
**** Reproducible Science for SBC
Write a small  manifesto for reproducible science, in  portuguese, for brazilian
CS conferences.

- https://github.com/ReScience
- https://www.nature.com/articles/s41562-016-0021
- https://www.semanticscholar.org/paper/A-manifesto-for-reproducible-science-Munaf%C3%B2-Nosek/a68ce412e92d87ca0116519651bbc484d98c76ae
- https://rescience.github.io/faq/
- https://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2012.pdf

***** Manifesto pela Reprodutibilidade da Ciência da Computação no Brasil
Conclusões produzidas a partir de  dados obtidos experimentalmente não podem ser
consideradas  validadas  até  que   seja  possível  reproduzi-las  em  condições
experimentais independentes. Esse princípio  orienta todo o progresso científico
baseado em  metodologias experimentais.  A  pesquisa experimental em  Ciência da
Computação está em posição singular para reforçar e promover a reprodutibilidade
científica,  pois experimentos  computacionais em  determinados casos  podem ser
acompanhados,  registrados, e  repetidos  com precisão  e controle  praticamente
impossíveis em  áreas como a  biologia e a química.  As organizações em  prol da
ciência brasileira têm portanto grandes  justificativas para promover e reforçar
a reprodutibilidade científica.

Os  [[https://www.acm.org/publications/policies/artifact-review-badging][esforços da  ACM]] são  um bom  exemplo dos  esforços iniciais  que podem  ser
realizados em prol da reprodutibilidade.   Diversas conferências e periódicos da
ACM adotam  um sistema de  insígnias para  marcar trabalhos cujos  esforços para
reprodutibilidade  de  seus  experimentos são  significativos.   A  nomenclatura
utilizada pela ACM é derivada  do [[https://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2012.pdf][Vocabulário Internacional de Metrologia]] (VIM),
e distingue entre resultados e conclusões que podem ser reproduzidos:

- Pela mesma equipe, nas mesmas condições experimentais (Repetibilidade)
- Por uma equipe diferente, nas mesmas condições experimentais (Replicabilidade)
- Por   uma   equipe   diferente,    em   condições   experimentais   diferentes
  (Reprodutibilidade)

O código e os dados que dão  suporte às conclusões de um estudo científico devem
ser  submetidos junto  ao documento  que  será publicado.   Esses /artefatos/  são
avaliados pelos  revisores e insígnias são  conferidas de acordo com  o nível de
reprodutibilidade   alcançado.    Outras    organizações   também   promovem   a
reprodutibilidade, como a  [[https://rescience.github.io/faq/][ReScience]], que recentemente promoveu o  [[https://rescience.github.io/ten-years/][Desafio de 10
Anos  da Reprodutibilidade]],  onde  pesquisadores foram  incentivados a  submeter
artigos com  a reprodução de  seus próprios resultados de  no mínimo 10  anos de
idade.

Ações  como as  tomadas pela  ACM  podem ter  um  grande impacto  no reforço  da
credibilidade  do método  científico e  no  avanço da  descoberta científica  no
Brasil.
** March
*** [2020-03-11 Wed]
**** Review for ICS2020
***** Summary
The  paper presents  an  interesting study  of the  performance  of a  redundant
operation detection tool,  CIDetector.  The tool is used to  identify regions in
the  machine code  generated by  a compiler  that contain  redundant operations.
These regions  are then modified to  remove redundancies, and the  percentage of
redundancy  reduction and  the resulting  speedups are  reported.  The  paper is
clearly structured and easy to follow.

The  paper  evaluates   the  detection  tool  on  14   programs,  composing  the
CIBenchmark, which  is also introduced by  the paper. CIDetector is  tested on 3
GCC  versions,  1 ICC,  and  1  LLVM versions,  by  measuring  the reduction  on
redundant operations,  caused by  changes in regions  detected to  produce these
kinds  of redundancies.   In  some scenarios,  eliminating redundant  operations
produces execution time speedups.

Strangely, the paper does not provide access  to any of its source code or data,
but mentions that the code will  be open-sourced provided the paper is accepted.
On top of being  a strange practice, this means I was  not able to independently
verify or validate any of the data or the code presented in this paper.

The paper does not discuss whether the  results reported are a mean of a certain
number of  executions, and no standard  deviation or confidence interval  of the
mean  is  presented.   It  is  strongly  suggested  that  these  statistics  and
discussions are added to the paper, in order to strengthen its conclusions.

Overall,  the  paper  presents  valuable insights  and  careful  evaluation  and
discussion of the results. I believe this  to be a borderline paper, which could
be accepted provided  the statistical analysis methodology  is clearly presented
and discussed. I also believe it would be interesting to compare the performance
of the code generated by different compilers.
*** [2020-03-13 Fri]
**** First results from Emanuel's work
***** Cloning the Git Repository                               :noexport:
#+begin_SRC shell :results output :session *Shell*
git clone git@github.com:phrb/matrix-multiply-test.git || (cd matrix-multiply-test && git pull)
#+end_SRC

#+RESULTS:
: git clone git@github.com:phrb/matrix-multiply-test.git || (cd matrix-multiply-test && git p<test.git || (cd matrix-multiply-test && git pu                                               <test.git || (cd matrix-multiply-test && git pull)<test.git || [33m([39m[32mc[32md[39m matrix-multiply-test && [32mg[32mi[32mt[39m pull[33m)[39m[?2004l
: fatal: destination path 'matrix-multiply-test' already exists and is not an empty directory.
: Already up to date.
***** Boxplots of Selected Flags
#+begin_SRC R :results graphics output :session *R* :file "/tmp/heap_vec_nolib_30.pdf" :width 20 :height 7 :eval no-export
library(ggplot2)

df <- read.csv("matrix-multiply-test/results/results.csv", header = TRUE)

ggplot(df, aes(x = as.factor(id), y = execution_time)) +
  geom_jitter(alpha = 0.6, size = 4, height = 0, width = 0.2) +
  ylab("Execution Time (s)") +
  theme_bw(base_size = 38) +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(size = 25))
#+end_SRC

#+RESULTS:
[[file:/tmp/heap_vec_nolib_30.pdf]]
***** LLVM Command to Spit Flags
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
llvm-as < /dev/null | opt -O2 -disable-output -debug-pass=Arguments
#+end_SRC

#+RESULTS:
: [32ml[32ml[32mv[32mm[32m-[32ma[32ms[39m < /dev/null | [32mo[32mp[32mt[39m -O2 -disable-output -debug-pass=Arguments[?2004l
: Pass Arguments:  -tti -tbaa -scoped-noalias -assumption-cache-tracker -targetlibinfo -verify -ee-instrument -simplifycfg -domtree -sroa -early-cse -lower-expect
: Pass Arguments:  -targetlibinfo -tti -tbaa -scoped-noalias -assumption-cache-tracker -profile-summary-info -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor -globalopt -domtree -mem2reg -deadargelim -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -simplifycfg -basiccg -globals-aa -prune-eh -inline -functionattrs -domtree -sroa -basicaa -aa -memoryssa -early-cse-memssa -speculative-execution -basicaa -aa -lazy-value-info -jump-threading -correlated-propagation -simplifycfg -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -libcalls-shrinkwrap -loops -branch-prob -block-freq -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -pgo-memop-opt -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -tailcallelim -simplifycfg -reassociate -domtree -loops -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -loop-rotate -licm -loop-unswitch -simplifycfg -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -loop-simplify -lcssa-verification -lcssa -scalar-evolution -indvars -loop-idiom -loop-deletion -loop-unroll -mldst-motion -phi-values -basicaa -aa -memdep -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -gvn -phi-values -basicaa -aa -memdep -memcpyopt -sccp -demanded-bits -bdce -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -lazy-value-info -jump-threading -correlated-propagation -basicaa -aa -phi-values -memdep -dse -loops -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -licm -postdomtree -adce -simplifycfg -domtree -basicaa -aa -loops -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -barrier -elim-avail-extern -basiccg -rpo-functionattrs -globalopt -globaldce -basiccg -globals-aa -float2int -domtree -loops -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -loop-rotate -loop-accesses -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -loop-distribute -branch-prob -block-freq -scalar-evolution -basicaa -aa -loop-accesses -demanded-bits -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -loop-vectorize -loop-simplify -scalar-evolution -aa -loop-accesses -lazy-branch-prob -lazy-block-freq -loop-load-elim -basicaa -aa -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -simplifycfg -domtree -loops -scalar-evolution -basicaa -aa -demanded-bits -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -slp-vectorizer -opt-remark-emitter -instcombine -loop-simplify -lcssa-verification -lcssa -scalar-evolution -loop-unroll -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instcombine -loop-simplify -lcssa-verification -lcssa -scalar-evolution -licm -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -transform-warning -alignment-from-assumptions -strip-dead-prototypes -globaldce -constmerge -domtree -loops -branch-prob -block-freq -loop-simplify -lcssa-verification -lcssa -basicaa -aa -scalar-evolution -block-freq -loop-sink -lazy-branch-prob -lazy-block-freq -opt-remark-emitter -instsimplify -div-rem-pairs -simplifycfg -verify
: Pass Arguments:  -domtree
: Pass Arguments:  -targetlibinfo -domtree -loops -branch-prob -block-freq
: Pass Arguments:  -targetlibinfo -domtree -loops -branch-prob -block-freq
*** [2020-03-16 Mon]
**** Tests
#+begin_SRC R :results output :session *R* :eval no-export :exports results
print("test")
1 + 2
a <- 3
a
#+end_SRC

#+RESULTS:
: [1] "test"
:
: [1] 3
:
: [1] 3

#+begin_SRC julia :eval no-export :exports results
println("test")
1 + 3
#+end_SRC

#+RESULTS:
:RESULTS:
: test
: 4
:END:
*** [2020-03-17 Tue]
**** Map Intervals
#+begin_SRC R :results output :session *R* :eval no-export :exports results
map_intervals <- function(x, interval_from, interval_to) {
    new_x <- x - interval_from[1]
    new_x <- new_x / (interval_from[2] - interval_from[1])
    new_x <- new_x * (interval_to[2] - interval_to[1])
    new_x <- new_x + interval_to[1]

    return(new_x)
}
#+end_SRC

#+RESULTS:

**** "Circle Regression"

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :eval no-export
library(dplyr)
library(ggplot2)

map_intervals <- function(x, interval_from, interval_to) {
    new_x <- x - interval_from[1]
    new_x <- new_x / (interval_from[2] - interval_from[1])
    new_x <- new_x * (interval_to[2] - interval_to[1])
    new_x <- new_x + interval_to[1]

    return(new_x)
}

df <- data.frame(x = map_intervals(runif(100), c(0.0, 1.0), c(-1.0, 1.0)),
                 y = map_intervals(runif(100), c(0.0, 1.0), c(-1.0, 1.0)))

df$y <-
#+end_SRC

*** [2020-03-24 Tue]
**** Meeting May 24
#+begin_export markdown
# Réunion 24 Mars

- Arbre « taxonomie »
- Structure de thèse
  - Il faut comprendre la figure quand elle est présenté
- Résultats GPR « 2 objectifs » précision DNN
  - Expériences avec des poids plus forts pour le taille (1/10 et 9/10 par exemple, voir 0, 1 pour vérifier que ça optimise bien la taille)
  - Calcul des indices de sobol et autres pas possible car pas assez de points
  - Estimer $\theta$ à chaque étape d'exploitation (donc ça utilisera tous les points de l'exploration initiale plus les nouveaux d'EGO). Pour vérifier si ça converge. Faire la même chose en virant les 120 premières étapes où on explore histoire de voir si le $\theta$ dans l'espace où on exploite a la même variance que l'espace entier.

- Résultats SPAPT
  - Ajouter l'amélioration relative pour GPR, RS (logscale, normalisation à O3)
  - Essayer de calculer les indices de Sobol. Peut être fait aussi bien sur les échantillons RS que sur les échantillons de l'exploration SOBOL initiale. Aucune idée de ce que ça va donner...
  - Regarder comment GPR explore l'espace (ggpairs). On s'attends à ce que certaines régions de l'espace soient plus explorées, que ça soit moins uniforme que dans le LHS/Sobol de départ. S'il y a une (ou plusieur) région particulièrement explorée, on voudra vérifier que c'est bien consistant d'une répétion de l'expérience à une autre.

On fait l'hypothèse que $Y=\beta.X + \varepsilon$
on estime $\hat\beta$. l'incertitude sur $\hat\beta = (X^{\intercal}X)^{-1}XY$ provient de la variabilité de $\varepsilon$.

$Y = (X^{\intercal}X)^{-1}X(\beta.X + \varepsilon) = \beta + (X^{\intercal}X)^{-1}X\varepsilon$  Et comme $\varepsilon \sim \mathcal{N}(0,\sigma^2)$, ça te donne ton incertitude du $\hat\beta$. C'est un ellipsoide de confiance. Là, le $X$, c'est les mesures mais pour faire une prédiction sur $x$, tu repropages ton incertitude dans $Y=(\hat\beta + ellipsoide).x$

$y=a.x+b$ avec $a \in [0.9,1.1]$ et $b\in [2.2,2.6]$.
#+end_export
*** [2020-03-27 Fri]
**** Design of Experiments Step-by-Step
Formulas in R:

y = (alpha * x1) + (beta * x2)
y ~ x1 + x2

y = (\alpha_1 * x1) + (\alpha_2 * x2) + (\alpha_3 * x1 * x2)
y ~ x1 * x2

y = (\alpha_3 * x1 * x2)
y ~ x1:x2

y = (\alpha_1 * x1) + (\alpha_2 * x2) + (\alpha_3 * x1 * x2)

#+begin_SRC R :results output :session *R* :eval no-export :exports results :tangle ed_test.r
library(AlgDesign)
library(randtoolbox)
library(quantreg)

y <- function(x) {
  # call supersim

  return(1 + (2.3 * x$inj1) +
         (3.2 * x$inj2) +
         (4.5 * x$inj2 * x$inj1) +
         (1.1 * runif(1)))
}

# temp_sobol <- sobol(n = sobol_n,
#                     dim = sobol_dim,
#                     scrambling = 1,
#                     seed = as.integer((99999 - 10000) * runif(1) + 10000),
#                     init = TRUE)
#
# rm(temp_sobol)
# quiet(gc())
#
# design <- sobol(n = sobol_n,
#                 dim = sobol_dim,
#                 scrambling = 1,
#                 seed = as.integer((99999 - 10000) * runif(1) + 10000),
#                 init = FALSE)

candidate_set <- data.frame(inj1 = runif(1000),
                            inj2 = runif(1000))

experiments <- sample_n(candidate_set, 4)

experiments$response<- y(experiments)

regression <- lm(response ~ inj1 + inj2,
                 data = experiments)


output_lin <- optFederov(~ inj1 + inj2,
                         nTrials = 4,
                         data = candidate_set)

output_quad <- optFederov(~ inj1 + inj2 + I(inj1 ^ 2),
                          nTrials = 4,
                          data = candidate_set)

design <- output_lin$design
design$response <- y(design)

regression <- rq(response ~ inj1 + inj2,
                 tau = 0.05,
                 data = design)
#+end_SRC

#+RESULTS:
:
: Loading required package: SparseM
:
: Attaching package: ‘SparseM’
:
: The following object is masked from ‘package:base’:
:
:     backsolve
** April
*** [2020-04-14 Tue]
**** Reunião HPE Fin
- Gastos de fevereiro?
- Bolsa:
  - Não é um pedido extra
- Hardware: problemas de covid
  - Acesso aos alunos: problema de segurança
*** [2020-04-17 Fri]
**** Sobol and Uniform samples
#+begin_SRC R :results graphics output :session *R* :file /tmp/sampling_histograms.pdf :width 8 :height 16 :eval no-export
library(randtoolbox)
library(dplyr)
library(tidyr)

map_intervals <- function(x, interval_from, interval_to) {
    new_x <- x - interval_from[1]
    new_x <- new_x / (interval_from[2] - interval_from[1])
    new_x <- new_x * (interval_to[2] - interval_to[1])
    new_x <- new_x + interval_to[1]

    return(new_x)
}

n = 216
d = 108

temp_sobol <- torus(n = n,
                    dim = d,
#                     scrambling = 1,
#                     seed = as.integer((99999 - 10000) * runif(1) + 10000),
                    init = TRUE)
rm(temp_sobol)

design <- data.frame(halton(n = n,
                           dim = d,
                           # scrambling = 2,
                           # seed = as.integer((99999 - 10000) * runif(1) + 10000),
                           init = FALSE)) %>%
    map_intervals(c(0.0, 1.0), c(1.0, 8.0)) %>%
    round()

design$method <- "Sobol"

rs_design <- data.frame(matrix(runif(n * 108), ncol = d, nrow = n)) %>%
    map_intervals(c(0.0, 1.0), c(1.0, 8.0)) %>%
    round()

rs_design$method <- "Uniform"

design <- bind_rows(design, rs_design) %>%
    pivot_longer(-method, names_to = "Layer", values_to = "Bitwidth")

ggplot() +
    geom_histogram(data = design,
                   bins = 8,
                   aes(x = Bitwidth,
                       y = ..count..)) +
    facet_wrap(. ~ method, ncol = 1)
#+end_SRC

#+RESULTS:
[[file:/tmp/sampling_histograms.pdf]]

**** Looking at Top500 Data
***** Cloning Repository
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git clone --depth=1 https://github.com/phrb/top500.git || (cd top500 && git pull)
#+end_SRC

#+RESULTS:
: [?2004l
: fatal: destination path 'top500' already exists and is not an empty directory.
: Already up to date.
***** Loading Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

df <- read.csv("top500/TOP500_history.csv")
#+end_SRC

#+RESULTS:
***** Looking at Data
****** Column Names
#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df)
#+end_SRC

#+RESULTS:
#+begin_example
'data.frame':	27000 obs. of  52 variables:
 $ Year                           : int  1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 ...
 $ Month                          : int  6 6 6 6 6 6 6 6 6 6 ...
 $ Day                            : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Rank                           : num  1 2 3 4 5 6 7 8 9 10 ...
 $ Site                           : Factor w/ 2467 levels " Institute of Information and Communication Technologies at the Bulgarian Academy of Sciences",..: 1287 1379 1513 1486 1516 135 1507 277 468 563 ...
 $ Manufacturer                   : Factor w/ 147 levels "Acer Group","ACTION",..: 141 141 141 141 98 98 141 76 28 28 ...
 $ Computer                       : Factor w/ 2825 levels " eServer pSeries 655 (1.7 GHz Power4+)",..: 790 799 798 798 2345 2344 796 995 2730 2730 ...
 $ Country                        : Factor w/ 60 levels "Australia","Austria",..: 58 58 58 58 26 7 58 58 58 58 ...
 $ Processors                     : num  1024 544 512 512 4 ...
 $ RMax                           : num  59.7 30.4 30.4 30.4 23.2 20 15.1 13.9 13.7 13.7 ...
 $ RPeak                          : num  131 69.6 65.5 65.5 25.6 ...
 $ Nmax                           : num  52224 36864 36864 36864 6400 ...
 $ Nhalf                          : num  24064 16384 16384 16384 830 ...
 $ Processor.Family               : Factor w/ 26 levels "","Alpha","AMD",..: 25 25 25 25 21 21 25 14 7 7 ...
 $ Processor                      : Factor w/ 463 levels "Alpha","Alpha EV4",..: 267 267 267 267 133 133 267 63 25 25 ...
 $ Processor.Speed..MHz.          : num  32 32 32 32 400 ...
 $ System.Family                  : Factor w/ 179 levels " IBM Power Systems",..: 174 174 174 174 122 122 174 104 33 33 ...
 $ Operating.System               : Factor w/ 85 levels "AIX","Amazon Linux 2",..: 10 10 10 10 59 59 10 30 74 74 ...
 $ Architecture                   : Factor w/ 6 levels "Cluster","Constellations",..: 3 3 3 3 6 6 3 3 6 6 ...
 $ Segment                        : Factor w/ 7 levels "Academic","Classified",..: 6 4 1 2 7 6 6 1 7 6 ...
 $ Application.Area               : Factor w/ 48 levels "","Aerospace",..: 37 37 37 37 37 46 37 37 37 37 ...
 $ Interconnect.Family            : Factor w/ 27 levels "10G","Cray Interconnect",..: 8 8 8 8 3 3 8 17 17 17 ...
 $ Interconnect                   : Factor w/ 90 levels "100G Ethernet",..: 37 37 37 37 67 67 37 71 71 71 ...
 $ Region                         : Factor w/ 16 levels "Australia and New Zealand",..: 6 6 6 6 4 6 6 6 6 6 ...
 $ Continent                      : Factor w/ 5 levels "Africa","Americas",..: 2 2 2 2 3 2 2 2 2 2 ...
 $ Power                          : num  NA NA NA NA NA NA NA NA NA NA ...
 $ System.Model                   : Factor w/ 539 levels "","Acer AR585 F1 Cluster",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Total.Cores                    : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Measured.Size                  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Processor.Cores                : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Accelerator                    : Factor w/ 7 levels "","ATI GPU","IBM PowerXCell 8i",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Name                           : Factor w/ 848 levels "","&#346;wierk Computing Centre",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Accelerator.Cores              : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Efficiency....                 : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Mflops.Watt                    : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Processor.Technology           : Factor w/ 26 levels "","AMD x86_64",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ OS.Family                      : Factor w/ 6 levels "","BSD Based",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Cores.per.Socket               : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Processor.Generation           : Factor w/ 75 levels "","AMD Naples",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Previous.Rank                  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ First.Appearance               : num  NA NA NA NA NA NA NA NA NA NA ...
 $ First.Rank                     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Accelerator.Co.Processor.Cores : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Accelerator.Co.Processor       : Factor w/ 59 levels "","AMD FirePro S10000",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Power.Source                   : Factor w/ 4 levels "","Derived","Optimized",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Rmax..TFlop.s.                 : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Rpeak..TFlop.s.                : num  NA NA NA NA NA NA NA NA NA NA ...
 $ HPCG..TFlop.s.                 : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Power..kW.                     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Power.Effeciency..GFlops.Watts.: num  NA NA NA NA NA NA NA NA NA NA ...
 $ Site.ID                        : num  NA NA NA NA NA NA NA NA NA NA ...
 $ System.ID                      : int  NA NA NA NA NA NA NA NA NA NA ...
#+end_example

****** Processor Speed
#+begin_SRC R :results graphics output :session *R* :file "top500_processors_speed.pdf" :width 10 :height 10 :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Processor.Speed..MHz. / 1000,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
                                        #scale_y_log10() +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Processor Speed (GHz)") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:top500_processors_speed.pdf]]

****** NMax
#+begin_SRC R :results graphics output :session *R* :file "top500_nmax.pdf" :width 10 :height 10 :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Nmax,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Problem Size to Reach RMax") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:top500_nmax.pdf]]

****** RPeak and RMax
#+begin_SRC R :results graphics output :session *R* :file "top500_rpeak.pdf" :width 17.5 :height 7 :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(RMax = RMax / 1e3,
           RPeak = RPeak / 1e3,
           RMaxT = coalesce(RMax, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak, Rpeak..TFlop.s.)) %>%
    select(Rank,
           Year,
           RMaxT,
           RPeakT,
           HPCG..TFlop.s.) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakT",
                                    "RMaxT",
                                    "HPCG..TFlop.s."),
                         labels = c("RPeak (HPL)",
                                    "RMax (HPL)",
                                    "RMax (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance (TFlops/s)") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.83, 0.09),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 3)
#+end_SRC

#+RESULTS:
[[file:top500_rpeak.pdf]]
****** Processors
#+begin_SRC R :results graphics output :session *R* :file "top500_total_cores.pdf" :width 17.5 :height 7 :eval no-export
library(ggplot2)
library(tidyr)

plot_df <- df %>%
    mutate(Accelerator.Co.Processor.Cores = replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    mutate(AllCores = coalesce(Processors, Total.Cores) - Accelerator.Co.Processor.Cores) %>%
    select(Rank, Year, AllCores, Accelerator.Co.Processor.Cores) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("AllCores",
                                    "Accelerator.Co.Processor.Cores"),
                         labels = c("Processor Cores",
                                    "Accelerator Cores"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Core Count") +
    scale_y_log10() +
    # annotation_logticks(sides = "l") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.67, 0.08),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 4)
#+end_SRC

#+RESULTS:
[[file:top500_total_cores.pdf]]
****** RMax / Cores
#+begin_SRC R :results graphics output :session *R* :file "top500_rmax_cores.pdf" :width 17.5 :height 7 :eval no-export
library(ggplot2)


plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores)) %>%
    mutate(RMax = (RMax / 1e3) / AllCores,
           RPeak = (RPeak / 1e3) / AllCores,
           Rmax..TFlop.s. = Rmax..TFlop.s. / AllCores,
           Rpeak..TFlop.s. = Rpeak..TFlop.s. / AllCores,
           RMaxC = coalesce(RMax, Rmax..TFlop.s.),
           RPeakC = coalesce(RPeak, Rpeak..TFlop.s.),
           HPCGC = HPCG..TFlop.s. / AllCores) %>%
    select(Rank,
           Year,
           RMaxC,
           RPeakC,
           HPCGC) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakC",
                                    "RMaxC",
                                    "HPCGC"),
                         labels = c("RPeak / Cores (HPL)",
                                    "RMax / Cores (HPL)",
                                    "RMax / Cores (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance / Core Count") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.85, 0.1),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          strip.text.x = element_text(size = 17),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 5)
#+end_SRC

#+RESULTS:
[[file:top500_rmax_cores.pdf]]

****** Achieved / Max Performance                             :noexport:
#+begin_SRC R :results graphics output :session *R* :file "top500_rmax_rpeak.pdf" :width 10 :height 10 :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = filter(df, Rank <= 5),
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = RMax / RPeak,
                    color = cut(Rank,
                                breaks = c(1, 100, 200, 300, 400, 500),
                                include.lowest = TRUE))) +
                                        #scale_y_log10() +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Dark2") +
    ylab("Achieved / Theoretical Performance (RMax / RPeak)") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.5, 0.025),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:top500_rmax_rpeak.pdf]]

*** [2020-04-27 Mon]
**** KL-Exchange Notes                                            :ATTACH:
:PROPERTIES:
:Attachments: kl_exchange_page1.jpg kl_exchange_page2.jpg
:ID:       cef38015-0504-43cc-99d8-74e5a69cfcfa
:END:
** May
*** [2020-05-16 Sat]
**** CSmith Histogram                                             :ATTACH:
:PROPERTIES:
:Attachments: 100_depth.csv
:ID:       0ece81f1-5ade-41e4-b217-c0d157e93922
:END:

#+begin_SRC R :results graphics output :session *R* :file "histogram.pdf" :width 10 :height 10 :eval no-export
library(dplyr)
library(ggplot2)

df <- read.csv("data/0e/ce81f1-5ade-41e4-b217-c0d157e93922/100_depth.csv")

ggplot(df) +
    geom_col(aes(x = depth,
                 y = occurrences)) +
    theme_bw(base_size = 28)
#+end_SRC

#+RESULTS:
[[file:histogram.pdf]]

#+begin_SRC R :results graphics output :session *R* :file "fit.pdf" :width 10 :height 10 :eval no-export
library(dplyr)
library(ggplot2)

df <- read.csv("data/0e/ce81f1-5ade-41e4-b217-c0d157e93922/100_depth.csv")

ggplot(df, aes(x = depth,
               y = occurrences)) +
    geom_point() +
    geom_smooth(formula = y ~ I(1 / x),
                method = "lm") +
    theme_bw(base_size = 28)
#+end_SRC

#+RESULTS:
[[file:fit.pdf]]
** July
*** [2020-07-03 Fri]
**** Review for CLUSTER
***** Summary
The paper  presents a series  of performance  comparisons between Julia,  a High
Level Language capable  of parallel and distributed computing,  and the pthreads
and MPI APIs for C and C++.

The paper  compares the  performance of Julia  with that of  the pthreads  C API
using atomic operations and simple  synchronization tasks, and also compares the
performance  of  Julia and  that  of  the MPI  C++  API  in microbenchmarks  for
communication, reads, and writes.

The paper presents  an implementation of HPCG,  a standard, communication-heavy,
and large scale benchmark for HPC.

The paper  argues that  the Julia  HPCG implementation  outperforms the  C++ MPI
implementation in certain experimental conditions,  and that Julia is capable of
performing certain synchronization operations faster than the pthreads API.

***** Strengths and Weaknesses
****** Strengths
The paper  is clearly written and  easy to read.  The  paper reports statistical
analysis choices and  experiment and experimental design, such as  the number of
repeated  measurements and  under  which criteria  were  some outliers  removed.
Interesting  discussions  are made  in  the  paper,  such  as regarding  be  the
underlying causes  of each observation,  and some of the  implementation choices
for HPCG.

****** Weaknesses
The paper  does not make available  any artifacts for the  verification of data,
code, or  statistical analysis,  mentioning these will  be made  available after
acceptance. Under  these conditions  it is  impossible to  verify or  attempt to
reproduce  results.   The   claims  that  the  Julia  language   is  capable  of
outperforming the  C++ MPI implementation  on HPCG  seem rushed, given  the data
presented.

***** Detailed Comments
I believe that a more comprehensive  experimental analysis of the performance of
Julia in  parallel and distributed  settings should be  performed to be  able to
claim improvements over well established technologies such as MPI and pthreads.

Although  motivating  and  interesting,   the  microbenchmarks  and  small-scale
threading comparisons  do not provide  a solid basis for  claiming improvements,
and  the  same  is  valid  for  HPCG. Further  work  should  aim  to  compile  a
comprehensive set  of large-scale, possibly real-world,  applications written in
Julia and C++ MPI.

The paper mentions that the variance of  the HPCG measurements were large on the
Julia  code. It  would  enrich and  better  support the  discussion  to add  the
confidence  intervals of  these  measurements,  and for  the  other graphs  also
presented.  It is  usual to compute an interval of  twice the standard deviation
of the mean, in order to obtain an approximately 95% confidence interval.
***** Suggestions for Improvement
- Add the  95% confidence intervals for  the mean, on all  plots, but especially
  for Figure 10

- Perform performance evaluations on a more comprehensive benchmark, in order to
  better support claims of improvement

- "Figure III-A shows the results." ~> Should be "Figure 4", at page 6
- "perfromance" ~> "performance", at page 10
*** [2020-07-20 Mon]
**** Extension Request
The experimental validation of the approaches studied during this thesis, namely
Design of Experiments and Gaussian Process Regression, took longer than expected
in  the  initial SPAPT  benchmark.  The  large set  of  problems  and number  of
configurable parameters slowed down a  comprehensive and careful analysis of the
relationships between parameters and performance for many problems.

The  Gaussian  Process  Regression  approach introduced  further  questions  and
restrictions for experimental  validation, and the method was also  applied to a
different  domain, the  selection of  bit precision  for layers  of Deep  Neural
Networks, where completing a single experimental run takes a full week.

I have  already started my efforts  towards writing the thesis,  but our current
assessment of  the state of the  work indicates that  I most likely will  not be
able to finish before the end of the third year's limit date.

Compounding the  experimental difficulties sustained  during the last  year, the
difficulties generated from  the COVID-19 pandemic greatly hindered  the pace of
my work,  as they did for  many others. I  continue my cotutelle from  Brazil, a
country that is at the moment the epicenter of the current pandemic.  During the
final stages of my thesis, I had to relocate to a different city and
*** [2020-07-25 Sat]
**** Review for Fabio (Middleware 2020)
***** Paper #72
Title: ROBOTune: High-Dimensional Configuration Tuning for Cluster-Based Data
Analytics

****** A. Overall merit
2. Weak reject (I don't want to publish this)

****** B. Reviewer expertise
5. I have published/worked on most/all this paper’s topics

****** C. Paper summary
The paper presents an autotuner for 44 parameters of the Spark data analytics
framework.  The optimization approach is a composite of Random Forests to
identify the most significant parameters, Latin Hypercube Sampling to better
cover the search space, and Gaussian Process Regression to determine
configurations to test, balancing exploitation of detected relationships and
exploration of the search space. The paper performs experimental evaluation on
five SparkBench workloads with three datasets each. Although the proposed
approach achieves small but statistically significant improvements in relation
to the optimization methods compared in the study, the paper does not present
performance or optimization baselines.  The paper does not compare the achieved
performance to the performance of default, or sensibly picked, Spark
configurations. The results of the optimizers are also not compared to the best
configurations found by naive methods such as Random Search.

****** D. Strengths
- Adapts well-established optimization techniques from Bayesian Optimization and
  Statistical Learning to the Spark tuning problem, exploring better-performing
  regions of search spaces and decreasing tuning costs with respect to other
  tuning strategies

****** E. Weaknesses
- Proposes an interesting high-dimensional optimization problem, with around 200
  dimensions, but the actual studied problem targets a more tractable search
  space, one order of magnitude smaller, with only 44 parameters. The paper does
  not mention the number of possible configurations
- Performance improvements over established work seem to be statistically
  significant although small, but the paper does not compare the results with
  proper baselines.  Performance is never compared to the sensible default Spark
  configurations for each workload, or to optimizations from more naive methods

****** F. Comments for author
******* Major Comments
- The paper does not compare the performance achieved by autotuning strategies
  to the performance achieved by more naive optimization strategies using the
  same budget, such as Random Search, which could also be based on LHS.  Without
  optimization and performance baselines it is difficult to measure the
  effectiveness of the autotuners

- The comparisons between the proposed system and others are not balanced.  The
  study allows the proposed system to reuse results from previous runs in two of
  the three datasets for each problem, which increases the effective
  experimental budget and the optimization time used by the technique. The
  author should modify figures to reflect this imbalance

- Page 8: "For ROBOTune, we treat each workload to be an unseen one for the
  first dataset, and for the remaining two, ROBOTune uses cached parameters and
  configurations through memoized sampling."
  - This means that the optimization time and budget comparisons presented in
    Figures 4 and 6 are not balanced between methods. If the proposed approach
    reuses measurements, its optimization costs must be updated accordingly

******* Minor Comments
- Page 3: "Eventually, it [Bayesian Optimization] locates the global extremum
  for the objective function, or nearly so within a limited number of
  observations."
  - Finding the global optimum in a limited number of measurements is not a
    guarantee of Bayesian Optimization

- Page 5: "Furthermore, LHS is dimension agnostic, meaning that the number of
  samples required is not tied to the dimensionality of the configuration
  space."
  - Obtaining an effective search space covering using Latin Hypercube Sampling
    is tied to the problem dimension. A sample with only a handful of LHS
    samples in high-dimensional spaces suffers from the same issues than uniform
    samples, that is, most samples would be located in the grid regions near the
    "shell" of the search space

- Wrong date on page headers: "Conference’17, July 2017"

- Page 5: "Another way of computing feature importances is through tree-based
  estimators. Unlike linear models, they apply to non-linear relationships."
  - Linear models can represent non-linear factor relationships, and are only
    restricted in the sense of linear, or linearizable, relationships between
    the estimated parameters

- Page 5: "We compare the coefficient of determination (R2) for two linear and
  two tree-based models in Figure 2."
  - It is hard to compare the qualities of fit of the models in Figure 2 without
    knowing the underlying models for the Lasso and the Lasso + Ridge (ENet)
    approaches, since adding polynomial terms would increase flexibility of
    these approaches and consequently increase R^2

- Reference could be improved adding publisher information: "[11] L.  Breiman, J.
  Friedman, C.J.  Stone, and R.A.  Olshen.  1984.Classification and Regression
  Trees.  Taylor & Francis.https://books.google.com/books?id=JwQx-WOmSyQC"

****** G. Comments for PC

***** Final Opinion

The response did  not address my most concerning comments  satisfactorily, and I
recommend  rejecting the  paper.   My comments  regard  mostly the  experimental
validation of the approach. Detailed comments are listed below.

****** Comments on the Rebuttal
> Comment-72E-2: Comparison to the default/naive?
>
> We compared our  results to the default in Section-5.2.  It resulted in frequent
> Out-Of-Memory/Runtime errors. We  need 5 runs for each dataset,  each run taking
> up  to ~10  hours. Due  to time  constraints we  chose to  focus on  established
> methods.

A fair comparison  baseline would be a configuration considered  good enough for
each dataset, that is,  a configuration that was used in other  work, or that is
well established in  the industry for this  type of data, or at  least that does
not  result   in  frequent   errors.   An  equivalent   example  would   be  the
Spark-equivalent of the  "-O3" compiler option for the GCC  compiler, which is a
common baseline  for works  on compiler  optimization. In  that way,  gains over
sensible  defaults  could be  properly  assessed.   Another  fair option  for  a
baseline would be picking the best  configuration found by a random search using
the same experimental  budget. The paper should be  resubmitted after sufficient
data is collected.

> Comment-72E-3: Comparison not balanced.
>
> This could be a misunderstanding. In Section-5.3 we exclude the initial one-time
> cost of parameter selection  as this is a fixed cost.  This overhead varies with
> the number  of datasets tuned, 3  used in our  evaluation. Using more (5  or 10)
> will change the overhead.

In my  previous comment I argued  under the impression that  the proposed method
leveraged  cached  results  from  previous  runs.  If  that  is  the  case,  the
comparisons between  approaches must consider  the experimental cost  of caching
these initial runs as  part of the optimization cost. The  cost of these initial
evaluations, that  are then cached and  reused, should be included  in the total
optimization cost of the approach.  If the  method does not in fact reuse cached
results in further optimization, please ignore my comment.
** September
*** [2020-09-13 Sun]
**** Running GPR on Steven's Data
***** Loading Data for Analysis
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(DiceKriging)
library(dplyr)

complete_data <- read.csv("dopt_anova_experiments/data/search_space.csv",
                          header = TRUE)
complete_data <- complete_data %>%
    select(-vector_recompute) %>%
    mutate(row_number = row_number(),
           load_overlap = as.numeric(as.factor(load_overlap)))

str(complete_data)
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ load_overlap      : num  2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...
 $ row_number        : int  1 2 3 4 5 6 7 8 9 10 ...
#+end_example

***** GPR Code
#+begin_SRC R :results output :session *R3* :eval no-export :exports results :tangle "dopt_anova_experiments/src/gpr.r"
library(DiceKriging)
library(dplyr)

complete_data <- read.csv("dopt_anova_experiments/data/search_space.csv",
                          header = TRUE)
complete_data <- complete_data %>%
    select(-vector_recompute) %>%
    mutate(row_number = row_number(),
           load_overlap = as.numeric(as.factor(load_overlap)))

str(complete_data)

global_optimum <- filter(complete_data, time_per_pixel == min(time_per_pixel))

initial_budget <- 120
initial_sample <- 24
added_training_points <- 6
iterations = round((initial_budget - initial_sample) / added_training_points)
print(iterations)

repetitions <- 1000
# sd_range <- 1.96
sd_range <- 0.3
nugget <- 1e-12

results <- NULL

delta <- 0.5 # (0, 1)
scale_factor <- 30

variance_dampening <- function(dimension, delta, time) {
    return(sqrt((log((dimension * ((time * pi) ^ 2)) /
                         (6 * delta))) /
                scale_factor))
}

variance_dampening_ucb <- function(dimension, delta, time) {
    return(sqrt(log(time)))
}

for(j in 1:repetitions){
    testing_sample <- complete_data
    training_sample <- NULL

    for(i in 1:iterations){
        if(is.null(training_sample)){
            training_sample <- slice_sample(testing_sample,
                                            n = initial_sample)
        }

        testing_sample <- testing_sample %>%
            filter(!(row_number %in% training_sample$row_number))

        invisible(
            capture.output(
                gp_model <-
                    km(formula = ~ y_component_number + I(1 / y_component_number) +
                           vector_length + lws_y + I(1 / lws_y) +
                           load_overlap + temporary_size +
                           elements_number + I(1 / elements_number) +
                           threads_number + I(1 / threads_number),
                       design = select(training_sample,
                                       -row_number,
                                       -time_per_pixel),
                       response = training_sample$time_per_pixel,
                       #nugget = nugget * var(training_sample$time_per_pixel),
                       nugget = 1e-1,
                       control = list(pop.size = 400,
                                      BFGSburnin = 500))
            ))

        gp_prediction <- predict(gp_model,
                                 select(testing_sample,
                                        -row_number,
                                        -time_per_pixel),
                                 "UK")

        # testing_sample$expected_improvement <- gp_prediction$mean -
        #     (sd_range * gp_prediction$sd)

        print(c(length(training_sample[, 1]),
                variance_dampening(length(training_sample) - 2,
                                   delta,
                                   i)))

        testing_sample$expected_improvement <- gp_prediction$mean -
            (variance_dampening(length(training_sample),
                                delta,
                                length(training_sample[, 1])) * gp_prediction$sd)

        new_training_sample <- testing_sample %>%
            arrange(expected_improvement)

        testing_sample <- select(testing_sample, -expected_improvement)

        new_training_sample <- select(new_training_sample[1:added_training_points, ],
                                      -expected_improvement)

        training_sample <- bind_rows(training_sample,
                                     new_training_sample)
    }

    training_sample <- training_sample %>%
        mutate(measurement_order = row_number(),
               experiment_id = j,
               slowdown = time_per_pixel /
                   global_optimum$time_per_pixel)

    if(is.null(results)){
        results <- training_sample
    } else{
        results <- bind_rows(results,
                             training_sample)
    }

    best_points <- results %>%
        mutate(method = "GPR_dampening") %>%
        group_by(experiment_id)

    write.csv(best_points %>%
              filter(time_per_pixel == min(time_per_pixel)),
              "gpr_dampening_sc30_d05_best_points.csv")

    write.csv(best_points, "gpr_dampening_sc30_d05_all_points.csv")
}
#+end_SRC

#+RESULTS:
#+begin_example
'data.frame':	23120 obs. of  9 variables:
 $ elements_number   : int  3 2 4 2 2 2 2 4 4 3 ...
 $ y_component_number: int  3 2 1 1 1 2 2 2 4 1 ...
 $ vector_length     : int  4 1 4 1 8 2 1 8 16 4 ...
 $ temporary_size    : int  4 2 2 2 2 2 4 4 2 4 ...
 $ load_overlap      : num  2 1 2 1 2 2 1 2 2 2 ...
 $ threads_number    : int  64 128 64 256 128 128 128 64 128 32 ...
 $ lws_y             : int  64 1 32 64 32 8 2 2 128 32 ...
 $ time_per_pixel    : num  1.11e-08 1.58e-10 2.34e-09 1.39e-09 3.40e-09 ...
 $ row_number        : int  1 2 3 4 5 6 7 8 9 10 ...
[1] 18
Error in t.default(T) : argument is not a matrix
#+end_example

***** Sub-spaces Reached by each Method
#+begin_SRC R :results graphics output :session *R* :file "dopt_anova_experiments/img/subspaces.pdf" :width 20 :height 20 :eval no-export
library(GGally)
library(dplyr)
library(grid)
library(patchwork)
library(gridExtra)

gpr_data <- read.csv("dopt_anova_experiments/data/gpr_nugget_best_points.csv") %>%
    select(-slowdown, -time_per_pixel, -measurement_order,
           -experiment_id, -X, -row_number) %>%
    mutate(method = "GPRN")

# names(gpr_data) <- c("slowdown", "method", "point_number", "time_per_pixel")

gpr_lin_data <- read.csv("dopt_anova_experiments/data/gpr_nosd_best_points.csv") %>%
    select(-slowdown, -time_per_pixel, -measurement_order,
           -experiment_id, -X, -row_number) %>%
    mutate(method = "GPR")

# names(gpr_lin_data) <- c("slowdown", "method", "point_number", "time_per_pixel")

df_all_methods <- read.csv("dopt_anova_experiments/data/complete_1000.csv",
                           strip.white = TRUE, header = TRUE) %>%
    select(-slowdown, -point_number, -vector_recompute, -time_per_pixel)

levels <- c("RS", "LHS", "GS", "GSR",
            "GA","LM", "LMB", "LMBT",
            "RQ", "DOPT", "DLM", "DLMT",
            "GPR", "GPRN")

selected_methods <- c("RS", "LHS", "GS", "GSR",
                      "GA", "LM", "DLMT", "GPR", "GPRN")

df_all_methods <- df_all_methods %>%
    mutate(load_overlap = as.numeric(as.factor(load_overlap))) %>%
    bind_rows(gpr_data, gpr_lin_data) %>%
    mutate(method = factor(method,
                           levels = levels)) %>%
    filter(method %in% selected_methods) %>%
    group_by(method) %>%
    ungroup()

gprn <- ggpairs(df_all_methods %>%
                filter(method == "GPRN") %>%
                select(-method)) +
    ggtitle("GPRN: Trend + Nugget")

gpr <- ggpairs(df_all_methods %>%
               filter(method == "GPR") %>%
               select(-method)) +
    ggtitle("GPR: Trend")

dlmt <- ggpairs(df_all_methods %>%
                filter(method == "DLMT") %>%
                select(-method)) +
    ggtitle("DLMT")

dlm <- ggpairs(df_all_methods %>%
               filter(method == "LM") %>%
               select(-method)) +
    ggtitle("LM")

p1 <- grid.grabExpr(print(dlm))
p2 <- grid.grabExpr(print(dlmt))
p3 <- grid.grabExpr(print(gpr))
p4 <- grid.grabExpr(print(gprn))

grid.arrange(p1, p2, p3, p4, ncol = 2)
#+end_SRC

#+RESULTS:
[[file:dopt_anova_experiments/img/subspaces.pdf]]

***** Histogram
#+begin_SRC R :results graphics output :session *R* :file "dopt_anova_experiments/img/comparison_histogram.pdf" :width 7 :height 9 :eval no-export :tangle "dopt_anova_experiments/src/build_histogram.r"
library(ggplot2)
library(dplyr)

#gpr_data <- read.csv("dopt_anova_experiments/data/gpr_nugget_best_points.csv") %>%
gpr_data <- read.csv("gpr_dampening_sc30_d05_best_points.csv") %>%
    select(slowdown, method, measurement_order, time_per_pixel) %>%
    mutate(method = "GPRD")

names(gpr_data) <- c("slowdown", "method", "point_number", "time_per_pixel")

#gpr_lin_data <- read.csv("dopt_anova_experiments/data/gpr_nosd_best_points.csv") %>%
gpr_lin_data <- read.csv("dopt_anova_experiments/data/gpr_03sd_nugget_best_points.csv") %>%
    select(slowdown, method, measurement_order, time_per_pixel) %>%
    mutate(method = "GPR-0.3sd")

names(gpr_lin_data) <- c("slowdown", "method", "point_number", "time_per_pixel")

df_all_methods <- read.csv("dopt_anova_experiments/data/complete_1000.csv",
                           strip.white = TRUE, header = TRUE)

levels <- c("RS", "LHS", "GS", "GSR",
            "GA","LM", "LMB", "LMBT",
            "RQ", "DOPT", "DLM", "DLMT",
            "GPR-0.3sd", "GPRD")

selected_methods <- c("RS", "LHS", "GS", "GSR",
                      "GA", "LM", "DLMT", "GPR-0.3sd", "GPRD")

df_all_methods <- df_all_methods %>%
    select(slowdown, method, point_number, time_per_pixel) %>%
    bind_rows(gpr_data, gpr_lin_data) %>%
    mutate(method = factor(method,
                           levels = levels)) %>%
    filter(method %in% selected_methods) %>%
    group_by(method) %>%
    mutate(mean = mean(slowdown),
           median = median(slowdown),
           ci95 = 1.96 * sd(slowdown) / sqrt(n()),
           max = max(slowdown)) %>%
    ungroup()

ggplot(df_all_methods) +
    facet_grid(method ~ .) +
    theme_bw(base_size = 15) +
    coord_cartesian(xlim = c(.9, 4),
                    ylim = c(0, 1000)) +
    geom_histogram(aes(slowdown),
                   binwidth = .05,
                   fill = "gray48") +
    scale_y_continuous(breaks = c(0, 1000),
                       labels = c("0", "1000")) +
    geom_curve(aes(x = max + .1,
                   y = 500,
                   xend = max,
                   yend = 5),
               arrow = arrow(length = unit(0.05, "npc")),
               curvature = 0.3,
               stat = "unique") +
    geom_text(aes(x = max + .2,
                  y = 550,
                  label = "max"),
              stat = "unique") +
    geom_rect(aes(xmin = mean - ci95,
                  xmax = mean + ci95,
                  ymin = 0,
                  ymax = 1000,
                  fill = "red"),
              alpha = 0.3,
              stat = "unique") +
    geom_vline(aes(xintercept = median),
               color = "darkgreen",
               linetype = 3,
               stat = "unique") +
    geom_vline(aes(xintercept = mean),
               color = "red",
               linetype = 2,
               stat = "unique") +
    labs(y = "Count",
         x = "Slowdown compared to the optimal solution") +
    scale_fill_discrete(name = "",
                        breaks = c("red"),
                        labels = c("Mean error")) +
    ggtitle("") +
    theme(legend.position = "none",
          text = element_text(family="serif"),
          strip.background = element_rect(fill = "white"))
#+end_SRC

#+RESULTS:
[[file:dopt_anova_experiments/img/comparison_histogram.pdf]]

*** [2020-09-18 Fri]
**** R session sample
#+begin_SRC R :results output :session *R3* :eval no-export :exports results
library(dplyr)
df <- rnorm(10, 10)
df + 1
#+end_SRC

#+RESULTS:
:
:  [1] 11.025812 11.525666 10.855048 11.342663 11.620696 11.578011 10.151870
:  [8] 10.311008 11.926957  9.512951

#+begin_SRC R :results output :session *R3* :eval no-export :exports results
str(df)
#+end_SRC

#+RESULTS:
:  num [1:10] 10.03 10.53 9.86 10.34 10.62 ...
*** [2020-09-24 Thu]
**** CI for Speedups

#+begin_SRC R :results output :session *R* :eval no-export :exports results
y1 = rnorm(mean = 20, n = 30)
y2 = rnorm(mean = 25, n = 30)

ci_speedup_inf = (mean(y2) - 1.96 * sd(y2)) /
    (mean(y1) + 1.96 * sd(y1))

    ci_speedup_sup = (mean(y2) + 1.96 * sd(y2)) /
    (mean(y1) - 1.96 * sd(y1))

c(25 / 20,
  mean(y2) / mean(y1),
  ci_speedup_inf,
  ci_speedup_sup)
#+end_SRC

#+RESULTS:
: [1] 1.250000 1.253689 1.066590 1.476254
** October
*** [2020-10-06 Tue]
**** Related work Pedro (from Arnaud)
***** Chat notes
- https://www.jmp.com/support/help/en/15.2/index.shtml#page/jmp/gaussian-process-imse-optimal-designs.shtml#ww90928
- https://sites.ualberta.ca/~dwiens/home%20page/publist.htm
- https://sites.ualberta.ca/~dwiens/home%20page/pubs/lof%20discrete.pdf
- https://bookdown.org/rbg/surrogates/
***** Minimisation (f is known)
- [[https://en.wikipedia.org/wiki/Stochastic_gradient_descent][Convex function or Pseudo-convex]]: Briefly, when the learning rates \eta
decrease with
  an appropriate rate, and subject to relatively mild assumptions,
  stochastic gradient descent converges almost surely to a global
  minimum
  - Lipschitz guarantee ?
  - [[https://en.wikipedia.org/wiki/Gradient_descent][Many variants]]
(momentum to avoid zig-zag, Backtracking line
    search so that objective strictly decreases, conjugate gradient
    or Newton if hessian, Nelder-Mead if derivatives cannot be computed
- C^2: converges almost surely to a local minimum
- Meta-heuristics like Hill Climbing, Simulated Annealing
  (stochastic neighborhood exploration, related to
  https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm),
  Genetic Algorithm.
  -
[[https://stackoverflow.com/questions/4092774/what-are-the-differences-between-simulated-annealing-and-genetic-algorithms][In  the  meta-heuristic taxonomy,  SA  is  a single-state  method  and  GA is  a
population method (in  a sub-class along with PSO, ACO,  et al, usually referred
to as biologically-inspired meta-heuristics).]]
- Constrained optimization (requires Lagrangian unless a variable
  change helps). Additional difficulty: avoiding to get stuck on the
  border and following it with ridiculous steps.

Convergence rate degrades with dimension and bad normalization.
***** Learning (f is not known and should be approximated from x samples)
- statistical learning: optimize the loss or maximum likelihood for
  model parameters. In general seeks a good prediction
  "everywhere". Tries to evaluate uncertainty on parameters and predictions.
- "machine" learning: efficient optimization of the loss (i.e.
  E_{x,\theta}[y_\theta(x)-y(x)])

Statistical and Machine learning generally assume that the samples x
are fixed inputs (observations vs. controled experiments), which
raises difficulties regarding generalization/overfitting,
discriminating causation from correlation, counterfactual analysis,
etc.
***** Design of Experiments (find the "best" x to obtain a good model)
Assuming a model for f, propose a set of x to measure to obtain the
best estimates for the model parameters \theta and the prediction.
- Linear models
  - Fractional Designs
  - Screening
  - D-optimal designs
- Unconstrained models (gaussian process)
  - LHS
***** Online and Reinforcement Learning
Choose x_i depending on previous observations and obtain feedback. Same
rules: interact with the world and get feedback, and try to find the
"best" configuration.

- Online machine learning: regret from 1 to T, no discount. Often
  relies on stochastic gradient.
  - Special simple case = Bandit
    - Stochastic: UCB/Thompson, log(T)
    - Adversarial: EXP3, sqrt(T)
  - Many variants Linear UCB, LinRel, Gaussian UCB
- Reinforcement learning: optimize (infinite discounted) regret,
  generally through Markov Decision Process (i.e., dynamic
  programming).
  - Well studied for finite-state space MDPs
  - Does not scale well in high dimension

In both cases, the balance between exploration and exploitation is
lead by (temporally aggregated) regret.
***** Fast exploration of expensive black-box functions
- Reduce dimension by exploiting knowledge on the geometry of f (e.g.,
  identify unsignificant parameters or parameters that need to be set
  to "obvious" values).
- [[https://smt.readthedocs.io/en/latest/_src_docs/applications/ego.html][EGO]]:
Efficient global optimization of expensive black-box
  functions. Journal of Global optimization, 13(4), 455-492.  EGO
  targets the minimization of the expected deviation from the extremum
  of the studied function.
  - EI, Kriging Believer (no guarantee) and GaussianUCB differ by the
*** [2020-10-18 Sun]
**** Brice's Links for Sampling on a Constrained Triangle
- https://cs.stackexchange.com/questions/3227/uniform-sampling-from-a-simplex
- https://cs.stackexchange.com/questions/14007/random-sampling-in-a-polygon
*** [2020-10-21 Wed]
**** Arnaud's References
- [[https://sites.ualberta.ca/~dwiens/home%2520page/publist.htm][List of publications on Maxmin Designs for Lack of Fit, Regression, etc]]
- [[https://sites.ualberta.ca/~dwiens/home%2520page/pubs/nonlinear%2520quantile%2520regression%2520design.pdf][Model-robust designs for nonlinearquantile regression]]
- [[https://sites.ualberta.ca/~dwiens/home%2520page/pubs/lof%2520discrete.pdf][Maximin power designs in testing lack of fit]]
- [[https://sites.ualberta.ca/~dwiens/home%2520page/pubs/uniformdesigns.pdf][Designs for approximately linear regression: Two optimality properties of uniform designs]]
- [[https://www.jmp.com/support/help/en/15.2/index.shtml#page/jmp/gaussian-process-imse-optimal-designs.shtml#ww90928][Gaussian Process IMSE Optimal Designs]]
- [[https://smt.readthedocs.io/en/latest/_src_docs/applications/ego.html][Efficient Global Optimization (EGO): Bayesian Optimization]]
- [[https://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf][EGO Original Paper]]
**** Surrogates Book
- [[https://bookdown.org/rbg/surrogates/][Surrogates: Gaussian process modeling, design and optimization for the applied sciences]]
